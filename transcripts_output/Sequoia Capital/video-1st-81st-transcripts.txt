--- YouTube Channel Transcript Export ---
Channel: Sequoia Capital (ID: UCWrF0oN6unbXrWsTN7RctTw)
Export Date: 2025-05-09 10:59:07
Total Videos Found (in channel): 93
Preferred Languages: en
--- Videos starting from 1st in this file ---
========================================

--- Video 1 ---
Video ID: _rjD_2zn2JU
URL: https://www.youtube.com/watch?v=_rjD_2zn2JU
Title: 9 Years to AGI? OpenAI’s Dan Roberts Reasons About Emulating Einstein
Published: 2025-05-08 18:10:25 UTC
Description:
OpenAI researcher Dan Roberts reveals how reasoning capabilities in AI are evolving, why test-time compute is revolutionizing the field, and the controversial shift to prioritizing reinforcement learning over pre-training. Learn how OpenAI is working toward AI that can make Einstein-level scientific discoveries within the next decade.

Transcript Language: English (auto-generated)
Dan Roberts uh is a former Sequoia team member and has been spreading the word about reasoning for many years to us the past two two and a half three years. We sat across from each other for about a year year and a half and I've learned so much from from Dan. I'm I'm really excited for you to share more broadly. I'll share one memory which is last year at AI Ascent. He was leaving Sequoia to go to OpenAI. He hadn't told anyone. He was keeping it on the DL. It was pretty material information. We had Alfred and and Sam up here talking and and Alfred said, "Oh yeah, and by the way, Dan's going to OpenAI." Saw his face. He was uh pretty mortified. So glad glad you're on the other side of that and sharing a little bit about uh reasoning for us. Yeah. Well, I believe you uh borrowed the opening that I was going to use. So, I'll just jump right into it. Uh so, uh as many of you know, in last September, OpenAI released a model called 01. I took this from our blog post and um let me get to the point. The y-axis here I'm sorry showing you plots um of of ML output but uh the y-axis is performance on some sort of mathematical reasoning benchmark and the x-axis is what's interesting. So the plot on the left said that the model improved when we trained it train time compute. This is something that everybody sort of who trains AI models is familiar with. The really exciting thing was this this plot on the right which said that the model improved also with test time compute. We we taught it to to uh reason and it would spend some time thinking and then the more time it spent thinking the more it would improve. And you know it's hot in here also like this this is uh so important that we we put it on a t-shirt like this is this is like a totally new dimension for for scaling um that's not just training it's also you know at test time. And so um okay so what does this what does this mean? So we have a thinking model. Let's let's do a thinking experiment. This is from so last month we released 03 which is a even better reasoning model. And my background was in in physics. And so so you can ask the model a physics problem. This is quantum electronamics. It can also see. So somebody poses a problem on that sheet of paper. And you've probably seen these models. It it this is the sort of thing that it does at test time. It can it can think about things. It can iterate. It can zoom in. And there's a fineman diagram on that paper which is you know a way of representing these calculations and then it thinks about it some more and then it you know it starts and it answers the question and at the end it gets the answer correct and it took about a minute to do that. Uh, as a as an aside, a colleague asked me to before they put up this blog post to check this calculation and it took me about 3 hours, even though this calculation is in four textbooks that I have to track down everything that it did, make sure that all the minus signs were correct and verify that I got the right answer. Okay, so what can we do? We can do think for say a minute and and do some some pretty cool calculations. But um, where do we want to go? Well, let's to imagine that. Let's do a thought experiment. Who does thought experiments? Albert Einstein. So, let's do a thought experiment about Einstein. Um, and let's imagine we go back to 1907 before he started working on general relativity. And we asked him the final exam question for general relativity. Uh, GPT 4.5 actually made this up, but I can verify that this is a a valid sort of question that you might ask. And, you know, we're open AI, so we wouldn't ask Einstein. We'd ask Einstein v1907- super-H high. how you know to make sure we get maximum reasoning effort and and maximum effort we would get. So Einstein, I guess he's he's a visual thinker. Something about elevators and freef fall. This is when you study GR, you learn about this and you do some calculations and um there's the rubber sheet in the ball and he got looks like he got distracted about quantum mechanics for a bit. Our models get distracted too. There we go. That looks like a you know starting to get to a black hole. I also don't know why he thinks about himself in all these scenarios. And there we go. That's that's like the sort of black hole thing that I was going for with the wormhole. That's the right answer. Uh, turns out GPD4.5 couldn't get this right answer. We needed 03. Uh, 03 was able to get it. I think my role at OpenAI is mostly just checking physics calculations rather than AI research. But okay, the point is though it gets the right answer or Einstein would get the right answer and uh it would take him about eight years and to solve the problem or what I'm saying is that you know he would discover and this is what happened eight years later he discovered general relativity. He'd be able to answer this question. So our models think for a minute now and they can reproduce textbook calculations and and perturbations thereof but like we want them to like make major contributions to the state of human knowledge and science. So um coming back to this plot uh how do we get there? Well now let's focus on the left plot. Um the performance of the model improves the more we train it. And the sort of training we do is is RL or reinforcement learning. And so the main thing that I want to communicate in this talk is that we want to scale that up. Uh uh a year ago we put out GPT40. There was uh compute used and it was all pre-training compute and as you can imagine then we started doing this thing that led to test time compute. So we we added some reinforcement learning comput RL compute for 01. I should say this is all a cartoon but uh you know directionally it's correct. 03 maybe had a little bit more RL compute. at some point in the future maybe we'll have a lot of RL compute and then at some far point in the future maybe we'll have just like be totally dominated and crushed by by RL compute and this so like this is this is I think is kind of a contrarian point of view like this is this is where we're going uh and we we mean it uh to to point to emphasize sort of the contrarian nature of this uh this is some of you who followed um AI research this is a slide that Yan Lun made some some number of years ago I guess 2019 based on the copyright um that that I borrow um you know I borrowed this from obviously um this is a complicated slide that's hard to maybe hard to follow luckily we have models that can summarize things for us uh the point is that pre-training is like this big cake and reinforcement learning is supposed to be like this little cherry on top and and uh you know that's that's basically what what this this plot was um the color scheme actually was accidental but I think it actually look works quite quite well and Uh, you know, this is where we're going. We want to we want to totally invert the meme here. Uh, uh, you know, we have like maybe the same size cake and we just want to crush it with a giant reinforcement learning cherry. Uh, okay. So, what's our plan? Well, um, can't tell you our plan. I sent my slides in and uh, the comm's team just redacted the whole thing. Uh actually I should say I I was kind of anxious that they would redact the redaction slide but luckily uh Brianna was very nice um the person I sent them to and so everything is is you know was was good and in fact you know what our plan is like like our plan I think we actually talk about it very clearly we're scaling compute so what what does that mean we're going to raise $500 billion we're going to buy some land in Abalene Texas build some buildings um put some computers inside some people here we spoke to earlier maybe going to help us with that and and you know we're going to train models hopefully have a lot of revenue from that and then build some more buildings, put some more computers inside and so forth. So we're scaling up our compute uh in in concert with that we also want to be we also want to um develop scaling science and and this is this is what I do at at OpenAI. This or one of the things that I think about this this plot was from our GPD4 blog post. It predates my time, but I think it's it's really inspiring and and impressive that this this dot down over here is the final loss performance of GPD4. These dots were experiments that they did along the way and this is a log scale, so they're much smaller scale and and this dotted line was the prediction. So, so they nailed the prediction, right? So, they they set out to train this model that's bigger than anything that anyone had ever seen and they knew exactly what it was going to do. And now that we have these new directions with test-time compute and and and um reinforcement learning training, we have to throw everything out the window and and reinvent what does it mean to apply um you know to scale up compute. Um so we're scaling we're we're scaling we need scaling science because we want to be scaling science. Uh this is a point that uh podcaster dwares Patel made that our models right now they sort of feel like idiot zones. They they don't you know they're not they're not discovering general relativity. Um, and uh, you know, I don't know why why why is this? Well, it's it could be that we're maybe just asking the wrong sorts of questions. A lot of things that we do in research is is about like, you know, the way the way in which you ask the question is more more important than the process and the answer. So, you know, we need to really get the question correct. Another another issue might be you could say is that we're training on too many competition math problems and and maybe our models are sort of like like jaggedly good at at different things. I guess in either of these cases, you're going to get integers and maybe a little unsatisfying. Um, but I I think really the thing that's going to happen here is that we're we're we're scaling up. Um, we need to scale this up further and and it's going to be really fantastic when we do. Um, just to to close, this is about what's next. And so next time on AI ascent, I was here last year. Hope love to be here again. Actually, this is easy because I think Constantine had this plot. He had the non semi-log y version of this this plot. But you know, so this is an exponential growth in the length of tasks that agents can do AI or that AI can do. It's doubling every seven months. So looks like they according to this plot, they can do tasks that are about an hour. Maybe next year where will we be? They'll be about um two two and a half hours between two and three hours. Uh you know, it's it's dangerous to to make predictions in AI. Everyone's always wrong, but maybe maybe I can extrapolate a line here. So you know uh where do we want we we're talking about this 8 years of Einstein and thinking and so eight years and and to to get there from now we need about 16 doubling times. So that means that like I guess the point is that in 9 years we're going to have a model that that will discover general relativity. Um so yeah thank you [Applause]

========================================

--- Video 2 ---
Video ID: jFZ9hJKJKtw
URL: https://www.youtube.com/watch?v=jFZ9hJKJKtw
Title: How OpenAI Built its Groundbreaking Deep Research Product ft. Isa Fulford
Published: 2025-05-08 18:10:02 UTC
Description:
Isa Fulford, head of the Deep Research team at OpenAI, explains how they developed an AI system that can search the web, analyze data and generate comprehensive reports in minutes instead of hours (or days). Discover the reinforcement learning techniques and tools that power this new ChatGPT feature that’s transforming how professionals and consumers research complex topics.

Transcript Language: English (auto-generated)
I'm so excited to welcome Issa Fulford who created a product that all of us know about use love openai deep research she is going to talk a little bit about the product how they did it where it's going a demo live for all of us she just redeyed in from Korea specifically for this event landed 5:45 this morning so please join me in a very warm sequoia welcome Welcome to Issa. [Applause] Hi everyone. Um I'm Issa. I lead the deep research research team at OpenAI. Um as some of you may know, Deep Research is an agentic capability in chat GBC that conducts multi-step research um online to solve really complex tasks. So you give it a prompt and then Deep Research will spend 5 to 30 minutes looking through loads of online web sources um reasoning about the content it finds and then it will come back with a fully cited comprehensive report at around the level of a research analyst. So it's able to do in a few minutes what would take a human many hours and so deep research is powered by a version of 03 that we fine-tuned specifically to be good at web browsing and data analysis. And I'll talk more about how we did this in a minute. And so a little bit of background about you know why we started building deep research. So a little over a year ago we were seeing a lot of progress internally with um reinforcement learning and reasoning models and we were training mostly on math and science and coding tasks. And we saw some generalization from training on those kinds of tasks to other domains. But we wondered what would happen if we trained directly on tasks that users do in their daily lives. Um and if we could make a really useful model by training directly on those kinds of things. And we felt that online browsing was a really good first place to try this because a lot of people do online browsing across a really broad range of domains and jobs and also in their daily lives. And also it's a nice kind of uh first sandbox where readonly um agents is kind of a good place to start because the kind of safety considerations are maybe a little bit more constrained. So um to do this we first needed to get people excited. So we hacked together a demo by just prompting models. I was actually working on this with Yash Patil and Thomas Simpson who's also here today. Um and we just made this demo of what the deep research product might look like. Um we didn't train any models, got people excited about it and then started the process of training a model to be good at deep research. And so this involved creating tasks um reinforcement learning tasks to actually teach the model the browsing capabilities and data analysis capabilities that we wanted the model to learn and also creating tools and giving the model access to those tools um to use during training. And so we needed to give it access to a browser so it could search and click on things and scroll through things and then also um a way to execute code to do like data analysis and plot graphs and things like that. So um I now I'll show you a few examples of deep research today. Um most of our users um use chatbt for professional use cases. So you know academics, VCs, so I've been told um consultants a lot of other things. But then also a lot of people have personal use cases. I would say that's probably my biggest use case would be like shopping and travel recommendations. So I'll show you a few example queries and then I'll I'll actually show you some some of my own queries that I've been I've been doing recently. So if we could switch to the screen that'd be great. So this is ChBT. I'm sure you're all familiar. Um um okay. So this is just the prompt. I actually had SHBT write this prompt for me. So I'm giving a presentation at Sequoia AISN today and I want to understand and visually communicate recent trends in venture capital investments in AI companies and then please analyze funding across um you know a number of different things that I want to investigate and then I want a graph um that meaningfully conveys a key trend mostly because I wanted you guys to know that it can plot graphs um so it's slightly contrived um and so you press start and the first thing that DP research will do is it will come back with some clarifying questions. The reason we designed it this way is because we felt that if you um the model is going to do something for you for quite a few minutes, you want to make sure that the output you get is exactly what you wanted and you we want the user to give as much detail as possible up front. And so we want to encourage the user to be very specific. So it came back with a few questions. So I'll say like just us um maybe both is fine and either and so now deep research will um kick off a research task and so in a minute we will be able to follow along with a summary of the its chain of thought and the actions that it's taking. So you'll be able to see interled chain of thought which is the model reasoning through the sources it's encountering and then also the tool calls that I make. So either searching or you know analyzing with Python tool. So it takes a minute to minute to start. I also have some pre-loaded examples that I can show in the meantime if this is okay. I'll show you one that I asked this week and then we can go back to this. So earlier this week um I was in I was in Korea and I actually did this query on my phone. I was looking for a night market that was maximum 15 minutes away from where I was and I wanted it to look at Reddit and also Korean sources that I'm not able to read myself and then within each of those markets find the best rated stores. So you can imagine that with normal search it's pretty hard to do a search that has all of these constraints in one go. But deep research is actually able to search the web and for each item like go pretty deep and identify like whether it fits the constraints and then synthesize it all back for me. So the final response it gave me was like this pretty long report with um a few suggestions of markets to try and then for each thing there are citations. As you can see, the citations actually um also um site the specific um lines from the um from the sources that are relevant to the the sentence that it's citing. And I actually ended up going to to this first one. So, okay, if I go back to the um original query that I just asked. So, you can see the model um is like thinking through and then we we just summarize the train of thought. says what it's planning to do, then it made a search and it's thinking. If you've also used 03, you you'll know that 03 is also able to do search really well. So, if you're looking for a more like medium search, I'd recommend using 03. Deep research will really is kind of like the far end of how long it will take to to do a search query. And the reason 03 is good at searching is actually because it's trained with the same tools and browsing data sets that we developed for deep research. And so I'm going to um skip to this to an example that I ran this morning. Um so this is this is the the final report on the um on the AI landscape. It plotted a graph which I'll show you. At first when I saw this I thought there must be a mistake but I think that this is counting the OpenAI investment which is why this graph looks really weird in March. Um but yeah, I I won't make you read through the entire report right now, but um I hope hopefully there's some good insights in in here. So I encourage you to try it as well yourself. And then um I just wanted to show that there like quite a broad range of different things that deep research is useful for. So this is an example from u like a biology use case asking about um specific gene therapies that have gained regulatory approval in the US for treating hemophilia with information and then the model is able to do research and then come back with um the correct list with like citations and explanations. So we're very excited to continue developing deep research. It's obviously not perfect. Sometimes it can hallucinate. So we're working on like improving um reliability. We're also excited to like integrate deep research into the main reasoning models. So, you know, as you know, 03 is already good at search. We'll continue to like upstream things that we work on into the the other the bigger reasoning models. Um, we're also really excited to bring um private context into deep research. So, your internal company knowledge um pay sources and then I think the next like big thing for us will be um not just synthesizing existing information but actually taking actions as well. So, um, thank you very much for having me and, um, I'll pass over to Zach. Thank you.

========================================

--- Video 3 ---
Video ID: bj62nmMh-_M
URL: https://www.youtube.com/watch?v=bj62nmMh-_M
Title: The Role of AI in Human Agency ft Carl Eschenbach, CEO of Workday #ai #podcast #tech
Published: 2025-05-08 14:30:33 UTC
Description:
While technology enables change, it still requires human agency and involvement to drive that change. 

Workday CEO and former Sequoia partner Carl Eschenbach explains how the company is evolving its platform to handle both human and AI workers and discusses the transition from merely working with technology to technology working for us.

Transcript Language: English (auto-generated)
That's why I think we move from a world where we're working with technology to the technology working for us and today it's front and center and we talk about how we're using AI but the power of AI is these models learn right and they start to do things autonomously on your behalf so you don't even have to do it so you go do new things and the last thing I'd say is this is this is factual technology enables change. Mhm. That's all it does. It enables change to drive change. It still takes people no matter what. Well put. People are going to be involved with AI. They have to implement it. They have to embrace it. They have to teach people how to use it. And they have to teach people new skills. Technology only enables change. It doesn't drive. It still takes humans. So this is one of the ironic things. I completely agree with the point and and maybe a different way to frame that. One of the ironic things ironic things about this agentic economy agency is the thing that is uniquely human right the ability to determine what is it that I want to achieve here that is still a uniquely human thing you can dispatch an AI agent to go reason through the steps and figure out how to get there but deciding what question should I ask what is the objective you know that's still a uniquely human thing I think another framework would be you know Google sort of commoditized knowledge open AI is sort commoditizing intelligence. Agency is the thing on top. You know, intelligence is making use of that knowledge. Agency is determining to what end. And that's the thing that is still uniquely human. And I think the argument in favor of AI is it gives you superpowers to go do that.

========================================

--- Video 4 ---
Video ID: 8VY-fifXNC8
URL: https://www.youtube.com/watch?v=8VY-fifXNC8
Title: AI's Trillion-Dollar Opportunity - Sequoia's AI Ascent Keynote in Under 3 Minutes
Published: 2025-05-08 11:00:10 UTC
Description:
At Sequoia Capital's 3rd annual AI Ascent partners Sonya Huang, Pat Grady and Konstantine Buhler outline why AI represents a market opportunity at least 10x larger than cloud computing, where startups should focus to win, and how the rise of AI agents will create an entirely new economic paradigm.

Transcript Language: English (auto-generated)
If you think about the physics of distribution, you just need three things. People have to know about your thing. They have to want your thing. And they have to be able to buy your thing. That's it. When the cloud transition got going, nobody was paying attention. AI is very different. November 30, 2022, Chat GPT comes out. The entire world is paying attention to AI. People are starting to break out, but for the most part, the opportunity is still wide open. We have believed and we continue to believe the value is at the application layer. But guess what? You got competition. We got a second scaling law. We got testime compute. We got reasoning, tool use, and inter agent communication that allows the foundation models to get pretty far into the application layer. But this is the race. This is where the value is going to be. And if you don't get in front of it, somebody else will. You are in a run like heck business right now. Now is the time to go at maximum velocity all of the time. More and more of us are getting value out of AI and we're all climbing the ladder together on how to weave AI into our daily lives. The breakout application category of the year was coding which reached screaming product market fit. A lot of the most exciting technology innovation in AI right now is happening at that blurry boundary between research and product and I think the two breakthrough examples of this over the last year were deep research and notebook LM to talk about where value will acrew in the AI stack. I remember debating this question with my wonderful partners at Sequoia at the time. My partners Pat in particular were adamant that the value was going to acrew to the application layer. I think you were right Pat. Vertical agents are a wonderful opportunity for startup founders who deeply understand the domain. And we see companies creating agents that are trained end to end to excel in a very specific workflow using techniques including reinforcement learning on synthetic data and user user data to make AI systems very performant at very specific tasks. And the evidence we've seen so far makes us really optimistic. A year ago, AI Ascent was all about agents. We were talking about agents and they were just beginning to form into businesses. The topic was really these machine assistants that eventually we predicted would come together as agent swarms. In the years to come, we think that this matures even further into an agent economy. An agent economy is one in which agents don't just communicate information. They transfer resources, they can make transactions. But in order to achieve that very big important next wave, we have a lot of important technical challenges. Persistent identity. The agent itself needs to be persistent. If you're doing business with someone and they change day-to-day, you probably won't be doing business with them for very long. Seamless communication protocols. Imagine personal computing without seamless communication protocols. No TCP IP, no internet. We're just now building that protocol layer. Security. If you can't meet the person you're doing business with palm to palm, face to face, that importance of security and trust is even further elevated. Eventually, these processes and agents are going to merge. You're going to have neural networks within very large complex neural networks. A network of these neural networks. And this is going to change

========================================

--- Video 5 ---
Video ID: xlQB_0Nzoog
URL: https://www.youtube.com/watch?v=xlQB_0Nzoog
Title: How AI is Reinventing Software Business Models ft. Bret Taylor of Sierra
Published: 2025-05-08 09:01:33 UTC
Description:
Sierra co-founder Bret Taylor discusses why AI is driving a fundamental shift from subscription-based pricing to outcomes-based models. Learn why this transition is harder for incumbents than startups, why applied AI and vertical specialization represent the biggest opportunities for entrepreneurs and how to position your AI company for success in this new paradigm.

Transcript Language: English (auto-generated)
Next up, we have Brett Taylor. He was part of the, I think, first APM program at Google where he famously rewrote all of Google Maps in a weekend. Then he founded a company. Then he went ahead and was just CTO of Facebook for a while. And then he founded another company, which we're lucky enough to be in business with him on, which is redefining the front door for businesses. and Brett will be joined by our partner RVY who used to deliver groceries and now gives people money. About the same. Please welcome RV and [Applause] Brett. Yeah, but a lot of groceries. Okay. Um I thought Pat was saying I was the legend, so I was pretty pretty upset when apparently it was Brett. You're a legend to me. Thank you. I really appreciate you saying that. Having Jeff Dean and then Brett Taylor is like having like LeBron and Steph of engineers. It's amazing. Or Ruof, two random rugby people that only you know, you know, like that maybe an analogy you'd get. Um, all right. I want to ask you a question. There's very few people that have been described as 10,000x engineers probably in general, but also extremely extremely few people who've done that and also been good at enterprise sales. Okay. Um, and I think one of the things that'll be fun today is we can like be like high flutin and talk about where the world's going, but we can also talk about like how to sell things. And so the question I have before we get to tactical advice on that is like what made you even think that you could do something other than just be an amazing engineer? Yeah, I actually think this is uh one of the greatest challenges to overcome as an entrepreneur. uh this is really reductive but I think if you talk to an entrepreneur they often have some unique insight on the business they're creating. So maybe it's a technical insight. If they come from a background in say business development, maybe they see an opportunity in the market uh that they can take advantage of as a partnership or business opportunity uh in the financial services industry. Maybe you see an inefficiency in the market you can take advantage of. The issue I see with a lot of entrepreneurs is you sort of become single issue voters. It's like every time there's a problem uh in your business, if you're a product person, you go change the product. There's always the like famed redesign like the dead cat bounce before your company gets destroyed. And it turns out that as you're growing your business at any given point, you may have a go to market problem, you may have a product problem, you have a competitive problem. And I think the hardest thing is when you come out of business with what you're comfortable with, you regress to sort of what makes you comfortable. For me, uh my most like vulnerable moment was when I was 29. and I became the CTO of Facebook. Uh I was one of the older people at Facebook at the time. Good. Well, I didn't know what the what I was about to cuss. I shouldn't cuss. I didn't know what I was doing. You told me this was recorded. I have to like I think that you should not give away state secrets about open a fine. It's good. And um I uh I wouldn't have self-described that I was sort of struggling in my job, but I think I was. I was sort of trying to do everything myself. Uh, and Cheryl Sanberg, who was the chief operating officer at the time, took me aside and kind of had a sort of come to Jesus conversation with me. Um, essentially saying, "You need to hold your team to as high of a standards you hold yourself. You need to stop trying to do the work yourself and you need to identify the people who aren't meeting your needs, replace them, but you need you're not scaling the way you're operating effectively." And it was kind of the conversation I needed to have at that moment. I felt really shitty for about an hour and then like woke up the next day with kind of a fire in my belly. And I realized upon reflection uh she didn't sort of diagnose it this way, but upon my reflection I was going into work every day and trying to say how do I conform this job to me and my strengths. Um and instead of thinking like what's the most important thing to do in this job today even if it's not something I find delightful or interesting. So, I sort of came at the job uh and I I always modeled it as giving myself advice like I was sort of my own board of directors and sometimes it would be people, sometimes it would be business, sometimes it would be technology. I was the CTO so hopefully it wasn't that some of the time. Um and interestingly enough, um I got more joy from it. Um I realized at that point I actually just appreciated having an impact more than I appreciated any act of doing my job. And so I ended up with this really nice reinforcement of, wow, I'm better at my job than I was last month because I'm not just doing what I like or what I feel comfortable doing. Um, and that ended up like a really virtuous cycle. So I've had a I've been a little bit of a chameleon. I've, you know, it's interesting. If I meet people from Facebook, they think of me as an engineer. If I meet people from Salesforce, they think of me like a suit, you know, like or I don't know, generously. I something the boss. And I'm not any one of those things. I just sort of showed up every day to, you know, be uh to have an impact. And as I said, I just I gain a ton of joy from it. And the reason I bring it up for the entrepreneurs in the room is I think at every stage of your company at the beginning, product is all that matters. You know, finding your first and happy customers. As you scale, what you need to be great at changes. And I think having uh the self-awareness and self-reflection to actually change what you spend your time on is actually I think one of the greatest challenges of not having your company sort of get away from you and continuing to be effective leader at that company. I I would tell you I think it's something just so you have it. It's something I have taken and sharing with my kids which is like don't let something you're good at become who you are. Right. And I think that that is something I've learned from you that I really appreciate and I think you're doing in the day-to-day. So tell people about Sierra. Why did you and Clay start it? What's the vision? What are you guys up to? Uh at Sierra, we help companies build customerf facing AI agents. Um we started on the hypothesis that uh every company's main digital interface will be an AI agent, you know, and maybe in in uh 20 years ago, your main digital presence was your website, your dot. Uh remember listing on Yahoo directory was the bees knees, you know, back in the day. Um I do I do remember that. Yeah. Thank you for Yeah. Thank you for noticing. Yeah. You know, and then for a lot of brands, whether it was like your social profile or your mobile app, you know, companies have a lot of different digital touch points today. Our hypothesis, I don't know if you measured it like what percentage of digital interactions happen on a touchcreen now versus a keyboard. It never replaces, but it certainly sort of displaces. Our hypothesis is you fast forward 5 years, the vast majority of digital interactions will happen via an agent. And as a consequence, every company will need their singular agent, not plural, the one that has their brand at the top that represents the whole of their customer experience. And um so we've uh worked with a lot of different companies from ADT home security to Sirius XM to help them build their primary customer-f facing AI agent. Um a lot of it is AI and it's an agent, but a lot of it is also business. You know, like what is the customer experience you're providing for ADT home security? your alarm might be going off, your alarm might be broken for SiriusXM, someone might want a better price on their plan uh because their promotional period expired. So, it's actually really interesting because just like your website, I don't think most people think of websites as a piece of technology nowadays, right? It's almost just an expression of your brand and your business, that's actually kind of the agents that that companies are building on our platform. Uh but I hope, you know, if we're successful, if you run across an branded AI agent in the wild, I hope it's powered by our platform. That's what we're trying to do. And one of the things that comes up a lot is like what are the foundation models going to do versus what are the application layer companies going to go do, right? And I think you have probably the most unique vantage point on this being the chair of OpenAI as well as building an application layer company. So can you help our founders here think about what is going to belong to them versus the foundation models? Yeah, I've um I'll say strong opinion loosely held uh on this. So I think there's really three big markets that I see and I'll end with the one I think is the most exciting. Uh first you have the foundation models and Jeff said something similar to this but I think there's going to be a ton of consolidation in the foundation model market. It's fundamentally just a capital intensive business just like building data centers. Effectively it is building data centers you know at the core of the expense. And as a consequence, it just, you know, if you have large-scale capex, you can generate more capex and it sort of benefits scale. And as a consequence, I think it'll end up a little bit like the cloud infrastructure market, a handful of players, relatively low margin, but very very high scale and sort of collecting taxes from everyone in the AI ecosystem. Uh I think the the next market is is really making tools, the proverbial pickaxes and the gold rush. Um and uh I think you know a lot of uh companies created prior to large language models like you know data bricks and snowflake will sort of be in that category but a lot of new ones as well. Um there's a lot of nuance there. I think that will be a market that can really uh be threatened by the foundation model companies. I imagine every cloud infrastructure company has some there's probably an AWS or Azure equivalent to what they do. And so you end up with a naturally sort of more challenging, but it can be very high scale. Um, but it's just a you definitely are closer to the sun, you know, the closer you are to the models themselves. And then there's the AI applied AI market. And I actually think this will manifest its form as as agents. Um, so uh, you know, there's companies like Harvey that make an agent for the legal profession. There's companies like Sierra that make it for customer experience. Uh, there's companies that make it for marketing. There's companies that do for visual effects. Uh my view is this is sort of the new software as a service. Um companies will the form factor of purchasing AI will be purchasing an agent that does a job. Um I I think there's a it's a really exciting time. I actually I think I have assume a lot of folks in this room are working on sort of applied applications in AI. Um the reason I think it's exciting are sort of twofold. One is I think it's the way software should be consumed. Uh I think right now a lot of particularly large enterprises uh in the excitement after chat GBT licensed a lot of models and and as we all know software is like a lawn. You have to tend to it. So I think there's a lot of buyer remorse right now from a lot of spend and a lot of proofs of concept but not a lot of value. Um the second thing is I think it changes the markets. you know, there's these markets where like selling enhanced productivity to attorneys is not like a huge tam and so you sort of run up against just a natural thing like even if you get you know 80% market share how big could this company be you know but if you're actually selling antirust review or or something I don't know much about the law so I just made that sound credible really you you are the you know crisis to the stars and you don't know anything about the law that seems you know it turns out the crisis to the stars um you know uh that has a ton of value um because you're essentially you know providing something that very high cost labor produced before and as a consequence I think if you look at the total addressable market of you know all these vertical specific domain specific agents the market is gigantic um and actually it's interesting if you look at the public markets if you look at the top uh you know five software companies that dominate um the S&P 500 that dominate the stock market. None of them are sort of pure play software as a service companies. Obviously, Microsoft is and it has some enterprise software. Amazon has AWS, but they're infrastructure companies. They're consumer. Um, and if you look at the software as a service companies, Service Now, Salesforce, SAP, Oracle, they're all sort of in that sort of 200 to30 billion market cap range, you wonder in this new world of agents, will we see our first trillion dollar sort of applied enterprise software company? And I I think the answer is yes. Uh because you're going from selling productivity enhancement to selling outcomes and outcomes are valuable. Uh some outcomes are extremely valuable. Um and so I think there's been a little understandably a little too much excitement around the models themselves. Not that I I mean obviously at Open Eye were very excited about it, but as an entrepreneur I think that it's sort of the the pickaxes are obvious, but actually the value is elsewhere in my mind. I think the value is taking these this amazing next generation technology solving a very valuable business problem that was high cost before and selling something valuable for an order of magnitude less money than the current cost is really easy. Um so I just think it's a great business and I'm I'm super excited about the future of the software industry as a consequence. What do you think is the right way to price those outcomes when you think about them? Right. I think that there's all the former seatbased models, right? And now Sierra obviously does it differently. How applicing model and then also talk about how applicable you think that is or isn't to other agent companies. Yeah, at Sierra we do we call it outcomesbased pricing. So uh for our median customer that typically means when the AI agent resolves the issue for the customer autonomously there's a pre-negotiated rate for that and if we do have to escalate to a person it's free. uh we do that just to align with the the business model of our customers and uh I actually view it as sort of a natural evolution of software. We went from uh box software with a perpetual license to you know once you delivered software in a browser most everyone's on the same version. you needed to sort of invent a new business model to sustain R&D which led to software as a service subscription based business and we just thought from first principles and said hey if you're selling software that completes a job what is sort of the secular business model for that and it felt like let's pay for a job well done you know sales people get paid a sales commission you know why not the AI as well um it it does really disrupt the way you build a software company in my mind though um if you look at particularly the enterprise software ecosystem there tends to be sort of an armslength relationship between software vendors and customers. Uh typically you'll sell a piece of software and be done. Uh then you'll have systems integrators come in and spend you know for large scale enterprise software deployments sometimes 6 months or a year to implement that software. Um and that's well I mean by the time the software is being used you know the vendor has been probably not gone they're probably going to sell them more things but effectively gone for uh 6 to 12 months that so as a consequence there's almost like an active disassociation with the act with the outcomes um and so you end up with a confluence of I think a lot of different factors that I think are interesting uh there was a question just before I walked up here about you know uh agents that can code what will that do to the professional services industry broadly um there's a question of if the right thing for AI software is outcomes, how do you actually align your company with that kind of accountability? I think it sort of blows up a lot of the model. Um but that's why, you know, as Steve Jobs said, it's more fun to be a pirate than it is to be in the Navy. And I think it's a really good time to be a startup because, uh most people in this room are not encumbered by a business model. Um which is a funny way of thinking about it. But if you look at the history of software, Salesforce beating seable systems uh you know service now beating the plethora of ITSM companies that came before it's actually much closing a technology gap in your product is is hard but not impossible. Changing your business model is really hard. Uh, and you look at Microsoft transitioning from Windows to Azure and how incredibly awkward that transition was and Satcha deserves a bunch of credit. Shantenu did a great job at Adobe going to ratible revenue, but there's like a graveyard of CEOs who've been fired because they couldn't make that transition in part because public company investors are horribly impatient in these things. And so I just think that what's exciting about AI is I think it's going to give rise to new delivery models, new technology models, but as importantly, new business models. And I think that if you think about what are the opportunities for startups relative to incumbents, don't lose sight of the business models. I actually think it's usually um one of the greatest advantages that startups have relative to incumbents. Yeah, I I think historically speed has been something, but I think on this point it's like you also have this enormous benefit of, you know, you're not encumbered by the old business model. Well, you referenced the uh previous companies that you've been in and you started. One of the things we've talked about is like there are things that are similar about building a company now and there are things that are different. What are some of the things that are the same as the previous companies that you've built that you're doing now? And what are some of the things that are different? Uh well, at Friend Feed, my not so successful social network, we built our own servers. Can you tell the story? Can you tell the story about the Facebook IPO real quick? just about friend feed for everyone where uh you got recognized. Um can you I'll I'll do that then. I'll do it then. I'll do that. Okay. It's a good story. It's a good story. We stand between these people and drinks, man. You got to make them laugh like let's go. You know, hit them with something. You're pulling out the deep cuts. Um you know, I think a lot has changed just in the infrastructure world. You know, I think literally we built our own servers uh at friend feed and you know like brought them into the colo. We brought our site down once by tripping over a power wire in the colo. Uh that was Sajjie Sajie's fault. We love him. No, I'm just kidding. It was like so it's just a different era. Um you know I do think it is uh as we've uh virtualized infrastructure uh and built you know just the software development life cycle we have today from uh cloud infrastructure like Azure and Amazon web services to GitHub and GitHub actions and you know all the incredible open source libraries um and then you have sort of the virtual infrastructure to help support your companies like you know ramp you know and rippling you end up where uh there's a certain like there's a very different level of focus now in starting a company. Uh it used to be you just had to do all these things to like build the scaffolding of your company and it was actually like a non-trivial amount of work. And the thing I find just so exciting about like the future of entrepreneurship is you there's just so little overhead now in terms of just like intellectual overhead. Uh, and I remember actually just how hard it was to get like credit cards for our employees. Like they had to all get a credit check. I'm like, are you kidding me? Like it's it's our bank account. Like what are they? And you know, now you can, you know, print a virtual, you know, uh, credit card with, you know, things like ramp. It's so easy. Um, so I think we've just seen every aspect of starting a company that was about sort of like the operational sort of scaffolding infrastructure to do so. Uh, many startups that you all have funded are now doing that. And so as a consequence, you can focus on just purely what defines your company and what makes it different. My color commentary on that is I actually think one of the mistakes a lot of smaller companies make is spending their resources and time on things that aren't core to their business. Uh and it's a it's more than a hidden cost. I think like every ounce of time you're spending on something that fundamentally should be a commodity to your business is time you're not spending on like selling your product to more customers or like working on like true differentiation and it takes a lot of discipline because a lot of those things are like catnip you know it's like fun to work on that part of your business and I always like to say like you know what Clay who's somewhere around here we always talk about like what will make us happy a year from now and that's always our metric and then some you could take it to 10 years if you want to be like no one knows what the hell is going to happen in 10 years. By the way, if you don't know what's going to happen in 10 years, nobody knows what's going to happen in 10 years. And I do think it's very clarifying and I just think it's I it always surprises me how much I tend towards some of those things that are, you know, wholly undifferiated at this point. Okay. Well, you mentioned some of the companies that Sierra works with, right? And some of those are companies that have been around for a long time, right? They are established companies. What does the T and ADT stand for? Telegraph. Telegraph. Okay. So, how do establish companies? It's cool, by the way. Home security over telegraph. I actually imagine someone coming to like the Sequoia of that day. It's like home security over telegraph. It's like YouTube for cats, you know, and like and uh and it and they built that business. It really was. And now they're actually doing home security with AI agents. What a cool privilege. I think it's awesome. And you know, it's a better acronym than AD AI, you know. Um Okay. So, but what do they have to do? What are the ones that are going to succeed? what are they going to do to in a world of changing technology and what are the ones that are not going to succeed? What is holding them back? So, and how can this group help all of those established companies succeed? Yeah. So, I think it's a really interesting opportunity for uh companies in competitive markets like home security um or a company like SiriusXM that's got incredible unique original content, but they have a lot of cord cutting and a lot of competition around them. You have this technology and AI that drives incredible amounts of productivity in departments that had previously been incredibly underserved uh by software. uh everything from you know largecale contact centers to we talked about things like legal departments large operations departments um and as a consequence companies that lean into this can really change the cost structure of their business um uh if you think about I mean there's companies that have like 20,000 person contact centers and you know this costs hundreds of millions of dollars every single year and if you can take all of that opex and put it back into your business you can lower prices you can invest in growth and so it's really interesting I think it is one of those opportunities where these shovelready applications of AI even if it's indirect because they can drive such structural changes to the uh essentially essentially the unit economics of so many companies businesses I think it will change market share and so I think that it's a little bit like the birth of the internet you know and you know Walmart did very well uh since the birth of the browser even in the face of Amazon, there were a lot of retailers like Blockbuster who did not. And so I think uh because most executives running these companies now lived through the web browser era, saw the birth of the smartphone uh and saw the growth of things like Instacart and Door Dash and others. They see this. They're not and they're they're modeling what is the future of our industry going to look like. And you know, you mentioned enterprise sales. I think the key thing about selling anything is speaking the language of your customer and having empathy. Um, I think one of the things I see a lot with entrepreneurs is they'll go into a meeting and sell their product. Um, if you ever see a really good salesperson, the first thing you'll see them do is ask a lot of questions. But then the next thing you need to do is actually listen and understand what they're saying and then reflect the value that your technology provides through the problems that they just described. And I actually think that actually a lot of the technologies in AI will immediately benefit a lot of these businesses, but they're being pitched by 20 AI vendors every single day. And they all sound the same. They literally all sound the same. you could see like the you know first call deck of startups that do completely different things and there's like the same words on the slides and so this is the I think the really opportunity right now is how do you actually stand out in such a saturated market I actually Clay and I talk a lot about this because we both started web design companies in high school you know like during the dot bubble I went door todoor flower shops he did more like graphical things for I think he made a little more money than you did in high school yeah I it's he did his successful. He also got sued by the California Raisins, but that's a whole another story. Um, so true story, his name's Clay. He named his company Claymation. Bad look. Um, sorry, Clay. I didn't know this a public. It's public now. Um, love you, man. Uh, and this guy doesn't know anything about the law. Yeah. uh we we talk a lot about the experience that you we were young and I not in the center of it during the dotcom bubble where you didn't just have Google you had alt vista all the web uh you had intome which was a B2B search player that was uh quite we respected quite a bit uh you had amazon.com and you had buy.com and half.com you had eBay and you know you had uh PayPal and all the other folks pursuing payments I think now like victors write the history book. So you you look at all these magnificent coming out of that era and you forget that it was like a fight to the death. Um the idea that hey maybe we should search the internet was not exactly novel. Um the but the actual like detailed execution of those ideas was paramount. And I think you know going back to like how do you help companies like ADT succeed? the first thing you need to do is like show up as a partner to them and help them solve their acute business problems. And that's a uncomfortable sort of conversation because I think a lot of people don't feel equipped to do it. Uh you know, one trick that I would just recommend is before you meet a customer, do deep research on them. And by that I mean like the actual feature deep research and chat dbt. Um you know, show up educated in these conversations. And I think you find the more you spend researching your customer and understanding their needs and less time you thinking about like the next feature going to launch, the better that conversation will go. And I think you'll end up finding that, you know, sweet spot between like where is the intersection of what you do differently and actual business value. That's awesome. Let's go to audience questions. I just realized that I've taken up a lot of Brett's time. So, uh, please go ahead. Thank you. Um, hi, I'm Robin. What are your learnings in building branded AI agents for specific verticals and how is it different to building horizontally? Uh, I really believe in verticals in AI. Uh I think that if you think about what the job proverbally of an AI agent is uh for a telecommunications company and you think about what's important to them uh versus uh say a commercial bank uh versus say a residential uh bank uh versus an insurance company who might be concerned about claims processing versus a health insurance company that's uh you know answering things like explanation of benefits. every single one of those applications is actually quite specialized and you know right now uh I think that the companies that can actually provide value quickly around these core workflows of these businesses that are actually valuable to them uh actually sort of when I talk about sort of like coming into those conversations and actually providing value really quickly just have a leg up. Um I am fairly skeptical of horizontal AI platforms. Um, it doesn't mean they won't succeed by the way. There's always an exception to every rule. But, uh, you know, things like lang chain I think probably will succeed. They're open source and you know, you end up sort of you need to sort of develop a horizontal business in a different way. But it just, you know, pure play enterprise business. I think it's very hard to go in with a horizontal platform because there's just lots of good enough. Um, there's lots of homegrown solutions. There's lots of not invented here. Um, and you know, having a slightly better mouse trap is rarely uh, you know, doesn't pass the like vitamin painkiller uh, threshold generally speaking. So, my rule of thumb is like the closer you get to uh, solving a problem for a company, the more successful your your business will be, assuming you're talking about an enterprise business. Um, and you know, the other thing that I I really learned uh from Mark Ben off at Salesforce is like if you have one successful customer, make it two, and if you have two, make it 10. And if you have 10, make it 100. Um, I do think there's a certain amount of tactical excellence that comes in when you're building a B2B software business. And ivory tower cleverness rarely wins. It doesn't mean you shouldn't have a long-term strategy by the way, but uh there's a certain if you focus on making your customers successful, that's where the truth is and where your like product should go. Um sometimes you can be led astray by that mechanism. You like the the first one was not representative. That's rarely the concern though. Usually it's uh being very very clever and sort of ivory tower with your strategy. So that's why I love verticals. Uh maybe one more. Hey Brad. Um I was really interested in the question around how you price uh AI agents. Specifically, have you seen customers being more open to you taking a cut of cost savings or of incremental revenue that you've created? And what are the main kind of push backs that you that you're hearing from customers? Because impact pricing has been historically quite hard. Um so wondering what you're seeing there. Yeah. Um we see interest in both. Um, you know, if you're owned by a private equity firm and have a big bunch of debt, you probably say the word Ebatel like 20 times a day. It's like what you do at private equity owned companies. And you probably care a lot about cost savings. Um, though, and if you're That's basically all you do. You just walk around. Yeah. Yeah. He actually really simplified it. He got it. Yeah. It's like, see, I I know finance too. Yeah. Yeah. Yeah. you have not ever let an oified version of yourself slow you down you know please and uh in contrast you know if you're in a very competitive market like if you talk to any CEO they generally care more about growth than they do cost savings and in fact I think one of the counterintuitive things there's lots of talk about with models becoming cheaper will demand for infrastructure go down no in fact demand will go up just because new use cases will emerge I think the same will be true of most cost savings in AI I think most sophisticated companies will recoup those soft cost savings and reinvest in growth. And so it won't actually be cost savings necessarily. It will actually uh flow to other parts of the business just because that's what capitalism does, right? It's like you know what saving a lot of money for what reason? Like you want to grow, you want to beat your competitors. Um so sort of first principles and and our lived experience is that you know topline matters more. There's just no doubt about it. And if you can drive a measurable topline outcome that's always more valuable. But cost savings do matter because fundamentally if you you know look at sort of productivity growth in the world over the past 30 years it hasn't been as uh dramatic as I think many people thought it would be after the '9s where we saw a huge uh spike in productivity growth. So fundamentally just like in the economy we're driving efficiency productivity in the economy and so the answer is definitely both. The interesting thing about cost savings is I assume those will be compressed over time. You know, right now most of your AI startups are being compared to the labor costs. 10 years from now when it's all AI agents, you're going to be compared to other AI agent. So the other thing about cost savings is it's a temporary drug, you know, because the costs of actually all these things are going to change because of the presence of of technology. Um, so it's a little bit like, you know, your mobile phone plans or actually always think about as like the computers we used to buy growing up in the the 80s and 90s. they would get cheaper and better every single time. And that's sort of where we are in this AI market. So our view is that like figure out what the outcomes are that your customers want and it really depends on the application honestly. Um even how you price it is interesting. One of my favorite stories, I don't know if this is actually true, but I uh I heard it and even if it's not true, it changed I think which was uh when LinkedIn was at one point um trying to grow the recruiting business, they started to do they started out with some usagebased model, but it turns out HR departments don't have a lot of uh license to spend like you know in a variable amount. So making it a subscription model just help with the procurement process. I love that example because it's a really nuanced and insightful go to market point which is that department HR is generally a cost center in most companies unlike marketing departments which have like you know money flowing from the the ceiling you know this is a department that has to go through a pretty rigorous procurement department so actually going to a subscription model was necessary to actually sell a product into that department it's that type of thing I think is where it's like is it cost savings or topline yes both also like who are you selling to? How do they buy? Uh who are the gate uh gatekeepers to approving how they spend? That's how you grow like an enterprise software business. You really need to understand who your decision makers are, who's approving it, and actually like how do they budget? Um and I have a lot of counterintuitive things, but it's like sometimes easier for companies to prepay for things than to pay on demand. Counterintuitive, but just I think that's like you really need to get into the details of your buying cycle to answer that in a in a accurate way. I'm going to wrap us up. Brett, thank you. Uh they say you should do business with people you like, trust, and admire, and you are that for me. So, I really appreciate you doing this, and all of us do as well. Thank you. Thank you.

========================================

--- Video 6 ---
Video ID: 9Eb_zp4Yfcs
URL: https://www.youtube.com/watch?v=9Eb_zp4Yfcs
Title: The Data Center is the New Unit of Compute: Crusoe CEO Chase Lochmiller
Published: 2025-05-07 20:15:44 UTC
Description:
Chase Lochmiller, founder of Crusoe, reveals how his company is constructing massive AI data centers in record time to power the next generation of AI. Learn about the energy challenges, innovative cooling systems, vertical integration strategies that are enabling the AI industrial revolution and why "the data center is the new computer."

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 7 ---
Video ID: egSh4TxS5go
URL: https://www.youtube.com/watch?v=egSh4TxS5go
Title: Ambient Agents and the New Agent Inbox ft. Harrison Chase
Published: 2025-05-07 20:15:33 UTC
Description:
LangChain CEO Harrison Chase introduces the concept of ambient agents, AI systems that operate continuously in the background responding to events rather than direct human prompts. Learn how these agents differ from traditional chatbots, why human oversight remains essential and how this approach could dramatically scale our ability to leverage AI.

Transcript Language: English (auto-generated)
Everything we're about to talk about is what's next. And the first person up here is Harrison Chase, our very own Harrison Chase, who's been at Every AIN. He's been talking about agents longer than anyone I know. We'd love to have you come up here and talk about agents, in particular, ambient agents, this new concept that you guys at Langchain are bringing to the world. Thanks for that intro and excited to be chatting. My name is Harrison, co-founder CEO of Langchain. We build developer tools to make it as easy as possible to build agents. Um, a lot of the agents that we've seen being built so far are what I would call chat agents. So, you interact with them through a chat interface, send them messages, they they run and respond and and that's great. They're great for a lot of purposes. But one of the concepts that I'm really excited about is the concept of ambient agents. So, what is an ambient agent? The way that I like to define an ambient agent is ambient agents listen to an event stream and act on it accordingly, potentially acting on multiple events at a time. And so what are the differences between this and normal agents? So there's a few. One, what are the trigger? So it's no longer a human coming in and sending a message. It's an event that happens in in in the background. How many of these can be running? With chat, you can usually only interact with one agent at a time. Maybe you open a few windows and you have a few running at the same time, but it's generally one. with ambient style agents because it's listening to these events. It's it's however many events are happening in the background. So, it can be a far bigger type of number. Another interesting point is the latency requirements around it. So, with chat, you message, you expect some response back pretty quickly or you get bored and you go to another website or something like that. Because these ambient agents run in the background, they're triggered by events. They can run for a lot longer period of time before you need a response in any shape or form. So there's generally much less strict latency requirements. And then lastly, I think it's interesting to think about the UX of these agents. So for these chat agents, it's mostly chat bots. That's a pretty familiar interface by now. I think there's a little bit of a question of how do you interact with these agents that are in the background because they they they are running without you knowing that they're running. But as I'll talk about in a little bit, it's still really important for you to interact with them in some form. So, just to make this concrete, an example of an ambient agent could be an email agent that listens to emails coming in and acts on them accordingly and maybe tries to respond or maybe tries to schedule meetings or maybe pings you or pings other people on the team. So, that's kind of like a concrete example of of one type of ambient agent that we're seeing. So, why ambient agents? I think they're interesting for a few reasons. First, they let us scale ourselves. So, if you interact with a chat agent, it's generally onetoone. You're doing one thing at a time. when you have these ambient agents, there can be thousands of them running in the background. And so that just lets us scale our impact a lot more. Two, they can they can get at kind of like more complex operations. So when you're interacting with a chat agent, it's generally because of the latency requirements, it's generally a simpler operation that it's doing. So you might have the human send a message, it goes to the the chatbot, the agent, it responds right away. Maybe it calls a tool, maybe two tools. The long more tools it calls, the longer it takes to run. can't do that with ambient agents because you don't have this as strict latency requirement. You can call a ton of tools and do more and more complex operations. You can add in other steps as well. So you can add in explicit planning or reflection steps and generally build up the complexity of the agents that you're building. One thing that I really want to highlight is ambient does not mean fully autonomous. So I still think it's really important that we are able to interact with these ambient agents. And there's a few different interaction patterns that we see people building towards. So one is approving or rejecting certain actions that these agents want to do. If you want to have an ambient agent that's potentially giving refunds to customers who are emailing in, definitely when it starts, you're going to want to have a human in there approving some of those things. Second is a more uh advanced option of this editing the actions that they do. So maybe they suggest something you don't want to approve or reject it, but you want to explicitly edit it and have it do that. Um, third, these agents can get stuck kind of like halfway down and so there should be an inability for you to answer questions that they might have, uh, just like you would answer questions of a co-orker if they're working on a deep problem or something like that. And then fourth, because these agents take a lot of steps, it might be very useful for you to go back to the 10th out of a hundred steps or something like that, interact with it there, modify what it's doing, give it some feedback. And so this is what we call time travel. and facilitating this is a a cool new interaction pattern we see. Um, so there's a few reasons that having this human in the loop is important. First, it just gives better results. So, if you think about deep research, which isn't exactly an ambient agent, but it is a longunning agent, there's a period of time up front where it asks you some clarifying questions to go back and forth, and that generally helps produce way better results than if it just went off whatever your initial kind of like question or statement was. And so having this human in the loop in the form of deep research, asking these clarifying questions, in the form of ambient agents, there's there's different types of patterns. This just gets better results. It also helps build more trust. Um, so if you're doing explicit actions like giving giving or sending payments or approving things, having the human loop just builds more trust. And then and then third, and this is maybe the most subtle one, is I think it it helps a lot with the memory of the agent. So when I'm talking about memory, I'm talking about learning from user interactions. If you don't have the user interacting with the agent, then there are no user interactions to learn from. Um, and so having this uh having this human in the loop helps inform a lot of the memory things that you want to be building into the agent so that it can do better in the future. And so with this importance of uh the human in the loop, I think it's interesting to think about what a good UX for this might look like. This is one thing that we've kind of built as a prototype at Langchain, which is the concept, we call it an agent inbox. It's an inbox for your agent to send things to. You can see when it requires actions. You can see some descriptions. If you click into a row, you can then see a more detailed description of of of what's going on, what explicitly it wants approval for or or whether you want to respond to it. And there's a few different interaction patterns here. Talking a little bit uh very briefly about some of the things that we're building that we think help with this. We've paid a lot of attention uh in Langraph, which is our agent orchestration framework to make it good at ambient agents. In particular, we've paid a ton of attention to the persistence layer that backs it. This enables a lot of these human interaction patterns because basically you can run your langraph agent. You can stop at any point in time. The entire state as well as previous states are persisted. And so then you can have the all the human and loop interaction patterns. You can wait for a second, a day, an hour, however long, have the user come in, see the state, modify it, go back to previous states, things like that. Um, we're spending a lot of time right now on Lingraph platform as infrastructure for running these agents. These agents are often way more longunning. They're often bursty because they're triggered by events. So, you could get thousands of events at a time. So, you need to be able to scale up and they're flaky in nature, not just because of typical software things, but also because of this human in the blue pattern. You want to be able to correct mistakes. And then finally, we're we're building Langmith as well for these agents. They're really long running. They can often mess up. They're doing more complex things. having visibility and observability into what they're doing is really really important. As a concrete example of this, uh, one of the things that I built on the side is an email agent. So, if you've emailed me in the past, uh, year or so, uh, it's it's drafted a response or sent a calendar invite. It's still human in the loop. I use the agent inbox all the time. It's open source and on GitHub. So, if you want to see how all these components come together in what I think is a pretty cool and unique and hopefully uh glimpse of what's next, uh I would encourage you to check it out. And with that, I will hand it off. [Applause]

========================================

--- Video 8 ---
Video ID: _2NijXqBESI
URL: https://www.youtube.com/watch?v=_2NijXqBESI
Title: The Physical Turing Test: Jim Fan on Nvidia's Roadmap for Embodied AI
Published: 2025-05-07 20:15:10 UTC
Description:
Nvidia's Director of AI Jim Fan introduces the concept of the Physical Turing Test and explains how simulation at scale will unlock the future of robotics. Learn about digital twins, digital cousins, and digital nomads in this groundbreaking talk from AI Ascent 2025.

Transcript Language: English (auto-generated)
Next up, we have Jim Fan. You all know him. Come on up, Jim. Jensen was talking about him just this morning. He is not only director of AI at NVIDIA, but also distinguished research scientist, and he'll talk to us about physical AI. So, a couple days ago, I saw a blog post that caught my attention. It says, "We passed a touring test and nobody noticed." Well, touring test used to be sacred, right? It's the holy grail of computer science, right? The idea that you can't tell the difference between a conversation from a human or from a machine. And then it just so happens that we got there. We just got there. And you know, like people are upset when um O3 Mini took a few more seconds to think or that Claude is not able to debug your nasty nasty code, right? And then we shrug off every LM breakthrough as just yet another Tuesday. You guys in the room are the hardest crowd to impress. So I would like to propose something very simple called the physical touring test. And the idea is like this, right? You host a hackathon party on a Sunday night and this is what you end up with. Uh your partners yelling at you and you're like, "Ah, damn. On Monday morning, I want to tell someone to clean up this mess and make me a very nice candle lit dinner so my partner can be happy. And then you come home to this and you cannot tell if this was from a human or from a machine's work. Right? Simple enough. The physical touring test. But where are we now? Are we getting close? Well, uh, look at this cumul robot getting ready for work. It didn't make it, right? And how about our dogs and the banana peel? Ah, yeah. And the robot instruct, you know, to to to make you a breakfast cereal. Well, it correctly identifies the milk. I I I will give that a minus, right? It's well intentioned. Oh, it spoon feeds you. It's a VIP experience, right? Look at that. I'm jealous. I got no one to spoon feed me. Yeah, this is where we're at. So why is it so damn hard to solve the physical touring test? You guys know that LM researchers complain a lot, right? They complain a lot. And recently some um some guy named Ilia, he complained. He said um the LM pre-training is running out of data. And he even called the internet the fossil fuel of AI. And he said we're running out of data to train LOM. Well, just spend one day with roboticist and you'll know that how spoiled the LM researchers are. We don't even get the fossil fuel. So, this is a data collection session at NVIDIA headquarters. There's a cafe in Nvidia and we have these humanoid robots set up where we operate them and collect the data. And this is what the data look like, right? The robot joint control signals. And these are continuous values over time. And you cannot scrape this from the internet. You can't find it on Wikipedia, on YouTube, on Reddit, anywhere. So you have to collect it yourself. And how do we collect it? We have a very um sophisticated way, but also very expensive way called teleoperation. Well, you can have a human wear something of a VR headset that recognizes your hand pose and streams to the robot. And in this way, you can teach the robot what to do, like pick up a bread out of a toaster and then pour honey over it. But you can imagine this is a very slow and painful process, right? So if you put it on the scaling plot, basically it doesn't scale at all. The real robot data is the human fuel. It's worse than the fossil fuel. You're burning human fuel. And what's worse, it's at most 24 hours per robot per day. And in fact, you'll get much less than that because the human gets tired and the robots get tired even more than the humans. So this is what you get and what to do, right? How to break this barrier? Where is the nuclear energy for robotics? We got to have clean energy. Can't live on fossil fuel forever. Well, enter simulation. We got to leave the physical world and then do something in simulation. So we trained this robot hand to do superhuman dextrous tasks as spin in a pen in a simulation and um well it's it's super human with respect to me because I couldn't spin pen and I I just gave up you know a long time ago in childhood and I'm glad that my robot at least in simulation can do it better than I do. So how do we train hand to do a sophisticated task like this? There are two ideas. One is you got to simulate at 10,000 times faster than real time. Meaning that you should have 10,000 environments running in parallel on a single GPU doing physics simulation. That's number one. And number two, the 10,000 copies of the environment cannot all be identical. You got to vary some parameters like gravity, friction, and weight. And we call that domain randomization. And that gives us the simulation principle, right? Why does it work? So imagine if a neuronet is able to control a robot to solve a million different worlds then it may very well solve the a million and first world which is our physical reality. So in other words, our physical world is in distribution of this training and then how we apply this you can build a digital twin right a onetoone copy of the robot and the world and then you training simulation you test it on the real world directly transfers right zero shop and you can do a hand and this is the most impressive task that we could do. So basically you have a robot dog on a ball and then um we transfer that to real world. This is uh at at pen uh at upen and basically someone walking the robot dog. Um our researcher super weird looks like a black mirror episode and this is this is called Dr. Eureka actually like one of researcher tried his dog on the yoga ball. At least we're super dog dexterity right now. Yeah the dog cannot do it right. And next we can also apply that to much more complicated robots like the humanoid. And these humanoid robots they went through 10 years worth of training in only two hours of simulation time to learn walking and then you can transfer that and it doesn't matter what the embodiment is as long as you have the robot model you simulate it and you can do the walking and can we do more than walking right? So as we are controlling our body you can track any pose that you want track any key point follow any velocity vector that you want and this is called the whole body control problem of humanoid and it's really difficult but we can train that right on 10,000 simulations running parallel and we can transfer that zero shot without any fine tuning to the real robot. So this is at the Nvidia lab. We actually need to slow down the video. So the first video is in real time and the next video is slowed down. So you can see the sophistication of the motion that it does. It imitates the human all these agile motions while standing balanced. And guys, how big of the neuronet network it is required to do this? It is 1.5 million parameters not billion. 1.5 million parameters is enough to capture the subconscious processing of the human body. The systemwide reasoning 1.5 million parameters. So if we put this on this diagram where you have the speed versus a diversity of a simulation I think we call this simulation 1.0 the digital twin paradigm where it is a classical vectorzed physics engine and then you can run that to 10,000 up to a million frames per second. But the issue is you got to build a digital twin. You need someone to build a robot to build environment and everything, right? That's very tedious and manual. So, can we start generating parts of the simulation? All of these 3D assets are generated by 3D gener model. All of these textures from stable diffusion or any diffusion you would like. All of these layouts generated by PR and LM to write XML. And putting all of these together, we built a framework called Roboccasta, which is a large scale simulation, a compositional simulation of everyday tasks. And everything here, right, except the robot, everything is generated. You can compose different scenes, but it still relies uh on this classical engine to run, but you can already get a lot of task from it. So now what we can do is we can have a human again do the tally up, but this time you tally up in simulation. You don't tally up a real robot. You tell it up in simulation. You replay that trajectory in simulation and you add all the great hardware accelerated rate tracing to make these beautiful scenes with lighting. And you can even vary the motion. Right? If you uh teleoperate and then move the cup from here to here, you don't have to demonstrate moving cup from here to here or from here to here again. And then putting all of these you have one human demonstration in a simulation through environment generation. You can multiply that to n. And for motion generation, m * n. And I promise you, this is the only math that you're going to do today. That's how we multiply the data. And then you put it together. Column one and three are the real videos from our real robot. And column two to four are from the roboc simulation all generated. So you can still tell that these textures are not real, but they're kind of close enough. And what do we call the things that are close enough? We call it the paradigm of the digital cousin. It's not the digital twin, but it kind of captures the right. So digital cousin and these simulations run slower, but there are this kind of hybrid generative physics engine where we generate parts of it and then delegate the rest to the classical graphics pipeline. Now simulate this scene, right? You got soft body, you got fluid, you got everything. It's gonna take a very long time for artists or graphics engineer to simulate this scene properly. So if we look at how graphics evolved, it took 30 years to go from the left to the right. And it just took video generated models one year to go from left to the right, simulating all the deformable right noodles, right? It lost some sense of humor here, but that's a price I'm willing to pay for latest Sora VO, right? All these strateg models only one year. And that's the power of scaling and datadriven process. Do you recall this video I showed at the beginning? I tricked you guys. There's not a real pixel in this video. It is fully generated by a custom model. So what we do is we take a general purpose open-source right state-of-the-art video generation model and we fine-tune it on domain data collected in our real robot lab and all of these are generated and now you can prompt the model to imagine different futures right to simulate the counterfactuals. So you see these two frames are the exact same but given different language the generated video is actually going to follow language and do the right thing even though this motion never happened in the real world. And then you can do this. It the video diffusion model doesn't care how complex the scene is, right? It doesn't care if there's like fluid or soft body and the same scene you can ask it to pick up different things. It will actually use the right hand to grab the object and put it in the basket. And these all generated all of these are generated. None of a pixel is real. It gets all these kind of reflection correct. Um right all all of all of those interactions correct. And uh one of my favorite is uh the robot playing ukulele over there. So basically um the the video model probably has seen millions of human like lots of humans playing ukulele and then it just simulates the robot finger to do that. Even though the hardware doesn't actually support it, the video gen generation model can do it. So if we put this in perspective, right, this is simulation 2.0 Z where it's got a lot of diversity but it could run pretty slow these days and nobody calls it but I'm calling it the digital nomad right which is wondering into the dream space our video diffusion model and what is a video diffusion model right it is a compression of hundreds of millions of internet videos into this kind of simulation of the multiverse so just like Doctor Strange right you instantiate the robot in the dream space and basically the robot can now interact with objects everywhere uh everything everywhere all at once. So you have this embodied scaling law. Um, okay. So Jensen left, but uh I think he's going to like this a lot, right? So you need a lot of compute to scale up the classical simulation and that's the sim 1.x series. The issue is as you scale this up, it's going to hit a wall because the diversity is limited in this handcrafted system. And then this is the neural world models, the sim 2.0 that's going to scale exponentially with compute. And that's a point where the neuronet network outperforms a classical graphics engineer. And together these two adding up that will be our nuclear power to scale up the next generation of robotics systems. Uh the more you buy the more you say the more you save. So at the beginning whoever says that the compute situation is going to improve not worse burn this figure into a retina and think again. And you put all those data into what we call visual language action model that takes in pixels and instruction and output motor control and uh you get uh what we open sourced at uh March GTC Jensen's keynote called the Groot N1 model and we run on the robot you know it could be romantic sometimes um yeah you can't imagine how much cleaning we did during during training so yeah it's able to grasp the champagne in in this one it did it perfectly Yeah, they do very well. And then it can also do some industrial tasks, pick up some of the factory objects and it can also do multi-root coordination. So group N1 is fully open source and actually the future series of the model will also be open source because we're following Jensen's paradigm of open source and democratizing physical AI. Great. So what's next? Where do we go after we solve physical AI? I will say the next thing is the physical API. You know, throughout human history, right, 5,000 years, we have much better tools, right? Much better society in general, but the way we make dinner and do a lot of hand labor are still more or less the same, right, from the Egyptian times. And maybe for 99% of the human history, we have this structure where you go from raw materials through human labor and you build civilization. And maybe in the last 1% or like 50 years, we have human labor shrinking and we have these highly specialized, highly sophisticated robot systems that can do one thing at a time. And it's very expensive to program, but they still live out our society. And that's what we have right now. And this is the future is to push that blue bar all over the place, all over there, and have the physical API, right? Just like LOM API, moving around chunks of digits of bits. The physical API moves around chunks of atoms. You basically give your software a physical actuator to change, right, the physical world. And on top of this physical API, there's going to be a new economy, a new paradigm where you have physical prompting, right? How do you instruct these robots? How do you teach them? Language sometimes not enough. You can have physical app store and skill economy. So let's say Michelle chef doesn't need to just go to the kitchen every day. He can teach a robot and then basically deliver Michelling dinner as a service. And I should quote Jensen here again that future everything that moves will be autonomous. And one day you'll come home right to a clean sofa and a candle lit dinner and um your partner's smiling at you instead of yelling at you for not doing the dirty laundry. That still motivates me every day, right? and um you bought two humanoid robots last month. It's running group N7 and those robots just fade into the background, right? Kind of like an ambient intelligence. It fades into the background and you wouldn't even notice the moment that we pass the physical touring test and that day will simply be remembered as another Tuesday. Thanks. [Applause]

========================================

--- Video 9 ---
Video ID: v9JBMnxuPX8
URL: https://www.youtube.com/watch?v=v9JBMnxuPX8
Title: AI's Trillion-Dollar Opportunity: Sequoia AI Ascent 2025 Keynote
Published: 2025-05-07 20:14:51 UTC
Description:
Sequoia Capital partners outline why AI represents a market opportunity at least 10x larger than cloud computing, where startups should focus to win, and how the rise of AI agents will create an entirely new economic paradigm. Founders need to adopt a “stochastic mindset” and “go at maximum velocity. All of the time.”

Transcript Language: English (auto-generated)
All right, my name is Packer Radio. I'm a partner at Sequoia. Sonia and Constantine and all of our partners at Sequoia will be your hosts for the day. Now, before we get into the real meat of the day, me and Sonia and Constantine are going to just share a few perspectives on some of the things that we've learned over the last year or so. And we're painfully aware that we're the appetizer, not the entree. I I got an email from one of the founders that I work with yesterday who said, "Oh, hey man, I'm going to be a little bit late to this thing. I'll probably show up at like 9:35." I was like, "Well, that's oddly specific. That's exactly when Jensen comes on." And so, so look, we get it. But we're going to share a couple thoughts and then we'll get into the real stuff. All right. First off, some calibration. What do we think is going on in the world of AI? Simple framework that we use to look at markets. What is it? The Don Valentine question. So what? Why does it matter? Why now? It may be inevitable, but is it imminent? And then finally, what now? What do we do? How do we take advantage of this? How do we play to win? And we've talked about each of these in years past, but we're going to update some of our thinking over the next couple of minutes here. And I actually I gota be honest with you, I had a real banger on the what, but Constantine delicately suggested that telling a room full of AI people what AI is not a great idea. So, we're actually going to jump straight to the so what. Okay, just for fun, who remembers this slide from last year. All right, thank you. Wonderful. Top row, cloud transition. Bottom row, AI transition. Left side of the page, yesterday. Middle of the page, today. right side of the page tomorrow. What does it say? It says that the cloud at 400 billion of revenue is bigger than the global software market when that transition began. If we reason by analogy, the market we're going after in the world of AI services, the starting point is at least an order of magnitude bigger. The end point 10 or 20 years from now has a chance to be absolutely massive. Important point. Now, we've actually updated our thinking. It's not just the services market that AI has gone after. It's both services and software. Means that both of these profit pools are under attack. We've seen plenty of companies that start with software, gets a little bit smarter, becomes more of a co-pilot, gets a little bit smarter, becomes more of an autopilot, and they actually progress from selling a tool into software budget to selling an outcome to selling work into labor budget. Both of these TAMs are up for grabs. Okay, who remembers this slide from last year? Oh man, I'm a little sad. That was only like three, four people. All right, there's one more. All right. All right. All right. Thank you. Don't be bashful. You guys can raise your hands. Okay. This layer cake represents the waves of technology that have stacked up over the last several decades to bring us to the present moment. There are two points to this slide. One, AI is in fact imminent, not just inevitable. The precedent conditions are in place. compute, networks, data, distribution, talent. We have all the ingredients we need. So, we're ready to go. Second point of this slide, these waves tend to be additive and so the opportunity is much bigger than it's been for prior waves. It's also coming much faster. Now, I hate this slide. X-axis time, Y-axis vanity metric. People use this to justify all manner of sins. Okay, but the observation is correct. Things are happening faster and faster and faster and faster than they have before. But not a lot of people have dug in on why. And so we just want to take a second on. If you think about the physics of distribution, you just need three things. People have to know about your thing. They have to want your thing. And they have to be able to buy your thing. That's it. Does anybody remember that logo? When the cloud transition got going, nobody was paying attention. Ben off had to do these crazy gorilla marketing tactics to get anybody to pay attention. AI is very different. November 30, 2022, chat GPT comes out. The entire world is paying attention to AI. Middle column, these are the combined monthly active users for Reddit and the artist formerly known as Twitter. didn't exist at the start of the cloud transition, barely existed at the start of the mobile transition. Somewhere between 1.2 and 1.8 billion people on these platforms today. Not the only way to find out about cool stuff, but a pretty good way to find out about cool stuff. Right side of the page, let's imagine you listen to Benoff. There are only 200 million people connected to the internet. Today, it's 5.6 billion. It's effectively every household and every business in the world. And so, what does all this mean? means the rails are in place and when the starting gun went off there were no barriers to adoption. This is not an AI specific phenomenon. This is the new reality of technology distribution. The physics have changed. Train tracks are in place. Okay, another slide from last year. What do we do about it? Where do we play to win? Two points on this slide. Number one, a lot of white space. Again, this is a slide from last year. There's a little less white space now. People are starting to break out, but for the most part, the opportunity is still wide open. Point number two, these logos represent the companies that got to a billion plus of revenue. We don't care about unicorns. We care about revenue and free cash flow. Got to a billion plus of revenue during prior transitions. Most of them are at the top of the page. Most of them are at the application layer. We have believed and we continue to believe that the same will be true for AI. The value is at the application layer. But guess what? You got competition. We got a second scaling law. We got test time compute. We got reasoning with tool use and inter agent communication that allows the foundation models to get pretty far into the application layer. What do you do if you're a startup and you're not building a vertically integrated business? Go from the customer back. Think vertical specific. Think function specific. Deal with complex problems that might require a human in the loop. But this is the race. This is where the value is going to be. This should be top of mind for everybody. Okay. How do we play to win? All right. You come on. You guys can laugh. This is a good one. All right. How do we play to win? Well, we showed this last year, so I'll presume you're not laughing because you saw it last year. So 95% of building a company in AI is just building a company. It's the same old stuff. Solve an important problem in a unique and compelling way. Get great people to follow you. All that good stuff. The other 5% is AI specific. And in the context of that race for the application layer, there are a few things to consider. This is the Leone merchandising cycle. Our partner Doug Leone, the goat, spent 40 years lovingly crafting this little piece of content. This represents everything you need to take an idea in your brain and turn it into a product in the hand of your customers. Okay, that idea has to turn into a product that has to be built by an engineering team that has to be taken to market and sold and supported. This is the value chain. Bottom of this page is the tech out point of view. Top of this page is the customer back point of view. That's how you can build moes across the entire value chain. Your customer is not sure what they want out of AI. You can have an opinion. You can give them an end toend solution that just solves the problem as opposed to throwing a tool over the fence. You can build data flywheels with the usage data of your own product. That's something that nobody else has. You can be of the industry for the industry, kind of like what open evidence does for the medical industry. You can speak their language. Harvey sends lawyers in to talk with law firms. And I got to be honest with you, we wouldn't recommend Ford deployed engineers, but you can do it. Tough way to live, but you can do it. You can put a big bear hug around your customers in a way that foundation models probably won't. By the way, we love foundation models, too, but we're assuming that most of you are not building foundation models. Most of you are building applications. Okay, I have two more slides, then we'll hand it off. So, we get asked the question a lot. What do you look for in AI companies? And again, 95% of it is the same stuff that we look for in any company. Okay, here's the 5% that's AI specific. Point number one, revenue. Vibe revenue can kill you. Everybody loves five revenue. Feels so good. Oh my god, we have so much revenue. Go look at it. Okay. Is it tire kicking or are you actually creating durable behavior change? Okay. Oh, I don't have the metrics to do that. Yes, you do. Inspect the adoption, the engagement, the retention. What are people actually doing with your product? And don't delude yourself into thinking you have real revenue when you have vibe revenue because it'll bite you. Okay, the good vibes piece of this is important, too. Actually, real quick, Andrew Reed, are you in here? Vibe check. How we doing? How's everybody doing? I don't care about you. How's everybody doing? Impeccable. I heard impeccable vibes. Okay, good. You need good vibes with your customers. What does it mean? Your customers have to trust you and you have to earn that trust. Trust is more important than your product at this point in time where we are in the cycle. The product will get better. If they trust you to make it better, you're in good shape. If they don't trust you to make it better, you're in bad shape. Point number two, margins. We don't necessarily care where your gross margin is today. The COGS component of that is probably going to keep going down. You know, cost per token is down 99% in the last 12 to 18 months. That cost curve is going to continue. those cogs are going to go down. I know it's stepping up with test time compute and all that, but that's going to go down too. The price component of that, if you are successfully marching from selling a tool to selling an outcome and you're going up that value chain and you can capture more of that value, your price point is probably going to go up too. And so your gross margins may not be great today, but you should have a good path to really healthy gross margins over time. And then point number three, data flywheel. Raise your hand if you got a data flywheel. Okay, what business metric does that data flywheel move? Okay, I see less certainty. I got good news and I got bad news. Good news is if you can't answer that question, I still love you. The bad news is your data flywheel is Either you don't have a data flywheel or it just doesn't matter. It needs to tie to a business metric or it just doesn't matter. And this is really important because this is one of the best modes that you can build. Last slide. Who can tell me how these two things go together? I'd be really impressed if you can. It's not logical at all. All right. Nature hates a vacuum. There is We'll get there. Nature hates a vacuum. There is a tremendous sucking sound in the market right now for AI. All of the macroeconomic stuff, tariffs and interest rate noise doesn't matter. The rising tide of technology adoption absolutely swamps any of the volatility that you see in the markets. Ignore it. Okay? There is a tremendous sucking sound in the market and if you don't get in front of it, somebody else will because nature hates a vacuum. And so all the stuff we just said about modes and metrics and whatever notwithstanding, you are in a run like heck business right now. Now is the time to go at maximum velocity all of the time. Thank you, Pat. So I'm going to use my f my section to focus on what's happening with AI right now. And we'll start with a quick year in review both from the customer back and from the technology out. So first a year in review. Back in 2023 we showed this chart of the ratio of daily to monthly active users for AI native applications versus for traditional mobile apps. And the punchline at the time was that AI apps had terrible engagement ratios. Hype exceeded reality in the data. We're very pleased to report that that punchline has now changed dramatically. It's been stunning to see, for example, the daily month to monthly active ratio for chat GPT climb the curve and now getting close to Reddit levels. I think this is extremely good news. It means that more and more of us are getting value out of AI and we're all climbing the ladder together on how to weave AI into our daily lives. And sometimes that usage is good and fun. I personally melted an embarrassing number of GPUs trying to jiblify everything. Uh but while the jib moment was fun and it was viral, the more exciting thing is all the deeper things that we're just uh scratching the surface of. So for example, advertising, the ability to create stunningly accurate and beautiful ad copy. Education, the ability to visualize new concepts with the snap of a finger. Healthcare, the ability to diagnose patients better with an app like open open evidence. We're just scratching the surface of what's possible. And as the AI models become more and more capable, the things we can do with them through this front door become more and more profound. Okay. Who here has seen the movie Her? Okay. Uh, and we have Brendan in the audience today. Um, we still don't have AI scar Joe, but 2024 gave us what I would call the her moment for voice. And voice generation went from almost there to fully crossing the uncanny valley. I've had a few folks say that, but keep your cards close to your chest. Let's see if I can truly blow your mind. You ever seen the movie Her? Oh, yeah. Her classic. Walking Phoenix really nailed that falling for your OS thing, right? Makes you wonder what the future holds. Sesame's voice demo was incredible, Brendan. I'm excited to see what you build. And it's crazy how quickly the gap between science fiction and reality is closing on us. And it feels like the touring test really just snuck up on us. Actually, hat tip for that to to Jim Fan who tweeted it and I stole it for this presentation. So, hi Jim. Um, finally, the breakout application category of the year was coding, which reached screaming product market fit. Anthropics Claude 35 sonnet dropped last fall, triggering a rapid vibe shift in the coding landscape. And people are now using AI coding for really impressive things. So, for example, this person vibe coded their own docend alternative. And so whether you're a veteran 10x engineer or somebody who has no idea how to code, we do think that AI is fundamentally changing the accessibility, the speed, and the economics of software creation. From the technology back, the bad news is that pre-training does seem to be slowing down. We've scaled pre-training by nine or 10 orders of magnitude since the Alexet days, and that means a lot of the lowhanging fruit has been picked. The research ecosystem is finding new ways to break through. The most important breakthrough was reasoning from OpenAI. We were lucky enough to have Noam Brown from the Strawberry team give us a preview of what was going to happen with reasoning at AI Ascent last year and this year we're happy to have Dan Roberts in the audience who's going to be giving another talk on what's happening with O3 and reasoning ahead. But it's not just reasoning, it's synthetic data, it's tool use, it's it's aic scaffolding. All of these things are combining combining to create new ways for us to scale intelligence. Enthropics MCP created strong ecosystem and networks and we're also excited to see what that does to accelerate agentic tool use. So all of this bigger base models, inference time reasoning, tool use, it's all combining to create AI that's capable of increasingly sophisticated tasks. The meter benchmark is one good quantitative uh measure of this. But I think what's even more powerful is talking to each of you about the types of things that are only possible now because of 03 or operator or deep research or sonnet. And then finally, a lot of the most exciting technology innovation in AI right now is happening at that blurry boundary between research and product. And I think the two breakthrough examples of this over the last year were Deep Research and Notebook LM. And we're excited to have the creators of both of those products in the audience with us today. Risa and Jason from Notebook who are creating a new company called Hu and uh Issa Hulford from OpenAI. Okay, let's talk about where value will occur in the AI stack. I remember debating this question with my wonderful partners at Sequoia at the time. I was personally the the midwit in the middle of this uh this chart saying like ah I don't know about the GPG rappers. And I remember my partners, Pat in particular, were adamant that the value was going to acrue to the applica uh to the application layer. And I remember thinking like, okay, Pat, sure, good luck with that. Uh but having seen how the last few years have played out, I think you were right, Pat. Uh you belong over here. Good job. uh if you if you see where value has been created, if you see companies like Harvey and Open Evidence really creating that customer back value, we very much believe that the application layer is where the value will ultimately acrue and that the battleground is intensifying in this layer of the stack with the foundation models increasingly competing here. Okay, quick aside, the joke's actually on all of us because the real undisputed king of the stack who was making all the dollars was the goat himself, Jensen Hang, uh who we're excited to hear from shortly. Okay, back to the application [Music] layer. Uh we now think that the first cohort of AI's killer apps has emerged. Whether it's chat GPT, Harvey, Glean, Sierra, Cursor, A Bridge, but there's a whole set of new companies on the rise across a very rich and diverse set of end markets uh including Listen Labs, including Open Evidence. We're excited to feature many of you uh today. Another prediction is that many of these new companies will be agent first and that the agents these companies are selling will evolve from prototypes that are often just pieced together today to ones that are really robust. And we see two paths that companies are taking to build that. Uh path one orchestration with rigorous testing and eval path two agents tuned on endto-end tasks. And we're excited to hear more on this from both Harrison of Langchain and Issa of OpenAI today. Our next prediction on the shape of agent companies in 2025 is vertical agents. Vertical agents are a wonderful opportunity for startup founders who deeply understand a domain. And we see companies creating agents that are trained end to end to excel in a very specific workflow using techniques including reinforcement learning on synthetic data and user user data to make AI systems very performant at very specific tasks. And the evidence we've seen so far makes us really optimistic in security. Expo is now showing that they have an agent that can outperform human penetration testers. In DevOps, Traversal is showing that they can create an AI troubleshooter that is better than the best human troubleshooters. Um, same goes in networking with meter and networking engineers. And so all of this, all these data points, they're early, but they make us very optimistic that vertical agents trained to solve a very specific problem can outperform the best humans today. One final prediction on agents in 2025. We're entering an abundance era. code as the first market category to tip will offer us a preview into what that abundance era actually means. What happens when labor is cheap and plentiful? Are we going to get a bunch of AI slop? And what happens when taste becomes the scarce asset? We're excited to see the continued progress of coding agents and what that does to the technology landscape, but also as a harbinger for how other industries will be changed by AI. I'll turn it over to Constantine. Thank you, Sonia. All right. Great job. with you. All right, good morning everyone. Thank you, Sonia. Thank you, Pat. So, we just talked about really big important topics. The so what, why does this matter so incredibly much. The what now, what's going on in the world, the state of AI today and its immediate future. Now, we're going to take a step back. We're going to think through predictions about what's coming in the mid and long term. In this section, we'll have three parts. We'll talk about what we see as the major next wave. Then, we'll get into the technology needed to achieve that wave. And then finally, we'll talk about what it means for each of us, our day-to-day lives. A year ago, AIScent was all about agents. We were talking about agents and they were just beginning to form into businesses. The topic was really these machine assistants that eventually we predicted would come together as machine networks. These machine networks are now broadly called agent swarms. They play a role in many of your companies and are beginning to form into a really critical part of the AI stack. Agents working with each other, against each other, collaborating, doing reasoning with each other. In the years to come, we think that this matures even further into an agent economy. An agent economy is one in which agents don't just communicate information. They transfer resources. They can make transactions. They keep track of each other. They understand trust and reliability. And they actually have their own economy. Well, this economy doesn't cut out humans. It's all about humans. The agents work with the people and the people work with the agents in this agent economy. But in order to achieve that very big important next wave that we're all going to enter, we have a lot of important technical challenges and we'll talk about three of them. Frankly, this room is going to be addressing those three as as you build. Well, the first is persistent identity. When when we talk about a persistent identity, we really mean two things. The first is the agent itself needs to be persistent. If you're doing business with someone and they change dayto-day, you you probably won't be doing business with them for very long. That dramatically different experience will take its toll. The agent is going to have to be able to persist on its own personality and its own understanding. The second type of persistence is understanding you. Similarly, if you're doing business with a person and they they don't remember anything about you, they can hardly remember your name, that's also a big challenge to trust and reliability. Now, we've been trying everything from rag and vector DBs to really long context windows, but everyone in this room knows that there are still major challenges in true memory and self-learning on that true memory and actually getting the agents to be able to be consistent where it matters and and only differ in the areas where they should be differing. The second major technology shift is we need to have seamless communication protocols. Now the great news is everyone seems to be focused on this now. But imagine personal computing without seamless communication protocols. No TCP IP, no internet. We're just now building that protocol layer. And there's certainly a lot of excitement around MCP. It's so great to see the big players collaborating and working together to come up with this is just one but a series of protocols that are going to allow for transfer of information, transfer of value, and transfer of trust. Finally, security. This is a topic that's going to be on the rise and is certainly on top of many of your minds. If you can't meet the person you're doing business with palm to palm, face to face, that importance of security and trust is even further elevated. You can't with agents. So, we're going to have an entire cottage industry formed around that trust, that security, and it will be even more important in the agent economy than it already is in our current economy. So, we talked about the technology needed to get to this big wave, to get to this agent economy. Now, let's talk a little bit about what it's going to mean for each of us. Well, first of all, it's going to change our mindsets. And and frankly, this room is already there on what we're calling the stochastic mindset. The stochcastic mindset is a departure from determinism. You know, a lot of us fell in love with computer science because it was so deterministic, right? You program the computer to do something, it does it. Even if that results in a sag fault. Now, we're entering an era of computing that's going to be stochastic. If you ask a computer to remember the number 73, it'll remember that tomorrow, next week, next month. If you ask a person or an AI, well, it might remember 73. It might remember 37, 72, 74, the next prime, 79, or nothing at all. The point here is that this is going to be materially different thinking from what we've done in the decades past. Second change is a management mindset. And this management mindset is going to be all about understanding what your agents can and can't do for you. Everybody knows that being a really good IC engineer is pretty different from being a great engineering manager. And this is going to be the transition that most of the economy is going to make into more complex managerial decisions like blocking processes and giving feedback. And I really hope it doesn't result in year-end reviews of agents. Let's try to avoid that. The third major change for each of us is a combination of the previous two. way more leverage with significantly less certainty. We're entering a world where you can do more, but you have to be able to manage that uncertainty and manage risks. And in this world, everyone in this room is extremely well suited to thrive. A year ago at AIScent, we talked about this chart and we were talking about leverage because we said individual functions at an organization will begin to have AI agents. And then we predicted that these functions would begin to merge. They'd cluster and entire processes would be completed by AI agents. We even made the prediction that we'd have the first oneperson unicorn. Now, that hasn't happened yet, but we have seen companies scale faster than ever before with fewer people than ever before. And we do think we're going to get to the highest level of leverage that we've ever seen in this economy. Eventually, these processes and agents are going to merge. You're going to have neural networks within very large complex neural networks. A network of these neural networks. And this is going to change everything. It's going to reinvent individual work. It's going to rewire companies. And it's going to recreate the economy. Thank you all for joining us. We're going to have an amazing AI sent today. And we're very grateful for you being with us.

========================================

--- Video 10 ---
Video ID: i5zEDZBqCtI
URL: https://www.youtube.com/watch?v=i5zEDZBqCtI
Title: Building the System of Record for the AI Era ft Workday CEO Carl Eschenbach
Published: 2025-05-06 09:01:00 UTC
Description:
Workday CEO and Sequoia partner Carl Eschenbach explains how the company is evolving its platform to handle both human and AI workers. He shares Workday’s three-pronged approach to monetizing AI through seat-based pricing, role-based agents and consumption-based API access. Eschenbach discusses why domain-specific agents with curated data will be more valuable than general-purpose models in the enterprise, and how Workday is helping enterprises navigate the transition to an AI-powered workplace while maintaining human connection.


Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

Transcript Language: English (auto-generated)
As soon as you go in and you talk to the enterprise, a CEO, CFO, CIO, whatever it may be, it immediately goes to an ROI conversation like this is going to save me a boatload of money. And in some cases, it is. That being said, if you only focus on the benefit of the ROI, what happens is immediately if you're an employee, you start to think, man, my job's being replaced. We need to flip that narrative as well. We need to start to think about how do we use this technology to drive ROI to get dollars to reinvest in the business to drive growth. We need to talk about AI from a growth value proposition, not just an ROI value proposition. And that's how you get the employees and agents and AI to peacefully coexisting enterprise and really drive growth for [Music] companies. Greetings. Today we're joined by current Workday CEO, former Sequoia partner, and perhaps the most beloved executive in technology, Carl Echinbach. Workday, as you may know, is a system of record for people and the numbers produced by those people. As we move into a world where people are complemented by AI agents, workday is on a path to become the system of record, not just for people, but also AI agents and of course all the work done and numbers produced by those people and those agents. Carl talks about Workday's three-pronged approach to monetize AI through seatbased pricing, role-based agents, and consumption-based API access. He also talks about how to implement AI and how to transform a company while preserving your culture and values. We hope you enjoy. Carl, welcome to the show. Thank you for having me. It's great to be here with both of you. I couldn't think of two better podcasters to be with, Sonia and Pat. Come on. We We couldn't think of a better venture capitalist who is moonlighting currently as a CEO to be doing this show with. Okay, moonlighting. I like that. Actually, I am moonlighting because I have responsibilities here with you and my other job. Right. Some days I wonder which I'm doing more. No, I didn't say that. I'm doing the other one more if I'm honest. But I appreciate still being part of the partnership. It's an incredible uh opportunity to still engage with what I think is the most iconic venture capital firm of all time, Sequoia. And just to be around it in some capacity and see people like you and all my partners is truly a blessing for me to still be involved. So, thank you. Thank you. We really appreciate that. All right. All right, we're going to start with a question on workday. Okay, so you guys are the ultimate system of record for people in a world where AI is replacing people. What happens to the system of record for people? Interesting. So if I may, we are the system of record for the two most critical assets of any company, their people and their money. Okay? Because we have the financials platform. Good distinction. Interestingly enough, they're one and the same. And we get asked and I get asked all the time, you're a system of record in the enterprise. AI is coming. It's a transformative technology. It's going to disrupt everything, especially in the enterprise. And companies like you might be in trouble. And I would say with a bias towards being biased, I think we're in a very unique position. And let me explain why. Today, 20 years old as workday, we've been managing and supporting our clients most critical assets. As I said, Pat, they're people in their money. Yep. And we now have a database, a highly curated database of more than 70 million users that is processing 30% of the job wrecks in the United States last year. And when you think about AI, at the end of the day, it's all about data. I think you guys would agree with that. Now, it's not just about data. It's the context of the data. Mhm. And when you have that much data and you have the context of it and you're in the actual workflow, the business process workflow, you can then use agents to drive actual results that people see value in doing in using. This is where we start to talk about things like role-based agents and the impact it can have on the enterprise. The other thing I'd say when you have 11,000 customers, you've been around 20 years, you've been that core system of record, it led to us becoming the system of truth. People trust us. And anytime you go through one of these major tectonic shifts from one technology to the next generation of technology, I believe especially in the enterprise, the enterprise customers look to us and say, "How are you going to help us with that transition?" Just like 20 years ago, you helped us with the transition from on premises to the cloud for HR and finance. Yep. So the combination of the data, the context of the data, we're in the workflow, the trust we have, and I think at the end of the day, as we talk about agents, and I'm sure we'll talk about it here, I think domainspecific agents with highly curated data are much more beneficial in the enterprise than just general purpose LLM models that are out there. Agreed. That's why we think we're uniquely positioned. I get asked all the time, are the incumbents in trouble because of the startups? And quite frankly, it's not an or, it's an and. There's going to be success across the board. You guys have a bunch of great companies that we work with that actually bring more value to the system of record. So, it's both. I think there's opportunity for everyone. Pat, and your point on trust is a good one, and that's definitely been a recurring theme amongst the application AI companies that seem to be working. Let me ask you a question on product though. So, let's say that I am an AI developer of some sort, or more practically, Sonia is an AI developer of some sort, and she shows up at workday because she heard that there's this treasure trove of information that you've amassed over the last 20 years, all of these records and all the context around the records. How much of that is actually available for you to go build cool AI stuff with? And how much of that due to permissions or information architecture or 20-year-old company stuff? How much of that is not available? Or or said differently, in actually building with AI, all the data that you've amassed is obviously an advantage. How accessible is it? And what roadblocks are there to making the most of all that data when you try to turn it into product? Yeah, it's a great question. Um, if you look at the amount of companies and technologies that access the workday system of record, it's a lot. For example, identity companies, for example, active directory. When you onboard a new employee, guess where they all come and do that? on top of the Workday platform. Today they do it through an open set of APIs, general purpose APIs that we put out there for people to develop on and connect with us. Going forward as we start to think about this world of AI and the potential of all of these agents entering the world in the enterprise, someone has to think about how you onboard them in a secure, responsible, ethical, compliant way and make sure their identity and access controls and all the data they get access to has governance around it. Today that's the risk. That is the risk in the enterprise. When we talk to CIOS, the first thing they say to us, we love agents. We love this technology. We love what potentially could happen with with agents. But we are concerned we're seeing agent sprawl across the enterprise. My analogy, Pat, is remember when we always used to talk about shadow IT creeping up and how do you get it back because of the security and compliance controls that you want in a corporate uh headquarters or a corporate function. That's what we're seeing. Now, what we're seeing is people want to bring agents into the enterprise. They want access to our records. They want to onboard them just like we would a human worker. They want to do it with a digital worker. And what we're building is something called an AI gateway where you're not going to be able to do that to a generic set of APIs that we have today. You're going to have to come through this agent, if you will, gateway to onboard into the enterprise through us. Yeah. Say more about this. So people people are using Workday or will be using Workday to onboard AI agents analogously to how they might onboard human employees. Bingo. I'm going to hire you as my next sales rep, Pat. So what we announced, uh, that was a good setup and you didn't even know it. Uh, about a month and a half ago, we announced something called an agent system of record. And think of it as the unification of a system of record between human workers and digital workers. Just like human workers get onboarded in the enterprise today. They come through workday. They get onboarded. They get assigned to whatever organization they're part of. They get their benefits. They get tracked. They get monitored. And they get looked at for performance reason. And then you do benchmarking of all your human employees to drive workforce management and capacity planning. The world of AI comes about. Agents come into the enterprise. Everyone says they're going to come into the enterprise. Okay. How do they get into the enterprise? Who onboards them? What organization are they part of? What access rights do they have? Who controls them? Who measures them? No one. Today, we think in the future and as we talk to our customers with this new agent system of record, that unification of human and digital workers on a common platform is the path forward. And then once that's in place, you can start to do workforce management, workforce planning around all of your workforce because it's no longer a workforce of human. It's a workforce of human and digital workers and that's what we've developed and we're in the process of bringing to market. That's so fascinating. So it's a system of records for your your people and your money but your people includes your your human people and now now your agents as well. Exactly. And there's a lot of fear out there Sonia that these agents as they come into the workforce are going to completely displace all the human workers. Right. And we're going to, you know, I hear some of my peers go out there and talk about we're going to see a 20, 30, 40% reduction in the human worker, right, going forward by all of the agents in this whole AI revolution. I fundamentally don't believe that to be the case. Yeah, it's interesting. I I remember the I remember 15 years ago hearing a Neil articulate the vision for workday and part of it was the reason people and financials go together and the analogy to the last generation before that which is you know the ERP systems of yester year SAP and such physical manufacturing was the economy and so the same system that took stock of all of your machines was the system that was your financial backbone and with the original workday day, we're moving into a knowledgebased economy. And so the same system that takes stock of your people is the system that provides your financial background backbone. And now as we move into an AIdriven economy, the same system that takes stock of your agents is the system that is also financial backbone. And because it's fluid between the people and the agents, it makes sense that it would all live in one place. it all and if you think about you know the workday platform everyone thinks about us having a HR platform and a finance platform but if you look at the architecture they're one and the same exactly as you just described now layer on top this new workforce called digital workforce on top of it that benefit you've got for the last 20 years continues but it brings these agents into the enterprise in a much secure way. Yes. Yes. And then practically speaking, how do you how do you take this to market? How do you how do you roll this out? Do you do you need some sort of a standard unit of pricing and packaging that an AI agent builder can adhere to? You know, how a person is a person. And granted, there are lots of different types of people, but everybody's got a date of birth, you know, everybody's got a title. um what are the common characteristics of AI agents that will allow them to sort of map into this framework that you guys are now and how do you price them as well and also how do you go to market right I remember being at Sequoa I'm going to back up and that'll answer your question do you remember when this whole actually I was talking to RU about this last week and everything was going to go PLG everything's going productled growth you don't need sales reps you don't need you know 37year sales veterans like myself anymore. You don't need the enterprise. You don't need the top down. I'll tell you the times are coming back. With AI, it's a top down sale in most cases. It's not productled growth up through the enterprise. You're selling to the heads of departments or heads of functions or a CIO. So, I think the distribution is coming back to a much more inhuman uh sales motion. You're seeing that. We're absolutely seeing I think that's a common thread if you look at the AI companies that are really working. It's fun because I told RU when I saw him last week, I'm like, see, I told you PLG is good, but it's not the panacea that everyone thinks. You are still going to need people like me someday. Yes. And they do, right? Um, so, so, so I just thought I'd wanted to talk about as far as pricing goes, there are multiple ways people are pricing AI and then I'll speak specifically about agents. Let me back up. Two years ago when co-pilots came to market, a lot of our peers, a lot of my peers and competitors immediately went to market and they basically said to their customers, we now have a co-pilot and because we have a co-pilot, we're going to raise our pricing 20%. And we made a decision and I specifically said I do not want to do that. I don't think it's the right thing. Co-pilots are going to become part of the core foundation of all products and it should be built in, not bolted on and it should be the value our customers get and receive by paying us a subscription fee on an annual basis. We have to innovate. Yeah. To be honest, we got beat up quite a bit. Why aren't you doing this? You have co-pilots. You should be monetizing. And I said because it's the right thing to do. When we bring true value, right, with our agents in the future and we can show you a strong ROI or a value equation, then you'll pay us. Fast forward to today, we price our AI solutions in one of three ways. Number one, we pay price it based on seats. So we are a seatbased company for the most part. And if we have an agent or an AI technology that provides value to all of the employee population, then we will do an uplift based on that agent or that technology. Uplift per seat per seat. Okay. The second would be based on an actual agent itself. Like if I can come to you Pat and say we're rolling out a payroll agent and you think about how many payroll administrators you have today, but we bring a role-based agent into the market. I'm making this up and we're gonna say it's $50,000 a year for this agent. Today you have a payroll agent or a payroll employee over here looking at everything. All you they're doing is administration and that cost $200,000. You'll say, "hm, I'll do this." So you can do it based on the role. Do some of those cannibalize the seatbased model of core workday? Meaning if I now have a payroll agent, I need to hire fewer payroll people and I would have been paying for a seat in workday for each of those payroll people. Does the agent business cannibalize the sort of original seatbased business? I don't think so. Okay? Because what AI should allow that payroll administrator to do is go off and do more high value tasks and drive more strategic outcomes for the company as opposed to just being replaced. If you think about what we do today, you, me, everybody, we spend the majority of our day working with technology, sitting at our desk, working on a phone, we're working with technology. The power of AI is going to happen when we transform that equation and the technology works for us. Yes. And we don't even know what's happening. Right now, we're interfacing with all of these, you know, chat bots and all of these different, you know, LLMs out there, the consumer, but in the enterprise, what's going to happen and the value is going to happen when these agents and AI technology start to work on our behalf. We don't do those tasks and we go off and do other things. So, we're going to flip the equation around. So that's where I think the value comes in for existing employees to go get new skills. Just like the AI momentum could be called the revolution. I think we're going to go through a skills revolution with the existing workforce to train them to go do other things. And and if you look at the history of time, people like all of us, all employees, they're very pliable. Yeah. They're adaptable. And what do we always figure out to do? How to leverage technology? Technology is the single biggest source of productivity gains in the history of time. For sure, right? That leads to GDP growth. That leads to company growth. The other thing we need to work on is this narrative that's out there. As soon as you go in and you talk to the enterprise, a CEO, CFO, CIO, whatever it may be, it immediately goes to an ROI conversation. M like this is going to save me a boatload of money and in some cases it is. That being said, if you only focus on the benefit of the ROI, what happens is immediately if you're an employee, you start to think, man, my job's being replaced. We need to flip that narrative as well. We need to start to think about how do we use this technology to drive ROI, to get dollars, to reinvest in the business, to drive growth. We need to talk about AI from a growth value proposition, not just an ROI value proposition. And that's how you get the employees and agents and AI to peacefully coexisting enterprise and really drive growth for companies. Y sorry we sidebar. Let me come back. There's another way to monetize it and how I'm thinking about it. Okay. So we got first off we have uplift on all the seats. Second one we have price per agent. Yeah. Think about that value based pricing, right? Based and then the third is consumption based. Okay. meaning how many times is something like I was talking to Brett Taylor as Sierra right they do it based on consumption and outcomes right and as you know Pat being on their board right we're thinking about the same things how many times back to your initial question are people hitting us to get access to our data so we can base on consumption okay so when all of these agents come out in the world and all of these agents say wow I need to get access to the data to be registered or whatever it is we got to get access to workday say okay great please do so but we're going to charge for that access going forward as opposed to just that open set of public APIs. So in that case it's a it's a platform business and it's sort of metered almost metered consumption model right and then you can price it to you go out a lot of people today are going out and saying here buy so many you know cons so so much consumption and then as you do it you burn it down over time right uh that's that's a big model that's out there and we're going to do all the above you mentioned AI for productivity okay you are easily top three most productive human beings I've ever met possibly top one probably top one honestly But you're certainly top three. Um, as an insanely productive human being, how are you using AI to make yourself more productive? Have you have you do you have any favorite products? Do you have any favorite workflows? Like what Studio Gibbly pictures? Yeah. Any great Studio Gibli pictures? Yeah. What are you doing with AI to make yourself more productive? Yeah. So, inside of Workday, and then I'll talk about how I use it, too. Inside of Workday, we've rolled out a whole bunch of, you know, productivity tools around AI. We have Slack AI, we have Zoom AI, we use Gemini all the time. Gemini is something I use all the time. So, an example, I'm preparing for earnings every 90 days, which is so much fun. Please join me if you guys want. Um, all the analyst reports come out, right? And, you know, how do you summarize them? How do you you throw it in Gemini and boom, and they give you save, you know, hours of reading. That's one example. Or you're on a Zoom call that you missed. Like, how does that get summarized back to you? I use those type of things all of the time. Yeah. And it just makes me that much more productive. Uh clearly one of the things we're doing inside of workday and I think it's it first of its kind at scale. Uh we're launching this month something called everyday AI where all 20,000 employees have to go through training on AI and how to learn how to use all of these tools to drive personal productivity and through that personal productivity gain workday gets gain. Yeah. Yeah. Right. So we and they have to test out there's an entire curriculum. It's a really interesting way to get people excited about AI, how to engage with it and leverage the technology as opposed to be afraid of it. And we think it's one way to drive, if you will, that peaceful coexistence between the technology and the humans to drive overall gains for the company. That's another example how the whole company has to do it. And I'm launching it in two weeks. Very cool. Can you tell us more about the agents that you're bringing to market? So, you mentioned the payroll agents. What what other agents should folks expect from you? We bought a company at this time last year about a year ago called hired score and they have what I would describe as a recruiter agent. So, if you think about recruiting Sonia in the HR function, it is talent acquisition or recruiting is probably one of the most expensive functions you have. Yeah. Right. in HR. This recruiter agent we brought to market, we can give it to a recruiting organization or talent acquisition organization and they in a very short matter of time can see a 50% increase in recruiter productivity. Just think you get a hundreds of resumes for a job. Today someone searching now it basically can do that in seconds and say here's the top two or three you should go after. But it's more than just recruiter productivity. It also has proved to accelerate time to hire by 20 30 up to 40%. So your recruiters are more productive and you can accelerate your time to hire. Then once you get them inside the building, we have two other AI products. One is called talent optimization and the other is a talent mobility agent. And when you implement them, we have seen upwards of 40% reduction in attrition. And the reason for that is for the employee, it takes a look at of all your skills, your background and your experience. And for the manager, you say, I have this project or outcome I'm trying to achieve and it starts to do the matching between the two. So what it drives is an increased level of internal mobility with your workforce which drives down attrition because people feel like they're getting new opportunities. So that's like a full life cycle of agents from recruiting to onboarding to talent optimization to talent mobility and all that we can monetize because people can actually see you see a reduction of 50% of you know you know um need for additional recruiters. That's pretty powerful. Yes. Can I ask you a question on you mentioned this idea of matching talent to other opportunities inside of their company? One of the ideas that we've heard from a few different companies at this point is this idea that thanks to AI the matching problem that is here is a job to be done here is the right person to do the job. Thanks to AI that matching problem is now possible. Historically, it wasn't because you'd have to collapse the dimensionality of that problem to things like what is the title, what is the geo, what is the pay scale, which really doesn't tell you whether or not this person is going to do a good job. And so I'm curious, are you are you a believer in the ability of AI to actually match the right talent to the right opportunity? And if so, is that a big theme for Workday going forward? We heard a glimpse of it with the internal product. Is there more of that to come? Absolutely. And it all becomes completely or highly dependent on skills. It's all about skills. It's not about pedigree or college or anything else like you know there like let me give you one statistic um and then I'll come back to say how we do it. Accenture, one of our biggest partners out there, who's a very reputable, as you know, system integrator, global system integrator. Today, their new hires, 30 to 35% of their new hires do not have a college degree. Hm. It's pretty amazing because what they've done, they're probably one of the most advanced companies I've worked with that looks at skills and job outcomes or job wrecks they're trying to fill and then they use AI to match the two, right? And that's what our product does internally. Let's say you have 10,000 employees. Because we are the system of record, we know all the skills you have and you have this, if you will, database of all the skills of your employees. So now when you want to try to hire someone, you first look inside and say, "This is what I'm trying to fill this job. These are the projects this person's going to work on and these are the type of skills that are required." Then you look at your own 10,000 employee population. Say, "Wow, here's all the skills Billy or Susie have." And you do that internal matching. And so as we move to a skills-based outcome world, right, this becomes more and more valuable inside of companies. um for people not to just hire based on pedigree is how we like to think about it. Carl, I remember when I interviewed for Sequoia, you were very memorable. Uh and I mean so much of so much of that interaction though is just a human can I see myself working with this person being inspired by being around them every day. Do you think that AI can actually and a agents can disrupt the actual interviewing process and and you know the the process of actually selecting the right candidate for the job and in terms of some of the human judgment elements? It's a great question because you know me I like doing this with my partners like you. It is all about the human touch and human connection uh in the relationship you establish with the other person. That's really what's important at the end of the day. But that's also why we always talk about and you talk about probably the same thing. There's going to be a human in the loop in all of this, especially in hiring. I personally would not accept someone into our company if they was 100% done purely based on skills and skills requirement. There has to be a human in the loop all the time. I think Yeah. And and I know we talk about where AI is going in the future and it will have emotions, it will have feelings, it will have empathy. I'm I'm trying to get there with everyone. Um but I still think this is required um ultimately when you're going to hire a human. In fact, one of the things I think that's going to change, and here's an example of where humans will not just necessarily be replaced by AI, but since AI is going to have such a profound impact on human productivity, it's going to free us up to move from a world of technology transformation to a skills transformation by people like us now going and picking up skills that I actually think we've lost in the workforce. Right? those life skills, those people skills you're talking about. How do you learn to network? How do you learn to give feedback if you're a manager? How do you collaborate? How do you lead with empathy and feelings? That's we lost a lot of that during co and now we got to get that back as people come to the office. How do we educate and train and mentor the younger people? You can't do it all the time over Zoom. There has to be a human connection to make that happen. So we're now talking about how do we use the AI transformation of the workforce to transform skills that we think are missing in the enterprise to bring the two together ultimately to drive better outcomes for both employees and companies. This is one of the optimistic views of AI that I think we very much believe in and we hear similar things. You know, Winston at Harvey talks about how the legal profession is going to kind of go back to what it was 50 years ago where it's less about processing transactions and it's more about human connection and the advice and the consuary role that people can play. You know, Christopher O'Donnell a day talks about how because AI generated CRM does a lot of the drudgery for you, you can actually focus on the human connection too. And so I think the the optimistic view and and sort of what I'm hoping for out of the world of AI is technology to your point earlier on how we work for technology and it's going to work for us. Technology kind of fades into the background because AI is doing a lot of the work and it sort of frees you to have the human connection be more well front. I think we move from a world where we're working with technology to the technology working for us and today it's front and center and we talk about how we're using AI but the power of AI is these models learn right and they start to do things autonomously on your behalf so you don't even have to do it so you go do new things and the last thing I'd say is this is this is factual technology enables change. Mhm. That's all it does. It enables change to drive change. It still takes people no matter what. Well put. People are going to be involved with AI. They have to implement it. They have to embrace it. They have to teach people how to use it. And they have to teach people new skills. Technology only enables change. It doesn't drive. It still takes humans. So this is one of the ironic things. I completely agree with the point and and maybe a different way to frame that. One of the ironic things ironic things about this agentic economy agency is the thing that is uniquely human right the ability to determine what is it that I want to achieve here that is still a uniquely human thing you can dispatch an AI agent to go reason through the steps and figure out how to get there but deciding what question should I ask what is the objective you know that's still a uniquely human thing I think another framework would be you know Google sort of commoditized knowledge open AI is sort commoditizing intelligence. Agency is the thing on top. You know, intelligence is making use of that knowledge. Agency is determining to what end. And that's the thing that is still uniquely human. And I think the argument in favor of AI is it gives you superpowers to go do that. Yeah. I I think it's well put. Again, especially in the enterprise, Pat, I think there's so much opportunity. I think AI will drive a step function change in human productivity. No doubt. No question about it. I think how we get there, there's different views. there's different opinions, right? Um, and I think, you know, we at workday and myself for sure think it's going to be a peaceful coexistence of humans and technology working together just like we've done in, you know, the past major tectonic shifts. I remember when cloud evolved and came about, everyone said everyone's going to CIO organization, you're not going to need any more people. I don't know. There's probably more people than ever working in, you know, the CIO's organization. People say developers aren't going to be needed. I'm not sure that that's going to be true. We'll need less junior developers, but really architects and senior developers are still going to do the final work and approve it or not. Back to Sonia's point, we're not going to have Carl vibes coding the dead code base. Yeah. No, I would love to see Carl. That would be a problem. I'm I'm There are some things I can do and there's a lot I can't. That's one I can't. Right. So, we talked about transformation a bit and how people are going to transform and AI is going to transform the workplace. Um, the timing of your transition into the CEO role at Workday was pretty interesting because it was January of 2023, I believe. And so, we're just a couple months post chat GPT moment. Yeah. And so, you you came into a company that was at that point 17 years old, 18 year years old, something like big company, already many billions of revenue and tens of thousands of people. And from the outside looking in it it seems like you have driven transformation on at least two vectors. You know one is driving just efficiency and velocity and just kind of basic you know cleaning up the business getting it in better shape sort of transformation. And then the second of course is the AI transformation. And so the question is because I think there are a lot of people you know looking at transformation over the next few years. What did you do to drive transformation? what was effective for you? What were some of the maybe what was a crucible moment or two that you had to sort of bust through to actually drive change in an organization with that much scale? Yeah. No, listen it is a big company um when I joined and it's a one of the most successful obviously software companies of our generation if not ever. Um and it wasn't Carl who drove any transformation, it was a team that drove it with me, right? and to get their support was probably what I had to do and work on most. Um, let me start with what hasn't changed and what will never change because I think it's very special and it will never be transformed. When Anne Neil and Dave, the founders of Workday, uh, who worked together and Dave was the founder of Peopleoft, when they sat down to start this company 20 years ago, Pat, the first thing they did is they said, "What kind of company do we want to build and what is the core foundation of the company and how we're going to scale from this day forward?" And the first thing they did is they sat down, they wrote values. They wrote six values of the company. And to this day, we still talk about those six values as the foundation for the company. We focus on our people first, our customers, two eyes, integrity, and innovation. Uh, and the last two is we like to have fun. And the sixth one is profitability. And profitability is the sixth one for a reason because if we do the first five right, we're going to be profitable. And even today, that's all I talk about when I'm in front of the company or in front of customers. We talk about how important a values-based company is. That being said, our culture is strong because of those values. But culture, while the values will never change, culture does change as companies scale and grow. And part of our transformation is to think culturally, how do we actually become a bigger company? Uh how do we run things more effectively and more efficiently and drive a deeper level of operational rigor? Uh how do we transform? A lot of work has been done because probably of my experience and background. I can't change who I am or where I came from on the go to market side. How we've segmented the market, how we've gone deeper into verticals. Uh how we've expanded and you know built a true partner ecosystem. I now say I don't want to build just an ecosystem around workday. I want to build an entire economy. Our partners are now not only deploying us, but they're all now taking us to market, they're selling on our behalf. They're being rewarded and most importantly they're innovating like crazy on top of the workday platform. We're unique that we're both a platform company and an application company at the same time. And the platform is starting to resonate as we open up the aperture. Some of the things we talked about earlier to allow people get access to the platform. They're innovating on top of us and then they build it once a partner can sell it to our marketplace and sell it to our 11,000 customers. Right? Build it once, sell it to many. So we've done a lot on that side. And then on the technology side, yeah, I stepped into this. Everyone knew about AI. We knew AI was going to become, you know, a big part of a company like Workday. It always, you know, has been on on the road map. In fact, you know, if you talk to Anneil, who I think is one of the most, you know, thoughtful, uh, mindful and smartest, you know, enterprise software guy I ever met. Like he's been doing AI and machine learning in the platform for a decade, right? That's what he said. So, this is just the next evolution. I didn't know stepping into this like this was going to happen this quick. And I think one of the things that we've done in some of the transformation and crucible things we're doing is moving with speed and velocity at scale. Yep. Like we're moving much faster. Uh we're we have a sense of urgency because we think we're uniquely positioned because of the data set we have. Um we think we got to move quick before anyone tries to come and disrupt us. So, we're focused on disrupting ourself and I think that's been a lot of fun. Um, unfortunately, you know, to support all of these growth initiatives and these transformations that we're going through. Um, one of the hardest things I've ever had to do in my career is a sizable um, you know, transformation of our people where we did a restructuring earlier this year, about 8% of our workforce, about 1,650 people. And listen, that was the hardest thing I've ever had to do in my career. When it's on your your watch and your time to do that at a values-based company is not easy. So getting all the the leadership and and my partner and Neil and the board to support that uh was a heavy lift. Um but the way we did it was incredible. The empathy we showed, the caring, the loving, uh, the embracing of our workmates that were leaving us, the packages and severance we gave them was done in such a way that I think people felt as good as they could going through this type of a transformation. And now we freed up all of these opex dollars to go invest in these transformations on both go to market and the product side. Um, it's been incredible. It'll be two and a half years here in a couple months. Uh it's been a great experience. Um and I'm grateful for the opportunity. I feel a sense of um you know pride to continue this you know endure this company well beyond you know my time here and feels like days there's a lot of pressure if I'm not lying but you know I have an amazing set of people that I get to serve alongside of that believe in our mission believe in the product believe in the technology and believe we're actually having an impact on the world because we manage more employees than anyone out there today that's pretty fun pretty cool So if the first 20 years of workday were sort of helping usher your customers through this on-prim to cloud transition in the next 20 years, I know this is oversimplified, but the next 20 years are helping to usher your companies into this world of AI. Does the nature of the moes that you build as a business change when you go into the AI world? or said differently, you've had this wonderful moat historically which is being a core system of record that a lot of other applications sort of orbit around. Does the nature of the moat for the next 20 years change with AI or is it kind of more the same? It's just the AI version thereof. I think the moat remains the data. When I think about data, data is the new UIUX for AI. Think about that. Data is the new UIUX for AI, right? Garbage in, garbage out. Highly curated data with context in great outcomes with agent. So I think it's a both. I think our moat is what allows us to build the next set of moes and that is role-based agents. Yeah. And if we can bring role-based agents because if you think a lot of today what you see in agents if you a lot of them are just super co-pilots. They're task agents. They call them agents, but they're doing a task. Yes. And it's automating some repetitive task, right? Think of RPA on steroids almost, right? Yes. When you go into a world where we leverage the mo of data and we have the context of the data and we understand the skills humans have, think about now building role-based agents that doesn't do a task, it picks up skills. Mhm. And because we know that, I think that's going to become our next big moat along with the data we have as we transform how people work alongside of agents and not just use them to solve a repetitive task. M so I think as the world goes into the future because of that moat I think people do trust us to help them navigate the unknown of AI to help them you know take advantage of it in the future specifically obviously we're not a consumer company but in the enterprise should we jump in a lightning round uh oh let's do it surprise you go for it where's the lightning come on which of your beloved Sequoia partners do you miss the Oh, wow. I got it. Ready? Yes. Emily, there you go. Nailed it. Nailed it. That That's actually That's the best answer you could have possibly given because I don't think anybody will take offense at that answer. Listen, I love all my partners. Uh, listen, I, as I said earlier, Sonia, I'm so grateful you gave the old operator a chance to come in. And I think some people were nervous about me coming in, but I dove in. I learned thank you to you two especially I'd attempt to write those damn memos and I'd be like all right that's enough let me get some help and you guys would just I remember the red lights coming back I'm like damn it I thought that was really good until I give it to you two. Um but I hope you saw that I came in with no ego and came in as an apprentice and I dove in and learned and cold called prospect. It was like I went back 30 years of my career bagging the phones and I didn't think I was any different than anyone else. And hopefully you saw that. It was amazing. It was an inspiration to all of us. This is a thing that I don't think people appreciate is like people I think people assume that oh well Carl is such an amazing operator. You plop him down at a venture capital firm. Of course he's going to be successful. I don't think what people realize is the degree to which you embrace the beginner's mindset and like truly learn the job from scratch. And and that was that was it was hard truly inspirational everybody. But I had people like you like and Doug and Ruof and Jim and you name all the out every partners like everyone like I remember coming out of partner meetings and I'd say get in this conference room what's a DA what's a mau what's this? I'm like what are you guys talking about right and then you learn it more and yeah but great it was amazing that's why I still appreciate being around all of you. Yeah. All right next question. I think that it's safe to say you would be a consensus pick for most people's Mount Rushmore of CEOs. No. God, no. Pat, who is on your Mount Rushmore of CEOs? Who are the four CEOs that you really look up to and admire? Wow. Um, one would definitely be top of mind comes Joe Tucci. Just an incredible human being, incredible leader. People will run through walls for him as I would. Um, he was pretty special. Um, I don't know. I haven't thought about that, Pat. I don't know him well, but I'm super impressed in with what Sachi's done. He's pretty impressive watching come up from the technology side and his success. Uh, I thought um been pretty impressive. Oh jeez, I don't want to call anyone out because then I'll miss someone. Um, you could just leave the four spot open and say, "Hey, you know who you are." Yeah, that's a that's actually a really good question, Pat. I must admit. Um I I don't know if I have anyone top of mind, but I I really uh get impressed with people who find a way to to have longevity as a CEO and that are very pliable or adaptable as things change in the world, right? They find they don't just kind of root themselves into what they were or how they used to lead or what they used to think, but they think about how to transform themsel. Just like we talked about business transformation, how do you personally transform yourself to operate in whatever era you're in. I think people who have a a long career as a CEO is is something I'm always impressed with as well. Um I don't know. Then the fourth one, you know, there's many I'll use your line. There's so many of them out there, I can't even think of answering only one. Fair enough. Favorite AI app that you use in your personal life? What What do you do with your your wife and your kids? Okay. With AI? No, I use I use Gemini because it's accessible. I mean, I use Chat GPT, right? I definitely use them a lot. You know, just asking questions. Yeah, I probably I mean, that's what I use probably more than anything else. Just a generic, you know, I'm gonna start sending you some AI songs. Yeah. Yeah. There you go. I think my kids use some they send me funny stuff and they're like I'm like what the hell are you guys doing? But uh but no, I I just use more of the the the business uh tools around AI uh to try to help me become more productive and get more hours in the day. Yeah. For any founders who are listening who are in the process of growing into being CEOs, if you had to pick one piece of advice to give them, what would the one piece of advice be? Yeah, quite simply, um it would be to focus on the success of others, not focus on your success. That's very well put. All right. Thank you, Carl. Thank you, partners, for having me. Thank you, Carl. An honor and pleasure to be here with you. Um it's great to be inside the Sequoia building again. Thank you. It's great to have you back. Thank you. Heat. Heat. [Music] [Music]

========================================

--- Video 11 ---
Video ID: LrMKsBtx5Bc
URL: https://www.youtube.com/watch?v=LrMKsBtx5Bc
Title: The Quest to ‘Solve All Diseases’ with AI: Isomorphic Labs’ Max Jaderberg
Published: 2025-04-29 09:00:35 UTC
Description:
After pioneering reinforcement learning breakthroughs at DeepMind with Capture the Flag and AlphaStar, Max Jaderberg aims to revolutionize drug discovery with AI as Chief AI Officer of Isomorphic Labs, which was spun out of DeepMind. He discusses how AlphaFold 3's diffusion-based architecture enables unprecedented understanding of molecular interactions, and why we're approaching a "Move 37 moment" in AI-powered drug design where models will surpass human intuition. Max shares his vision for general AI models that can solve all diseases, and the importance of developing agents that can learn to search through the whole potential design space.

Hosted by Stephanie Zhan, Sequoia Capital

00:00 - Introduction
02:33 - Key questions in RL
06:16 - Multiplayer games
11:05 - Finding the right recipe
13:13 - Working with Demis
15:47 - If everything goes right
21:44 - “A holy grail model”
26:08 - The breakthroughs of AlphaFold
35:00 - What kinds of data are missing?
37:34 - Limiting factors in drug development
40:00 - How do drug designers use AlphaFold 3?
43:46 - Building an interdisciplinary team
46:43 - AlphaFold Server
49:22 - The GPT-3 moment in AI biology
52:42 - When will we see the first AI-generated drug in clinic?

Transcript Language: English (auto-generated)
We have set up the company from day one to really go after this big ambition. This isn't about uh developing therapeutics for a particular indication or a particular target. Um it's really thinking about how do we create a very general drug design engine uh with AI something that we can apply to not just a single target or even a single modality but we can apply this again and again across any different disease area. Um, and that's what we're stepping towards at the moment. [Music] Today we're excited to welcome Max Yotterberg to the show, chief AI officer of Isomeorphic Labs, which launched out of DeepMind with a goal of revolutionizing drug discovery using AI. Last summer, they released Alphold 3, a stunning breakthrough that allows us to model not just the structure of proteins, but of all molecules and their interactions with each other. That led to Demisabus winning the Nobel Prize in chemistry last year. Max describes their vision for what a holy grail model for drug design and what agents for science look like. He draws parallels to his experiences building Alpha Star and Capture the Flag and the research directions of building agents and games more broadly. Specifically, with 10 the^ of 60 possible drug molecule structures, we need to build both generative models and agents that can learn how to explore and search through the whole potential design space. Max also describes his vision for what a GPT3 moment for the field might look like, describing it more akin to Alph Go's famous move 37 when we start to see things that exhibit superhuman levels of creativity in AI drug design and that stun even humans ourselves. This is one of my favorite episodes yet. Enjoy the show. Max, thank you so much for joining us today here in London. No, it's a pleasure to be um with you here. Yeah, it's fantastic. Um, awesome timing too with the launch of AlphaFold 3 and with Demis winning the Nobel Prize in chemistry, which is a true testament to everything that you and your team have done over the last couple years. Yeah, 2024 was definitely a busy year for us. Um, lots of big breakthroughs. Nobel Prize was just incredible to see, you know, I think amazing recognition for this for this seminal piece of work. Yes. Um, well, I'd love to start with talking a little bit about your own personal story. You've had an incredible career in the world of deep learning from the very start. um authoring many seminal papers while at DeepMind including for Capture the Flag and Alpha Star um breakthroughs in the world of deep learning. Can you walk us through some of the key questions that you had in your field of research around deep reinforcement learning at the time? Yes. So at at at Deep Mind, yeah, I I worked on a whole host of stuff, you know, early days of uh computer vision and and deep generative models. Um but it was really reinforcement learning that ended up hooking me there. Um DeepMind was the place in the world to be working on reinforcement learning at that time. And you know really the question in our minds was how can we actually get to a point where we could get an AI um that could go and go off and do any task you wanted it to do. And um you know the the dominant paradigm at that point in time was supervised learning. Yeah. And supervised learning is very different from reinforcement learning. They're both learning techniques. But supervised learning, you need to know what the answer to your question is and that's how you train the model. So in supervised learning, you give an example and then you supply the model with the answer to to that question. Um, now that can be great if you already know everything about the problem that you're training this AI to do, um, this neural network to do, but most times you don't. Yeah. I mean there there's just so many problems in the world where we don't know what the answer is. Yeah. Uh we don't know what the solution is. And if you think about, you know, I think about how I want AI to be applied to the world. Um yes, it's going to be great to be able to apply things where we're already good as humans here, but really, you know, the the big frontier is can we start applying AI to places where, you know, humans don't know how to do this stuff or, you know, there's a limit to human um performance there. And um you know that's where reinforcement learning is one of the key tools and has real promise here because in reinforcement learning you don't need to know what the answer to the question is. You just need to be able to say whether the answer that the model gave you was good or not good. Yeah. Maybe even how good or not good. And um so this opens up a completely new you know um field of problems to train these models against. And so reinforcement learning and and really you know starting from what was one of the big breakthroughs of DeepMind in in the early days was was you know working on games like Atari. Yes. The question was okay so how can we scale this up from you know the world of Pong um and Space Invaders to things that really start to look like real problems in the world. Yeah. Um and so there was an amazing track of research um as we scaled up these methods. Yeah. Did you know that Sequoia was uh the first investor in Atari back in the day? Oh, really? I didn't I didn't know that. That That's incredible. Yeah. Yeah. No, those those Atari games were, you know, great fun actually to um to sort of go back and play in the context of, hey, we've got an agent and, you know, draw I'm just going to have a game of Pong on the side as well. So, um there's a wonderful wall at Sequoia in our office where um we have all these names of um legendary IPOs and and M&As that have happened. There's one um uh I think it's called the pizza company uh and I love asking folks if they know what that is and it's actually from Chuck-e-Cheese's uh which was an original Sequoia investment at the time. Oh, amazing. Amazing. Um so Capture the Flag and Alpha Star uh were incredible breakthroughs at the time. Can you share a little bit about what exactly those breakthroughs were and maybe why you chose those specific games? Yeah. So, you know, if you think about the history of of AI um using video games, you know, why do we use video games at all? Video games are these sort of malleable, perfectly encapsulated worlds that as researchers and scientists, you know, we we can manipulate them. We can test out different algorithms in them. We can set up different situations. So, the the perfect test ground for us to develop new algorithms. Um, and then you can imagine as a, you know, RL researcher, as someone who's like thinking about how can we get AI to be as general as possible, you're always thinking, okay, we've cracked Atari, how do we get a more complex game? Yeah. Um, and the thing that I was personally obsessed with is I want these agents um to be able to zero be able to do any task. And this is a slightly different paradigm from the uh from what people were doing at the time with training on Atari where you normally reinforcement learning you think about here's a game now you get to train on it and get good at it and then you apply that same algorithm from scratch training on different games. Yes. I'd love a different scenario where instead we train an agent and then we can lift it and put it on any new task. Yeah. And that agent will be able to perform well in that task without any more training. Yeah. And so to do that, what you're really asking for is generalization over task space. Yes. And that means you need lots and lots of training tasks. So the training data in this RL for agents becomes tasks. Yeah. Not images, not pieces of text, but tasks. And so you can imagine you could go and sit and um you know take a whole game studio and try and hand author you know hundreds of different tasks lots of little mini games uh in these virtual worlds. Um and we did that you know we we were doing lots of that. And then you can think yeah we can actually uh go further than hand authoring. we can procedurally generate um these tasks and games uh generating worlds and and and um maps and and and different objectives and we did that. Um but you keep running into this complexity ceiling that there's only so much complexity that you can hand off or you can design. Yeah. Humanly. But that's where multiplayer games come in. Yeah. because as soon as you go from single player to multiplayer, it's not just the agent playing. You've got another player in this game. Um, and that other player or other players can take on many different characteristics and many different behaviors. So, every different player, every different strategy that you're up against changes fundamentally the game and what the agent is trying to do. You know, I I I go back and think what, you know, why are people still obsessed with playing chess? Yeah, you know, why does a professional chess player still keep playing chess? It's the same game, but it's actually not because you're playing completely different opponents day after day and new people into the world. Um, so the game is continually changing. So, multiplayer games and multi- aent games really encapsulates that huge diversity of tasks that you might encounter just from other players um being there. And so capture the flag was actually one of our first foray into um how can we use multiplayer games to really stretch what our reinforcement learning algorithms can do to really force us to think strongly about how we can generalize to new tasks, how we deal with these multi-agent dynamics. Um so capture the flag was a fantastic breakthrough. Um really showed that we could get to human level performance for these multiplayer firsterson games. Yeah. And then of course Starcraft added on a huge amount of complexity um and was sort of the next frontier that we had to um go after for this. You were so early in this so many of these concepts are very very relevant today in the world of language. How does it feel to to see some of this work continue to be played out? Yeah, it's brilliant. Um it's it's it's just fantastic actually. You know there were so many things that we were talking about seven years ago. Yeah. Yeah. Yeah. you know, 2015, 16, 17, 18. Um, and to see all of these core fundamental concepts being really useful and really applicable today in the world of large language models. Um, you know, the and and resulting in performance that we could only really dream about at the time. That's that's incredibly satisfying actually. Yeah. So then, you know, in your own words, you said that um you moved from building toys to then finding real applications. when did you know that you found the right recipe? So, you know, I I I just love deep learning. I I've been obsessed with deep learning for, you know, 10 15 years now. Um, and the thing that I love about it is that you have these underlying core concepts, these fundamental building blocks that are somehow incredibly transferable between different application spaces. Yes. Um so you know it's the same building blocks that we were using in computer vision in 2012 as we were using in uh you know generative model early generative models in language you know then reinforcement learning etc etc. Um, so what I was seeing just again and again was this ability to take these core concepts, these same core concepts, take incredible people um, who understand how to, you know, they're almost like master chefs of putting these these concepts together and these different building blocks together. take team of incredible people um and go after you know really really challenging problems you know problems that you go to conferences at the time and you and you talk to leading researchers in the field they say no no no this is 10 years away you know and in the back of your mind you know okay we actually we basically cracked it wow and I saw I saw that happen again and again and again um you know you take amazing people amazing algorithms amazing compute on really challenging problems and we can find recipes now to crack so many problems. Um, and so it just got to the point where um, and I've always been quite obsessed with the application of these methods. I I want to see this technology have, you know, real transformative positive impacts in the world. Yeah. Uh, and so, you know, we need to start actually going after that and and, you know, the time has been right for, I think, a few years now. Yeah. Well, so you've now had a decadel long relationship working together with one of the greatest scientists, technologists, and founders of our lifetime, Demis. Um, he called you while you were still at Oxford. Uh, and then your company, Vision Factory, and DeepMind were both acquired by Google back in 2014, around the same time, and that's when the two of you started to work together. Um, now for over 10 years. What was it like or what has it been like to work with Demis? Yeah, I mean Demis is a incredible person. Um, you know, a real character and a real visionary. Yeah. Um, and you know, also amazingly human and relatable and I think that that really inspires people. So, you know, it it only takes a five minute conversation to for him to sort of really bleed out the the depth of ambition that he thinks about um and just the the immediiacy of of of the potential to get, you know, to step towards these ambitions. So I think um he he has this great ability to inject a lot of energy into you know a group of very smart people. Um get people to see beyond what's right in front of them. Uh you know I remember moments sitting uh well standing in the lobby of one of the early Deep Mind offices. I think this was the it was a a toast. We were a celebration we were having for the first Nature paper from DeepMind. Wow. And um Deis was saying, you know, this is actually just going to be the first of dozens of nature papers. And at the time, this was the first basically the first machine learning paper in nature. This was the Atari DQN paper. And the prospect of dozens of nature papers, you know, it seems a bit farfetched. And actually, he went further and said, "And and and we're going to be winning Nobel prizes as a result of this." And that was 10 years ago. Yeah, that's incredible. The forethought that he has, um, he's got what I call like one of these rollouts minds. Maybe it comes from all of his experience playing chess, but it's he's always, you know, rolling out into the future. What what are the steps now that are going to lead, you know, to this big ambition. Um, so yeah, it's been it's been fantastic. I've been working with him for about 10 years now. Um, you know, still work really closely together on isomeorphic labs. Um, and the ambition is as big as ever. It's so interesting to hear that um you had this ambition and that he had this ambition from the very start. Um and it's incredible that it's played out that way. Um well, I'd love to talk a little bit about isomorphic. You're now embarking on one of the most ambitious missions of our generation to reimagine drug discovery and drug development with AI. If everything goes right and you realize your vision for isomorphic, what does the world look like? Yeah, you know, we we think really big isomeorphic. Um, we want to be solving all diseases here and and and and genuinely that that that scale. And the point is that this technology that we're building, you know, and AI as a whole field is going to be completely transformative in how we understand biology, in our ability to manipulate and craft chemistry to modulate that biology. Um so we really think about a future where we are solving all diseases where you know AI is not just helping us you know discover and create and design new therapeutics but also just understand so much more about our biological world about how our you know cells are working um what are the root causes of of disease um and therefore opening up new pathways that we can um think about modulating. Yeah. Um so uh we we have set up the company from day one to really go after this big ambition. This isn't about uh developing therapeutics for a particular indication or a particular target. um is really thinking about how do we create a very general drug design engine uh with AI something that we can apply to not just a single target or even a single modality but we can apply this again and again across any different disease area um and that's what we're stepping towards at the moment. How does um setting out with this ambition of being general change how you built in practice from day one? Yeah, it's a good question. Um, when I think about some of the status quo of AI in drug design, there's a lot of the there's been a lot of use of uh machine learning models in chemistry and biology, but I would call them a lot of the first generation of of these sort this sort of application to be more local models. Yes. where you might have some data about a particular target or about how a particular class of molecules is is is behaving and you'll fit a small you know multi-layer MLP against this data. Yeah. Um to help you generate some predictions that lead to your next round of design. Yeah. Um this is the complete opposite approach of what we were trying to do. So from day one we were setting out to create models that generalize across chemistry and across target space. So uh you know and a key example of this is is something like alphaold and alphaold 3. Yeah. Where this is a model that you can apply to um a whole different host of targets. You can apply it to any protein in the in the proteome in in the universe of proteins. uh you can apply it to any small molecule that you can think of designing without needing to fine-tune it, without needing to fit any local data. Yeah. And so you can you can imagine that completely changes the way that chemists can use these models if you don't need to be adapting this model to every single application. Yeah. So, every single one of our internal research projects, and by the way, when I think about, you know, what we're going to need to get this breakthrough drug design engine that we've been building, uh, we need like half a dozen alpha folds. Wow. Alpha fold is just part of the story. Wow. So, from day one, we've been setting up these internal research programs going after these halfozen um problems. Uh we've had significant breakthroughs obviously in alpha fold and structure prediction but also in other key areas. Um and in all of these these models are general they can be applied to any target. They can be apply and and then what we're finding actually they can be applied to any modality or lots of different modalities at least. Yeah. So that's the first time I've I've heard you say half a dozen alpha folds. Can you share a little bit more about what that means? Yeah. So you know Alpha 4 was obviously a massive breakthrough in in understanding uh biomolelecular structure. So how what what is the structure of proteins and now with alpha 3 structure proteins with small molecules and things like DNA and RNA. Um that's a fundamental step change. It allows us to get experimental level accuracy of a really core concept of biochemistry. Yeah. That unlocks a whole bunch of thinking and design work for chemists. Yeah. But you know, my comment here is actually we're probably going to need something like half a dozen more of these sort of breakthroughs. This sort of getting to experimental level accuracy of different core concepts of biology and chemistry. Yeah. To be able to put this together into um something that's really transformative for drug design. Drug design is really really hard. Yeah. It's not just a single problem. It's not just about understanding the structure of a protein. Yes. Um it's not even just about designing a molecule that will modulate that protein in the way that you want. Yeah. You want this molecule to be able to, you know, ideally be taken as a pill and go through the body and be absorbed in the right way and reach the right, you know, cell type and actually, you know, go into the cell and and and and not be broken down by the liver in a certain way. So there's just so much complexity. Yeah. To hold on to as a drug designer. Yeah. Um, and each one of those is like, you know, an AlphaFold level style breakthrough that that we've been creating. So interesting. Um, well, I've also heard you use the words a holy grail model for drug design and agents for science. Can you explain a little bit more about what you mean? Yes. So, um, some of these research areas that we've been going after, um, you know, predicting structure and properties of these molecules and how all of these like biomolelecules interact and play out over time. Um, these really are sort of holy grail predictive problems for drug design. Um, and we've made some incredible breakthroughs there which have, you know, really stunned our chemists and and and stepch changed how we do drug design internally at ISO. Um but what's I think a really interesting thing to think about is that you could be you could create the best possible predictive model of the world like an experimental level even better than experimental level model to predict a particular property about a molecule for example to be able to predict the outcome of a real experiment. So we could have a whole suite of those but that still wouldn't solve drug design. Um and and the way to think about this is is you know there's this number 10 the^ of 60 which is perhaps all of the possible drug-like molecules that you could that could exist. Um you know that that's maybe that's maybe that's maybe you know a bit you know takes into account a lot of things. So we could even reduce that by 20 orders of magnitude. Yeah. Get to 10 to the 40. Yeah. That's still a lot of things. Yeah. And um even if you had the best predictive models in the world, so let's say you could screen a billion different molecules, you could go and test a billion different molecules, that's 10 to the nine. So you know, now we're still like 10^ the 31 molecules left on the table. Yeah. So even with the best predictive models, you're still not even scratching the surface of molecular space that you should be exploring. And this is why we need to go beyond just predictive models of experiment but also models like generative models like agents that can actually navigate that whole 10 to the 40 10 to the 60 space. That's so interesting. Using our predictive models obviously to understand how to navigate that but so we don't have to exhaustively search because we can never exhaustively search the whole universe of molecules if that makes sense. Just in the same way that Alph Go couldn't exhaustively search all of the possible Go moves, right? Unlike chess where you could exhaustively search all possible chess moves. Yeah. Yeah. But yeah, molecule design is much more like go than it is like chess. Um, so that's where generative models come into play, agents that utilize generative models that utilize search techniques as well as these amazing predictive capabilities to really open up the entirety of of of molecular space. Now to me it's actually still amazing that even without AI we managed to find drugs in this 10 to 60 space 10 to the 40 space. Yeah. Um it just says actually there's there's probably a lot of redundancy. There's a lot a lot of potential designs. Yeah. You know if you think about a particular disease indication a particular target um there should there should be many designs that exist that would be good for that and and would be the right sort of product profile for this therapeutic. Um, and I think the real potential here is is for these generative models, these agents as well to be able to search through this space and really uncover that whole potential design space. That's so interesting. I I think in very simplistic layman terms, you're both modeling, learning and modeling the game and trying to build the best player to solve different types of games. Yeah. So, I mean, you know, I'm I'm incredibly biased by by by by games. I've been playing video games since I was a kid. Grew grew up in that world. Um, but, you know, that's exactly how I think about it. We've got to be creating our world models, our models of the biochemical world, our biological world. And then it we don't stop there. We actually then need to be creating agents and generative models that can work out how to explore. Yeah. Yeah. How how to traverse that and to basically uncover these, you know, amazing needles in the hay stack of in chemical space which could be, you know, life-changing therapeutics for so many millions of people. I love that. That is our punchline today. So, Alphold 3 is truly groundbreaking. Um, you've taken us from being able to model just the structure of a protein to now being able to model the structure of all molecules and their interactions with each other. Um can you share a little bit about how we should think about that in terms of the impact in accuracy in speed and efficiency and also potentially in being able to explore problem spaces that we couldn't solve before this? Yeah. So yeah, Alpha Fold 2 was a you know the biggest breakthrough right to be able to understand the structure of proteins. And then there was something called alfold to multimmer which then allows you to understand not just the structure of proteins by themselves each individual protein but the structure of proteins as they come together and what we call complexes. So how these proteins fit together that opens up you know and helps us answer a lot of questions in biology. But there's still a big hop to designing therapeutics. And one of one of the big classes of therapeutics is what's called small molecules. So these are molecules that are not proteins. These would be things like caffeine um or paracetamol things that of more often you can take as a pill. Um and the way that these therapeutics work these these small molecules is that they go through the body, they go into the cell and they actually come and attach themselves to these proteins. Uh you know these proteins they're the fundamental building blocks of life. They form these molecular machines by interacting with other proteins. Yeah. And so you can you can imagine that if you have another molecule, your your drug that comes in and attaches itself to a protein over here, then it might disrupt the ability for that protein to interact with another protein, part of its normal machine in day-to-day life. And so you're modulating the function of that protein. Yeah. With this small molecule. Yeah. And that's the essence of of of of drug design and how therapeutics work. And so you can imagine as a chemist your your job or a drug designer, you're trying to design a small molecule that's going to fit to this protein over here and disrupt how it normally functions or in some cases enhance how it normally functions. And so it'd be really helpful to understand how this small molecule interacts with the protein. Yeah. What's the structure that it might make? What are the interactions? These literally physical interactions that are being made. Mhm. Um and so that really inspired, you know, the creation of AlphaFold 3 where now we have a model that not only predicts the structure of proteins, but how these proteins interact with small molecules, also other fundamental molecular machine building blocks, things like DNA and RNA. um and this basically opens up the ability to structurally understand which is a core part of drug design. Um small molecules, it opens up new classes of targets uh you know there are things like transcription factors which are proteins that sit on DNA um and and read DNA. And you can imagine now trying to design a small molecule to change or disrupt the function of of something like that. And to so to do that, you'd really want to be able to see literally in 3D how this all looks. Yeah. And if I make changes to my little molecule. Yeah. Um how will that change the way it interacts with this protein in this biomolelecular system? So Alphold 3 is now very very accurate. Um allows us to answer a lot of these questions purely in silicone. Yeah. Purely on a computer where before you would have to go to the lab, literally crystallize this stuff. This can take six months. It can take years. Sometimes it's even impossible. Uh now at ISO, our drug designers are, you know, literally sitting, you know, with their laptop browser based interface being able to understand, make changes to their designs um and see the impact of that. Incredible. So there are a couple interactions that um Alphold 3 is focused on um proteins and nucleic acids, proteins and ligans, and antibbody to antigen. Can you give us some good examples of of um the impact that alth 3 now has on the interaction of these different types of proteins and molecules. Yeah. So um protein and lians that's the same as protein and small molecules. Those two terms lians and small molecules are synonymous. That allows us to understand how small molecule drugs interact. Then we can think about um you know protein protein interactions. Um there's a whole class of therapeutics called biologics. These these are things like antibodies. Um that allows us to understand how they might interact with our targets. Opens up new modalities. Um and and that also encapsulates the the the sort of pro the the antibbody antigen interface. So if you're designing an antibbody uh you want to understand how your antibbody design is going to interact with the protein surface. Yeah. There. So it's the same model that we can use across all of these different uh applications. What are the nuances of training a model like Alphold 3 and what are the benefits of using a diffusionbased architecture? Yeah, it's a great question. Um there are a lot of challenges we had to overcome to get Alphaold 3 to work. uh one one of the most interesting things was actually just how do we uh take uh something like AlphaFold which was only working with proteins and then input these new modalities these new data types of RNA DNA small molecules so we had to work out how to tokenize not just proteins which we kind of knew how to do but how to tokenize then DNA how to tokenize small molecules for things like DNA and RNA that's a little bit more obvious um we could tokenize in the bases but But then for small molecules, we would really go to um we tried a whole bunch of different stuff really ended up that this atomic resolution tokenization worked super well. And then you have the question of okay, how do you actually um predict the structure of this mixture of different molecule types? Yeah. Uh you couldn't use the same framework as Alpha 2. And uh this is where diffusion modeling just really shun. Um here we could actually model every single individual atom and the coordinates of every atom individually um and have a diffusion model be producing those 3D coordinates. Uh and the tokenization that we talked about is conditioning the uh inference of that diffusion process. So interesting and um this this was a a huge breakthrough. So, you know, we're talking about on our leaderboard just a massive step change, particularly in small molecule protein um uh interaction accuracy. It was a massive step change and something that really unblocked the rest of the project. Wow. Um so, data, compute, and algorithms, we know those three are important in all other adjacent fields. Um but I was surprised to read an interview with Demis where he shared that we're not data constrained in biology. Um can you share your point of view on that? You know I think it doesn't matter what field of machine learning you're in you're going to feel some data constraints and I think the point here from Demis is that it's it's not a real bottleneck as in we can make progress with the data that is out there that the data we can generate and real progress can be made. Um it's not, you know, oh, we've got to sit and wait 50 years for like the world to generate data before we can actually make impact here. No, we're not seeing that at all. There are modeling spaces that um where the data has been sitting around for years. Yeah. That we that we can see that we can make, you know, really substantial progress beyond anything that people have experienced before. Yeah. Now, does that mean there's no opportunity for data in biology? Absolutely not. like this it's going to be a fundamental part of of how we um you know how we continue to develop these these models and these systems will be what data we go out and generate um and there I think there's there's just a massive opportunity in my mind um the data you know the data for machine learning in biology hasn't actually been created yet yes there's a lot of historical data but there's a huge but that historical data hasn't been created for the purposes of machine learning. And so when you're going out and thinking how do I create data to actually train my model, you're thinking in a very different way to how people have gone out and generated data in the past. And that there's a big opportunity there to explore. What kind of data do you think we're missing here right now? And do we think do you think that we need anything in synthetic data? Yes. So um I'm a massive fan of synthetic data. Actually, I have been for since the very beginning of my career where, you know, we were I I was generating synthetic text data just to overcome the fact that, you know, I was a PhD student with access to a couple of thousand images and Google had millions and millions of images and so instead I just generated tons and tons of synthetic data and and that unblocked things. And you know, we're seeing the same thing in the especially the chemistry space. Yeah. Where we have good theory. We we actually you know know a lot about physics. We know um you know we we have the theory of of quantum chemistry and quantum mechanics and we can create simulators out of that. Uh we can approximate that and create more scalable molecular dynamic simulations. The this gives the basis for you know a whole host of synthetic data. Then we have the models themselves that you know especially we have generative models. this can actually generate data that you know we can use scoring systems to help um uh you know really enhance the information content of this data. Yeah. But I think one of the the big open spaces will be on um what's called invivo data. So data that you would normally measure on a on a real animal something like a mouse or a rat. We you know that you there's some historical data on that but you can't generate tons of you can't really generate any at all. Yeah. Right. So then there's a big opportunity to look to new data generating technologies. There are some incredible people doing things like organoids on a chip. Um so ways of starting to measure things that you would normally measure on a on a real animal but you know completely um on a on a chip. So you know I think so interesting. Yeah. There's going to be a whole host of like new breakthroughs in data generating technology in biology and chemistry. that's that's going to, you know, have big impact on how we think about modeling that world as well. Are you working on any of that internally or are you hoping that other players uh fill in some of that gap? So, in internally, we we actually don't have any of our own labs um in isomeorphics. Yeah. Yeah. In isomeorphic labs, but we um we work with a whole bunch of other companies. Yeah. Um you know, we we we generate a lot of data ourselves, a lot of proprietary data. Um we've seen amazing impact of that. It makes a lot of sense. Um so there's a point of view that modeling structure of molecules and modeling their function and the modulation of function is very important but not necessarily always the limiting factor in drug development. What's your point of view on that? Yeah, as I touched upon before, drug design is really, really complex. And as before, you even get to drug development, which is where you take those designs and you start putting them into real people, clinical trials. There are so many bottlenecks throughout this whole design and development space. Um, drug development is, you know, how do how do we start to approach clinical trials? How should we test these drugs out in people? How can we do this in a really timely manner but still a really safe manner? Um there's a lot of bottlenecks there that I think the industry as a whole. We will need to work out how to innovate in that space especially as our predictive models of how these molecules will interact with people, how toxic they will be. As these predictive models get better and better, we will have to change the way that we approach clinical trials to really make use of that. Yeah. ultimately to get you know therapeutics into the hands of patients who really desperately need them. Yeah. You know even in even in the design um of of molecules themselves as we talked about before it's not just understanding the structure of these molecules. It's not even just understanding how these molecules change the function of these proteins. But we need to understand how these molecules change the function of pretty much every single protein in our body. Right? Because if we take this as a pill, it's going to go everywhere. And that's the major to major cause of toxicity is when yes, you you've designed this amazing molecule that like perfectly modulates your specific target that you know is key to your disease, but also affects other things, but it also affects other things. Um, now of course you do a lot of screening to protect against that, but um the more we can predict that the better. What's really exciting from my perspective is if we're creating these general models that understand how this molecule interacts with this target but also any other target then why can't we just use that same model to understand how these molecules interact with the rest of our body right so interesting so what is now possible with Alphold 3 for drug designers how are you using it internally so Alphafold 3 gives our drug designers the ability to understand how their molecule designs really interact with this protein target and this is the target of disease. And so our drug designers can make changes to the design and then see instantly how that changes the way that this molecule physically interacts with the protein target. And that's really really powerful. Before Alphold 3, you would be completely blind to this. You wouldn't actually probably know how your molecule is interacting with your protein. you'd be using your best intuition. Maybe somewhere down the line in the in the drug design project, you would get uh your structure crystallized with a particular design. That means going out to a real lab six months later, if you're lucky, getting a resolved 3D structure, but even then, that's just the 3D structure of a single design, not every single change that you make. Yeah. So, Alpha 3 completely changes the way chemists can do this design work. But I would stress that's that that's that's nowhere near as far as we want to go because it's not just about what these molecules look like in terms of interacting. We actually want to know how strongly these molecules interact with this protein. Yeah, we want to know other properties of these molecules. We want to understand how the way that these molecules interact with this protein and how that changes the fold or the confirmation of the protein, how that changes the function of the protein, how it might actually change the dynamics of the cell. There are so many questions and these are these other alpha fold like breakthroughs that we're working on um that also go you know we have created incredible models for that our chemists are using in this design process. Interesting. So you're designing some drugs internally. what targets and programs are you focused on? So, we have a really exciting uh internal program of of drug design projects. These are focused on immunology and oncology. Uh we've been making some incredible progress there and it's been really exciting to see especially how these models have transformed the way that we're actually approaching drug design on these these programs. You're also working with um Eli Liy and Novartis and recently you announced an expansion with Novartis' partnership. Can you share a little bit about what these partnerships look like? Yes, so we we signed these initial partnerships um two partnerships, one with Eli Liy, one with Novartis that that was fantastic. They brought some really really challenging problems to us. I think it's no secret that um you know the sort of targets um that for example Novatist brought to us these are these are sort of targets that you know the field and Novatist for example have been working on for you know 10 years plus. Wow. Um so these aren't sort of oh we we'll try things out problems. These are for real you know hard things. Um, last year was an amazing year both both for our internal projects but also for these partner projects to really see how well these models are working. It's allowed us to really you know uncover new chemical matter. Um, working out new ways to modulate these these targets that people have worked on for a long time. Um, you know, it's been amazing to see uh this new deal which has expanded on the artist collaboration which I think is a real testament to um some of the success of the early days of these partnerships. Congratulations. I think it's an incredible milestone especially just one year in. Yeah. So I'd love to talk a little bit about the team. Um, you've built a truly excellent team composed of the highest caliber talent across many different fields, AI, chemistry, biology, and you've also brought in outsiders into the field to help question traditional thinking. Can you share a little bit about how you thought about this? Yeah, so the space of AI for drug design hasn't really existed for very long. Um, so the chances of finding a world expert at drug design who's also a world expert and machine learning or deep learning is basically zero. Zero. Yeah. Just because these fields these fields haven't coexisted um for long enough. I I genuinely think about a new um sort of field of science that ISO is breeding because we are you know we have these people who really live and breathe the intersection of this. Yeah. Um so you know because but because we can't hire these people you know I really think about how do we bring the world experts at drug design and medicinal chemistry and the world experts at machine learning and deep learning and get these incredible people sitting side by side um because it's not just enough to have these amazing people sitting in their isolated teams. Yeah. We need people sitting side by side speaking each other's languages. Yeah. With a lot of empathy, a lot of curiosity, uh curiosity to understand this new science to really build intuitions in your own language. And we've seen just such amazing things come out of this dynamic where you you really have, you know, a generalist machine learner who doesn't know anything about chemistry or biology. Yeah. start to come in and understand the problems of a medicinal chemist and a drug designer. And um when I think about even hiring machine learners and machine learning scientists and engineers for the research that we're doing, I'd say you know 60 70 80% of the people on our team have no prior knowledge of chemistry or biology. Maybe you know high school or uh university level. And um that can actually be a real asset. Yeah. Because you come in sort of a little bit naive. Yeah. And as long as you're curious, I think one of the key things is asking, you know, the curious questions, asking this like stupid questions and then and then that allows us to come at the problems from first principles. Yeah. It almost allows us to break through the the the the dogma of um you know previous experience and how people traditionally approach these problems. We can think ground up from scratch. Um and that's a lot of the mentality of how we think about creating these research breakthroughs. A little naive and highly curious and high agency is a very good thing. Yes. Exactly. Exactly. So in November last year, you also made a very big move in launching the Alphafold server um which releases code and model weights for academic use. Can you share a little bit about why? Yes. So I mean AlphaFold has a long long lineage of um being open for this you know academic and scientific use and uh it was it was really important with this latest breakthrough of Alphaold 3 that we make sure that this scientific community has access to this functionality because you know yes Alphaold 3 is going to be incredibly useful for drug design. already is but it's also useful for you know many other areas of fundamental biology and just understanding biology and people are using these you people are using our 43 server and model it in very very creative ways um so you know it's very important for us to make sure that there is that sort of free use for non-commercial academic work um and it's been incredible to see the the the the take up of that and the use of the server I'd love to talk a little bit about future. Can you give us a tease of what else is to come with AlphaFold? You know, in in terms of, you know, structure prediction as a problem. Uh I, you know, in in my mind, I I want to completely solve this. Uh I think Alfold 3 is a fantastic step on the way of that this significant breakthrough. Um but you know that it's not 100% accuracy. Yeah. Um what does even 100% accuracy mean in this space? Like with a lot of areas of science as as you start to push the boundaries you you you see that the problem opens up into even more problems. That's that you know that that's the addictive part of doing science right um and I think that you know Alpha 3 is a good example of that where as you start to get these capabilities you see that actually there even more deeper problems that we want to be working on. um and and stepping towards. So yes, understanding structure better and better and more accurately is is is always going to be interesting for us, but then it's also, you know, not just necessarily about static structure. So Alpha 3, it models these crystal structures um which almost static crystallized versions of these these these molecules, how these molecules interact. But in reality, we don't have crystals inside of us. We you know, these molecules are in solution. They're moving about. They're dynamic. So you can think, okay, well maybe understanding the dynamics of of these systems is actually also going to be really interesting. Yeah. So yeah, what does a GPD3 moment look like in AI biology and when do we get there? So if I think about GPT3, this is really a generative model. So something that's generating text and the GPT3 moment for me was was you know crossing over that boundary between yeah we've got generative models of text and they generate some stuff and it looks like text but I'm not convinced that it's generated by by a human. Yeah. And GPT3 started to be that first point where you're like oh this is like this kind of looks like a human. And so your this generative model is actually recreating the distribution of data that it's trained on. And what is a generative model? Generative model is something that fits the manifold of data that is trained on it. So when I think about this applied to biology, um you can think about these generative models actually starting to recreate at that GPT3 moment, recreate what things would actually look like in reality. And that's quite exciting because that means that these models are spitting out things that either they actually exist in the world. Yeah. And we can kind of validate that or maybe even discover new things that exist in the world or they could exist in the world. Yeah. Which means that they could be things that we could design or manufacture or create that would actually be stable and work and exist in our physical reality. Yeah. And I think the the cool thing about um this in biology is that unlike with language where with language when it generates something at human level quality we can understand that because it is human derived. But a lot of problems in chemistry and biology we even struggle to understand ourselves. And so when we get to that GPT3 moment I think it will look a lot less like GPT3 but much more feel a lot more like move 37 in AlphaGo. M interesting where we're starting to see things that are beyond human understanding but that do exist in the real world that exist in our physical reality um but are beyond sort of human comprehension right and that's just going to be mind-blowing. In fact, you know, we're starting to see that internally with our generative models that we're creating designs that a human drug designer would say, I'm not so sure about that. I much prefer this. And then you test it out in physical reality and the generative model is correct and the human is wrong. That's fascinating. I love the move 37 analogy when the model starts to see elements of creativity and and past the human. Exly. Move 37 was this um amazing move during the Alph Go games against Lisa Doll. Uh it was, you know, the 37th move of the game and it stunned the world. Stunned the Go world because it was uninterpretable by a human. Yeah. It looked like a mistake. No one had ever played this move in the entirety of, you know, thousands of years of human history playing Go. And it turned out as you unrolled the game that this was the critical move that allowed AlphaGo to beat Lisa Dol in that match. Yeah. And we're going to see so much of that sort of behavior coming out of these models, especially when we're applying them to things outside of of native human understanding like chemistry and biology. Yeah, I love that. Also our punch line today. So when will we see our first AI generated drug in clinic and also in phase one, two and three trials? So we're making amazing progress um on on our drug design programs. And you know the thing I think about actually is as we start to get a whole bunch of these um uh AI designed assets, these molecules get into clinical phase, how can we actually start to think about engaging in that clinical development um to you know get these molecules to people you know as as as as fast as and as safely as possible because there's so much unmet medical need. Um, so yeah, here I think about, you know, what are going to be new ways to engage with regulatory bodies? What are going to be new ways to incorporate our predictive models for not only how this molecule works for the disease, but how, as we talked about, how it interacts with the rest of the body, you know, the types of toxicity it may induce. I think there'll be a lot of opportunities to think about just uh streamlining and speeding up this process. Yeah. Maybe even completely changing the way we think about human clinical trials as we you know our AI models become so we can design these molecules so much quicker in a much more targeted manner with so much more knowledge about how they work. Yeah. So that that will change the game. But I think we've got a long way to go as an industry to really work out how that changes. Yeah. Last question. As isomorphic succeeds and potentially as a whole field succeeds, what happens to the traditional world of pharma? I think they become, you know, in some sense pharma will be using AI. I think I think there's there's no world where in five years time uh you will be designing a drug without AI. Yeah. Like that is an inevitability. It'll be like um you know trying to do science without using maths. AI will be this fundamental tool for biology and chemistry. It already is. Yeah. At least in in isomorphics world um that everyone will be using. So it's not going to be oh is it farmer or is it AI that it's going to be one and the same in the sense that the whole industry will will adapt to that. Yeah. Amazing. Max, thank you so much for joining us today. This was a fascinating conversation. Yeah, it's been a pleasure. Thank you. [Music] [Music]

========================================

--- Video 12 ---
Video ID: NLu6NiVj-2E
URL: https://www.youtube.com/watch?v=NLu6NiVj-2E
Title: Vibe coding an idea: the art of the possible w/ Paid CEO Manny Medina #ai #podcast #tech
Published: 2025-04-23 14:00:37 UTC
Description:
In this episode of Training Data Paid CEO Manny Medina discusses transforming intangible ideas into executable actions through the power of AI, why companies building AI agents need to rethink their approach to both technical architecture and pricing strategies, why complex workflows amplify hallucination risks, the importance of moving beyond activity-based pricing, and more.

Transcript Language: English (auto-generated)
Everyone is like innovating and wondering like living the art of the possible with like models improving every seven months like you have a new toy to play with like every seven months. I think the ability so this is understated the ability to vibe code an idea to just at least show how it works and then actually build it has made the the iterations in the conversations so much faster. like you know you can just like say like we think we should build this or I have you know have a lot of energy around this thing and you can just write it and you can see it work and you can tweak it a little bit and then present it in front of everybody and sell it like you're selling internally to another to another person. So this ability for us to be brass stacks in terms of like where we're building for whom we're building being so small and getting so much work done in a small team and being so close to our customer because we're early it's just fun. I think I think that's the best word to describe it.

========================================

--- Video 13 ---
Video ID: H1YzF75CcNI
URL: https://www.youtube.com/watch?v=H1YzF75CcNI
Title: Optimizing AI Workflow and Token Economics w/ Paid founder Manny Medina - Training Data  #ai  #tech
Published: 2025-04-22 16:42:26 UTC
Description:
Manny Medina, founder of Paid, discusses why the cost of tokens in AI models is unlikely to decrease in the near future, despite popular belief. 

Manny talks with Sequoia partners Pat Grady and Lauren Reeder about the importance of deeper level thinking in AI and the limitations of current workflow-based models and emphasizes the necessity for models to handle most of the work to reduce errors and ensure greater accuracy.

Transcript Language: English (auto-generated)
like everybody's telling me that the cost of tokens is going to go down and this is going to become a commodity and whatever whatever um I think in a world of reasoning where is inference time compute worth more than training time compute I just don't see how the at least in the immediate future I just don't see how the this the token price goes down because you're going to require like deeper level thinking so what the problem with agents right now is that you you sort of like workflow them right so you you know you you go to you know land graph and you you put your boxes is and what what what the does and like you string them together. But the fundamental problem with that is if you have a hallucination at the very beginning of that chain, you're screwed. Like you have all sorts of like you know bad activity happening all through and like that there's no amount of evil that is going to like rescue you out of that. So like the better way to do it is to have the the the the model do most of the work. You see what I mean? As opposed to have like these little boxes of work. Have the model do most of the work. a good you know evil um framework to make sure that you know that it's doing the thing that it's supposed to be doing and then enroll it out in that world where you have less error rate the the the I don't see that the the cost per token going down I see that if nothing else is going to go up as the models get more advanced and like smarter

========================================

--- Video 14 ---
Video ID: ESa1TdS3dI4
URL: https://www.youtube.com/watch?v=ESa1TdS3dI4
Title: Pricing in the AI Era: From Inputs to Outcomes, with Paid CEO Manny Medina
Published: 2025-04-22 09:00:20 UTC
Description:
Former Outreach CEO Manny Medina discusses his new company Paid, which provides billing, pricing and margin management tools for AI companies. He explains why traditional SaaS pricing models don’t work for AI businesses, and breaks down emerging approaches like outcome-based and agent-based pricing. Manny shares why he believes focused AI applications targeting specific workflows will win over broad platforms, while emphasizing that AI companies need better tools to understand their unit economics and capture more value.

Hosted by Pat Grady and Lauren Reeder, Sequoia Capital

00:00 - Introduction 
01:45 - What’s working at AI app companies?
06:48 - Which markets will transform sooner?
10:47 - You can vibe code a ServiceNow
12:48 - What’s working with pricing and packaging?
17:37 - Pitfalls of charging-as-a-tool
19:05 - The pricing maturity curve
24:32 - The cost side of the equation
29:34 - The mission behind Paid
32:18 - What is Paid?
37:49 - Fun is the culture of Paid
39:44 - Lightning round

Transcript Language: English (auto-generated)
your customer will always default to some like the easiest way to buy which is either you know some kind of fixed price or a consumption price for the first year to see if it works but if it does work it is up to the AI agent builder and creator to go back to the same customer and say let's align on things that are important to you and charge for it [Music] Greetings. Today we're joined by Manny Medina, founder of Paid. Paid helps Agentic AI companies make money by dialing in their pricing and costs. Manny experienced the problem he's now solving with his previous venture, Outreach, where he first encountered the challenges of pricing and margin management. challenges that are becoming top of mind as Agenta companies move from experimentation into production. Paid provides the infrastructure that helps AI companies transition from simple activity- based pricing to more sophisticated value-based approaches, allowing them to capture their fair share of the value they create for their customers. In this episode, we'll explore Manny's contrarian views on AI pricing models, his framework for the four pricing approaches that are working today, why he believes specialized AI agents targeting narrow problems are printing money, and what it takes to build a successful business in the age of AI agents. We hope you enjoy. All right, we're here in London for a very special episode of Training Data with paid founder Manny Medina. Manny, welcome to the show. Great. Thank you for having me. This is awesome. All right, we're gonna start off with a high level question. Okay. In the world of AI apps, Yep. these are your customers. So, I think you have a pretty good sense of what's working and what's not working. Yep. What's working? So, I think that we are right now in if you were to take the analogy of hedgehog versus fox, we're in hedgehog land. I love I love where this analogy is going. So like if you take up a very narrow problem and then you hedgehog into it and you become the best at that one thing that is printing money right now like everyone that I'm seeing. What's a good example? Um a good example for instance I love what Quandre is doing. I love what Expo is doing in your portfolio. I love what um Happy Robot is doing. Um there are very narrow cases of problems that have a lot of people in it. Mhm. that don't have a clear solution for software. Like there's no software to solve that problem. Um it's peopleheavy and and it's interesting because they're not replacing people right now, but they're replacing BPOS. So whatever you see BPOS having a big role that is food for the apex predator that we're seeing right now in the world of AI, which is AI agents. And for the uninitiated, what is Quandry? What is Expo? What is Happy Robot? So, Quandre does policy renewals. Okay. Uh, another one that is adjacent to it is a company called Owl that does um they they they're in the insurance space too, but they when they but they review claims, they review claims data. Again, super tiny like you look at it and you pass because you think that the world is really small, but when you look at all the amounts of claims that people do and the ones that get left over, you know, they're taking care of those. Um, Happy Robot is is doing is calling truckers on behalf of brokers. Hm. So if you want to send beer from Milwaukee to Boston and you call you call a broker to find your trucker, Happy Robot spins up 2,000 agents. Agents call the truckers, which is, you know, a guy and a dog and they negotiate with each other until they book the load and then they call the trucker all the way all the way to delivery. And of course, once that Milwaukee beer gets to Boston, the Bostononians are going to send it right back. Exactly. Gota throw it over. Exactly. But somebody has to move the beer around. So like so this this is a thing. Um and and Expo of course does uh pentesting and you know again they took out the the pentesting agencies. They they rank them. They look like the best and then they run it for you. So these are in my mind very narrow problems. So like Expo doesn't come and say I'm going to be uh a full cyber security solution which eventually they will be but that you're saying like I'm just going to continuously bang against the door of every single app and back and back end that you have and I'm just going to u figure out a way to penetrate it and that's going to happen you know with the highest quality possible um and it's going to happen continuously which is unavailable right now and is the converse also true the things that are not working or things that are too broad in scope at the moment I there is a lot of traction in things that are broad in scope in terms of like companies coming out and they're doing okay. They're quite well like Harvey is doing quite well. Um and they're they're not so broad in scope like they're narrow into like the legal system. Um, but I'm excited for, you know, what Crosby is going to do for instance. You know, they are they're tackling the problem in a in a in a in the the similar space but with a different approach just like full out replacing you know the the the lawyer you know for commercial for commercial contracts. Um, but if you go too broad like for instance AISDRs is a little too broad and that encompasses a lot of things and means a lot of different things for a lot of different people. There's there's going to be swirl. So I don't I don't I don't like to characterize it into like, you know, what's working, what's not working. I characterize into what's working now and what's not working yet. Okay? You see what I mean? Like what's going to what is going to be refined and eventually come through. Like for instance, the one that I that I'm waiting for with baited breath is AI assistants uh for like EAS like who's going to be my AIA. And you know, I think there's Lindy and there is Fixer out there, but they're not quite quite quite there yet. Yeah. You see what I like they're not they're not um for instance I I love I I love fixer but I have three time zones that I work through like I you know my my co-founders in India sometimes and like most of our customers are in Pacific zone I'm here um and like getting that to work out just right doesn't quite work for me but if I had like one time zone and I had like one line of business like a like if I'm a real estate broker they are perfect like perfect fit for that particular thing so like big problem narrow application nailed it. We have a on the EA front, we have a founder to introduce you to. He's he's in stealth, so I can't say his name, but okay. It's coming. Perfect. All right. Love to see it because I This is one of those problems that I'm just going to wait. I'm not like I you know, I had an EA forever and I'm just I'm going to bite the bullet and just gonna wait for AI to catch up. So, I'm I'm ready guys. Send me business. Do you have a sense for which markets are likely to be transformed sooner rather than later and which markets are going to be more resistant to AI transformation? Um, so I have this I heard this thesis when I came up with the idea um for for paid. It was right around when Strawberry dropped and you guys wrote that paper on on the the uh the shift to um to cognition actto one. I I remember where I was. Like it's one of those things that like I I read the paper and then I I listened to somebody reading the paper and like I remember where I was like there's so many episodes in your life you remember where you were when that happened. Like I remember exactly where I was when that happened. It just hit me. Um and I I was I was listening to I think it was u from Kleiner who had this hypothesis that I actually fully disagree with. So fully disagree. Yeah. So his hypothesis is that AI is going to start with the highest paying jobs. Okay? Because that's where the money is, right? So it makes logical sense, right? You go and target the the most expensive job and you displace those, you know, developers, lawyers, accountants, doctors. I actually disagree with that hypothesis. I think AI is the highest paid people will buy AI as a side thing and ditch it with the same regularity that they ditch other things in their lives. I think AI is going to stick the landing where it actually takes over our role fully that nobody else wants to do. So for instance, nobody wakes up and wants to be an insurance actuary or an insurance adjuster. Our partner Ruf Boto would beg to differ. Yeah, perhaps perhaps the only licensed actuary in the world of venture capital. All right. So So I'm sure there's an exception to that, but no, like these are jobs that are hard to that are hard to backfill. Yes. So, you know, when Quandry does is um this policy renewals people like they're just like they're replacing people that are exiting the business and they're not filling it back on, right? Um the same thing was like nobody wants to wake up and like work for a BO banging out the phone. Like they do that job for like six months on on the way to a different job. So the turnover on these jobs are really high. So what I'm observing from my end is that the companies that are doing really well are addressing pools of labor that are either disappearing because of retirement or they want to do something else. Yes. Or they're B or they're run by BPOS. Yes. And in that segment I'm seeing a lot of stickiness. I'm seeing a lot of expansions. I'm seeing a lot of growth and really good economics. Yes. Like I can charge whatever I want. You know my upper bound of my price is the labor cost. My lower bound on my price is my margin. and and people are going to town on that. So like in terms of like people experimenting with like outcome based etc that's where I see the majority of my of my traction in the broader applica the reason I disagree with like you know the rich job replacement is because everyone is going to go there. You see what I mean if OpenAI needs another source of revenue or Trump needs another source of revenue they're going to go after lawyers and accountants. Why? Because it's hard and they pay a lot and they buy everything. So like I feel like that market is going to be hot for now and then it's going to be super competitive and then you're going to have a lot of people in it. See, I I wonder if you can have your cake and eat it too. I wonder I wonder if both you and Mimoon are correct in the sense that for the higher paying more creative jobs. Y I think the co-pilot approach and I don't mean Microsoft co-pilot. I mean the the approach of AI that has given people superpowers seems to really be working. You know Harvey and legal or open evidence and medicine. Whereas for the jobs that are lower paying and a little bit less creative, the full autopilot approach seems to be working where you're fully replacing the work with an autonomous agent that can do it better, faster, cheaper. And so I wonder if it's both. And then as far as modes for the co-pilots, I think you're right that because those are big categories where there seems to be a lot of money, there's going to be a lot of competition. I think what we're also seeing is collaborative workflows, which are kind of the eighth wonder of the world as it as it comes to software, right? You know, collaborative workflow flows still work, right? Right. You you tend to get pretty deeply embedded into your customer and then once you're in there, you're providing a ton of value and they don't they don't really want to switch, right? No, it's so it's true, but I I think the world has changed a little bit in that, you know, I don't know that you're going to see Asanas anymore or or service nows or like any of those like you know they work in the collaborative world workflow when it didn't exist and they were you know revolutionary. I think now anyone can spin up, you know, you can vibe code a service now. And this is a good topic. So vibe code, are you a believer or not? Where are you on vibe coding? I think I we're dude, we spun up paid in like a month and a half. You are a vibe coder. And half of it was vi coding. Yeah. I mean, no, we have to throw it away. But this is the beauty of V coding. You can throw it away and respin it. Yeah. You know what I mean? Like there's no like there's a lot of people getting hung up on like the whole debugging thing. You don't debug VIP code. just throw it away and you start and you put it in production and when it breaks you start, you know, it's wonderful. You can always start. This is very comforting to your customers. Yes. Well, I don't have a lot yet. So, so I don't have a big problem, but eventually it will be a big problem and to start is is is a very easy thing. So, but back to the the workflow question, I think that you're right. So, if you become a definitive workflow for X, Y Z, like it's definitely a sticky point. I just think that the the the the speed of competition is really high right now. Yes. And co-pilots have the the explanability of the value of a co-pilot is really hard to land. Like how do I how do I differentiate one co-pilot versus the next versus the next versus the next and they're all can come and say I'm I'm the same as X but better or cheaper. And that just that's a recipe for swirl. So unless they specialize in verticals and say like if you are um you know if Harvey were to say like for I don't know um patent law I am the best and I got 78% of the market. Boom. Right. But now you're moving into the very narrow application with very narrow set in a very rich market is rich because you own the whole of it not because the whole market is rich. You see what I'm saying? So that's that's my but look we're all hypothesizing here like I make money either way. So, so I'm happy for all of it to work out. So, you're trying to jump in to help with this on pricing and packaging. What are you seeing work today? So, I'm seeing four things stick in the landing with a plum with Gusto. One is, you know, clearly charging by activity. You know, that's an easy one is, you know, has a credit consumption type model and you can you can show the activity that was done fairly easily. The other one that I'm seeing more is changing by workflow. When you string a number of activities and you say this workflow is cost this much, like a document review, right? Because then you can separate documents that are small from documents that are long and complicated because they have different consumption patterns and it feels like you're getting closer to value based pricing as opposed to costbased pricing. Exactly. Exactly. So like the moving to a workflow allows you to move out of the the thread mill of charging for, you know, for pure work to charging for actually that is worth something. somebody and then eventually you'll get to some kind of outcome. Yeah. And what I I've been recommending to my customer, it's not out there yet, but I'm going to give it a push, is to charge instead of charging per outcome, get an outcome bonus. Meaning, if an outcome happens of a particular quality that is measurable, charge for it. That way it it be it opens the door to a conversation of value alignment. And the moment you open another door conversation followment and then you start getting into more bespoke contracts with each of your customers which are super hard to rip out and historically that's been a tough pricing model to pull off. Do you think AI changes that? I think AI changes completely completely changes that because in the in the past um we wanted to put everyone in these little boxes you know called SKS and then we wanted to count SKS and HQ will have a you know a discretionary amount of discounting whatnot. Um that's a that was a world of rows and columns and you don't need that anymore. You can, you know, if you go to the largest companies again like a service or a sales force, all their large contracts are bespoke, you know, so you know, you send off uh, you know, Paul Smith or, you know, their CRO to go and sit down with, you know, the counterparty, he comes back with a deal and the deal has all sorts of complications in it that doesn't belong into the like the the CPQ world, you know what I mean? And I don't know why in the agentic world you wouldn't do that all the time with the customers that you want to go big with. And there is no re you know you can always put a chat interface to say interpret this contract for me and give me the annualized value. You can inquire you know the all the the body of contractes you have done and get a sense for like your unit economics and your growth and like what this looks like. So I think that custom contracts is is a is is is here. Okay. You had four things. There's activity based, there was workflow based, was outcome a third. Outcome is a third and the other one the fourth is pay by agent. Pay by agent. How does that work? And so I've been working with a lot of the AISDR companies to get into to in introduce this as a concept because an a lot of the what they're doing is replacing say 80% of what a normal SDR would do and an an SDR you know fully loaded will cost you somewhere between 70 and $90,000 a year. So you can pay instead of saying a platform fee say like you know I'm going to deploy x many agents the agent is going to do this amount of work that is equivalent of a $90,000 a year uh sr I'm going to charge you 20,000 per agent the agent is going to deliver this much work and you can pay me a bonus for meeting for meeting booked. Yeah. How do you how do you define I guess how do you define the job of an SDR? Do you just say hey your human SDRs have a certain quota. This agent's going to hit the same quota. Yeah. When you hire an SDR the first thing you get is activity. Yeah. So you get x many calls, x many meetings, x, you have a book of accounts, you have, you know, uh, contacts within each that account and each each of those contacts get an activity, right? So that's where you get the, you know, 100 calls a day type of SCRs. You can do the same thing with an AISR. You can say like you're going to you're going to draw a a boundary around the activity that this agent is going to do and then you're going to charge for that agent because the output is the same, right? And then and then your job is to say look instead of paying 90,000 pay 20,000 for the agent and then pay me a bonus for meeting booked pay me a bonus for you know opportunity close one which is very similar to to the equivalent of hiring which is the hiring of a of a of a human in the flesh sr and you get to dip into the into the um headcount pool of budget as opposed to the tools pull of budget. So you're not in the RevOps purview in terms of like where my head where my budget is coming from. You're on the HR which is far larger. So that's what I'm sort of like trying to steer people away from um from you know charging like like it was a tool because then you're constrained right like then the CRO has a budget for so many things and you get a sliver of that as opposed to like full headcom replacement. What are the pitfalls with that you're seeing with companies that are ending up in the charging as a tool bucket rather than charging for the work? Um that then you get pigeon hole into seats. I guess like what are people doing that makes them get pigeonholed? A lot of what I'm seeing selling is the vast majority of AI agent companies are doing PC's right now. I mean they look like contracts and they look like their money blah blah blah blah blah but in reality every company in the planet got a AI mandate. So somebody in that company went and you know pursue some software and then they bought it and they bought it as software and as a trial vibe revenue. Yeah. 100%. Vibe revenue. Okay. So like so now you have the the the vibe revenue curve and now soon we're going to come into renewal land and that's going to separate you know the wheat from the chaff. And at that point you're going to figure out who real who got real stickiness and who doesn't. And that's where the the real monetization scheme is going to come in because they're going to figure out like who made money, who didn't make money, what kind of value I deliver or not. They'll see the outcomes and then we'll see the Right. Exactly. 100%. And it's funny because, you know, a lot of my early customers um I told them that how I thought about the world and I said like, "Yeah, that will never happen." And now they're calling me back and be like, "Hey, I just got my first contract that only pays me per outcome and I calling you because I don't know who else to call." So, I feel like this is going to be pulled by the buyer. You see what I mean? As a way to mitigate risk. Yeah. Because AI is is risky in in a number of dimensions and outcome actually lowers your risk. Is this in the the four things you mentioned activity, workflow, outcome, agent, is that a maturity curve of sorts like is is the is the idea that you want to get to selling agents or maybe you want to get to selling outcomes? It's a fascinating question. So the answer is is um a tit I think it's there is a maturity element into it but it's a little bit of like making your own adventure element to it in that in that um there's a bottom line so everybody has to get out of of selling by by activity. If you stay there somebody will come along and say I'll do the same thing for cheaper. Yeah. And then you are in in a nightmare scenario in which there is tons of others who look just like you. Messaging is just like you and the only way to separate the weed from the chaff is to try. Yeah. And now you're like turnurning from one to the next. So, so yes, there is a maturity level in that if you don't move out of that bottom one which is easy to sell, you'll get competed out. Um once you get into workflows then you're into value based pricing and then you're defining what the workflow is and why is that important and then you are in a deeper conversation and that forces a much better alignment. Uh and that requires some so I guess the answer to a question is yes that requires some level of maturity to have that conversation with your customer. Your customer will always default to some like the easiest way to buy which is either you know some kind of fixed price or a consumption price for the first year to see if it works. But if it does work, it is up to the AI agent builder and creator to go back to the same customer and say, "Let's align on things that are important to you and charge for it." Do you have any sense of which markets are likely to stay with value based pricing and which markets are likely to collapse into costbased pricing? Have you seen sort of early indications from the companies that you're talking with or working with, you know, of markets that are trending in one direction or the other? So what I'm seeing right now is that for those who are targeting BO budgets, yes, to win that business, they go at the BO price and lower it and they say, I'm going to do the same as a BO cheaper 247 and I'm going to accumulate all the data of that the BPO used to do. Yep. But I'm see but again I'm seeing that as an intermediate step like that is not the full step. That's just a way to get into the market and win it. Um then I wonder the BPO is not gonna just sit there and like just take it you know I mean like this is this is relatively large companies and they can just as much buy somebody's technology and deploy it. So I wonder what will happen once the VPO turns around and like deploys their own agents replaces their own people uses the data that they have internally to train or or to make it better um and then go to town and and defend themselves. Um, I was actually uh talking to, you know, BCG about this and they wouldn't tell me that they're doing a lot of engagement with BPOS, but I sort of like sensed it and I don't know that they're just going to sit on their hands and like see their business go away like that will never happen, right? So, like I wonder how does the game sort of like levels if you would like when what is the balance of trade? I I don't I don't know the answer to that. But I I think that to get into a market you see a lot of initial you know let's price it the easiest way for somebody to consume but to advance into the market you immediately switch to something else like in the alo that everywhere like everyone came in and charging by some kind of like token or credit or like activity but then it became a blood bath and there's like 50 of these guys and they all kind of like sound the same. Unless you start putting making some assertions of your quality and putting your pricing to back that assertion, meaning I'm going to get you five qualified meetings or I'm going to charge you per agent that replaces a human being, then you're going to get, you know, competed away. Yeah. And as you move into these more the later maturities of pricing models, how do you see people measure and actually implement these things? Um, I think that's kind of the beauty and that to each customer the definition of success is going to be a little bit different and a little bit of like I feel like I was like John the Baptist walking around like being locus and money and like like looking like a crazy man telling people that they should do bespoke contracts and everybody was looking at me like I'm crazy. Now it's coming around to say like be you know the ability for you to understand your customer's business and price and and contract around it and have the engine behind it to support it is actually a competitive advantage in a world in which you all have the same tools. But it wasn't obvious now. It wasn't obvious when I started like you know remember you we we talked in like September last year like it wasn't obvious back then. It's getting a little bit more obvious now as we're turning on the lights in different parts of the market where people are saying, "Yeah, we need to do something that, you know, is very unique to me so that I can protect that that um that asset or that contract." Second of all, you know, I think it was Seth Goden who said, "Pricing is part of your story." You know what I mean? And how do you differentiate your market is your story has to be different. And if your story is different and your pricing is the same, your story ends up being the same. You see what I mean? So aligning with and and this is why like Sierra doesn't have pricing on their on their on their main page because they're sitting down with each customer and they're finding out what's important to them. Is it time to resolution? Is it that the customer resolves a ticket and then buys something? Is it, you know, a c set? Is it MPS? Like what is it, right? And then you you build a box around and you say that that's that's your outcome function. And it's just the same as a objective function in in in in in uh machine learning. Like you create this function of where you want everything to go and then you keep iterate to towards it. Yep. Yeah. Well put. All right. Talk a bit about the cost side of the equation and how margins play into this. Um so it's really interesting because this is another one that I'm I'm contrary like everybody's telling me that the cost of tokens is going to go down and this is going to become a commodity and whatever whatever. Um I think in a world of reasoning, yep, where is inference time compute worth more than training time compute? I just don't see how the at least in the immediate future, I just don't see how the this the token price goes down because you're going to require like deeper level thinking. So what the problem with agents right now is that you you sort of like workflow them, right? So you you know you you go to you know land graph and you you put your boxes and what what what the does and like you string them together. But the fundamental problem with that is if you have a hallucination at the very beginning of that chain, you're screwed. Like you have all sorts of like, you know, bad activity happening all through. Yeah. And like that there's no amount of evil that is going to like rescue you out of that. So like the better way to do it is to have the the the the model do most of the work. You see what I mean? As opposed to have like these little boxes of work. Have the model do most of the work. Have a good, you know, evil um framework to make sure that you know that it's doing the thing that it's supposed to be doing and then enroll it out. In that world where you have less error rate, the the the I don't see that that the cost per token going down. I see that if nothing else is going to go up as the models get more advanced and like smarter. Well, or it could be the horse race between volume and price and price for a fixed workload may go down over time, but workloads increase, right, because you're throwing more compute at things at inference time or because you're just doing more sophisticated jobs. Yeah, I think over the long arc of history, I think you're absolutely right. I think in the very short term as we're trying to figure this out, I just don't I just don't know is sort of the short of it. Yeah. And and the second thing that I'm seeing actually which is my big aha moment is that um agents are using one modality and the moment you nail the modality you want to use all these other modalities, right? So you you do text and then you want that to do a phone call or to take an inbound call and to do call or to like an avatar or whatever that is and then you're incurring costs that are not LLM but are thirdparty APIs. So the full cost of the full service you got your cloud cost you got your LLM cost and then you have like all this other stuff that you have to buy to make it sync in different in the different modalities in which it's used or even like you know buying data or like you know buying whatever and that is what's driving up the cost. So if you look at a say an avatar company or a phoning you know agent their cost is not quite the LLM their cost is how long are you going to stay on the phone and like um the number of dials that you do or like how long does the avatar run etc. like those are those are compounding costs that you have to look out for. So like um the problem that we have right now in in cost mode is that because everything like all the activity of the agent goes down through this evil framework that as acts as a proxy before the token goes to the LLM. You don't know who's incurring what cost. So you don't know what customer is profitable to you, what customer is negatively impacting your margins. You don't know what agent is doing good job, what agent isn't. You see it as a whole. You see what I mean? And that's is why like the margin problem is such a a bear to to solve. Well, and generally speaking, margins are a reflection of the amount of value that you're providing to your customer. Do you see that in AI apps right now or is there a mismatch between the amount of value that companies are providing and the margins that they're able to get? It's a mismatch because people don't know how to price and people don't know their cost. Yeah. So, this is why the problem compounds a little bit in that, you know, agent companies are relatively new. So they come to the market just trying to like get business, right? So I I'll price in whatever way you want to buy and only later I find out whether I win or or lost. You see what I mean? So I think that this is this is just the beginning of the game that as they understand the unit economics, they're going to price better. As they understand the value delivery, they will price better. But they just don't see it yet. So for instance, we build this thing in pay that you know we look at an activity or a workflow and then we make this call to to the service that tells us what is a human equivalent of the same work. Mhm. In what country, right? So you know if the work is kind of hard you know it goes looks up the type of work how long how much would that person make per hour and how long does the work take and returns a value. M so we give our customers this little bit of pricing guidance that says the the human equivalent of the same amount of work is this you can price up to that point you see what I mean or at least use that to go and say I need an increase in in in in what I'm getting paid because this is actually pretty expensive but without that it's it's uh it's kind of tricky and it's not like you have Simon Coocher like Price Intelligent alongside you guiding you all along right they come in once a year and then they they they peace out you know you're left holding the Whereas you actually need guidance on this stuff every time you talk to your customer to get that you're getting you're getting your your prices worth like the the problem we're seeing is that the value is accuring to the to the customer not to the pro not to the agent business they're capturing all the value of all the savings and that needs to change. Yeah. So you started a company to help this. Tell us a little about why and what your mission behind it is. um when we were rolling out agents at outreach and I wanted to understand the business the business fundamentals of the agents my margins the value that I'm adding and how am I adding value adding value the underpinning software supporting me was not helpful was not built for a world in which pricing needs to evolve um where um the agents are doing are delivering more than just you know bits and bites are delivering this ful outcomes and that just stuck in my head as I moved to London and I step aside from being a CEO and I sort of like noodle on that and and the problem kept on bugging me. So I'm like is is this a problem that was just me or you know who else had this problem? So I spent, you know, a couple months just calling friends and, you know, other founders. Um, and I always have this trick, right? Like I I know a lot about sales, right? And they know a lot about agents. So we will do a cross education, you know, ask me like an AMA, like you ask me anything about like building out sales teams. And I ask them about like what it's like to run an agent business. And I found that this monet like running the business itself was all spreadsheets. So when I and and that's number one. Number two, um, some of the problems were actually difficult, but not intractable difficult. There was just a lot of work. You see what I mean? So, I love when there is a problem that is big, people are hacking around it, and the problem is not like simple as like, you know, you know, a couple lines of code kind of solves it. Um, so we that that sort of was the inspiration behind it. Um, solving the problem. Number three, I wanted to build a company that I get to work with like the new founders like you know um because they were building a company themselves and that kind of energy just drives me. So like you know the most fun thing to do when I was running outreach was customer conversations. Um, but the customer conversations at the founders level are so raw, are so full of like wonder and mystery and energy and like I can just see my energy tank just going up and up and up in every single one of those conversations. I'm like, I want to do this for a living. Like this is this is a great place to like exist. Um, and the market is going to be big and it's a a small enough market that I can just call call everybody and like most people will return my call because it's founder to founder stuff, you know? I mean, I seen your problem. Maybe I can help you. maybe you can help me and boom, you're in you're in a conversation. So like it didn't almost feel like selling. It felt like I was solutioning. Um and at the end of the day like I I told them where I'm building this thing and they like yeah let me try it and like I had a 100% history on people who said let me try it. I'm like oh let's go. The worst could happen if somebody buys something you know I mean like we haven't actually said explicitly what is paid. So maybe maybe just answer that question. What is paid? Yeah thank you. Um so paid is the business engine for AI companies. So what we do at paid is that we build the entirety of your billing, invoicing, monetization, um pricing, even at this point we're doing collections, revenue recognition. So the entirety of your back vendor management, margin management. So the entirety of the back office you need to get your business up and running to understand the unit economics and run your business. That's what we're building at paid. And our first foray into the market was our monetization engine and our margin management engine which is the biggest problem that we're seeing in the market. But we're building the adjacent. So the problem compounds, right? Like the moment you solve monetization and and uh invoicing, then you need to solve collections. The moment you solve margin, then you need to solve vendor management and so on and so forth. So you we're just building the unified layer that allows you to run your business in one place. How much of what you learned at outreach transfers directly to building paid and how much is new? Um the human aspect is the same. Building teams is the same. Inspiring is the same. Selling ahead of capability is the same. Um the what's new is how savvy the world has become in terms of company building. Uh I think you made this joke in in our in our podcast that you know you look at a AI agent company, you take the hood off and it's just building a company. Yes. And the building company part has gotten so much smarter like everyone that I talked to like is way more advanced than I was at the same stage in my building of outreach where people have seen you know what happened in SAS the whole arc and they're building completely differently. I was going to say is that a good thing or a bad thing because I think the positive case would be people are better informed. They're just making better decisions and they're expediting a lot of the aspects of building a company that are not unique to their specific company. I think the negative case would be that people are blindly pattern matching and not understanding the causal determinants that actually lead to healthy businesses over time. Do you think it's more the former it's a good thing or more the latter it's a bad thing? Maybe somewhere in the middle case dependent. Um to be frank I haven't stopped to think about that particular question. But if I were to put a a qualifier to it, I think that the initial conditions are different. Meaning they're starting from a point in that I'm going to keep the company small. I'm going to own more of my destiny. So we're going to raise in a different way. We're not going to be growth at all cost. I see I've seen it from several founders already that they take more time honing in who their ideal customer profile is. Y and making sure that they scale that versus scaling everything. Yeah. So when I started outreach, um I was always trying to figure out what what are the edges of my market. Who shouldn't I be selling to? And it turns out I can sell to everybody. And that actually makes the problem worse, right? Because I can sell to a startup. I can sell to a very large company like Adobe was one of my earliest customers. As much as I can sell to the Hillary campaign, as much as I can say to a repo company, they all have communication problems. They all have workflow problems, but they all have completely different ways of getting to them. um honing in the product for their particular solution and it was really hard for me to let go and I sort of like wallow in the sea of business that I had from every walk of life where if I were to relive my company I would like really hone it in with intention and say like okay so what does make outreach a billion dollar you know run rate company and then like stack from there um I'm seeing these new founders coming with that point of view I'm like what is it the the narrowest profile that I can sell without a lot of friction and then going up from there. Now, the problem with that is then they they may be restraining their ICP. I mean, they may not be trying they're not be experimental enough to grow the company to a from single product to multi-product to platform. Yeah. But quality tends to scale, right? If you really nail it for a small audience and you can start compounding from there, that's probably better than being mediocre for a big audience. 100%. And like it and it just makes life more fun because your road map is a lot clearer. Yeah. Your you know, your tickets, they all look the same. So like the scaling of all the other operations is actually easier when you have one type of customer that you're serving with excellence and then you get known the word of mouth gets bigger like all that happens uh more organically again like I wish I knew that you know I started outreach and it was faster to to like all right let's narrow narrow narrow narrow narrow let's stay super focused on this two things or one thing all right Manny so how does one get started with paid so it's a great question we are now on boarding manually uh to make sure that we have all the pieces dialed to your business. Um, so you apply and there's a short questionnaire as like you know monetization margins and you know what breaks you in. We'll schedule a conversation for on boarding and the the beauty of being an early stage startup is that all the onboarding is done by me and my my small team. We make sure that we're capturing your agent work correctly. We're making sure that we're guiding you with best practices because we understand the market at this point. We've seen pretty much everything under the sun. So we understand your market, how you usually charge and give you a few ideas. Uh get your your invoicing trail going, get your margin trail going so you have visibility into you know how you're making money and you know we'll be a short call away at all times. So just apply make sure that you have an agent business and some customers and then we'll take care of you from there on. And what what's it like to be part of your team? Maybe what words would you use to describe the culture of paid? I know this is super trite but the first word that comes to mind is fun. Uh and and the and the reason it's fun is because we're serving an industry that in itself is fun. Like you know, everyone in everyone that I talk to that is building agents out there like they can't believe they're doing this for a living. Yeah. You know, they can't believe they're like like these are like what a time to be alive. What a time to be alive. Like they're doing stuff that and they're getting paid to have fun. So like all our conversations are super fun. like nobody like like in I remember when I was in SAS there's a lot of people who are stressed out and there's all this like you know milestones and whatever so maybe because everyone is early stage right now so there is not a whole lot of growth rounds happening so nobody has like a a number that they have to hit or like an efficiency they have to hit um but everyone is just like enjoying the discovery so it's a little bit like that book from uh basis of like innovate and wonder like everyone is like innovating and wondering like living the art of the possible with like models improving every seven months months like you have a new toy to play with like every seven months. Um I think the ability so this is understated the ability to vibe code an idea to just at least show how it works and then actually build it has made the the iterations and the conversations so much faster. like you know you can just like say like we think we should build this or I have you know I have a lot of energy around this thing and you can just write it and you can see it work and you can tweak it a little bit and then present it in front of everybody and sell it like you're selling internally to another to another person. So this ability for us to be brass stacks in terms of like what we're building for whom we're building being so small and getting so much work done in a small team and being so close to our customer because we're early it's just fun. I think I think that's the best word to describe it. Very cool. Love it. All right, let's jump into the lightning round. You ready? All right, let's do it. All right, question number one. Who is on your Mount Rushmore of founders? Um Jeff Bezos for sure. Okay. Um he's a he's a he's a a big a big name founder in my in my life. Um I have a lot of friends that are founders. I just I just really that really inspire me, you know. Uh like Todd also from Pendo, he's he's a great founder that I just find he's a great human being as well. Um I really like what you know Sam Alman has done with uh with OpenAI. I think the ability to just you know push out innovation with regularity and and be a showman in in the world. I think it's it's something that I aspire to be. Um, you know, the Collison brothers are really special to me because they're so voracious in their appetite and their reading and the and how they think about the world that, you know, they're so um, uh, even though they're they're very young, they're so wise that I really look up to them. Awesome. What's one piece of content that every AI founder should read? You know, I've been thinking about this one a lot because the thing that changed my perspective in AI is actually a very old book about a statistical natural language processing that it was written by a guy called last is his name is Rich Manning. It it was mandatory reading at Stanford uh computer science program on the natural language program course. I don't know that it is anymore because it's so old, but it's like one of those like oldie by goody kind of thing where they tell you how to, you know, you know how a marov chain works, how you know the, you know, the length of u of the look back and look forward to try to predict the next word. It's like the early days of machine learning and a lot of the stuff that we're doing right now is still based on this statistical approach. Y so I know that you know a lot of advances have been done and like the you know we now you know cut tokens in many different ways and optimize in many different ways but I feel like if you don't understand where all came from I don't know that you're going to get where this where it's going. So that book is it's actually not a hard read even though it sounds like a hard read it's a fairly easy read. It's written in a fairly you know normal and accessible way and I can't believe more people haven't read it yet. What AI product can you not live without? Oh perplexity. That's an easy one. What What do you do with perplexity? Oh, we do everything. Like we It's so weird. No, seriously. It's so weird. Like fashion advice or like you know my f like Yeah. So the hardest thing for me is to have to take a driving test again. Like come on. Like you know I know it's the wrong side of the road and everything but like having to relearn everything is just absurd. And I've been re you know pushing away learning how to drive here in in the UK. And my my wife had to beat the bullet because one of us had to drive at some point. And she just went to perplexity and look for like, you know, advice on finding a good person to help her out to drive. And like she found this is super guy that has helped her go from zero to driving in like no time, which I' I've, you know, I didn't know perplexia can do that, but like it can. So it's it's taken over pretty much every aspect of our life. At that end, of course, um I don't write anything without anthropic anymore. Like Claude is my constant companion for pretty much everything I do. It's like my It's like a friend that I talk to and I shouldn't be talking to. If we were to have imaginary friends, Claude is mine. Models are commoditizing. Yes or no? Uh no, not yet. Not in a world of reasoning. I I think that we're just scratching the surface of where this can go. And there's a lot to go. Um like if you see every new model coming out the input tokens are like far more expensive than the the one they leave behind like but by by a factor of like six or seven or eight like not by a small amount and I think that this is just going to continue as we discover new things. On what date did we or will we reach AGI? I think it's I think it's kind of here already just in a underused kind of way. M um we haven't really bottom out everything a model can do. So I think it's just it's behind door number three or something like I think it's it's already here. We just haven't acknowledged that it is. Yeah. And then what was your what's your most optimistic future state for AI? Describe it. Um it's a it's a scaffold for human imagination. I think we also haven't really you know absorb how much smarter we get when we have somebody else doing a lot of the thinking and discovering for us. Uh it's sort of like when somebody props you up on the shoulders like all of a sudden you can see farther and and longer is a AI is doing the same thing for us. We can we will now be able to see farther and longer and like come up with like things that were impossible before and but can't such a time to be alive. Yeah. Last question. One piece of advice for AI founders is stay focused on a very narrow set of customers. Don't worry about TAM. Disregard and they say this all the love in the world. Disregard BC advice about big Tams. Small Tams will be big TAMs as long as you deliver a superb experience. Awesome. Thank you, Manny. Thank you. Thank you. That was awesome. [Music] [Music]

========================================

--- Video 15 ---
Video ID: v-_58dabswU
URL: https://www.youtube.com/watch?v=v-_58dabswU
Title: Arc Institute's Patrick Hsu on Building an App Store for Biology with AI
Published: 2025-04-15 09:01:16 UTC
Description:
Patrick Hsu, co-founder of Arc Institute, discusses the opportunities for AI in biology beyond just drug development, and how Evo 2, their new biology foundation model, is enabling a broad ecosystem of applications. Evo 2 was trained on a vast dataset of genomic data to learn evolutionary patterns that would have taken years to find; as a result, the model can be used for applications from identifying mutations that cause disease to designing new molecular and even genome scale biological systems.


Hosted by Josephine Chen and Pat Grady, Sequoia Capital

Transcript Language: English (auto-generated)
One of the things that we you know that the field of computational biology is often asking is you know if you have a genetic mutation in your genome if I sequenced you whether that's via you know 23 and me or you know or or some other genetic test right you'll find mutations in your genome how do we actually interpret those and understand you know what the what the functional consequences are right sometimes you'll get a rare genetic disease those are causal genetic mutations that are known to cause a a devastating disorder that might be musculardrophe or cystic fibrosis or uh breast cancer, right? But you know most of the mutations that you have um there's sort of this uh you know we call them variance of unknown significance which is fancy kind of scientist what the hell is going on, right? And uh you know it turns out the model has an opinion about those mutations and what the hell is going on with them. And it turns out it's sort of state-of-the-art in doing that. [Music] Today we're joined by Patrick Shu, a pioneer in genome editing, crisper technologies, and the emerging field of generative biology. He's the co-founder of the Ark Institute, where cutting edge AI and biology converge to reimagine scientific discovery. Patrick and his collaborators created EVO2, a revolutionary biological foundation model that can interpret and generate genomic sequences across all domains of life. By training on the fundamental information layer of life, DNA itself, EVO can identify patterns in genetic code at scale, and predict effects of both coding and non-coding mutations that can mean the difference between health and disease. In this episode, we'll hear how Patrick's vision goes beyond creating better drugs to building a comprehensive understanding of biology at all scales. Patrick, welcome to the show. Thank you for coming. Thanks for having me on. Excited to spend some time with you today. I think maybe the most obvious thing to start with is, you know, people have heard about CS and bio for the longest time. Now it's all about AI and bio. What are the results? like what what should we actually be expecting to see and where are the drugs? Why are we not seeing the drugs yet? It it takes time, right? Well, here's the thing. Even if we had perfect drug design molecules, um, you know, coming out of these pipelines and fancy models, it was still, you know, you can design a trillion molecules, right? 10 trillion, right? But you still have to actually test them, right? Initially in animals and then in people, right? And so that's the real the real bottleneck and even if you pack top of funnel with all the things it just takes years to actually go through the regulatory apparatus right and so I think there are a few intermediate checkpoints along the way in order to kind of realize this potential but it might be worth taking a step back and just saying you know this is a bit of a soap box of mine that ML for bio is not just drug design right this is actually ultimately I think a very important but narrow part of the potential of biology and not just as a field of STEM and in the way that it affects human lives, right? And do you mean basic just like understand the human body or where where else do you think the applications are beyond drugs that treat all of us? Yeah. One of the things that motivates me academically is the idea that we actually have a a unifying theory for biology, right? So unlike the physicists who have been kind of scrimping and you know kind of poking for one for a century, right? We have this in biology and it's so obvious that we find it, you know, sort of an just an obvious force, right? This is of course evolution, right? And then so it acts on biology across all of its different length scales from entire planets, right? Biology can, for example, terraform planets, right? All the way down to, you know, ecosystems and populations to individuals to our tissues to individual cells to individual molecules, right? So that's this unifying force that is actually very deep and rich and actually you know you can learn a lot from right how do we activate that unifying theory how do we put it to work so we've been thinking about this in the lab and uh recently have been training a series of models that we call EVO inspired by these forces of evolution that tries to connect biological sequences using this sort of you know modern sequence modeling paradigm directly to biological function with the idea that evolution passes down its effects of natural selection throughout generations of life via DNA mutations. And so, you know, last year, of course, you know, uh multiple Nobel prizes were awarded for AI and for AI in biology, in particular for protein design and for predicting the structure proteins to David Baker and to Demis Sabis and John Jumper, right? But and if you read those citations, they both explicitly state for proteins, right? And you know, we love proteins, right? These are of course some of the most important fundamental molecular machines, but our realization as for me as a as a genome biologist, um if you will, right? Um the idea is that proteins are encoded in DNA along with RNA and with regulatory DNA and all of the things that you need to make life, right? And so we asked could we train a model on genomes with a long context model so that it could reason over all the different bases and uh molecules that are embedded inside of genomes to learn about the molecular interactions and how they lead to biological function. Now that was very scientific or academic right but we can talk through specific examples of what we were able to actually do with this in a way that uh grandma can understand. uh you know like predicting the effects of breast cancer causing mutations right it actually is uh bestin-class at doing this right or being able to design new crisper gene editing systems or uh you know I think uh you know in addition to its zeroot capabilities I think people are building really an app store for for biology on top of all of these kind of foundational layers one of the kind of funny things in modeling today is how everyone's model has to be more foundational than someone else's model. There's a bit of a pissing contest that's that's happening, right? But maybe our model is more foundational than the other models. But, you know, because your DNA versus protein I I I mean or you know, and then below that there are these all atom diffusion models. So maybe those are even more fundamental. I don't really know. I think what matters are the capabilities and doing something that actually feels useful. And I think, you know, those are just some examples of what we thought was cool and useful from the model. Can you actually walk through a couple more of those use cases like which are some of the most exciting ones and why is it possible today with EVO but wasn't possible before and the model's open source and so where has it been picked up like where are people running with it with some of those use cases that Josephine mentioned? Yeah. Yeah. So the model so I can just maybe talk about what the model is right. So you know it's um it's an auto reggressive uh sort of uh multiconvolutional hybrid model right but you can think of it like uh you know just a really efficient long context model that's trained at least in this version auto reggressively right and uh basically it does this next token or next base prediction and it turns out just like in natural language or in vision or in robotics and embodied intelligence this general machine learning paradigm is able to find higher order patterns right and so just like you know uh if you're doing next word prediction you can learn about grammar or color um and world navigation right you seem to learn some rich set of representations about biology by predicting the next base or the next amino acid residue or the next gene right and the model learns something about the molecular logic that gives rise to a cell right and so one of the things that we you know that the field of computational biology is often asking is you know if you have a genetic mutation in your genome if I sequenced you whether that's via you know 23 and me or you know or or some other genetic test right you'll find mutations in your genome how do we actually interpret those and understand you know what the what the functional consequences are right sometimes you'll get a rare genetic disease those are causal genetic mutations that are known to cause a a devastating disorder that might be musculardrophe or cystic fibro osis or uh breast cancer, right? But you know, most of the mutations that you have um there's sort of this uh you know, there we call them variance of unknown significance, which is fancy kind of scientist. We know what the hell is going on, right? And uh you know, it turns out the model has an opinion about those mutations and what the hell is going on with them. And it turns out it's sort of state-of-the-art in doing that. Interesting. Wait, what's an example of one of these mutations? what what did the model discover and how did you verify that what it discovered was accurate? Yeah. Yeah. So, so you know one one example that we showcase in the paper is a gene called broco one right it's sort of a famous gene that's known to cause breast and ovarian cancer and uh you know if you have the sort of specific causal mutations in broco one many uh women elect to get double mystctomies right and this is obviously you know a serious and major life decision and medical decision for you and for your family right and you know the question is you know if you don't have one of the known to be benign mutations So you're fine and you just go ahead and get an annual mammogram and just check and monitor, right? There's this entire middle distribution of these VUS or variants of known significance, right? And uh you know there is this uh kind of gold standard database from uh you know the scientific literature. It's known as ClinVar and it basically has a list of all of the different genes that are known to cause disease and you know which of those mutations in those genes can cause uh disease state or not, right? And we can basically use this as a ground truth database to assess the predictions of the model for you know new mutations that you introduce into the gene and whether or not those would be pathogenic. Right. And so when you develop a new type of model, you have to um create a lot of the evals as well. And so that was actually something that we put tremendous effort into. And obviously you guys see this horizontally across AI in many different domains is you know folks will build things to the benchmarks. No one really I think likes building benchmarks. It's really gory. It requires a lot of taste. It takes a lot of time. You have to continually update them as the models get better. And you know we dealt with a very similar uh sort of challenge here which was making evals that would be similar to the the AGI or intelligence evals where it actually feels meaningful when you're actually able to do it like Amy or Putin problems or you know things like that. What what would be the equivalent of demonstrating true biological understanding that a cell biologist would fuel emotion if you were actually able to solve that right? um you know what what would it look like to make all molecular biologists feel what the NLP people felt a few years ago. Right. Right. Though that's sort of the the core of you know what we're kind of noodling through. Yeah. And I think you mentioned this briefly but you know there's you guys are working on the DNA layer and you guys didn't actually do anything in the lab. You didn't have a lab in the loop like there was no RL. Talk us through the decision to do that and then kind of the decision for people who are doing protein models or affinity models. a lot of them have a lot more lab in the loop. Talk us through kind of the differences between some of those models too. Yeah. So, so we started with DNA because we think it's the fundamental information layer of life, right? Um the second is it's also just pragmatically where we have the most data data. Yeah. Right. Where does the data come from? It comes from the entire scientific community, right? And so there are these uh kind of open- source governmentf funded and maintained databases um you know known as the sequence read archive right where basically you know when you publish a paper you have to submit your data and all of the sequencing data that's been created over the last 25 plus years right goes into these databases right and that has all of the genomes that the community has ever sequenced for bacteria for bacterial phagee for viruses for humans, monkeys, fish, flies, you know, the entire Noah's arc or managerie, whatever, right? We've got all those genomes and you know, so so the experiment that's already been done, if you will, is the experiment of evolution, right? That you know, there are different mutations um that are what make us different from each other. That's human genomic variation, but also the mutations that make us different from chimpanzees or from worms or from bacteria. And the model can just look across this you know you know trillions of tokens large data set and learn those patterns. And that was sort of the, you know, insight of this sort of EVO series of models that that we've been training um at ARC um is to kind of ask if you could predict that next base, right? Then that might be the difference between being healthy or having a cickle cell anemia mutation, right? or it could predict the next amino acid residue. And that could be the difference between having a catalytically active binding pocket for a key enzyme right in your, you know, body physiology or that being a null mutation where that thing doesn't work anymore. Or it could also be the sort of the next gene, right? So these are different levels of abstraction, right? that uh completes some biosynthetic pathway or it's a you know a different gene that's you know you know that's been removed by some transposon or jumping gene mobile genetic element that's excised it so some sort of viral interference if you will right so there's just you know across large databases you find new patterns and it turns out those seem to be biologically meaningful yeah so if you're going from DNA and you know the function in many ways you you mentioned even the DNA model can actually predict binding affinities. Do you even need the structure the protein structure models at all as an intermediate step or can you just go straight from sequence to function? So, so structure is um another way to ab have an abstraction of function, right? And so you have concepts of convergent evolution for example, right? where something has similar function but they have different sequences and slightly different structures that act out the activity or you know function of that protein. Right. And so this sort of you know sequence to structure to function token or mapping of protein language modeling I think is very beautiful. Yeah. Right. It takes advantage of you know the central dogma of molecular biology alphold series. Right. Right. Well, it it takes advantage of just our supervised textbook understanding of how biology works, right? And know the interesting oxymoron with these models for me is the way that we use them is, you know, just like use chatbt, it's text in, text out. You know, EVO is DNA in DNA out. Yeah. And it turns out we don't speak DNA very well. Sort of imagine if you were using a model like chatbt in Russian, right? But 1% of the words were in English, right? That's kind of what it feels like. That's the vibe of using EVO is actually you don't really know what's going on. And so you're you have to build lots of annotators and interpretability, like techniques to try to interpret and read what's happening. And so the way that we even use and prompt the models is really primitive, right? And so the way that we do fancy prompt engineering workflows to, you know, get more utility out of these models is something that we're just in the very early innings of exploring how that works with these biological language models because we speak DNA with an extremely heavy accent. Yeah. Who do you think will do some of that work? Because the model is currently open source. Do you envision a company formed around this? Will it be individuals who are at these pharma companies who learn how to prompt this? like how does that ecosystem evolve? Yeah, I hope everybody right uh first of all, right? And I think the you know tools become useful when they meaningfully lower the energy barrier of adoption right it's like a it's a catalysis type of activity where you know everyone uses blast for sequence alignment right everyone uses alphafold to look at protein structure right uh you know everyone uses crisper to do gene editing everyone does ngs to read DNA or RNA right so I think there will be a zoo of different models for not just modeling molecules or mapping sequence to function but I think every step of the scientific method right and so I think you know you know if 2025 is the year of AI agents right I think you know there's lots of interest in agents for science and not just agents for interpreting molecules but also for doing the meta aspect of how scientists work and and operate and we're also very excited about that at arc and recently released some of our first AI agent work. Yeah. What is the most important role for ARC to play in all of this? Um, we started the institute to be able to have this mothership that is able to attack long-term research capability breakthroughs and to be able to have the long-term thinking and the multidisciplinary expertise in order to actually execute on those goals. Right? I think if you look in biology, one of the interesting things is that a lot of the biggest mechanistic or basic science breakthroughs do happen in a university context, right? And I would say that's interestingly kind of in contrast to what happens in AI or CS in general. Yeah. Yeah. Why do you think that's the case? Um that that might be a 10-hour epic podcast, right? I I that's the one minute vaccination. I have a lot to say about this but um I think the in in short it does happen in universities. I think you know 30 years ago the questions that basic science uh was interested in and uh industry was interested in were quite different actually and I would say today they seem to heavily overlap right and there are some things that you know folks don't typically do in university lab like you know people don't tend to study PK or TOX or CMC right or you know hardcore drug manufacturing type things but folks are interested in molecular glues, induced proximity and degraders and new drug concepts, right? folks are also interested in new machine learning models um you know and in new delivery mechanisms and you know inflammation and stress and you know all kinds of things like this and so there's much more heavy overlap but I think the way that the type of product that selects what you do upstream is very different they're structurally different you know end of the day you have to optimize for grandfunding and first or co- first author papers and you know you know that type of stuff in the academic setting whereas you know you do have to make a drug and have you know dozens to hundreds of people lying behind a molecule or a program to reach the holy land right and I think that does lead to differences in strategy and I don't know personally by the way I think people really kind of overemphasize like what's academia and what's industry and the reality is these are like heavily overlapping distributions today and how people operate And so I think the differences are a little overblown, but you know, they do have fundamentally different incentives that drive different behaviors. And a lot of what we try to do and blending our academic side of the house with our technical staff uh side of the house, which is built much more like you'd see in industry, um has been, you know, kind of part of what we hope will make ARC a model that others want to copy or replicate or propagate. Yeah, makes sense. And you've had some fun people come through on the technical side of the house, including people like Greg Bachmann. Yeah. No, it's been it was a joy. Yeah. So, Greg joined us during his sbatical uh from uh OpenAI really the kind of you know, the first vacation that he had ever taken since starting OpenAI. Of course, it's to work more. Yeah. So, so I actually have a funny story about this. Um you know, hopefully he hopefully Greg will allow me to tell it. Um but you know when he when he first came by ARC and we were talking to him about you know um you know biological language modeling and how you know the sort of machine learning breakthroughs of you know all these other domains might just port over directly to understanding you know molecules. Um he was you know really kind of you know jazzed by this and also by the idea that his very specific uh capability and expertise would be kind of really meaningful in actually making this happen. And um but you know in in in the you know initially he was you know um he was you know saying you know this is really the first vacation I've ever taken. I can't promise too much. You know I might not be able to spend so much time on this. Um you know you know I owe I I I need to take Anna on vacation. Um and then and then Anna um gets up and goes to the bathroom um you know in the middle of this very long meeting. And then he's like all right here's my email. Get me on the repo. Of course, of course. Yeah, that's funny. So, so yeah, just built different. Yeah, it was such a joy to to learn from him. What have you found works well in getting the different disciplines to work together as one unified team? Yeah, know it's an interesting question and we've thought about this deeply at ARC um as a convening center, you know, not just between the three flagship research universities here in the Bay Area in Stanford and Berkeley and UCSF, but also between basic science and the biotech industry, but also biology and the technology sector. For example, um you know, our CTO, uh Dave Burke, just started um a few months ago and has really kind of been leading the computational modeling of virtual cells where we're trying to simulate human biology with these AI foundation models. And you know Dave used to um he actually has a PhD in biomedical engineering um from you know many moons ago um but uh you know most recently ran engineering at Android and Pixel and so you know I think we have also built an entire operational side of the house with from finance to legal to lab ops to facilities to university relations and academic affairs and you know we run our own space around administration our own ops and you we we try to do you know much of that like a tech company, right? And you know, we've also, I think, recruited in on the off side of the house many people who maybe ordinarily wouldn't work in a basic science or discovery setting, um, but are kind of motivated by the mission to be able to take fundamental breakthroughs and have a product sensibility where we can get these out into the real world. And we're not optimizing at ARC for nature and science papers, right? If you give Berkeley or Stanford professors millions of dollars to do more science, that's almost the default expectation and output, right? And what we really care about are things that could be tangible, right? What does success look like? Just more people using the products you create or yeah, what is success for art institute? I think there are lots of ways that you can parse scientific productivity, right? You know, journal publications is of course a important and fundamental part of sharing work with the community and peer reviewing it and all of that good stuff, but technical blogs, you know, code repos, protocols are are just platforms that people can use, right? And I think we want to make technologies and platform capabilities that are broadly useful to be able to create new mechanistic insights, but also actually try to cure some diseases, right? And you know I think over time if we can be an Edison shop that's inventing or finding lots of cool things right there you know hopefully we'll be you know real world value in those things and there are lots of you know partners who specialize in this. Speaking of curing diseases earlier you were talking about even if you had perfect drug design or infinitely accessible infinitely intelligent drug design there's still a long process that has to happen after that before you can make an impact on humans. Um can you talk a bit about where you see opportunities in that value chain and if you had a magic wand and you could just accelerate progress by 10 years which of those bottlenecks might be alleviated by things you see coming down the pipeline. Yeah. So happy to talk about this in the pharma context which are you know large decentralized sprawling bureaucracies much like universities or you know the you know congress or the SF city governments right and you know I think you know um some parts are you know you know incredibly you know functional and then I think everyone would agree in these different uh organizations some parts are you know less efficient right and so I think the first thing that hopefully we can see is that we can have models that can improve efficiency in discrete individual steps, right? So, can we have a more efficient process for target ID, right? In particular, right? Can we have a more efficient process for data analysis, right? Can we have a more efficient process for um you know like information and literature review and summarization, right? For molecule design, absolutely making a better binder, right? And then figuring out the drug properties of you know those different molecules that could be selectivity that could be you know pharmaccoinetics right halflife expression manufacturability what have you right and you know there will be models or modelg guided approaches for each of those steps right and then there's you know I mean I think if you look at how AI is actually kind of being used today in pharma companies a lot of them are actually just taking massive regulatory documents, summarizing them, and then using AI to help them write more of them. So, there's this compression and decompression of information in a structured fashion that has actually been leading to enterprise adoption in this setting, right? And I don't know, I think that I don't know that says something about the process of where people find things useful. So you know one example is if you talk to some pharma execs right um not the drug discovery organizational leaders but the you know the the the budget people right um you know who kind of hold the enterprise purse strings right they'll say well you know I don't spend actually that much money on drug discovery I actually spend most of my money on drug development so tell me something about drug development which is really where most of my dollars go how can AI help me with that right and I think that is actually like a very deep uh comment, right? Because it says something about where um money is spent and where value can be found. Um you know, the first thing to realize is, you know, our industry probability of success is like 10%. Right? And so, you know, I think a lot of the things that people talk about or complain about or comment on in drug discovery and development falls out of that fundamental uh statistic. Yeah. Right. Like why does the FDA heavily regulate? Why is it so focused on safety? Well, if 90% of the time it doesn't work, they're going to care a lot about safety, right? And I think the promise of AI is if we can go from 10% POS to 20 or 30 or 50, right? As you move through those steps, can we do you think we can um over time? I think here's the thing that I find kind of interesting is we have done a lot of biology with what is not that far from guess in check right if you look at what happens in the wet lab like the actual experiments that are happening right you you you're just kind of in the arena trying right and yeah trying out random hypotheses and seeing what happens yeah and this is The missing reasoning trace in the scientific literature is you don't know what didn't work. Everything is narrativized and written in a story of, you know, inexurable logic and vision leading to scientific breakthrough, right? But everyone who makes the sausage knows that's not what actually happens most of the time, right? And you know, if you actually work with the, you know, research um, you know, kind of folks at the bench, right? You know, and this is actually the case across all technical industries is that highfidelity reasoning trace of the true process is kind of not written down anywhere. And that would actually be very useful for these reasoning models and for closing the loop and doing, you know, you know, multi- aent frameworks blah blah blah blah blah. But that's kind of what we'll need. But if you actually look at what happens, it's guess and check, right? And so a model with even a modeicum of predictive value would be transformative. when with even moderate predictive value which by the way we don't have right like I think biology is a very pragmatic salt of the earth experimental discipline right you see this in the culture of peer review show me the data you can't you know pontificate in your discussion section because you know you haven't shown any of this stuff right is if you read old papers right they were so clear and visionary and um you high fluting in a way that I think you know papers today are you know we we have this culture that is very pragmatic and I think having models that have predictive power will you know I think a obviously be useful for accelerating the efficiency of science it also change the culture which I think hopefully will hopefully will change the culture which I think will be really interesting why do you think it'll change the culture of the models because you'll believe people's predictions or pontifications depending on how you it's just another kind of evidence point basically just like how model hallucinations could be you know predictions or they could be nonsense and garbage right and that depends on how much you trust the model right got it why do you think it's the case and we've collected a lot of data too to your point there's obviously we basically see what works and we oftentimes don't see what doesn't although hopefully lab notebooks are recording that in some way shape or form maybe maybe hopefully Okay. Um, but somehow still very very regularly even when things work in cells, things work in mice, they fail in humans often times. Like why is that still the case? Is it we just don't understand biology deeply enough? Like why is there still that drop off and that drop off hasn't really changed over time? Well, these are imperfect models, right? And um you know we we set up this set of filters um in the drug discovery process where you know first show it works in cell lines then it show it works in primary cells or in an organoid then it show it works in a mouse then show it works in a monkey then test it in people right and you know you know by the time you've gotten there like five years and hundred million dollars has gone by right and I think that's that's very challenging right and that's where I think predictive models will really help, right? Because the reason why we do all of these steps in linear series is because we don't have predictive power and so we have to do things in the arena and it just, you know, you have to do it in real life, right? And growing cells and growing animals takes months to years to actually do those experiments. And so the promise of having predictive model isn't just predictive power. It's that you could actually simulate things in a multi parallelized fashion, right? That's the whole idea behind parts of machines of loving grace, right? That I thought, you know, Daario really did get right and is the idea that if you had something that could be a trusted oracle, right, that you could just run, you know, 10,000 agents, right, at the same time, right? Do we have enough data for that full closed loop to create a trusted oracle? Um I think we will see more examples of this coming out over time. Right? Today folks building AI agents for things are doing you basically trying to close the gap between small you know yes step X and step X plus one right or X plus4 or whatever right and the businesses are trying to find the most commercially valuable set of you know steps that is a set size that's as small as possible in step number in order to you know make a company and I think we will have agents or co-pilots at each step in the scientific ific method from hypothesis generation to experimentation to data analysis and the ability to close the loop is and write the paper or make the discovery and then decide what to do next I think is quite far away but I think something that's very efficient at traversing the steps I think will really take off and so as a concrete example right one of the things that we recently released at ARC is our virtual cell atlas right, which is the world's largest data set of single cells, right? That we're using for training these cellular foundation models, right? And uh the way that it happened was we created an agent that was essentially it's like a a crawler kind of like you know kind of a search crawler but it's able to crawl the kind of you know sequence read archive and then process all of the highly unstructured and messy metadata and uh you know kind of reanalyze and systematically reprocess all single cell data. And this is something that is just running on a cloud bucket instance just cranking away right you know in a tireless fashion right and it's the kind of stuff that a talented computational biologist wouldn't want to do because it's so grind set but actually the scale at which we're able to reach is communitywide and that was that the leverage and efficiency that our team of you know just you know you know really two lead researchers could achieve with one agent I think was was a huge mental unlock for me. And so we want to be at the frontier of actually deploying these and making breakthroughs and I think the the meta aspect um that folks are going after right now will shake out over time but I care about using these to actually make breakthroughs as opposed to chart the endtoend closed loop path. Yeah. You were mentioning earlier the um sort of the pragmatism that a lot of research papers have now versus grandiosity or something a bit more visionary back in the day. I wonder if some of that is related to the specificity of the work that people are doing now. Meaning it feels like we've gotten more and more specialized over time. Yeah. And I wonder if some of that is taking us away from breakthroughs because in a lot of cases you need the knowledge from different domains or different disciplines to achieve those breakthroughs. And I guess maybe the question is one of the nice things about LLMs is that they can incorporate an enormous amount of information and they're sort of inherently generalized even when you apply them to a specific domain. How much of the efficacy that we might get out of some of these models is simply related to their ability to go across all these different specialties? Yeah. If you look at, at least to me, the the best scientists that I've had the pleasure to collaborate with or learn from. Yeah. Um they really do two things. They they they they're able to come up with really creative ideas and they're able to execute on them. Yeah. The reason why they're able to come up with really creative ideas is because they're able to make connections between things that other people wouldn't make. And in fact, if you got a room of 10 really smart people together to chat science, like that's, you know, a weekly lab meeting in any group, right? If you actually analyze the anthropology of what happens, there's usually a small subset of people who are hearing all the things that are being discussed and then actually saying this is the connection or the conceptual bridge between these things, right? Yes. And so there's, you know, that's sort of like an out of distribution generalization, right? It's like this thing that I heard was really novel and let's recognize that and then try to see what can generalize out of that observation. Right? And that comes from people who tend to either read a lot or reason a lot, right? And so there's some aspect of pre-training, right? You need to just read a lot of papers, right? read a lot of chemical biology papers, read a lot of molecular biology papers, read a lot of AI papers, read chem like physics papers, right? And do so across domains so that you can traverse those boundaries, right? I think you know there's this design problem of how do you build a multi-disiplinary team, right? And the reality is there well for example you want to work at the interface of bio and ML there are way more ML people and way more bio people than truly bilingual ML and bio people right I have the extreme fortune of working with some of those at arc right um and you know they're they're just rare right but those those translators right can actually help you power you know the the you know the the the rest of the population Yeah. Yeah. What do you look for in people at ARC? It depends on the role, right? I think, you know, depending on how you're trying to to to match specific project needs, but in a way I I could tell you all the ways that we try to intellectualize our recruiting process, but it actually comes down to very simple things, right? Um, and it's the same thing that I look for in a research technician or an executive on at least on the science side, right? It's really like are you thinking about science outside of the lab, right? And have you done something end to end before? Right. And then the third is, you know, do you have the the the grit to actually kind of walk the path and get it done? Yeah. Right. What do you mean by the end to end part? Um I think it's very easy to go from step one to two or three to five or you know 12 to 15 but you know going from 1 through 15 winnows down the population significantly. And so you know you know I often say you know the the last 20% of a project is actually 80% of the work. Yeah. Right. And it's because finishing something and then honing your killer instinct from finishing things multiple times really matters. What should we expect to be coming out of Arc Institute in the next six months and then over the next few years? Well, I'm I'm I'm tremendously excited about lots of things. And I think the thing that maybe many people don't know is the degree to which we have, you know, really been trying to build biology out at ARC. I think people have maybe heard about our our gene editing work or our machine learning work, right? But a lot of what we're actually trying to build is this general concept of applying high throughput scalable technologies in the context of multi-systems interactions, right? Um really working at the neuro andimmune interface, right? And so you know we hired uh you know two incredible scientists um out of Penn last year um who study the process of interosception right. So proprioception is you know when you kind of close your eyes where your limbs right and interosception is the idea of you know um you know I feel the weather in my knee or my tummy feels funny right you know kind of midwives tales type stuff that actually has really deep science and of course it's totally unknown and it's how does your body talk to your brain and vice versa right and it turns out there's a deep mechanistic basis for this and as you know I think when people think about programing biology, they think about it in the drug paradigm of how do I get a binder that binds to this protein or how do I get an crisper to edit this gene, but if you think about what happens with hormones or with ampic, right? Right. You're able to program the way that you think and feel and behave in really powerful ways that controls not just satiety but energy, mood, you know, you know, um, you know, muscle synthesis, you know, um, all focus, all kinds of things. And I think how do we actually program physiology is something that I've been spending a lot of time thinking about in in our lab. Yeah. What's one unexpected connection that you think people don't don't think about um one example is um the you know so so exercise right um and so Kristoff one of our PIs you know had a beautiful paper where he showed that there is a specific species of gut bacteria that produce a certain type of molecule that connects via your entic nervous system which is the nervous system that lines your gut that goes to your brain in order to release dopamine and it is this functional circuit that creates runner's high or exercise reward. And when you delete these bacteria, you cut off this ENS to brain circuit or you you cut off the ability of the brain to release the dopamine at each of these steps individually, you can block the runner's high, right? And so it really traces in an intact animal. Well, this is a mouse study, right? this you know full body circuit but that also goes in reverse. So when you have deep psychological stress right um that can lead to signaling from the brain to aststerytes that intervate your gut that releases pro-inflammatory cytoines that leads to gut inflammation and then yeah can give you ulcers right so stress causes ulcers we've actually kind of known this but this is the mechanism how can you treat it right because there are folks who get recurring ulcers right and there's a brain two body axis by which this signals right and I think this happens all the time you know some of which is conscious most of which is unconscious right and you can actually start to figure out the dials and knobs and that's actually if in a way like a new paradigm for drugs yeah or how to think about using drugs right it's not just this highly reductive pharmaceutical you know kind of binder to biomark marker type of thing, but you know the that's giving you more of the holistic kind of almost eastern medicine flavor of just how do I feel healthier? Yeah. Right. That I think is in vogue in the longevity community today, right? Like what is health span? How do I improve it? How do I improve my diet, my nutrition? Uh so I just feel better, right? You know those things are also can have deep scientific grounding and needs to have that. Interesting. So how do you think it'll how do you think people will be treated in the future? It's you will have a full panel. You'll know exactly what's going on in your body and then you'll decide different inputs to influence the whole thing like how do you how do you tie in functional medicine things going on longevity with the drug industry as it is today? Like where does that interplay come out? I mean, I think we'll want AI doctors, right, that are able to integrate information multimodally, right? And so just like you have your CGM that monitors your glucose with high temporal resolution, you have your order ring or your Whoop that talks about your you know various biomarkers or you can go to Quest or you know function health or whatever and get blood tests right that you know measure what's going on with your liver function or your cholesterol or you know um your testosterone or estrogen or you know other types of hormones. Right? Right now, all you really know is that these things are going up or down and whether or not they're in standard or reference range, right? That doesn't tell you very much about what you're supposed to do, right? And I think, you know, one thing that has been really missing in personalized genetics or, you know, consumer genetics is the ability to take information content from your genome sequence. Yeah. and meaningfully integrate it with your health biomarkers in a way that gives you the genotype and the environment that can be more predictive of phenotype, right? That GXE equals P equation you learn in high school biology, right? But none of that is actually kind of accessible to mom and dad or to even us, right, in the in the in the setting of how do I actually live my life? And I think we need to go from measuring people with, you know, higher content approaches to connecting that to, you know, genetic signatures and make more accurate predictions. Right? So that's sort of like been the theme of our conversation today. And what do you think that'll look like? Do you think that'll look like any of the existing longevity efforts or do you think there's some new beast entirely that's going to be created to serve that purpose? Yeah, I mean so I think if you look at you know 23 and me was you know recently you know file chapter 11 right and it's I think one of you know I think it's an amazing uh pioneer and visionary kind of pioneering effort in you know how to take genetics and you know put it in the hands of millions of people right I think the thing that I think I would really love to see in the world is something that can take all of that information with all of your different, you know, kind of body measurements and then actually, you know, connect that to diet and sleep and give you personalized recommendations about your health in a longitudinal way, right? We have very fragmented data sets for being able to do this today. And I think being able to collect this data at scale across populations and over time with temporal resolution will I I don't I don't want to be one of these unhinged big data will solve everything people. Yeah. But but it will but it will. Yeah. I do wonder if there's more stuff on the cross functional side to your point like if you know you have a gambling addiction I don't think anybody's thinking about maybe Mjurro Zmpic could help with that but it does help with some some of these things right like I do think there's something about the cross functional nature that is interception that's fair there needs to be some some organization that actually makes accessible yeah yeah I don't think that obviously exists today I think folks are building a different hands on the elephant for this. But, you know, you guys should start this company. We should. Honestly, it's a pretty good idea. It is a good idea. Um, all right. Let's ask Let's get a couple of predictions on a couple different time scales. So, we'll start with 2025. Mhm. And then maybe 2030 and then maybe 2050. What is the most interesting thing we're going to see in the world of AI meets bio in 2025, by 2030, and by 2050? My hope is by the end of the year I mean and this is already happening right we can you know just we can design full IGG antibodies right not single chain binders like nanobodies but just the real antibbody medicines that you know we kind of know and love today that we can just design their CDR regions they're going to bind really well um you can oneshot it and you can kind of do point and click on that you know that surface of your enzyme I can just bind it right you know one chat. I think the thing that will mature over the next couple years is that we can actually design enzymes denovo. I think that will be that will be really interesting and you know also lots of efforts but and again this is all in the world of proteins and I think one of the things that mo most people who think about this stuff are very protein coded and so a lot of our our our work is to sort of zoom out from proteins and think about cells right and so I think building the the sort of the the PDB of virtual cells right is something that we've been focusing a lot on at ARC that will take some years from today to mature. Uh so PDB is a protein datab bank right and it's the sort of gold standard database of atomic resolution solved experimentally solved protein structures that was used by uh you know deep mind to train alpha fold right and so it's the pre-training data that allows the model to reach some soda capability like protein structure prediction um at uh angstrom resolution right so you know what is that for you know virtual cells which we think would help us design better drug targets increase therapeutic probability of success, right? Um I think you know that will that's sort of my 2030 uh sort of prediction is that we have you know accurate and useful virtual cell models that make uh cell biologists feel emotion right um the the 2050 idea and hopefully this happens much much sooner than that you know I think you know there's lots of chat about scientific super intelligence or you know the the endtoend kind of uh recursion of the scientific method Um, I'd like to see that right with, you know, lab in the loop with a fully automated wet lab that's vertically integrated. Do you think, do you think it's possible by 2050 we can simulate with 99.9% accuracy, you know, the impact that a particular drug is going to have on a particular target, you know, validate that in a wet lab in a fully automated way in a matter of hours, not months. like what do you think the dream scenario is for going from zero to impact you know in the future of drug discovery if you imagine 25 plus years of technological progress yeah I mean I think we've laid out different aspects of the vision over the course of our conversation today um where things are really slow right like toxicity long-term follow-ups right these are the you know it depends a lot on the disease right if you're doing some acute ute oncology thing that's very different from some really chronic autoimmune thing, right? And so I think the only way that I can imagine you speeding this up is if you have a model with strong predictive power, right? And you know, so a lot of this hinges on our ability to make models that can actually do that. Yeah. And I think you will unlock different step sizes of capability based on how good they are. Is there any reason we wouldn't have that by 2050? Um yeah, basically if we make the wrong data I would say is like one obvious one. You know you can model the mouse in all of its glory to great perfection. It still will not be the human. That's, you know, one one sort of, I think, you know, trivial example that is something that we still do though because that's just what's practical. And so I have this other soap box about how we actually just need to be doing way more experiments in humans. And what do you think it will take for us to be able to do that? Is that just a regulatory thing? is that I think there will be some aspect of creativity involved in addition to better regulatory innovation. So, so one example would be you know there there there are these uh kind of you you can take samples from brain dead patients for example and then now you can just get lungs that you can peruse and keep alive for a week and then just do experiments in that lung right and you know there there was a paper recently published about this should we do lightning round yeah let's do it first one favorite new AI app in that you've tried in the last three months so this is maybe cheating but I'm a DIU of uh Open AI deep research, right? I just I I find it just by far the main AI app that I find useful enough to use in my day-to-day work, right? And so there are lots of other fun AI apps or things that I pay attention to because I'm interested in AI, but the thing that actually changed how I work is uh these deep research models. And they have, by the way, so much more room to improve and to run. Yeah. And uh yeah, I think it was the first time I felt real emotion thinking, "Oh, wow. Okay, someday maybe I will be automated." Yeah. Yeah. I feel that every day. Um who would be on your Mount Rushmore of scientists? Um this is maybe a bit smarmy. Um but it's the the folks I get to work with uh at Ark. I I I I realize this is incredibly smart me, but it's it's really genuine. I feel I feel so lucky to to go to lab every day and just be around, you know, just passionate, bright, kind, and incredibly ambitious people. Yeah. And it it levels up my game. What do you think is going to be the killer application that scientists will use by the end of this year? Deep research. Sold out. All right. nothing that you guys are going to create for work. Um well, I think these virtual cell models will be incredibly useful. Um we I don't think we or anyone will have working models in the sense that they I I think I think it'll take some time for them to mature to the point where they're actually fundamentally useful. Right? There are currently research problems that will ripen um over over some time. Yeah. but we'd like to deliver those. What's the most important thing you've learned at Ark Institute? So, there's a Scottish proverb that I'm gonna butcher um when I paraphrase uh but it's basically, you know, you know, be happy when you're alive for you're a long time dead, right? And that's real. You know, I think that that really hit for me when I when I read it. Um and you know, it was one of those, you know, you're lying on the couch, the phone is six inches from your face, just beaming lux into your eyeballs right before you're supposed to fall asleep. And I don't know that just it it reminded me that despite all the complexity of, you know, trying to work really hard to do useful things, you're supposed to have fun. And I think that should I think we need more of that in life, not just in research labs where I think it can be so easy to be super critical because that's the training and how you make progress is to hate on everything and everything has a problem and figure out why this can go wrong. But being optimistic and happy is not a path to mediocrity and mistakes, but it's actually how you have the emotional capacity for persistence over time to reach those those long-term goals. Love it. That feels like a good place to end it. I know. Well, hopefully this podcast made you happy, too. Made us happy. I'm always happy to see you, bud. Thank you again for doing this. Thanks, guys. [Music] [Music]

========================================

--- Video 16 ---
Video ID: hnBoab2-2ao
URL: https://www.youtube.com/watch?v=hnBoab2-2ao
Title: A Better End State than AGI? Replit CEO Amjad Masad on 1 Billion Developers
Published: 2025-04-08 09:00:51 UTC
Description:
Amjad Masad set out more than a decade ago to pursue the dream of unleashing 1B software creators around the world. With millions of Replit users pre-ChatGPT, that vision was already becoming a reality. Turbocharged by LLMs, the vision of enabling anyone to code—from 12-year-olds in India to knowledge workers in the U.S.—seems less and less radical. In this episode, Amjad explains how an explosion in the developer population could change the economy, society and more. He also discusses his early days programming in Jordan, his unique management approach and what AI will mean for the global economy.

Hosted by David Cahn and Sonya Huang, Sequoia Capital

00:00:00 - Introduction 
00:03:00 - A billion developers 
00:07:18 - How changing coding changes the economy 
00:19:50 - Making a coding product work on mobile
00:24:20 - Early insight on the impact of LLMs
00:28:57 - Launching Replit Agent
00:40:43 - The landscape of coding tools
00:49:12 - Vibe coding
00:50:41 - Functional AGI
00:54:50 - Growing up in Jordan
01:07:15 - How Amjad manages Replit
01:11:06 - Weirdos and misfits
01:20:24 - Lightning Round

Transcript Language: English (auto-generated)
you know um what is special about humans and what's uh replicable in in the machines and at least in the in the near term um my view is that AI is going to get really good at um uh sort of two things that are highly represented in the data and things that you can construct a very good RL environment for. So what what can you construct a great RL environment for? Um like obviously with Alpha Zero games right uh games are famous for you can you can have these self-play sort of algorithms that develop over time. Um now with reasoning models um you know math is an environment that especially with lean uh it's like a code uh almost like an expression of of math that can be executed uh that that's like a great you know RL environment I think code execution as well so running the code and and then you know um doing reinforcement learning on that um and and things that are already represented on GitHub and and things like that. Uh but but there's a lot of other domains where we actually still don't know what we're how we're going to make them better. Like uh new fundamentally new ideas, new knowledge. Um it's not entirely clear how we're going to get there. You can't can you use RL for these like more software things? Perhaps you create a word model. You can approximate these things. But I feel like the sort of the ideas um and the creativity and the sense of like coming up with really novel things and understanding the world world in very complicated intractable way and you know coming up with an idea that could fundamentally change how things work or change the world I think will still have be the domain of the of the human. [Music] Hello and welcome to Training Data. I'm David Khan and I'll be the guest host on today's episode interviewing Amrad Masad, the founder and CEO of Replet. Amrad's vision of the future is a world in which a billion people on the internet become developers. And in today's episode, we imagine how these billion developers will reshape the economy, society, culture, and more. A lot of people talk about AGI as a utopian vision of the future where people aren't working and there's universal basic income. But what if there's a different vision of the future? What if people are working, they're working as developers, and they radically change segments of the economy that we never thought we could revolutionize? Things like healthcare, education, industrials, and more. That's the topic of today's episode. Enjoy. I'm Jad. Welcome. Thanks for coming on the podcast. Thank you. My pleasure. You and I have known each other I think five years now and I had the chance to invest in you four years ago at CO2. So I've been able to see the journey. One thing that's been true for you since the first day I met you and I think for maybe a decade even before I met you is you've been talking about this idea of a billion developers coming on the internet which is a bold idea. Maybe it's gotten more consensus as AI has come around. Maybe tell us a little bit about that idea. When did that intuition first hit you and how has that journey evolved? Before we get to that, I remember I actually remember the first time we met. It was in our Bryant office which is actually kind of a home a loft. We're in the basement sitting downstairs and so we worked slash lived in this like really small place in San Francisco. Um and um in terms of uh the billion developers, it's just like you know uh ever since I uh I was a kid, I started programming really early on. Like my first experience with computers was uh when I was 6 years old. Um and like by seven I was trying to make things uh with it. The first program I made was for my younger brother to learn math and I've done such a good job at it that he works at Replet today. Uh so um uh it just always felt like making software is the natural thing to do at a computer on a computer and I was actually surprised that this is the domain of the expert as opposed to this thing that anyone can do. Um and uh you know through thinking about why is that the case uh it just felt like a lot of tools were complicated. Actually they were getting more complicated over time. So if you think when when I was a teenager kind of building a business, it was Visual Basic uh and I can like you know make an app and Visual Basic with database and everything kind of shrink wrap everything into an EXE and and sell it. Um and then the web web came along and it just felt like a lot more complicated. Uh it was more powerful. You can deliver things over the internet. And then the uh complexity didn't stop. Like if you think about a JavaScript application today, um you know there's there's a lot of things you need to do. You need to spend perhaps hours uh if you're new, you might spend days kind of setting up the development environment and learning all these asteric things like you know what is webpack and what is transpolation compilation and all that. Um and it just felt uh kind of worse than when I when I started. Um and there was kind of no reason I I I didn't feel like there was an intrinsic reason for that. there's all these perhaps social phenomena that made it so that programming is uh a lot more complex and one is this decentralized nature of open source when open source took over and it wasn't Microsoft product managers kind of designing how programming should look like but I felt like even then you can you can have you can have the open source ecosystem being this decentralized innovation machine uh but you can h you can create um experiences on On top of that, by mixing the best of the open source to create amazing experiences. Um, in the old language of of open source hackers, there's the cathedral and the bizaar. And the idea of the bizaar is this complicated mess. And this is where, you know, open source software lives. And the and the cathedral is like something like Apple or or Microsoft where where you're designing it top down. Uh, but I was like, well, that's a kind of a false dichotomy. We can build cathedrals from bizaars, right? uh and and and that's been kind of the driving motivation for for replet and I felt like okay if you make programming something that um more people can do by removing this complexity a lot more people would want to use it and the nature of computing changes uh because no longer is there's this big divide between what it means to be a developer and what it means to uh uh to be a user of applications which was the original vision for computing Um, and so this was the drive behind that. And it just, you know, also the opportunities presented by being a programmer is is kind of amazing. And I'm sure we'll get into my story, but like the fact that I was able to make all this money uh when I was a kid, was able to get an 01 visa to get to the US and I felt bad opportunity could be a lot more accessible to people. Maybe take us just to start like take us 10 years into the future or I don't know, you tell me how many years from now it is when there are a billion developers. what does the world look like? How does the economy look? I mean, I think you have this sort of imagination about all the ways that we're going to build software and the ways that businesses are going to be built are different. And to me, it's it is somewhat of a utopian vision of the future. I know tell us a little bit about what that looks like. Yeah, I try not to be too utopian. Uh, but a a few things on that. Um, one is the nature of if if essentially anyone can program and most knowledge workers would want to develop or make make applications or solve problems using um using software and AI. Uh the nature of what it means to be a company kind of changes. uh because if you think about companies today, we have these roles and we have these silos and kind of the way companies are structured are based on the sort of factory pipeline from the sort of industrial revolution. Actually, if you look at society today, a lot of it hasn't really been updated since uh since the industrial revolution. So the main innovation slash sort of work innovation from that era is the pipeline is the idea you know you do this one thing and then you pass you know whatever you're making to the next person eventually there's a car or a toy whatever at the end of the uh at the end of factory chain and you know society is sort of designed but that's like the main design principle that we have and so you look at the school for example you start at like you know kindergarten you go to first grade second And it's like everything is like kind of created like that. And even in companies it's like uh oh you have the you know the you know the product manager you know creating a PRD and then it goes to the designer and then it goes to uh an engineer and then it goes to a release engineer and then the product is out there goes to marketers goes to UX researchers and then uh and so always the silos and the and this pipeline model if you have journalists that can make uh that can make increasingly more complicated things and can use computing to to its true core to solve problems. I think I think you're um you're going to have people in the organization that can solve problem across the board. So if you're someone in sales and you want to uh you want to you have an idea to drive more sales, you might spin up uh sort of like an SDR agent that does this one specific thing that's based on an idea that that you have. There's no there's no this separation. Oh, well, I need to go to my boss to uh to go kind of uh hire someone to do the SDR. You can actually spin up an agent that does that exact thing that you want to do. Um perhaps maybe you're on a customer call and a customer is asking uh how can I do this X Y and Z with your with your product and perhaps your API or SDK. Now traditionally you have to go back to engineering and you have to ask them how to do that and you have to kind of but in this world you can spin up something like replet agent and you can say well yeah prototype this thing for me and show the customer in real time by the way all the stories that I'm saying are real things uh from our customers and so they uh you could think about it as just a a company is a set of uh generalist problem solvers solvers and you see that in startups right we see that in startups but I think that's going to be the pace at scale. Um the other thing is um the way we construct software uh I think will change. So if you think about uh how we construct software um again when you look within one company you see this like sort of fact theory pipeline but you also when you look at uh the economy in general you also see the same sort of separation between different companies and how the products get made. For example, the supply chain, right? Uh in software, we have some sort of supply chain where you have like a, you know, database company and then you have a sort of infrastructure hosting company and then you have a you know whatever front-end uh company that's delivering actually goods and services. Um and and I think uh the going from the industrial age to the network era, I think that the way we construct thing will become more like a like a network. Um and so uh you know you can imagine the way software is constructed is if we have something like uh crypto, bitcoin, um stable coins, things that are able to lets people to transact uh without having to know each other, trust each other. You can imagine software being constructed where I am uh sitting in front of software agents and I'm going to say okay well I need to to create this product uh and the agents is going to be like oh well I'm going to go grab this database from this area this you know thing that sends SMS or email from this area and by the way they're going to cost this much and as an agent they actually have a wallet I'm going to be able to pay for them and when I'm going to publish my software my software is monetized and whenever there's like a dollar that comes in it kind of flows it through the entire network. Um, and so there's this ambient services that I'm that my my software agent is able to compose um without necessarily having any sort of centralized system. And all these services are operated by hackers and people that are making money on the internet. Um and so so I think I think the nature of software itself and the nature of companies and perhaps the nature of the economy would change when everyone can be a generalist uh software and AI agent creator. Yeah. Maybe taking that last point on the economy to its logical extreme macroeconomics itself changes in a in a world where everyone's a developer. You have all these stories. A lot of your users are abroad. They're in other countries. You have users in how many countries do you have guys users in now? Uh every country. China is is is uh is the you know is harder but we have some some users there but basically every other country so how do you think like the global economy changes the way the economy works today the people you're empowering how does this change how these economies work so I I think one concrete thing um I think Petal talked about this paradox of um uh the internet where the internet was supposed to be the great equalizer yet it centralized all the wealth in Silicon Valley and this this one one area. Um, and so why is the case like this this thing that is purely virtual could have been and perhaps should have been uh much more decentralized. Um, and there there are all these things you can say about network effects and and and things like that. But you know ultimately I think that uh part of the problem is yes we think about this technology as accessible but it's not as accessible as we think it is. Um, for example, there's um a university student in India that uh was living in some uh rural area, but he was going to school to study computer science and but didn't have a computer or laptop. He had an Android phone at home. Um and he uh he would go and replet and um start programming and learning how to code on his phone because we have a mobile app. Um and then he started picking up u tasks from our bounties uh platform. So bounties is the ability for people on our platform to provide uh services uh to to to others platform. Thinking behind it is like we have a lot of people with time then we have a lot of people with money and no time. So a lot of people with time and money and it's like obvious trade. Um so a lot of people learn to co platform and they want to earn. So we want to present opportunities for them. Uh and so we we had an entrepreneur here in the US um build actually a technical recruiter uh building a recruiting app but he was running into problems uh and he was able to go on our bounties platform hire this kid and for this kid to kind of fix some of those problems and for this entrepreneur to be able to ship his applications. So and that kid made money made more money than his entire family would make for an entire year. Um and so yes, the opportunities will will get distributed. Obviously, this is more providing labor services, but you can imagine it actually creating a company uh and creating something that could that could be scalable uh and that way uh perhaps uh that this uh massive wealth generation machine that we call the internet will be accessible to more people in the world. Yeah, that's amazing. When I think about your vision, I think it's fundamentally this empowering economic vision. I think it's a vision of wealth. It's a vision of prosperity. It's a vision of giving opportunity to people like and we'll talk about your story, but people like you when you were a kid. Um I'm curious to question to you like what do you think is the macro because you know you're more of a finance uh brain. Like what do you think the macro implications of of everything that we just talked about? I think that if we go down this route of having a billion developers, the question is what are all the sectors of the economy today that are not techified? If you will, they're going to get techified. And so you think about healthcare, you think about education, you think about these enormous percentages of GDP that fundamentally have not been touched by Silicon Valley and touched by Silicon Valley technology. And the thing that I think about is well the barrier to entry was high for these industries. And so in order to build something for these industries, you had to go sell to them. It was difficult. It was tricky. And so what were the lowest barriers to entry? Consumer. You and I can go download Facebook on our phone. And e-commerce. We want to buy things. And it's pretty easy to make money buying things. And so you think about social and e-commerce as sort of the initial engines that kind of got going as the economy was techified. And this is where the you think about the wage rate for a software engineer is being a proxy for how valuable software engineers are. Um and so the wage rate has continued to go up. It's sort of one of these surprising things. You would think that as the more supply of software engineers came into the market, the wage rate would go down. And what that tells you is the value that software engineers drive is actually higher than the wage rate. Mhm. Um, and so I think companies like Replet and as you see this next wave of developers come, you're going to have this incredible prosperity from the value that they're going to go create. The thing that excites me is what are all the sectors of the economy where we can go create this value such that the kid in India who's using replet is going to extract some of that value and create prosperity for his family but also such that the consumer at the under end of that experience call it a healthcare experience call it a government experience whatever it might be is also acrewing value and so we've seen that value creation but to the extent that there's 100 million people I think who've created GitHub accounts today and there's going to be a billion and so you have this incredible will increase. Um I think there's a lot more to come. There's also like a cultural impact, right? Like where uh Silicon Valley, you know, as much as we have this global view of the of the world and we can uh because we have so many immigrants, we can actually relate to to a lot of those people, but there's limits to that. You know, I remember when I was working at Facebook, um, we were redesigning the photos experience on on desktop and, uh, we designed this amazing kind of vertical scrolling experience where everything was scrolling after the, you know, iPhone came out. Um, and everyone was excited about it and then we went and did an AV test and all the metrics tanked. Uh, it's like what is this? Did we create some crappy product that didn't didn't work? And then the UX researchers did did some tests and then nothing came back as not working. Everyone was was happy about the product. Then one product manager actually looked and dug into the metric and found that most like most like you know the majority of Facebook desktop users use it on the on the netbooks or like those um uh those laptops where it's like wider screen and they don't have a lot of vertical space. And that was mind-blowing to me where everyone in as Silicon Valley have these like amazing sort of MacBooks uh and uh and so so their their limit to how much we can relate and I think they the products that we build is often uh not as suitable to these culture and creates this flattening of of the world whereas when you have sort of innovation decentralized I think they're going to be able to create applications that are that are more local that can like benefit benefit their their communities and I think there's less of that today. Yeah. And you've architected your product this way. Maybe you could I think this is like one of the secrets of Replet that people don't fully understand is how do you make a product how do you make a coding product that's actually good on mobile, right? Is is sort of people think of this landscape and there's a lot of hard work that goes into that. How do you make it work on all these devices, right? Maybe talk to us a little bit how you how you've done that. Yeah. Well, initially I mean the the main breakthrough of the uh you know open source project that we become wrapped of the company was that I you know I I was the first to compile a bunch of programming languages to JavaScript to run in the browser. So this technology becomes WM later on. But you know uh I was on the ground floor. It was like a research project by Mozilla. It was called Ncriptton. Um and uh me and my friend were able to compile C Python to JavaScript using using this technology and we contributed a lot to it. We had to create um sort of a Unix emulation layer in the browser. It was like very complicated project. Uh but it captured people's imagination. We put it up on hacker news. it went super viral and uh people got got really excited about it. I remember one highlight at the time was Brendan Ike, the inventor of JavaScript, like tweeting about it. I was like, "Wow, you know, we're like kids in Jordan like building this thing and and people got people, you know, very important people got excited about it." Um, and at the time I thought that was like the the best thing. the the problem is um when I tried to load it on on on phones uh they would crash especially low-end uh like Android phones and uh I even wanted to work on like these Nokia Symbian phones at the time and it's because we were downloading tens of megabytes of of JavaScript. Um so it worked for a lot of people. people are excited about it, but the problem with client side execution is that a lot of people's machines are just not very good. Um and so when we when we went to when I went to uh work at at code academy and maybe I'm skipping a little bit in our story uh uh we we again we wanted more and more people in the world to to learn how to code and we started having users in Africa and other places like that and the computers would not handle tens of megabytes of JavaScript to download and then so I started building the sort of backend execution environment such that when you're what what you're using on your phone is really a thin client And any code you're typing is just going to the server executing and like coming back to to the client. And then the you need to make it so that you hyper optimize the JavaScript application such that it's very small. It does incremental loading. All of that stuff with React right now is really easy. But back in 2011, it was actually quite hard to do. We had to invent a lot of kind of web technologies to kind of make it make it work. Um, and you know, still to this day, like if you go to the app store and download the Raflet app, it's actually one of the smaller apps on the app store, uh, it's less than 100 megabytes, I think, uh, whereas most apps is on or of gigabytes. Um, why do you think that is like the IDE market is one of the I think the last markets to to move to the cloud. And if you talk to a lot of Silicon Valley developers today, they still want, you know, the local thing on their laptop. you know you cannot make me run this thing in the cloud whereas every other piece of software we've used people have eventually given in and you know Google sheets and uh Figma and all these amazing uh cloudnative companies why do you think developers have been kind of the last last market to move to the cloud what is the saying where like the shoe maker is walking around barefoot or something like that there's a at least in Arabic there's the saying where uh you know people who make things for other people are often not satisfying their own needs I think there's some of And I think there's some cultural aspect like you you know you can't really uh you know I can't really pinpoint a real sort of technical problem that can't be solved or can't be mitigated that prevents people from like coding in the cloud. I think a lot of it is cultural. There's this sense of control over the stuff on my machine. Um and you know there's this old kind of joke in programming where progress in programming happens one generation at a time. Uh so you need the old generations to sort of retire. So for the new generation by the way all the folks that are learn to code on on on rap code academy and things like that I think you they're coming into the market now they have no problem with with you know coding on on cloud products. I love that. Another question back to the origin of the company. Did you foresee that we were going to have LLMs and you know sonnet and all these amazing models to actually transform coding back when you started the company originally? So um when I was working on uh at code academy and then I went to work at Facebook uh and before that on on the open source droplet what I was doing is wrangling code. So I was writing compilers, transpilers, parsers um and those things are very fiddly and very complicated uh programs. Um, and it just felt like, you know, it just felt like this this laborious thing that like machines should be better at. It's like, you know, parsing this character and like, you know, putting it in in a node and trying to understand the structure of the program. And I read this paper in in 2012, which I think sort of is an underrated uh paper that should be a legendary sort of on on the order, maybe not on the order, but kind of similar to to attention is all you need. um it's called on the naturalness of software and and and this group they they're making the argument that uh NLP can be applied to to code and they make a statistical argument um about how code kind of um is statistically like natural language there there isn't a lot of daylight between them and so they build an engram model and they use the engram model to uh do uh completion and by the way you scale engram engram models like um engram models were the original language models and uh famously Google did this like trillion parameter engram model that was like pretty good at translating the very very simple simple things that's trying to uh you know predict the next uh character based on frequency y um so uh they constructed that thing and then it was performing well it wasn't performing as well as IDE completions but then they used uh static methods like IDA completions to rank the completions from the uh from the language model and the result was superior and programmers rated as superior and it actually did more completions than this uh static methods on their own or the classical kind of algorithmic methods. So at that point it just felt like okay we're you know that's the future like you know you know deep learning and things like that we're coming on the scene and neural networks and if this very simple thing is able to to to to to have such results I'm sure we're going to have better things and so try to do something like that uh but it didn't it didn't work really well at the time but actually when I pitched uh you know I think the Ced deck it is out there on the internet but one uh one uh one slide in it I try to do like the Elon Musk master plan which everyone does these days but basically it's like you know the Elon Musk master plan which really amazing uh it's like oh we're going to build the roadster it's going to be expensive we're going to use that money to fund you know the next generation and that'll create economies of scale and then so on and they did it uh basically it was like uh we're going to create uh this website for hobbyists learners and teachers and we're going to grow it that way and that way we're going to get a lot of data about how people use and learn programming then we're able to train machine learning models and it's going to be this like AI powered uh you know uh tool and then that would be a tool that's like more powerful than traditional software uh you know tools and therefore would let people come to the side learn how to code make things and deploy them. Um and that was like in 2015. Um and so every year I would try to do something with machine learning and uh the first time that I got a glimpse of something working was GBT2. Um and there was a lot of experiments at the time. uh you know a lot of the early community around GPT kind of had that feeling that okay this is working because you saw GPT1 you saw GPT2 it scale even GPT2 there was like two sets of weights one more parameters you could tell that scaling is working and so when GPT3 came it was amazing it was like you know groundbreaking event for a lot of us it was the chat GPD moment uh because Chad GD was not it was like a UI innovation perhaps some finetuning Uh but it was it was really clear that it was here and it was coming and I started orienting the company to take advantage of that. Do you want to talk to us a bit about replet agent? I mean that feels like a transformative moment in this company and almost like the whole company you got these millions of users you had this community and almost like the whole company was built for this moment. Talk to us a bit about replet agent. Yeah so um every year so we implemented co-pilot like features. were actually the first uh startup to uh train code code models outside of Microsoft and OpenAI and we open source them for a small time there were soda models there were three billion parameters um and and that was really exciting but the the thing that I thought would be transformative is agents uh I actually had a you know thread that went viral at the time in 2022 about how software agents will change programming and perhaps the economy um and so uh every every year we're trying I remember we tried it with GBG3. Um the context window wrangling that we had to do was insane. Like we had to um every time we generate like a class or a function or whatever, we'd have to like summarize it like into one sentence and put it back into context to be able to kind of iterate. Uh so that didn't work. uh 2023 you had AutoGPT and things like that come out and you could tell like oh the models are getting better at like you know like being coherent a little longer but it was it wasn't working very well. October 2023 I I I gave a talk uh a TED talk uh and I talked about how how in the future people make software and in it I talked about agents. I kind of predicted uh sort of test time scaling as well where I was saying like um uh you know there's going to be a trade-off between you know cost speed and accuracy and uh sometimes you want to like pour more compute or time into into a task um and you're going to have these plans and the agent will iterate on these plans and and build a software. You're going to be a human in the loop where you're the sort of the creative engine. It was the plan. it was the road map for what we're going to build and 24 I think around the time 40 came out um it felt like uh like we're almost there and so I I essentially put the entire company on that and in fact we actually did a layoff uh in in May 2024 partly to to kind of cut burn and focus uh but the other part is like I knew in my bones that that was the way to go and we were doing a lot of other random things that were unimportant Um, so between uh sort of March, the first time I got a demo from someone on the AI team, uh, Zen on our team, he he showed me something and I I could immediately feel like we're we're there. It was like almost like a baby a baby kind of agent. Uh so we spent uh seven, eight months working on it and then um it just felt like we were on this treadmill fixing one bug after the other, you know, trying to make it coherent. Uh the transformative thing that happened is like June 24 when Sonnet 3.5 came out and Sonnet 3.5 had two important properties. Actually, the main property that that people don't really talk about all that much is Sonnet was able to generate uh a uh like thousands of tokens like I think up to 32,000 tokens of code coherence code like so you can it can oneshot a repo almost. Uh whereas with GPT40 when you're doing agents you're writing one function at a time and then testing. If you remember the cognition demo, it's like, oh, they write one thing, they go to the browser, they write one thing, they go to browser. And that was like that was intractable. That was very very expensive. And we thought that's not not going to work. And the thing that sonnet wrote at the time is like writing high quality code and not being lazy about it because the problem with OpenAI's products were lazy. They there's a real problem laziness where it's a it it like you ask it to do something and and and at one point it adds a comment and says the rest of the code here bro I asked you to make that program why are you asking me slash it is like a human yeah and uh so so it was this uh huge unlock and we felt like we're going to be able to ship soon uh you know there's always this tension you know the quality and when do you own a ship? And so we set um we set a deadline for my birthday uh September 5th and and and it was so contentious actually at least one person in the team kind of quit because they thought we're not ready and he was partially right. Um and we launched uh agent and there was a lot of excitement because really it's the first software agent that works on the market uh the very first and and people got really excited because uh oh you can try this thing and you can get a glimpse of the future. Oh this thing can bring it in a database and like provision it for you. It can run a migration. It can like deploy my app. Um but then as it escaped the kind of early adopters uh it was you know a lot of people were disappointed because it was actually kind of crappy and uh so we sprinted between uh September and then uh December we had something we're really proud of and we exited the beta um and although we're we're growing well then we we grew really fast uh from there and people got a lot of value out of it. Now we have version two coming and uh it's rolled out to to some beta users and uh we've done AB tests and it is um it is anywhere depending on the metric we're looking at between 50 and 500% better. Wow. Uh so uh it created 50% better daily retention, 50% conversion, 50% better uh engagement. But the the metric that was really great was um uh new users are twice more likely to deploy an app that they made with agents and um power users are you know five six times more likely to deploy an app uh that they made uh and deployment of replet is very very important because that's one of what makes replet great is that you can go from an idea to a deploy thing and we see uh people who deploy are 10 times more likely to retain on on replet. So that metric we we track very well. So now I look at V1, I'm like this crappy thing like we got to we we got to ship V2 as soon as possible and I'm like on the team and so we're starting to roll it out now and it it's it's really really good. Can you give us some of the intuition for how you've gotten it so much better because I imagine it's still Sonnet 3.5 that still seems like the sort of model that everyone likes right now under the hood. like how have you maybe what's the behind the scenes for how the agent actually works and how you've gotten it to be so much better when the base models are very similar. Yeah. So um at the 3.5 sort of era the um uh we had to build this like a fairly complicated multi- aent system and we're talking about langraph earlier where you know there was a state machine component to it. Um, and there's a lot of non AI uh uh state transitions that we're having to make. So that the more you put into the eye, the the worse decisions it often makes and less coherent it is. So, so context window was still very important uh because there are actually studies showing that although you know these models are uh kind of advertising a million tokens actually after something like you know 32,000 tokens reasoning in in a lot of benchmark just like tank like crazy right um and so uh there's a lot of context window wrangling and so we had a manager and it had a different context than the editor than the debugger because we had to kind of um make sure they they're isolated and they have their different memories and and things like that. Um and you know the tooling between all these things had to had to be different. We built our own sort of protocol for the agent to call tools because actually tool calling was not very reliable at the time. It would like hallucinate tools. or hallucinate arguments or so there was a lot of engineering behind it to to make it work when uh computer use uh from anthropic came out so that was 3.5 v2 the moment we laid our eyes on it we felt that there's something important that wasn't getting marketed that was uh that was start of a of a of a transition which is they actually it looks like they actually fine-tuned it for long uh for uh long horizon horizon in reasoning because if you want something to do computer use, you're going to have all these images and context and it's going to have to kind of continuously like click on things, reason, think about what to do next, click on the other thing. And so you can actually roll out a long uh chain of tool calls uh without that big of a degradation in in and reasoning. Um and so we started rearchitecting the uh the model of the agent to be uh uh let's call it um less less multi- aent and more single threaded uh because the because the models are getting good at it and that's like a much bigger simplification. It added other challenges, but it's a much bigger subocation over the the existing model. And so kind of the lesson to to learn is that you have to constantly rewrite the systems because you want to make use of the next version of and you want to be able to predict what's coming down the line. When 3.7 came out, uh, you know, cursor had a big problem with integrating it. Everyone hated it on cursor. I think just they just recently fixed it. And the reason is because again 3.7 is more agentic. And so when you try to use it and this composer style request response, it is not very good at that. It's actually worse at that because it's trying to kind of make decisions that but you're not giving it the space to to kind of do this looping. Um and and so and so you there's a really kind of tough lesson for engineering and product teams. If you overoptimize for the present capabilities, you're gonna enter a local uh maxima that is uh quite hard to move out of. There's like an innovator's dilemma problem on the order of months. You know, innovator's dilemma used to be like on the order of decades, right? Uh where you know, a company has an innovation, reaches a certain uh height of success uh based on a product that they made that's people really love. But then there's a disruptive technology and they don't make use of it because it might be destructive for their current business where it cannibalizes their business and so they tend to not pay attention to it and fight it. The classic example is like um Kodak had a digital camera product but they didn't actually launch it because I was going to cannibalize their film business which was the thing that's making the money actually. Um and this kind of thing is happening on the order months which is really hard to wrap your head around. And so the the move fast break things is actually very very important today right more than ever and you you have to have you have to be okay with your with sometimes like delivering crappy experiences. You mentioned cursor. What do you make of the entire lands it feels like the landscape of coding tools has just exploded in the last six months and you know developers are reaching for for new IDs. You know many more people who didn't consider themselves developers before are entering the market. what's your view of the market landscape and how how do you think of as you know the ideal person that chooses replets versus one of these other tools? Yeah, so so I think um there are incremental uh innovations and there are sort of more uh disruptive clean slate uh innovations and I would put replet in the latter category. So um you to you take VS Code and you build an amazing AI experience on top of it. Really cursor is like a fantastic experience. A lot of people on our team use it. Um the the thing is it is by definition incremental. You took a piece of software that you know Microsoft has been building for over a decade and you added this you know much better experience on top of it but you're still you're still kind of it is still literally this additional layer. uh with um with rap agent we actually took a clean slate uh approach to that and I was like okay what do we need to make it that people aren't coding at all uh like you know we we want to kind of drive towards this vision of of no coding not only no coding no DevOps no it like we don't want you to set up a database we don't want you to write migrations like writing migrations is the worst thing ever like you know if people don't talk about this but one of the worst things about building software ware is being able to keep your version control, your database schema and your you know sort of deployment settings and configuration in constant in lock step. Um and most actually most outages when you read the postmortems from Google cloud or AWS it's like a configuration error and usually it's outdated configuration the software changes but they forget an environment variable configuration. Uh and so when you have an agent actually do these things is actually much better at at doing that. And so the system that we built for the agent is this uh sort of transactional lock step system that um does you know uh these transformation one at a time. When you hit revert and replet it actually uh reverts the code but also reverts the database changes um and other environmental thing environment variables things like that. So uh there's all these things that we thought about where uh you as a as a developer coming to replet the thing you have to worry about the most is your ideas. Mhm. you know, you don't have to worry about where do I get object storage and how do I configure my buckets, right? Whereas when you're sitting in cursor, um there's either someone in your company, a backend engineer worrying about these things and you're basically hooking into these APIs or uh you're having to build them from scratch and like it it would help you hit those APIs but you still have to architect the larger system. So I think there's like a fundamental category difference between these things. Uh these products are hooking into existing systems and require a lot of existing support around these systems whereas replet is trying to be the final tool you have to adopt in order to build a piece of software. One thing I've heard from users when I talk to them about replet agent and deploying on replet, one of the things that people love is that you can go straight to deployment and that replet does the whole thing. You start from nothing and you end up with a fully deployed system. Can you talk a little about that? How do you think the deployment space plays into this? You guys in some ways are not comparable to a lot of these other companies in the sense that you can go all the way on replet. How do you think about that last mile? Who cares about that and and how does that play into the long-term vision? Yeah. So, um, in the same way that I can say that cursor helps you with coding, doesn't help you with the other stuff. Versel helps you with hosting, doesn't help you with the other stuff, right? And again it's like this back to the cathedral and the bazaar is sort of like a bazaar and you need to kind of find all these tools and configure them and figure out whereas uh sort of replet I mean another sushi analogy there's alakart and there's alicas replet is alakas we're going to make we think we have great taste and we're going to make the design choices for you and it's not for a lot of people if you want to make all these choices don't come up with I mean there's a lot of other products out there So um I in terms of like the deployment system the uh if you want to reach a billion people eventually like making software at priority uh most of them will not know how to set up a deployment environment and therefore the replet product needs to have a deployment environment basically had that's how we approach product in general you know um in the early days of replet there was always this narrative of um it's students it's young people as people learning how to code. It feels like in recent years that's really changed and maybe AI has enabled that. I was talking to a Netflix engineer who's telling me about how he's using Replet. Paul Graham has obviously always been a big advocate of Replet uh and one of the early believers in the company. How do you think the user persona has changed and how will it continue to evolve over time? How amazing of a visionary is is Paul? Like how can you look at a crappy uh toy like Raplet and just see that oh that this one day could could be really big or the idea that people are leaving uh you know are sleeping on mattresses and other people's homes that's going to be like$100 billion dollar company. Um it's incredible. Yeah, it is it is fascinating. Um in in ter he's also incredibly good human and he's been uh very supportive uh during very difficult times and in the company when when others uh weren't. Um it was frustrating for a long time that like oh Replet is as is a toy a hobby toy students you know kids teenagers on the one hand I was excited by it because like we had a group of users that are willing to experiment that are willing to kind of try to find the future of software the future what we just talked about of you know being able to have a economy built into the software infrastructure and we tried a lot of that and I think it's going to work we still have like the main missing thing was the agent but all these things are going to work and there's this co-evolution that happened with these users that was was very important. The other thing that that perception was important because um sort of competitors weren't paying attention. Uh they looked at rep like oh that's a toy thing why would I pay attention to it? uh a very close uh uh friend of mine and someone who's uh looked up to in the in the ind industry visit us the other day and uh visit you know a lot of our friends um and he he was telling him he was using replet to kind of prototype ideas for a new company and everyone was like well isn't that isn't that like the toy thing that I'm working on was like well no it's very useful for me and I you that's how I'm able to iterate on these ideas very quickly so now yeah the perception is is changing Um and partly look I mean we we took explicit decisions to make the product more premium. Actually what's fascinating now is a product replet is the most expensive product on the market. Uh with cursor composer I think one request is 4 cents with replet one request is 25 cents. Uh and the reason we did that is like we just we really want to build an agent. We don't want to build codegen tool. And if you want to build an agent, it's going to be expensive. It's gonna iterate and and um it's going to call a bunch of tools with every request. Um and so uh it did alienate some some users and that partly uh also shifted the perception and and kind of more professionals uh got on board. But if we're going to reach a billion people, we need to we need to go down market again. But I think you know I think we started in one end and now we're starting in another end. Uh and we'll meet in the middle I guess at some point. But I guess that replet is the sort of the more roadster now and as we drive cost down by you know perhaps using more open source models and and things like that it's going to become more and more accessible to people. What do you think of vibes coding? I don't really like this term. I think you you shouldn't fight it because uh I actually didn't like Gen AI either. It's just I don't know. Same. Yeah. I don't know. It just like cheapens the possibilities. Yeah. It's just like oh this thing that generates things. Well, actually you can build agents and not just generate things. It can actually reason. Um and vibe coding is um makes sense if you're sort of starting from a position of coder and you're Andre Karpathy and you don't want to kind of worry too much about the code and you keep hitting enter whatever but if you're if you're starting with rockload you're actually not starting from a position of code you're starting from um an idea you're starting from an idea that you're iterating on and uh and then you go in and the agent is unfolding this code in front of you. You actually when you're using replication, you don't have uh the luxury to to to look at the code. So, so uh now we have another product called assistant and assistant is for more advanced people and that you can do more bite coding there because it's like request response and you can like kind of review the code and and do all of that. Um, if I were to explain rapid, it's just vibe. Like don't code vibe. Not vibe coding, just vibe. You've always been a bit of a contrarian. One idea that's very popular now in Silicon Valley is AGI. There's this idea there's going to be no software engineers. And you sort of have the opposite view. There's going to be a billion software engineers. Talk to how do you think about AGI? What is AGI in your view? And uh, how do you what do what would you say to all the people who say I talked to a lot of software engineers and they're like, well, I'm not going to have a job. I'm worried about not having a job. I'm worried about no one's going to have a job. We're all just going to be on universal basic income and all this stuff. Like Replet is in some ways the opposite vision, right, of like we're actually all going to have jobs. They're just going to be very powerful and we're going to be able to do all these amazing things. I I think it's like a fundamental is philosophical difference. uh like what you know um what is special about humans and what's uh replicable in in the machines and at least in the in the near term um my view is that AI is going to get really good at um uh sort of two things that are highly represented in the data and things that you can construct a very good RL environment for. So what what can you construct a great RL environment for? Um like obviously with Alpha Zero games, right? Uh games are famous for you can you can have these self-play sort of algorithms that develop over time. Um now with reasoning models um you know math is an environment that especially with lean uh it's like a code uh almost like an expression of of math that can be executed. uh that that's like a great you know RL environment I think code execution as well so running the code and and then you know um doing reinforcement learning on that um and and things that are already represented on GitHub and and things like that. Uh but but there's a lot of other domains where we actually still don't know what we're how we're going to make them better like uh new fundamentally new ideas, new knowledge. um it's not entirely clear how we're going to get there. you can can you use RL for these like more software things perhaps you create a word model you can approximate these things but I feel like the sort of the ideas um and the creativity and the sense of like coming up with really novel things and understanding the world world in very complicated interactable way and you know coming up with an idea that could fundamentally change how things work or change the world I think will still have be the domain of the of the human and AGI will have AGI but it would be a functional AGI meaning it would do the jobs that h a lot of humans are doing today by virtue of the training data being available and by virtue of some of these jobs having like ground truth that you can uh you can train on uh and the reason I call it functional AGI is because it's fundamentally not general in that you can throw it in a uh super novel environment and for it to uh efficiently learn uh things especially when they're you know when there's not explicit feedback um and be able to uh to be successful in that environment which uh which which you know the definition of the universal AI uh which it doesn't feel like we're trending that way but I do think that you can reach something u so the definition of AGI at a lot of these companies is doing economic ically useful activities in front of a computer, right? I mean, they it's like a remote worker is what AGI I feel like we're going to get there. Uh but um if I have a remote worker, I'm going to like create a 100 remote workers and implement all my ideas. Yeah. And and I still it's a tool. It's useful for me. Is it going to replace me? Well, if I am like a code monkey, it's going to replace me. But if I see my place in the world as as someone who can generate ideas and create products and services because I understand what people want and how how the economy works and all of that. I think I think that's still irreplaceable. I want to talk to you a little bit about your life story. You touched on coming to the US, the O1 visa, growing up in Jordan. And maybe start at the beginning for people who don't know your story because it's a really inspiring story and in some ways your whole life has built up to Replet. It's not just a company. I mean your wife works at the company, your cousin works at the like this is like your whole life and your whole mission all in. Bring us to the beginning like how did how did this all come together? How did you get to the US? How did you get into Silicon Valley? Yeah. Yeah. So my one of my first memories um just as a as a child I don't know if you remember your first memory but I remember very vividly that my father uh kind of getting this this machine and like opening this this box and like putting it together and I was fascinated by it even before I knew what it does. And I I walk walk over him and I kind of look over his shoulder, him sitting at the keyboard and sort of uh finger uh sort of typing. He had like a big manual like you know you throw it at someone it could really hurt them. and he would he was reading into it but one by one and he was like finger typing um cd mkdir you know these dos commands and um and I remember feeling that it just wow this is like uh this is a machine that you can talk to like you know what is a doss it is like a rappel you know this is where uh the rapid name comes from read val print loop like you can uh have a essentially a conversation with this machine. Uh so my father would like go and like attend uh classes to learn uh computer computers and he would like you know every night he would come in with all these notebooks. Uh he's like such a such a big nerd. Um the apple doesn't fall far from the tree. I I'm different than my father. Um uh so my father is a Palestinian refugee. grew up uh with like this u intense focus on education. Like if we're going to make it, we're going to we're going to have to be like the best educated. We're going to have to be like better than anyone else. We're going to so is like a a students and working really hard. Um my mom is sort of the uh the opposite. My mom is like a sort of a free spirit. Uh she was a um into poetry. She taught me a lot of poetry uh when I was a kid and I was actually able to um it was like there's an art to telling um Arabic poetry uh and uh and so I was this mix of two things because I can be very intuitive. I can be uh I can go uh on purely in intuition and and take a lot of risk based on some kind of idea or vision or something that I that I feel feel good about. And I also have this like very analytical mind and I can sit and be be such a uh you know uh pain in the butt about about being so rigorous about certain things and that's that's really my father's uh inspiration and and so um you know you know one day my father comes home and I I'm sitting in front of the computer and he was he was mad. He put all his savings into this machine and and and I was like, you know, opened the computer apart, uh took the parts out, put them back in, and I was like, "No, don't get angry. I know exactly how to use it. I know exactly how it works." And I I showed him how to use it. I showed him essentially how to create things, how to open applications that he's been studying all this time. And he was like, "Okay, this is fascinating." You know, all right, it's yours. And the first uh program that I wrote, I think I mentioned it earlier, was to uh teach my younger brother uh math. And and I I thought, okay, I can use this uh this thing, this conversational machine, which I still think about it that way obviously with AI that that really happened. Yeah. And this idea that you can sit in front of it, learn something, play games, do something fun. Um and so uh that was the first program that I built. And then um when I was a teenager, I was really obsessed in um Counter-Strike. So I would go to these internet and land gaming cafes and I would like, you know, whatever money scraps of money I have, I would like put it in into that and just play play a lot of Counter Strike and and strategy games and things like that. I got very good at them to the point that it was like a source of income. I was actually winning tournaments and and things like that. uh you got into esports uh early on. So that's like another branch of my life. But um one of those one of the things I noticed uh about um uh uh about those businesses is they were running on pen and paper. I'm like you have all these computers you can just write software for it. So I did write this like client service software that did accounting that uh gave people you know accounts and user username and password and manage their their their time on the system and their um and also did security so that people can't format their computers or do you know install malware and started selling that uh and made a lot of money on that. I sold it to a lot of uh businesses uh in Jordan. Um, by the time I got I got to college, I had this idea that, you know, AI is gonna get so good, you know, we're not gonna have to write write software. Um, it just felt like um software is this like pedestrian thing that that that you do. It's like not that interesting. It's just like you just do it to like make things. And at the time like these these wizards of code generations were coming out from Microsoft. So when I went to school, I actually studied um more on the electrical engineering side, especially because my father thought that computer science was not a real field because the engineering association of Jordan would not admit you into the as an engineer if you don't have like uh you know electrical engineering. Uh and by the way, my father is the vice president of this organization. I didn't know that. I learned something new. He really loves that uh engineering organization. So but but throughout my college experience I got I got really into programming languages. I started reading Paul Graham started reading hacker news. Paul program wrote a lot on on lisp. He actually has a book called on lisp. Uh and pgram's view of programming languages is more of an art rather than than a science. Like these artifacts are aesthetic artifacts and not just functional artifacts. And that also played into the reason to create replet because I wanted to try all these programming languages and there wasn't a place on the internet internet to be able to try all these programming languages and you know uh after I created replet that breakthrough that I talked about earlier it went viral in the US a bunch of um tech companies started adopting it if you remember 2010 111 there was the MOO kind of hype you know we have AI hype now there's a like blip period where there's like to massive online courses, right? Udacity came out at that period. Corsera came out, Code Academy. Uh, and I got a bunch of um, they all started using Replet, by the way, and I got a bunch of offers. I eventually decided to uh, come to the US, got an 01 visa, uh, because a lot of my work was published in the news and everywhere else. And so it was it was it was possible to get an 01 visa. Landed in New York early 2011. uh the only money that I had was uh taken from me at the airport and the reason is in the uh airport in Aman uh I I had like perhaps $700. Those are the money that I'm going to the US with. And the uh folks at the airport did not know what a no one visa is. apparently like no one in Jordan had ever gotten no one visa to get to the US and they didn't think it was like a um a resident visa. They thought it was like a visitor visa and that I needed a ticket back and I was like no it's like I can go there I can work it's a work visa and they didn't believe me. I was like go look it up on the internet. They didn't believe me and they made me buy a ticket back and that was something like 500 $600. So I arrived there with like about a hundred bucks and my salary was $80,000 70 or $80,000 in in New York City. That's pretty big. Yeah. Well, not in New York City when your rent is like $2,000. I think at some point you had um someone offered to buy the company for a lot of money. How did you decide to turn that down? That seems like that was a pretty big decision coming from this background. You grew up, you made it to the US, you got this job, then you have an offer to buy your company for life-changing amount of money. Insane amount of money. We were like six people at the time and you know the numbers that were thrown around is between 500 million to a billion uh dollars. We're six people. Um and so it would have made me uh insanely rich, right? Um so uh and it was a tough time. My uh I wasn't really happy about the culture then like we grew to six, seven people or something like that from from the three that our family essentially. And um we hired people that I didn't like very much and and the culture was changing and and I kind of wanted to do a reset. Um and um at the same time uh my mom was diagnosed with with cancer back in Jordan and I had to go back the entire family had to go back which is half the team. Wow. um to spend time with with my mother. And it was very stressful time. And um and um the thing that uh made me which is like the absolute rational thing to do is stick the money, go home and and and um you know live I guess happily ever after or something. But um I felt two things. Uh one is my dreams, right? I I've had the dream of being in Silicon Valley for so long. Like the first time I knew about Silicon Valley is through a lowbudget movie called the the Pirates of Silicon Valley where it's the dramatized fight between Steve Jobs and uh and Bill Gates. And I was like, "Wow, this Silicon Valley place is like there must be like flying cars and like really you like mass advanced technology." of course you come here and it's like it's like the suburbs. Um but but I always wanted to be here. I thought the innovation I was really I was reading about all these entrepreneurs and I I I felt like this is the most important thing and those people are heroes. Um and I felt like the weight of Silicon Valley also on my shoulders because um Paul Graham, you know, Mark Andre and other people that I really respect uh invested in the company and like they all have like very high hopes for it and and and they they're all really excited about it and um and I felt like I don't want to let people down. Uh, and I felt like if I sold the company, it would have be it wouldn't be I wouldn't have achieved the potential of it and um and and maybe I would regret it in the future. And um and it's like, okay, being rich is is good. You know, I think I think money is actually great and and improves your lives in many ways. Allows you to focus on on the things you love. Um but you know, what are you going to be? you're going to be another, you know, rich Silicon Valley dude. And there's a lot of them, you know, going to like write invest do angel investing. It was like your life is kind of a little boring and and you you never kind of want to take the pain again unless you're El Musk to kind of like go start a start another company and put your life's force and energy into it. And so for all these reasons, uh we decided to to turn it down. Now you have 40 million users. Yes. and and uh you know we achieved the valuation over the uh what were we going to sold for and I think the company is actually underpriced now it's a lot of grit to get here from six people I want to talk a bit about your management style you have a unique management style I remember seeing one time on the internet you you said like I'm now the VP of engineering of replet I feel like over the years there's always been you're a hands-on leader your leadership style maybe has become more popular but it wasn't it wasn't popular six seven years ago when you were doing this like how did you develop your leader ersship style. What advice do you have for founders as they think about running their companies? In some way, it's a deficiency. You know, uh Jensen now talks uh talks a lot about uh him not being sort of a great manager and him having all these reports and only uh you know, talking to everyone in these big meetings and and being, you know, giving feedback publicly, not not really doing performance reviews in the traditional way and and all of that. Um and in some way I would say it is a perhaps lack of skill in terms of like how to do traditional management. Um the management style that I have is similar to like how I would like lead a open source project or how I would lead uh a sports team back home. I've always been a leader uh and I've always been a sort of a hands-on leader where it is this duality of uh micromanaging and trusting people like it is actually not at odds to to do these things and the most inspiring leaders both can go dig into the details and give very precise direction but then like really trust people to deliver on those things and also trust people to have their own innovation, their own ideas. Um the way I managed the company from the start is I had a um a text file inside rapid. I used it as a sort of a notebook. Um and the text file had everyone's names and the one thing that I think they should be working on or the one thing that I expect them to deliver on. And every week when I meet everyone, we would go around the table and I would tell them did you do this thing? Uh and it's either yes or no or something there go right or or something like that. And then the other question, what are you going to do next week? Uh, and so it's like, okay, they did that last week or they didn't do it. Why did they fail? What happened? And here's what you do uh what they're going to do the next week. By the way, my exacts still write that. Everyone in the company, every week on Friday, they write, "Here's what I got done this week. Here's what I'm planning to do next week." And so uh and so I can still keep in my head what most of the company can do uh or is working on. Like I can walk around and tell you this this guy's working this this this person is working on this. Um so partly is I can keep a lot of complexity in my head and I can like really be able to kind of um make these um these you know very deep decision about how like a sort of one button should work uh inside the product or or how uh certain marketing ideas we should we should run and and so I can go between all these different departments and and and be able to go all in into the details s and then kind of zoom zoom back out. Um, and also, you know, uh, just having really high expectations. If someone if week over week that person who said they were going to get that thing done and they they couldn't, it's like an obvious reason to let them go, it's like not that complicated. And so, Replet has an high attrition rate, especially in the first few months of people joining. um 20 30% of people are going to either leave or get let go because they're not able to keep pace of the environment or they get confused about how to work in that kind of environment. The other thing I think that's unique about your culture, I remember I attended some all team dinner at an all you can eat barbecue restaurant. I don't know if you remember this. And there's all these young people around the table and some of them didn't even go to college, graduate college. I mean, you seem to recruit for raw talent. Yes. And that's something that a lot of people talk about doing, they want to do, but it's hard to do. How do you do that? How do you filter people? How do you find these people? Um, how do you give them leeway? How do you train them? Talk to us a little bit about how that how that happens. Yeah. First of all, I am uh uh you need to be able to work with weirdos and misfits. Um, and I was able to do that whether it's instinctively or whether I relate to them being sort of myself a weirdo and misfit. Um and uh and I can see talents certain people like even back in in my college years like finding those people that like have hidden talents and being able to harness it somehow. I have this feeling of like um if if someone has a talent that is not being put to good use, I feel like a sense of waste and like you know this this needs to be harnessed uh somehow and I think that's sort of a good management skill. Um but uh in terms of so so first of all you shouldn't have allergy towards these people uh I remember uh my wife and co-founder Haya when we hired Mason he was like 18-year-old kid he um he was you know I joke that he's a runaway kid from from Santa Cruz which is partly true he left Santa Cruz uh in high school he wasn't happy he wasn't happy with his family with the school he went up to the city he went to um one of those boot camps uh those boot camps were using were using replet all of them and one of them was sending me bug bug reports I'm like look I don't have a lot of people who work on this why don't you send me your best engineers it's like look I'm going to send you this kid he's really awkward but he's one of our best so he came in and I remember ha one day was like oh this kid like doesn't doesn't open the door doesn't keep the door open behind him he like literally slams the door on me he's like well he's not really thinking about you he's like thinking back code as he's walking around. Right? So, uh and still to this day, uh I see people doing that. But, um the first thing is most people I think just reject these people outright when they can't communicate with them or they can't uh relate to them. Um and then the other thing like uh you know, they're going to spike on certain things and then they're going to be not great at at some other things, right? And so uh but you know you can construct a team where it all fits together. Uh where the the peaks uh of of someone is the values of someone else, right? uh someone is really good at, you know, shipping thousands of lines of code and someone else is really good at testing it and very methodical about code reviews. And you know, if you have one who's like a cowboy slinging code and another is like a little more careful uh and like meticulous and rigorous and perhaps a little annoying of of how good how meticulous they are, put these two things together, there's a lot of tension, but at the end of the day, you get you get a good good product. So you want to balance the team uh in a way um and uh you you want to go to to hire from places that uh other people aren't going to and you know that's that gives you an advantage because everyone's competing over the existing talents uh on the market. So for us a lot of it was go going to replet. A lot of it was running hackathons or running uh sort of prizes on replet and uh and you know some kids win and we fly them out to to SF to work with us and you know we've had anywhere from 16 17 18 21 years old uh you know join the team and even on the older side um uh people who haven't had uh traditional jobs before or were ND game hackers that that joined the team Um so yeah I mean it's it's a sort of a diversity of sources. If I am starting a company today I would try to like find um niche communities. Uh this is where you'll find like some really underrated uh talent whether it's a niche community around a niche crypto project or a niche um program language. Um that's where I would look first. You mentioned your wife Haya a couple times. Uh what's it been like building a company with your wife? Um depends on the day. Uh we um we've worked together uh you know we met at work uh back in Jordan. We were working at this uh company where recruited um actually a foreigner came to Jordan as part of a job from Belgium and he decided that he loved Jordan a lot. He loved the desert and he wanted to build a company there. So I was the first employee there and Haya was the second. She was a designer. I was an engineer and he pitched us on this idea. We're going to do consulting and then we're going to build a product on the on the side and that and then we're going to become a product company. By the way, every company that has his idea never becomes a product company. I'm sure you've seen some of them. Um and so uh the company was very dysfunctional. He was nonpresent. And so it was like the inmates were running the asylum. Uh so I and I ended up like working on a lot of projects together just just for fun. And um and um and then we we started dating, which dating in Jordan is not real dating. You like go out for a cup of coffee. She had like a 7 700 p.m. curfew. And so and so um and then by the time I started working on on the open source uh replet, she actually contributed the um the logo and a few other designs. She helped with a few other things. And then when uh I got the um uh visa to come to the US um we we got married and very young. I was like 24 at the time. Um and then she she joined me in the US. We continued working on projects uh the whole time. We worked on art projects, worked on games, all these other things. Um, and then when the time came to to start the company, uh, I was kind of like looking for co-founders, uh, because I'm like, "Oh, yeah, YC says that you you need to have co-founders and I guess it's like best to have co-founders." And I was like going in these co-founder dates and trying to meet people and all of that. And then she was like, "Well, I can, you know, I can I can start the company with you." I was like, "It's going to be really painful. Like, it's really, really painful. Like, are you sure you want to go through that?" And she's like, "Yeah, yeah, of course." Like, "I saw you, you know, with Code Academy and like, you know, I can do that." Uh, three or four months later, she's like, "I underestimated how hard this thing is." But uh she she obviously lived up to the to the challenge and I think overall it um it uh strengthened our relationship because when I was at code academy and I was you know working you know uh 12hour 14-hour days she she didn't really relate like what kind of job requires that kind of commitment and then with replet we were both doing that. So, it's not like, you know, your your partner is going off in the morning, not returning until midnight on the weekend. They're they're wasted and they they can't really do anything fun and and so you don't really have this great relationship with them if you have a startup founder and you're kind of you have a regular job. But if you're both founders, you're actually going through this together. Uh and I think um I think that ends up being uh being good. And uh you know at the time it was actually working against us because venture capitalists do not like um you know husband wife uh founders there wasn't a lot of examples at the time obviously the most famous example is um Paul Graham and Jessica uh but then now there's like the Canva founders and a bunch of other companies that that made it work but there are challenges too like um you know right now we're fully sort of fully committed to the but also we have kids and so how do you how do you manage how do you manage this that that adds quite a bit of stress and pressure uh on us. Um, and then the other thing is when something is going wrong in the company, like a lot of people can go home and can disconnect because they, you know, find comfort and and talking to their spouse about other things. But when Hay and I go home, we're talking about the problems and the problems kind of percolate and and and they end up seeming bigger and and worse. And so you you start to have you start to need rules around when do you actually talk about work and you're constantly breaking those rules obviously because you're thinking about something. So I I think it is a challenge and I think it is at the same time there's there's something very uh very unique and and and and good about it. Yeah. Well, it's impressive how well you made it worked and you guys have a really great dynamic. Thank you. All right. Lightning round. We have some lightning round questions. Uh are scaling laws going to hold? Yes, it it uh scaling different things. We're just going to keep keep finding things to scale. What is the best piece of advice you've gotten? Uh Paul Graham uh asked me this question. I I was stressed at the time and he told me, "Is this your life's work?" Like, "Are you going to be working on this 10, 20 years from now?" And I said, "Yes." And he said, "Well, why are you worrying too much about the daily uh tribulations?" like uh if to just settle into the fact that even if things are not great today, you're going to be able to fix them and you're going to be able to turn things around as long obviously as long as you're not dying. And that's why you know Paul always talks about not dying and surviving as a as a company. What is your favorite new AI app? Uh I'm kind of a lot because I make a lot of things myself and I use Replet to to make them. Uh I use I use the basics like you know chatup tea perplexity but then I spin up I spin up pieces of software every day uh to to use I guess uh manis is an interesting demo. Yeah. Um it's actually pushing on this idea that we talked about of like how long can models work while saying while saying coherence and I think they showed that they can go for an hour uh with with some coherence. Since you brought up Rebblelet apps, what's a cool Rebblelet app that you've seen recently? Uh there are a lot of business things that that are are, you know, are are fascinating. Um one cool thing that I um I I went to New York. I was meeting a few investors and and customers and I met with uh Sears Home Services. That's a century old company. I didn't know it still existed, but apparently the home services department still existed. I met this very cool trendy team. They kind of easily work at a startup. They're working at Sears. And they told me that six months ago they finished the cobalt migration and and it took them 6 months. And then I started asking him like what kind of other tools do you use? They use any SAS software and they don't have any ERP software any like these modern SAS SAS software. and they leaprogged an entire generation of um of software to uh start using Grapplet to create agents to manage their business. That's really cool. Yeah. So they they have these uh field workers that every morning they wake up and they're like, "Oh, how do I, you know, optimize my routes to kind of earn the most and service the most customers?" So they built like an AI tool that actually gives them the most optimal route that they use every day, for example. Yeah. By the way, that that team is nontechnical. They're all operations uh people. It's insane that you go from cobalt to replet and you skip everything in between. I mean it does does we were talking about this at the beginning of the conversation, but it gives you a sense of the sectors of the economy that are going to get transformed by this technology. Favorite book? I am a strange loop. Uh Douglas Hustad. Yeah. Uh, I actually disagree with the conclusion slash uh premise perhaps of the book, but it's still like one of the best books exploring concepts like AGI, concepts like consciousness and the soul and what it means to be a human versus what it means to be a machine intelligence. Um, what is the foundation model that you're you like most or most impressed by? Uh, I mean Claude is the uh is the best. I mean 3.7 is is really the best at doing agent stuff. person who's influenced your life. Uh I think uh my mom had this uh supernatural belief in me like she uh she would talk about all the great things that I was going to do even when I was a very small kid and she uh she gave me this again uh um unwarranted at times confidence uh in myself. recommended reading on AI um you know Twitter uh like uh it's really awesome you know I I think it got degraded recently but um you uh but but like there's so many papers that you find on Twitter or so many snippets of information just like like go read those papers skimm and I think you'll learn about what's coming down the line by just reading the literature last question who is the most underrated person in AI I think um Michael Katasta the head of AI now present at Replet um like I don't think he's very public but um he was um one of the early pioneers of um LMS's uh for code um and and he's a great leader as well and and visionary around where AI for code is uh is headed. Super impressive guy. I love every conversation I have with him. All right. Well, thank you for coming on the podcast. I really appreciate our friendship. Thank you for doing this. Thank you. I mean, I appreciate your belief in us and and I think you saw something special really long time ago and like all these things that you notice about what's special about Replet and and I really appreciate that. Thank you. We're still 4% there. 40 million users. We got 960 million to go. Yeah. Exciting times ahead. Thanks, AJ. Thank you. [Music]

========================================

--- Video 17 ---
Video ID: xl6ALoIMRoI
URL: https://www.youtube.com/watch?v=xl6ALoIMRoI
Title: Legacy CRMs have you working FOR them. Day.ai is reimagining what's possible with AI-native CRM.
Published: 2025-04-01 17:57:32 UTC
Description:
The fundamental problem with traditional CRMs isn't just poor UX—it's that they've inverted the relationship between tool and user. Legacy CRMs have you working FOR them, not the other way around.

In our latest episode of Training Data, Christopher O'Donnell explains why even the best implementations only capture about 50% of the data they should. This creates a perpetual data gap that undermines the very purpose of these systems.

Day.ai is reimagining what's possible when AI does the heavy lifting—automatically populating data, providing context, and actually working for you rather than demanding constant maintenance.

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 18 ---
Video ID: MiYrWLaAf1I
URL: https://www.youtube.com/watch?v=MiYrWLaAf1I
Title: Why CRM Needs an AI Revolution, with Day.ai Founder Christopher O’Donnell
Published: 2025-04-01 09:00:20 UTC
Description:
Christopher O’Donnell believes the fundamental problems with CRM—incomplete data, complex workflows, siloed work products and the fear of leads falling through the cracks—can finally be solved through AI. Founder of Day.ai and former Chief Product Officer of HubSpot, Christopher explains how his team is building a system that automatically captures the full context of customer relationships while giving users transparency and control. He shares lessons from building HubSpot’s CRM and why he’s taking a deliberate approach to product development despite the pressure to scale quickly in the AI era.

Hosted by Pat Grady, Sequoia Capital

Transcript Language: English (auto-generated)
If you ask a hundred CRM users, you know, classic user interview questions to get to the core pain, it's all always a hundred of them will give you some flavor of I'm scared about things falling through the cracks. H all of them. Now, why would we care about something falling through the cracks? Makes us look bad. You know, we're we're coming into work, we want to add value, but we want a sense of respect. You know what I mean? That's what's common across all of these departments from engineering to sales CS. Everybody just wants to kind of feel like they belong, feel like they're they're worthy, they've earned their paycheck, uh they're respected by their peers, they're not going to get fired. And this whole new world allows us to operate kind of at that level. Yeah. of belonging and respect and weighing in creatively on kind of how we want these conversations to evolve without having to worry. Just like taking notes, like I don't have to take notes in a meeting. I can make eye contact. Oh my god, that's incredible. [Music] Today we're joined by Christopher O'Donnell, architect of HubSpot's CRM, the one competitor to successfully challenge Salesforce's dominance. Now Christopher is building Day AI, an AI native CRM that reimagines how customer relationships are managed. What makes Days special is how it eliminates the data entry burden that plagues traditional CRM by automatically capturing conversations across email meetings and messaging platforms. It constructs a comprehensive customer database without manual input. It's essentially self-driving CRM that gives salespeople the superpower of perfect memory and preparation. Christopher's vision goes beyond automation. It's about transforming how people connect and make business relationships more human by allowing people to be fully present. All right, we got a special edition of training data. We're on the road in Boston to see Christopher O'Donnell. Christopher, welcome to the show. Great to see you. Thanks for coming out to Boston. Okay, before we get going, okay, I'm going to set some context. Now, some of our listeners are aware of this, some may not be. There are three categories of enterprise software that sit above all the rest. There are three kind of super categories. There's CRM, there's ERP, and there is productivity. And in the world of CRM, you've got this monster called Salesforce, which is the single most valuable company to have emerged from the cloud transition. Salesforce has been a dominant force in CRM for a couple of decades with one exception. There is one company that has actually mounted a successful assault on Salesforce and that one company is HubSpot. HubSpot has the most beloved CRM product in the market with a billion plus of revenue growing at a nice clip with nice margins. It's fundamentally changed the shape of the HubSpot business. And the reason I mention all of this as context is because the man who built that business at HubSpot is Christopher O'Donnell. So Christopher, can you start by giving us a little bit of the story of how the CRM product at HubSpot came to be? Yeah, sure. Sure. So I joined HubSpot through an acquisition in 2011. Um, and I had sort of three chapters at HubSpot. The first chapter was really on the heels of the famous ICP decision. And this is what they teach at Harvard Business School uh and other business schools. The kind of HubSpot case around the uh owner Ali versus Marketing Mary. Yep. Um decision. Yeah. You you you know this decision very well. Um and and I think it worked out very very nicely. So I came in right after the marketing Mary decision, the second contact uh second access for for contact pricing and so forth. And they really needed an email system. And so that was kind of the first chapter was Whitney Sorenson who's CTO there uh to this day and I kind of mounting this assault into marketing automation, marketing email. Um and it was from zero uh rewrite replatform. We had about four months to get off of a well-known enterprise email app and and we managed to do it. I was hands-on. I built the frontends because there was no one really else to do it and uh and Whitney did all the backends and everything. And we always kind of had an eye toward this universal contact database. That felt like the vision and this sort of um one stack unified way of doing it that felt like it was going to grow outside of of marketing email. Uh so about four months to using it internally about seven months to bring it to market we were able to take the company public and arguably win that category. Uh Marquetto was the big competitor and they went public and then went private and have kind of faded away. The second part of uh my time there was what you're describing and it was interesting. I I started to build a product management practice. This is end of 2013. Um, and as that kind of came online and I was able to give the marketing stuff that we had been working on to a team and and let that start to scale out, uh, kind of like the the fisherman throwing the fish back in the water started over. We did a startup within a startup. Stanford's actually doing a case on it now, which is I think going to be really really interesting. They have some uh, folks there that study this kind of innovation. It was very innovators dilemma literally from the book, you know, ripped from the pages of the Klay Christensen. How do you get that what Brian Helen would call the second Scurve? You know, how do you do that in an environment where incremental investment in the core product is going to yield margin? It's going to yield results. And HubSpot is one of the very few companies that pulled that off. And so what what did you do to make that work? Because I think the default is that it doesn't work. And in HubSpot's case, it worked to wild success. Yeah. In hindsight and talking to the Stanford people, in hindsight, it looks like it was very smooth and and brilliant and it was rocky and hard. Uh, you know, it was really messy. I give credit to the founders for their decision to do a startup within a startup. So, we did what the business school professors would call the the skunk works. We literally hung up a pirate flag behind us. We had a small team. It was at the beginning just me and Mike Peachy, my co-founder at at Dayai. Um, and then kind of grew the team from there. We were on our own stack. It's a cool story. I mean, it's it's it's really neat how the strategy played out. There was uh a big question about core CRM versus sales acceleration. Yep. And we were seeing a lot of tools, you know extremely well. we were seeing a lot of uh wallet share going toward these kind of plugins to CRM that would maybe give you more enrichment data or help you do better presentations or write better emails. And so we started there and did email open notifications, the simplest possible thing. We did it on our own stack. We did our own Stripe billing um and had a lot of fun with it. built that kind of into a sales acceleration suite and a year or two into it also made the decision as a company to do core CRM system of record. This made a lot of sense to product and engineering. We were very on board with this and had built things in a way to to set ourselves up to be able to do this. Um and so it kind of merged. It was a sales acceleration suite on top of CRM. We grew the the team to about a hundred people, went from zero to about 40 million in revenue, and then were acquired back into the mothership. And that happened department by department, which I think is really interesting, which in a traditional normal acquisition of another company wouldn't be how it's done necessarily, though it's actually not a bad idea. Um, an interesting way to think about getting um an acquisition to be successful. So, sort of engineering went first, then product. you know, up the stack through sales and finally to customer support. That was the last thing to kind of integrate. And part of our mandate had been to reinvent go to market and explore these bottoms up adoption methods, bottoms up monetization. And there wasn't PLG back then, right? Nobody had a PLG blog. There was no such thing. And so we were kind of discovering that for ourselves while some other companies were as well. you know, Expensify and Atlassian and there are a bunch of examples in hindsight, but um but again, there was no real textbook. So, we were sitting there like, "Oh, we could generate demand from this feature and rotate it to sales." And then Peachy and Mark Rober, who was a huge part of this chapter, uh came in one day and had this spreadsheet where they had figured out the lifetime value by product area limit that people had hit. I still remember where I was sitting when they showed me this on a laptop. They're like, "Look at this model we made." And that was just a massive breakthrough. Um, and that led into the third chapter where I was fortunate enough to be steward of the overall team. Um, and uh, did that for four or five years. So, yeah, it was it was really interesting. It was a great opportunity. And on the CRM piece of that, because I think that's I think that's exceptional in part because CRM is such a big obvious category for people to go after. Nobody else has been successful. You guys were successful. I think that makes it really exceptional. The second thing that I think makes it really exceptional is the default is for act two or a startup within a startup to fail. And based on what you just said, there were kind of three things I heard as maybe big strategic decisions or big principles that certainly didn't cause it to work, but maybe helped set it up for success to some degree. One was the startup within a startup and that being a real thing. You guys were legitimately by yourselves as opposed to feeding off of the resources of the mothership. Second was a little bit of the endound, you know, starting with the sales acceleration tools and getting the system of engagement so to speak before going into the system of record which is of course CRM as opposed to a full frontal assault and starting with the system of record. And then the third thing is the the PLG and to your point at the time that wasn't necessarily a thing. So you kind of had to invent the playbook as you went. Um let's transition a bit into day. And so with day I see I see similarities and I see differences. One similarity I see is PLG. One difference I see is the full frontal assault because my understanding with day is that you're not sneaking around the edges and then popping out of a cake and saying hey we're a CRM. You're actually starting with the CRM. So maybe we start with that before we get into that. Why does the world need an AI native CRM? Let's start with that. Yeah. Well, it's an incredible time uh to be doing this from scratch. And there are 14 16 17 fundamental things about all of the existing CRM that are never going to change and are huge drawbacks. Um you know, fundamentally these systems are supposed to be working for you. Yeah. And they end up being things that we work for. Uh there are a few fundamental problems. There's the data problem where there just isn't canonical data. Uh and we can talk more about this because it it's a big topic and and a really interesting one. But you know even the best CRM implementation on the planet is probably 40 50% of the data that it even thinks that it should have um let alone what is now possible. Uh so that's the first problem is how do you actually populate this thing so that it has all of the information so that you can do your job. The second problem is user workflow. You know where do you start? How do you get something done? How do you answer a question? Um typically if you see a heavy CRM user at work, you're going to see 40 tabs open with that CRM, you know, all the way across. And so, uh, it's very easy to lose your place. It's very easy to lose the plot and you're not working back and forth with it to really understand a relationship. And that leads into the third part, which is walking away from it with some sort of work product. Y you know um CRM users are not sitting down and doing things asking questions getting answers and then walking away with an incredible internal memo or a knowledgebased you know article or an email draft or prep notes for a meeting. You know that's that's happening somewhere else. Maybe it's happening in notion or you know Google Docs or something. Um, and it seems like a really obvious thing that you should be able to sit down. The CRM has the full history of everything that's happened between the company and the customers. It's extremely obvious where to start. We can talk a lot more about that. And at the end of that thread, whatever it is, prepping for a meeting or following up on an email or reviewing all of the deals, uh, if you're a sales manager, understanding what are you going to do this week, where are you going to dive in, where are you going to help out? Um, you know, I have a fundamental belief that people really do want to buy things. Yeah. You know, I I've always built sales and marketing tools from the perspective of a buyer. Um, I'm in those databases as like a decision maker. you know, I'm I'm getting those calls more than I'm making those calls. And I'll tell you, buyers, we love buying stuff. I love buying things, you know. Um I I like buying Data Dog and configuring it and learning how to, you know, have richer traces and doing all this kind of stuff. I mean, these are this is like Christmas. Um and it doesn't feel that way to reps, you know. I also like sales reps. I like working with a rep. I like building the rapport. Um, and so it's this kind of combative relationship that I think is gunking up the economic engine of a bunch of earnest good faith people trying to, you know, deliver value and and push their companies forward. It doesn't need to be this hard. Uh, and a lot of it comes down to trust, improving relationships. That comes from keeping promises, remembering details, and there's a lot of fear there, too. You know what I mean? Sales is a hard job. There's a lot of rejection, there's a lot of disappointment. Um, and that bleeds into everything else that bleeds into the life of that person in in their family. So, I I think all this could be a lot more fun, frankly. Um, you know, we're incredibly blessed to live in this age of software and be able to use cool things at work and have interesting conversations with interesting people and we ought to be able to do it a lot better uh with no extra effort. Yeah, I would bet there are a bunch of people sitting in Salesforce Tower in San Francisco thinking to themselves, "Wait a minute, we're going to be the AI native CRM." What would your response to them be? Why can't Salesforce built this? You know, look, I I I do think that anybody can do anything and there's nothing stopping anybody. I mean, we're just a handful of people with laptops. Um, and you know the flip side of that is the data model is going to really need to change. So if we think about CRM traditionally, a really good way to think about it, Eric Mson, our founding engineer, uh, talks about it this way and I really like it, which is we have compressed the data, right? If you have a lot of data, one way to store it somewhere is to compress it down. So you can think of legacy CRM data as, you know, a photograph that has been downsampled into pixel art and it's just a few little blocks. It's like, you know, one of those uh crypto apes. Yeah. Yeah. You know, kind of thing. Um maybe not as valuable. Uh and so it's really radically downsampled. Why? Because people have to put this data in. And so you're really limited in terms of what you can ask for. you're only going to add fields into the CRM to collect data if you think somebody is going to have the wherewithal to actually fill it out. Yeah. Um otherwise, it's going to remain empty. And so that's kind of the fundamental principle. And I think the legacy CRM companies are going to do a pretty good job of going from kind of 8bit to 16bit. But there's this opportunity to to say, well, hold on a second. We now have what we need to build PlayStation 5 with ray tracing. You know, uh it's it's like going from Super Mario Brothers to Elder Ring is really what's possible. You know, if I show my parents modern video games, they believe it is reality. They don't really immediately grasp that it is a video game. So that data decompression in the CRM, why is that happening? How is that happening? Um, but in terms of expanding this, why do we need this? To understand relationships and to allow people to do these things, give them these workflows, allow them to ask questions, allow them to walk away with work product, you need as close to the actual reality of those conversations and relationships as possible. You need the full picture. You need to invent not just a camera to take a photograph. You need a a 3D reality scanner. um and you need a way to store that, right? And so that that kind of pulls into a couple of elements. One is everything is about context. So if you want to build a chat interface where you can ask a CRM a bunch of questions, which we have, we are not the only people who will, you know, try to do this and build this, you need the data that you're working with to be of a certain type. It needs to be very detailed. It needs to be in natural language. it needs to not assume what's going to be valuable in a particular context in the future. And so the primary aspect of this compression of the data over time has been reducing it down to a checkbox. You know, reducing it down to a drop down. And with AI, you don't need to do any of that. You can just have all of the raw conversations and maybe you can transform them in some ways so that they're more convenient, more portable. Um but all of that needs to be interlin. All of that needs to point back and forth with citations and sources. There needs to be providence. Um you need to understand why an answer to a question is you know if you say what you know what's my team going to do this quarter and it shows you a chart that chart can be absolutely perfect and it will not be a usable product if you can't inspect that data and go into the details and understand all of the assumptions that the AI is making and point back to all of these different things. Right? If a CRM is generating a to-do list for you, which we have, we won't be the only ones doing it. Why was this generated, you know, um why is this bug in the in the support inbox? Um oh, it's this moment in this call that generated this thing and then we dduplicated against this thing that came from a Slack message where somebody said a similar thing. Here's how they said it, right? And so it's not just, you know, six tables in a relational database that have foreign keys that point to each other. It's this constellation uh endless kind of universe of data points that are all interlin. Uh so to the question of why can't somebody do it? They they could. I think data migrations are extremely painful and large incumbent you know hundred billion dollar trillion dollar companies have done them. Netflix has done one. uh you know, Square, you name it. You know, people have gone through these data migrations. Um they're generally moving from one technology to a relatively similar technology. Uh the idea that the use cases around the core data of a company, a system of record company is so wildly different. Yeah. So suddenly I don't think there's an example of that. And if I am a sales rep, an account executive, what's the biggest way in which my life will be different with AI native CRM versus whatever I was using before. I think the way to think about that would be, you know, how is life different with a meeting recorder and without a meeting recorder. Uh if you there are a bunch of these meeting recording bots out there and many of them are really really good and if you read the verbatims uh you know they're largely fivestar reviewed a lot of stuff is five star reviewed these days I'm a little skeptical but um but these are great products and if you read the verbatims it's not about this feature it's not about that feature it's I can be present in a meeting now y so that's the benefit and that's a glimpse into the type of benefit that you're going to get across the entire business. Um and and I think people are still tied to this idea of data entry is cumbersome. We should make data entry better. That's a little bit like the 16-bit Pac-Man. Yeah. Um you know, data entry as a concept will entirely go away. Mh. Um, now being able to shape things and and sort of say, well, this part of this is not quite right or I kind of agree with this take over here in the system learning and adapting. I mean, we need to have control over these systems and with these systems and be able to work and coexist with them. Um, but we don't need to be entering this data. All of that administrative work goes away. We don't need to be writing an email from scratch. We can be looking at the email and weighing in on what we think. We can be maybe developing as a team the way that we want to email and it kind of moves up a level and so everything becomes much more strategic and much more intentional and we aren't going to forget things. Yeah. You know the classic thing if you ask a hundred CRM users you know classic user interview questions to get to the core pain it's all always a hundred of them will give you some flavor of I'm scared about things falling through the cracks all of them. Now why would we care about something falling through the cracks? Makes us look bad. You know, we're we're coming into work. We want to add value, but we want a sense of respect. You know what I mean? That's what's common across all of these departments from engineering to sales CS. Everybody just wants to kind of feel like they belong, feel like they're they're worthy, they've earned their paycheck, uh they're respected by their peers, they're not going to get fired. And this whole new world allows us to operate kind of at that level. Yeah. of belonging and respect and weighing in creatively on kind of how we want these conversations to evolve without having to worry just like taking notes. Like I don't have to take notes in a meeting. I can make eye contact. Oh my god, that's incredible. Now take that to prepping for the next meeting. I'm prepared for the next meeting because I remember all of these details. And by the way, these other things have happened and that's really valuable context and I can incorporate that really easily um you know just by asking the right question or clicking a button to meeting prep, right? Yeah. Uh so the level of effort basically goes to zero and the level of presence and human respectability and and self-esteem, you know, shoots up. I think that's going to be the biggest change. Yeah, it's interesting that a lot of the examples you gave have AI actually making people feel and behave more human and it's a nice example of like AI enabling people, AI providing superpowers to people. Um I want to ask you a question about day itself and specifically you started the company you and Peachy started the company maybe five six months after the chat GPT moment. You started I think April 2023 or thereabouts. Um, what was it that inspired you to start a company? What did what did you what did you see? What made you want to build from scratch again? Kind of what inspired that? I think it was as far back as 2017 that I said out loud, I'm never building a product from scratch without PG. Uh, so he kind of came on the job market. Uh, and I could kind of make that work. the chat GPT thing was capturing our imagination and it just felt really good like the stars aligned. In retrospect, I don't think we could have picked a better time. Yeah. Um it was a little early. So it was that right at the end of the spring of 23 and the chat chat GPT had its moment but the the fundamental stuff of what we're doing in this trade of taking natural language and building and editing and updating structured output that then gets used you know to sort of rinse and repeat. Um that was not possible yet. Yeah, that became possible in June of 24 35 sonnet. You started to see glimmers of it with uh GPT4 and function calling and you could kind of coax it into uh returning JSON and that kind of thing, but we really had a year of understanding the problem space and starting to understand what was going to happen with the AI that that had not yet. So that was that perfect window. It's kind of like if if you're surfing, you catch the wave a little early and then you swim like hell to try to drop in on it. And I I I think we kind of maybe got lucky there. Um I It's funny. I was looking at some old code that I was killing the other day and I was looking at the chatbt 35 Turbo era and the prompts that we had in there and it's like all caps just just begging for it to return, you know, a type- safe object. Uh and then yeah, in June of last year that became possible. Yeah. You know, and you could say, "Hey, look, you know, Claude, here's what we're trying to do. Here's the context we have. This is the type of output that we need to be able to work with heristically, right?" And it's that hopping back and forth between heristic and and non-deterministic worlds that yeah, makes our days colorful and long. um you know that was when it was uh really possible and and it was July basically July 1st I remember because I took a few days before the 4th to just go deep and you know I went up to New Hampshire and just opened up my laptop for 72 hours and started working on uh you know some of the pipeline stuff and actions to do stuff. Um so that's kind of when we let it rip. Um before that it was mostly the the meeting recording um uh aspect of the system which we knew we needed for a bunch of reasons as an entry point into user workflow. It's a really good one because it's that moment of contact. And so if we can, you know, have some seat at the table, we knew that would be strategically interesting. We knew that these were really easy to adopt. Um it turns out they're very sticky. We have basically 100% uh user retention. I don't think I've told you this, but um uh maybe no, I showed you some data on this. That's why that's why you still hang out with me. Um but it they are actually pretty sticky and and people will try different ones but have stuck with ours uh because of a few particular qualities of it and how it's integrated now into that deeper CRM stack. Principally though we needed to capture that data and we needed to be able to capture the raw data and massage it, transform it, you know, kind of cook it, chop the wood as we say and uh use that to start to feed the CRM. So that and then Gmail and we had been doing Gmail in that first year too which is um you know I wouldn't wish on anybody. Uh it's hard right? Uh Google calendar and Gmail ingestion is is really tricky. And then we added Slack and continued to add data sources from there. But it was really July of 24 that we started on on core CRM. And in the basic idea and what you said with with the meeting recorder and Gmail and Slack is be present where your users are and ingest as much context and information as possible so that you can sort of autopop populate or even auto construct this CRM that they need. Is that the basic idea? 100%. You know, Peachy and I have had this idea for a long time of the Spotify of sales. Yeah. You know, the idea of somebody who grew up with Spotify that you would choose a CD to buy. Yeah. And then that would be your CD. Yeah. And you know the ones that you prefer you would keep uh in your car. So that of those eight discs, you could easily access them and you know, and we all had the the visors. Did you have one of those visors, you know? Oh, I had the thing that sat in the back seat, you know, the little binder full of discs that kind of sat on the floor in the back seat. Yes. Yeah, that's what I had. Yeah, exactly. I think this stuff is going to feel that ridiculous, you know. Um, uh, Dan Chen, one of our early users who who you talked to, and I think he made this comment to you as well. the next generation of people coming into the workforce. It's not going to be this idea of data entry is too cumbersome. They're going to look at these things and people are going to explain something like Salesforce to them. Yeah. And they are going to think that they are on punct you know it's like hold on a second. So I do these zooms and I do these emails. Yeah. Yeah. Okay. And then I like DM with my prospects and we do all this. Okay cool. And then I like I tell the computer all of that by like I put that into the system. They're like, "Yep, right here. It's that button right there." And then there's a form and you got to fill it out. You really have to fill it out. Um it's going to be completely insane to people. So, back to the ingestion. If you're going to do that, you know, Spotify had to go out and do contracts to get access to all of the all of the data. Um, for us that is binding to these inputs and they're natural language inputs. You know, it's uh plain text. And then bringing in some of what we know about the CRM data model, even though as I've sort of hinted at it under the covers looks totally different, you are going to want to be able to see a list of people and a list of companies. You know, you're going to want to be able to add a column. um if for no other reason than inspecting it so that you can trust it, you know, uh you need to be able to do all of that. So that's all 100% automatic. You know, folks come in, they sign up, they invite a couple of co-workers, they off their Gmail, maybe they add the bot to Slack, and within half a day, you know, four hours, eight hours, the entire CRM is built. Yeah. you know, contacts, companies, deals, tasks, everything. Deals is the interesting one. Um, uh, because you have to have this idea of of like business process context. And that's kind of advanced mode, I think. You know, building company records, contact records. Not that it's easy, but it's, you know, it's a lot of looking at the web. It's a lot of, you know, that type of thing. And you have domain and email address to work with. when you start to get into to-dos and opportunity management and all the rest of it, it's like, whoa, okay, hold on a second. You know, what does this mean? What is what does due date mean? What does this stage in this pipeline mean? Yeah. Um, and so getting it right requires a lot of input from the user and they may not show up having all of these answers. It's not like everybody shows up and says the entrance criteria for our fourth stage of the business development partnership pipeline is the following. you know, they don't know. They don't know. And so, you have to kind of get that metadata out of them and then use that as you're looking at all of this data. Yeah. And continually return to the data set as things change and re-evaluate things. Move a deal from one stage to another because a particular meeting happened where particular things were said. Like I said before, you need to show proof of why you did that. Um, and then on top of all of that, the user has to be able to say, "No, no, no, it should be over here." Um, you know, let me weigh in on this, or let me make a hard edit to this. And so, it it really is very, very different under the covers. Yeah. It's almost like self-driving CRM. It is self-driving CRM. And you know what we learned? you do know this. I know you know this. Um is that so that was kind of that arc from July through then we got into automated opportunity stuff in November and up through you know that I can't believe it was a month. Oh my god. So by December what we what we were learning was full self-driving is too scary. Yeah. It's too scary. And so from New Year's to now, the big push for us, it's it's been getting that last 10% of data quality, for sure. Um, you know, avoiding false negatives on saying something's an opportunity. Uh, getting to-dos right. We call them actions. Getting actions right. But it's really more about this control and configurability layer. Yeah. and and getting the balance of the be best of both worlds where okay the data is right and I can also correct it. I can also understand why it is what it is um and make sense of it. I can debug it sources and reasoning right um a very simple example of this is uh perplexity you know if you do a search on perplexity it'll show you the web pages that it's using to build the answer. Yep. Um, and this now becomes kind of the core product challenge I think for all of these AI native apps across disciplines is managing transparency and managing control as you are prompting and working with the LLM being able to approve the output being able to uh have rules and instructions about the output so that it sort of fits your standards. I think on some level we're all in a similar game. Yeah. Yeah. Which all comes back to trust. you know, can you trust what the AI is doing on your behalf? Um, let's talk about building with AI a little bit. Can you share maybe some of the surprises or some of the magical moments along the way? Yeah, so I I am a heavy uh personal user of the AI coding apps. Um, I've tried I can't say I've tried all of them because there are so many, but I'm regularly checking back in uh with with most of them. I most heavily use cursor um and have been pretty deep in that for over a year. Um these things are evolving incredibly quickly. Yeah. Uh week by week they will have not just pros and cons and sort of one's ahead of the other in the horse race, but they're dramatic uh movements. You know, one week wind surf will do something kind of cool. um but oh people don't really like the pricing model and then the next week cursor says okay well here's you know a new way of doing rules management and system instructions that's like way ahead of what anybody else is doing but then a new model comes out and it responds to this stuff differently and so it's like oh you actually shouldn't use any of that in these types of circumstances. So it's um it's really a moving target. It's fascinating and thrilling to kind of watch. One of the things that's happening right now as we speak is that um less context and less instruction layer in a lot of cases is better. And so apps that have less of that off the shelf, right? Like fewer features, fewer stuff going into particularly 37 sonnet, um you get a better result for a lot of questions. And so, you know, you have companies that have been adding all of these features to let you do all of these system instructions and then it's like, whoa, how do we respond to this? Um, so man, you know, long days and nights for everybody working in that field. Uh, you know, one interesting thing about that playing field I will say is that they are all working off of the same raw input. They are all able to look at a code base. Yeah. And what's interesting for us is there's no code base you know we have to create the codebase and then we have to do the kind of coding stuff on top of it which is uh which is really interesting. Can you talk about how going from a world of software that executes things deterministically to a world of software that is in some ways deterministic and in some some ways probabilistic? How does that change the art of building software products? I will say that generally speaking, software engineers are not liking this. Uh it's it's not the kind of curveball that they were hoping for, I think. Um you know, I mean, and and there are ways to kind of coax it into being more deterministic. Uh so in terms of surviving the day-to-day and really making progress, a lot of stuff, you know, you turn down the temperature and you kind of say, "Okay, you know, um especially when you're doing natural language structured output, but it's still non-deterministic." Yeah. Uh I think hallucinations are not nearly the problem that we would have thought a year ago. Okay. You know, a year ago having this conversation saying, "Well, this chat GPT thing is just saying some nonsense." you know, what are we going to do about this? A lot of that's gone away in this type of use case. I can't obviously speak for every use case, but if you're saying, here's a whole Gmail conversation, you know, is this uh a promotional email? Is this a cold email? What's going on? Are there pending action items and so forth, there isn't an enormous amount of hallucination. Um, you might get a slightly different result. And it is tough because there isn't a lot of tooling out there for you know non-deterministic stuff. I think you know from a from a a venture perspective and so forth the the new generation of tooling is super interesting for sure because this idea of eval kind of in this race but you know these are really interesting products that are really valuable to product builders like us um that are going to do really well. So as that tooling matures, that's another kind of tailwind to it. Uh but you know, if nothing had changed from where things were a year ago, uh it would be very very scary. How have UX patterns evolved in a world of AI? The UX patterns I I'll say a couple of things about. One is consumer grade. So we have talked in B2B SAS for a long time about you know consumer grade and the truth is what we meant was kind of a coat of paint a design system a big investment in information architecture and trying to arrange things in an intuitive way and so forth. We did not mean, you know, Facebook native mobile app level transitions and autoscrolling and, you know, all this kind of stuff. And so the work that folks at Anthropic and Chat GPT and Perplexity and these types of companies, they're building consumer products. Yeah. And the quality is extremely high. They are very wellunded. They have the best engineers. Um, a lot of those engineers are coming from social media and consumer apps. Um, and so the level of polish is insanely high, which only in the last, I would say, month am I coming to fully appreciate because I'm matching up our stuff against, you know, what's out there. And in the past, when you've done that and and kind of looked at what's out there and where the bar is, you know, you can get over the bar. Yeah. And in B2B SAS, you can do it. maybe you need to invest more in design, you know, value it more, give them more of a seat at the table. Um, now it's a little different and things are going to need to be unbelievably fast, unbelievably smooth. Um, so that's one in terms of UX. The second thing I would say is control and transparency balanced with automaticity is the core tension. So if you ask a user, do you want all of this to be done automatically for you? Of course they're going to say yes, when you do it automatically for them, they then have a lot of questions um of why things are a certain way, what they can do about it. And there are a lot of levels in there, you know, do you want uh to let the user just flag something as a miss? um maybe not that valuable. I mean, everybody's doing that, you know, but how often am I in Claude Web saying thumbs down? It's it's it's never like a thumbs down in Claude web. It's, you know, let me push you in this direction or let me ah you're missing this piece of context. Okay. Yeah. Let me paste this document in or whatever. Yeah. And so you have to be able to do that. You have to be able to manage the context that the system is using. um you need to be able to undo you know and correct the data. So, we have a document internally uh that Daphne made, Daffany Funston, and it's called the rules of the game, and it outlines like, hey, anything we do, you have to see why the AI did what it did. You have to be able to override it as a person and know that you're overriding it, not have it flip back. Um, because you'll run into that, too, right? Oh, I moved this thing, you know, into this stage because, you know, I thought it was correct, but then the AI moved it back. Yeah. Okay. Well, what do you want? Do you want the AI to be able to continue the decision-m after the user has weighed in? Sometimes, not all the time. Again the data model is uh sort of ridiculously uh involved to be able to give the user very basic things like hey if I say that you know the status on this thing is such and such it needs to what update the status and never change it or take that piece of information into account for every status that it prints moving forward you know in that case probably the latter. Um, so yeah, the the UX patterns around control, uh, progressive disclosure, you know, it's show me the clean shiny thing that's automatic, but then let me turn it around, you know, let me flip the hood and see what really happened leading up to this. Um, and so I think that kind of modality is going to be really interesting. You mentioned the document that Gwen put together, which is a little bit of a glimpse into how the day the company works. Let's talk a little bit about day the company. Yeah. And maybe starting with two things that are on my mind. One is I think you guys are a very good example of this idea that a lot of people have talked about which is you can just do a lot more with a smaller team and particularly now that we're in a world of AI you can do a lot more with a smaller team. Team is very small but it's very high caliber people and you work extremely well together. The second thing, which is one I want to ask you about, is I tend to be a big believer in inoff culture. You know, we we Sequoia have been back in the office since May of 2021. You know, we're five days a week. We think it we think it works. You guys are not all in the office together and yet you seem to have extremely good flow. And so, I guess the thing I'm curious about is how do you achieve that? H how do you get this small group of people in different corners of the country to come together and actually have extremely good flow? Yeah. No, it's a great question. And the spoiler is I bet we'll end up in the office. Yeah, I bet we will. Um, you know, fewer sites, I think, fewer time zones is generally good. And we've been kind of letting the universe sort that out for us. And it may be that the universe just sorts it out and that we end up in an office together in Boston. And going into that office, I think having had the experience that we're having now, the flow will be very different. Um, Pichy wrote a really cool post on LinkedIn about the offsite that we had recently. Uh, and a couple of things about it, characteristics that I think are emblematic of the way that we work, you know, our company culture. Yeah. But it's it's really just, you know, the way that we work. Um, and to your point, I would say they are very high caliber people. There's a little bit of a a thing to these particular people in terms of their level of emotional intelligence, how much they like the work for the work's sake. Uh the willingness for everybody to be so directly exposed to customers like this is not all for everybody. I, you know, I wouldn't, you know, stand on a soap box and say everybody should do this. Um, but it works really well for us and it's the kind of people that we want around. You know, if if an engineer never wants to reach out to a customer to verify that a bug is fixed, like they're just not going to have a ton of fun. You know what I mean? Uh, so that's a really important layer is that the primary source material, it's kind of what we're trying to do with the product, right? And we use our product for this, too. I if you say, you know, oh, that's a great idea that that would kind of get us the thing for Dan. Yeah. Everybody knows what you're talking about and everybody's following along on the plot and in and seeing what you're synthesizing. So that that's kind of a an unwritten contract that we've all entered into, which is, you know, I am willing to hold an enormous amount of customer conversations in my head so that we can have these conversations. Um, I find it makes it really fun. You know, it it makes it possible to come in and do a day of work where you feel like you've actually made a difference for somebody. Now, it's software, so making a difference for one person if you've chosen the right person, you know, it's going to make a difference for a lot of people. Uh, and and the team's nice, you know, Daphne joined. I asked her a few days in how it was going and she said, "Everybody's really nice." Um, and that's part of it, too. It's I think part of that comes with seniority. It's the emotional intelligence part of it. It's the, you know, people are kind of here doing it for similar reasons. Um, what would I don't know if you've codified the company's values. If you have, what are they? If you haven't, what would you say they are for the people who are on the team? What's their lived experience of what the company cares about? Yeah, the Lyft experience is um very truly customerdriven in a literal sense. Yeah, you know that our customers talk about the same same hour bug fixes and same same day feature releases. Yeah, it seems like the feedback cycle is incredibly fast. That's the fun right there. Yeah. you know, um, if you're going to do something anyway and it's the right thing to do, you can do it right then and get word of mouth and get momentum and and thank somebody. I mean, this is early software. This is early stage software. People are are investing their time and energy into this and we need to reciprocate, you know, we need to show them that it's worth it. If we do that, it'll be really fun for them, too. and they'll feel like they're a part of something because they are a part of something and their feedback is creating this this larger thing. It's one reason we've kept it a little bit on the smaller side because you can't do that forever with, you know, not even a certain volume of people because I think you can I think you can do this with a very large volume of people but once the product is at a certain maturity and once you've really nailed who those people are um you know this last year and a half has involved a lot of kind of understanding, okay, what's this like for solopreneurs, you know, you know, there's a case to be made that this that we should try personal CRM or, you know, very small business founder CRM or that we should start with VCs, you know, because they're early adopters and boy, they're certainly willing to give it a try, you know. Um, and so we've kind of been updating ICP and doing that, staying focused on the people who we take the most seriously in terms of having strategic uh, weight. Yeah. It's easy to take those people very seriously and build a thing for them that hour. Yeah. Because you are giving them credit for being right. Yeah. In a very macro sense, like you have it. There isn't, you know, a product management off-site and, you know, sticky note sorting exercise to be doing around it. It's like you are the user, you know, you are correct, we will do the thing and it happens to be really fun for them, which makes it fun for you. Um, the other thing Peachy noticed about our off-site is we pull the work up like and and I think the piffy answer to your question of how have we been getting by remotely. It's a lot of slack huddle. Yeah, it's a lot of slack huddle. Very low stakes. We don't have any recurring meetings. Um, and when we get on huddle, I'll get on huddle with Gwen. We've had a bunch recently and we'll work for six hours, you know. Um, and and it's kind of magical to be able to do that. So, and there's a whole practice of pair programming. Yeah, I actually don't know anything about it and in the body of knowledge. I should stop and probably read a book on it. But that is actually really great. You know, you don't want to go out for a huge hike or mountain climbing thing by yourself for a number of reasons. And, you know, diving in to do some of this data model stuff, it's buddy system. um that's worked really well and I don't think that the gulf between the customer and the code needs to be as wide as it is. You know, we're used to these sort of senatorial trappings where you ultimately have go to market and and uh uh R&D sort of as the Republicans and the Democrats. Yeah. you know, and everything is some flavor of managing expectations for their frontline people who are the the voting base that have all the power in a SAS business, right? And and so there's a lot of distraction that comes with that. There's a lot of subtext. There's a lot of peeling it all back. Again, everybody having good intentions like everybody solving for the customer like best possible case, you have this huge gulf. And so we're we're trying to build and and I would say at this point really have a culture where you know here's this question from a customer. Uh here's a doc on it. We do a lot with docs. Yeah. We do a lot with the written word. You can't compete with the clarity of the written word. I think um but pull the code up, you know, pull the code up. It it's not like this secret backroom thing for just the software engineers. like look at how things are labeled. Look at the conditionals of when we show that button and when we don't. Y let's look at it together. It's not, you know, rocket science. Some of it's going to be pretty opaque if you show it at a company level. A lot of it isn't, you know, a lot of it isn't. Uh and so that lets us get to technical decision- making and polish a lot faster, too. And again is kind of fun and rewarding. One more question and then we'll jump into the lightning round. So, um, one of the mantras that applies to some businesses, and I think it applies to today in some ways, which I believe is stolen maybe from the Navy Seals or some other branch of the military, is the idea that slow is smooth and smooth is fast. And you are anything but slow when it comes to working on the product. But you've resisted the temptation to juice the vanity metrics that a lot of startups feel pressured to juice. Said differently, your customers today are as much design partners as they are customers and you've been very deliberate about crafting a product that meets a certain bar of excellence or certain bar of quality before releasing it out into the world. And I guess the question is where did that strategy come from? Is it AI specific? Is it just the way you like to build products? What's sort of the philosophy behind the slow is smooth, smooth is fast strategy? It's not typically what I would do. It's not um I like the Y combinator wedge kind of thing. Um I like incremental improvement. I like broad customer exposure. uh with the exception of when you are doing software that is of a certain scope and stake that the investment that you need from somebody to get any feedback is very high. Yeah. Um and the scope is also very wide. Uh I mean part of it is I don't necessarily want the entire world to know how wide it is. Um but I guess they'll find out from from this. You talked about the three main spaces. Yeah. I mean we're doing the entire CRM. Yeah. Um it's every function lot of surface area AI native everything you know. Um it's probably going to happen faster than people expect. Yeah. So doing things in that way where we are trying to meet this bar with a particular customer. This is system of record software, you know. Um, I don't know how Parker did rippling. I'm guessing he didn't say, "Let's launch payroll in one state." Um, maybe he did. I'd actually be really, really curious to hear the story from him. If you're doing HR payroll, you know, benefits, there is a level of completeness uh that you need to get to, you know, even be in the game. CRM has some of that. CRM, you can get a little bit cute with some use cases and say, "Okay, here's a meeting recorder, but there's a contact sidebar and it has, you know, personal history and everything like that and it's a CRM record and you're sort of Trojan horsing in." Um, so it's not what I would typically do. I think it's very appropriate for us now. Um, and I also think that, you know, we're setting ourselves up for the very, very long term. the thought experiment of you know if you are in a race to a millionaire or r how do you think about things think about the y combinator which okay and you say all right let me break your brain you're in a race to a 100red millionaire r from zero how do you think about things you immediately start to think about things differently and you think okay well we're going to need something that people just don't cancel that has a certain ASP a certain adoption pattern, you know, and so forth. It doesn't force you out of the way of thinking go to market or SAS economics or anything like that. So when you say you're now in a race to 10 billionaire R from zero and you have seven years, you know, to beat the record, whatever. Uh record I looked it up, by the way. It's I get from what I can gather, Bite Dance did it in eight. I think Meta did it in that sounds about right in in nine or something. I mean, which is insane. But if you say you're in a race from zero to 10 billionaire R now you think about things in an extremely different way, you know, this needs to be that entire top level space. Every line of code needs to make every other line of code somehow more valuable. And so little patches and little feature things where you're doing it in a non-strategic way, you can't afford to do that. All goes away. And so, you know, if your number one feature request on MeetingBot is output templates because these other apps have output templates, you maybe you resist doing it that way because output templates are a core part of interacting with LLMs. And when you do output templates, you're going to do output templates. Yeah. And it's going to be legit and it's going to be proper and make sense and be a permanent thing. Um, so soon we're going to have this feature that people have been waiting for having done it in that way. Yeah. Right. You can't say uh let's draft an email with this email editor and not be thinking about marketing email and knowledge base website, you know, internal wiki. you have to set yourself up to do things in in that way as well. Um, and that is kind of my style. Like I do like thinking that way, but um that's the other big slowest smooth is fast factor. All right, lightning round. All right. Who do you admire most in the world of AI? Sarah Guo. All right. I love it. Um, yeah. Who do who do I admire most? I don't know. I think you already nailed it. Yeah. I know. I know that. Sarah's incredible. She's extremely extremely helpful. I will give I will give a lot of credit to Daario for what felt for a long time like bubble wrapping his models so much for safety, you know, and and we're I think we're starting to forget that narrative. Uh, you know, Claude was for a long time unusably bubble wrapped. You know, you'd say, "Okay, now in the middle of this play, we're going to have this, you know, choreographed martial arts thing." And Claude would say like, "I'm done. I'm out." You know, and people on Reddit and everywhere were were, you know, rolling their eyes and laughing. And this was before 35 Sonnet. Yeah. When they really like set everybody straight. Um and so you look at it today and and using those models having grown up with them in the expectation of safety and ethics is incredibly important. Like we're doing so much for our customers automatically and they can say anything they want into this system and they could theoretically use it for any purpose. the fact that we can sleep at night knowing that a massive amount of that kind of ethical compliance is handled because we're using those particular models. Uh I I think it was I think it was courageous, you know, and and I'm thankful thankful to it. So I' I'd probably say him. That's a good one. Um one of the topics that's been uh debated I think recently in the Twitter sphere and elsewhere, should designers know how to code? And I think you're well positioned to answer this because I I almost think of you as an artist first and foremost, you know, crafting exactly the right experience for the person on the receiving end of the product. And so I think about you as coming toward engineering from very much a design and and product lens. Should designers know how to code? I think the biggest change happening in product design, which as a field is radically changing, is writing. I think that's the biggest thing. I think that UX content there's a big debate about whether that's a thing and we started to build these teams out Slack, HubSpot, other places started building out these writer teams to get all of the brand and tone and voice and button text and everything right. That's correct in that that that is a real discipline. However, I think that is a major part of the product designer's job going forward because, you know, 10 years ago, it was asking the designer in Sketch probably back in the day or Photoshop or whatever to design a date picker. No one's going to design a date picker right now, right? You're going to use a component library, have a design system. That's a solved problem or like you shouldn't be doing doing software if uh if you're not doing that. And so it frees up a lot of career bandwidth for designers and I think they're going to need to adapt. So I think they're going to need to get really good at content um you know microcopy flows you know moving over time. That's a weak spot for design I think traditionally is kind of throughout time and this is the consumer folks are really good at this. So I think they're going to have to get really good at interaction design as well. Uh in terms of coding I don't know. I mean, I don't know if anybody's going to have to code. Outside of the world of AI, what is the most extraordinary product you have ever encountered? Okay. The most extraordinary product, honestly, is the Augusta stove. Really? Absolutely. Tell us more. I mean, look, it it's it may be a good example because it's like completely functionless. It it is just a hunk of cast iron with a pilot light that keeps it hot. That's it. There there are like no moving parts at all in it. Um but what it does, the benefit to the user is create a sense of hearth in the home. Um, and it has little features to it where if you have wet snow boots from, you know, your kids playing in puddles or whatever, it's meant for you to put those on the top of the stove. There's a place for that. Uh, it heats the home. You know, this idea that everybody kind of gra however you lay your house out, everybody gravitates toward the kitchen. Well, the AA is always on. It's hot, you know, and it's immediately accessible. It has four ovens that are always on. And so you think about things differently and you cook oatmeal over 24 hours and you cook a chicken over 16 hours and you make a sandwich and then you melt the cheese on it for a guest. You know, it it's really kind of fun and cool. It's extremely safe for kids. So my kids are are actually killer cooks and can make themselves, you know, uh whole meals. They cook meals for their grandparents and stuff at 10, 11 years old. So I And I like the example because there's nothing to it. Yeah. Yeah. Very cool. All right. Last question. It's a Mount Rushmore question, and you can take it in one of two ways. Number one, who is on your Mount Rushmore of product people? Or number two, who is on your Mount Rushmore of founders? It's a it's a tough one because you have to walk the line of sicky, you know. Um, and I think for me the answer is going to be the same. you know, the the founders that are on any kind of Mount Rushmore. I'm not really in any place to make a a Mount Rushmore at this point at all, but uh certainly Steve Jobs. Yep. Um he's complicated. I have an opportunity through a mutual friend of ours to be learning more about him um and kind of how he interacted and what the experience was like for those around him. It's not a leadership style I really want to emulate, but um you know the results pretty impressive and the engagement I think is really impressive. The trust of knowing what's best for the user. It's a little over my line. I still need to talk to people to figure out whether they like something or not. Um but Steve Jobs is up there. I' I'd put Paul English up there for similar reasons. I would I would because I think Paul uh Boston guy is cool. You know, Paul left a long shadow. I'm not friends with Paul or anything. I've I've met him. I've hung out with him. He probably has no idea who I am. Um but uh he will after he listens to Yeah. Well, I hope so. Hey, Paul. Uh I hear you're a great great dude. Um the one time we hung out, you said some really brilliant stuff. The the recruiting and the customer focus. Yeah. I think that left a really long shadow. Um I hope he knows that, you know, the the story of the really loud annoying phone in the middle of of the room. I mean, our culture is some version of that. It's really just the extension, you know, down from from that and the diaspora of that kind of value system. Um so there's that and then also what the way he recruited and the way he talks about recruiting I think is really um underststudied. uh you know the idea that if you have the candidate you have the right person that's your only job and you need to be in person with them with an offer letter within hours you know not weeks and uh and I think that's another advantage that we can have culturally over over larger companies is you know moving very quickly with amazing people y so he's up there and then I'll throw in Rick Rubin I mean I don't he's probably founder of he probably has some production company or something I don't know but he's a really interesting guy because he's a he's a Steve Jobs with a completely different vibe and a completely different level of humility which I think is interesting. So Rick Rubin has produced you know probably half the records we grew up liking and listening to um and you know and he's in the studio to the question about designers knowing how to code. I mean this is the greatest music producer maybe of all time and has no idea what any of the knobs or dials do. doesn't know a single chord on the piano and trusts himself in his taste completely and totally. Uh there are no focus groups or anything like that. Um so I'd put him up there. I'd put those three guys up there. We got room for one more. Sarah Blakeley. Really? Okay. 100%. I I was just telling my daughter about this cuz um my 10-year-old daughter, she was dropping some science on me, man. She was saying, "You know what? if we took maps and that claude thing you were showing me and you know and she starts riffing on it. I almost texted you so I was like you should you should get in on this one early. Um but uh I was telling her about Sarah Blakeley and and the idea I mean very humble, very approachable, really inbound kind of leader with a really positive message. Um but she just did it, man. She just did it and she had a belief, you know, women's undergarments should not be designed by men who knew nothing about it. Like that is okay, great. You are a billionaire, you know. Um and and she's done a lot of good with it. So I' I'd put her up there as well. That's right. There are four on on the mountain, right? He's up there, too. Awesome. Christopher, thank you for coming on the show. All right. Thanks for having me. Good to see you. [Music] [Music] Heat. Heat.

========================================

--- Video 19 ---
Video ID: G0xSAdgZQBQ
URL: https://www.youtube.com/watch?v=G0xSAdgZQBQ
Title: Wordware's Filip Kozera is trying to bring structure to human-AI collaboration #ai  #podcast #tech
Published: 2025-03-26 14:00:17 UTC
Description:
Wordware’s Filip Kozera is putting AI development in the hands of knowledge workers, helping them become “word artisans” who use AI to amplify their creative impact.

Transcript Language: English (auto-generated)
I think bringing structure to human AI collaboration is something that I've chosen to spend the next 10 years of my life on it's an exciting problem because right now we're trying to mix the structure of programming which is very rigid and deterministic with something that's intrinsically fuzzy and that Maring of these Concepts and choosing the right affordances and the right obstruction layers for that communication to be incredibly easy yet enable people to do complex things is hard and this is what we are trying to do with wordware as the engine to enable the right mix of the two

========================================

--- Video 20 ---
Video ID: HzOj3Bx1XiQ
URL: https://www.youtube.com/watch?v=HzOj3Bx1XiQ
Title: Wordware combines the best aspects of software with the power of natural language #ai  #tech
Published: 2025-03-25 22:33:38 UTC
Description:
What Excel did in the ‘80s to data analytics and numbers is what Wordware is trying to do to AI. 

Filip Kozera, co-founder of Wordware, is building tools to help bridge the gap between human creativity and AI.  Filip shares his vision for why English is becoming the new assembly language for LLMs, why he believes that the future belongs not just to coders but to people he calls “word artisans” who can communicate effectively their creative vision to an AI system, and what that means for the future of programming computers.

Transcript Language: English (auto-generated)
uh we have 30 million software engineers in the world and we have 750 million active users of Excel and now uh you'll ask like how does Word compare to Excel and what Excel did in the 80s to data analytics and numbers is what we are trying to do to AI. So in the 80s you either had to have a team of data analytics people or engineers or you using a calculator and I would say the calculator equivalent here is the chat GPT that you every time you need to redo the conversation and you every time you need to instill your own needs into it. What Wordware is trying to do is saying, "Hey, a lot of the things that you do are repeatable, similar to Excel, and you can encode your taste into it with Wordware.

========================================

--- Video 21 ---
Video ID: s3L4tmuUdKg
URL: https://www.youtube.com/watch?v=s3L4tmuUdKg
Title: From Software Engineers to AI Word Artisans: Filip Kozera of Wordware
Published: 2025-03-25 09:01:04 UTC
Description:
Filip Kozera sees parallels between Excel’s democratization of data analytics and Wordware’s mission to put AI development in the hands of knowledge workers. Drawing inspiration from Excel’s 750 million users (compared to 30 million software developers), Wordware is creating tools that balance the rigid structure of programming with the fuzziness of natural language. Filip explains why effective AI development requires working across multiple abstraction layers—from high-level concepts to detailed implementation—while preserving human creative control. He shares his vision for “word artisans” who will use AI to amplify their creative impact.

Hosted by Sonya Huang, Sequoia Capital

00:00 - Introduction 
01:47 - The hottest new programming language is English
05:31 - Not no-code
10:47 - George Lucas and GPT-7
13:49 - The next billion developers
19:14 - The UI for AI
24:20 - ICP as analytical creative
28:19 - Transformers are the new transistor
30:54 - Programmable documents
36:02 - Lightning round

Transcript Language: English (auto-generated)
uh we have 30 million software engineers in the world and we have 750 million active users of Excel and now uh you'll ask like how does wordware compareed to Excel and what Excel did in the 80s to data analytics and numbers is what we are trying to do to AI so in the 80s you either had to have a team of data analytics um people or Engineers um or you're using a calculator and I would say the calculator equivalent here is the chat GPT that you every time you need to redo the conversation and you every time you need to instill your um own needs into it what wordware is trying to do is saying hey a lot of the things that you do are repeatable similar to excel um and you can encode your taste into it um with wordware [Music] today we're joined by Philip kazera co-founder of wordware who's building tools to help bridge the gap between human creativity and AI Philip shares his vision for why English is becoming the new Assembly Language for llms why he believes that the future belongs not just to coders but to people he calls word Artisans who can communicate effectively their Creative Vision to an AI system and what that means for the future of programming computers Philip thank you so much for joining us today I'm excited to learn about you and wordware um and get your take on how you know programming computers is fundamentally going to change with large language models so thank you for in it for the invite let's dig right in I want to start with Andre karpathy he had a tweet in 2023 that went viral the hottest new programming language is English what do you make of that and what does it mean relative to what you are trying to build at word um I think syntax will not be as important you know everyone will be somewhat of a coder people used to have to know python now English is enough however you still need to know what you're trying to say and in that way I would say um you know not everyone will be able to use it because some people don't have that much to say and I would maybe rephrase it a little bit in a little bit more of an exciting way for us is that the Assembly Language um to llms is English but you still have to structure it in the right way and you still have to use some of the concepts even from typical programming in order to actually make sure that it does what it's supposed to do you heard it here first okay English is not the hottest new program programming language it is the new Assembly Language and at wordwar you were trying to build that new programming language uh to work with that new that new Assembly Language I think bringing structure to human AI collaboration ation is something that I've chosen to spend the next 10 years of my life on um and it's an exciting problem because uh you know right now we're trying to mix the structure of programming which is very rigid and um deterministic with something that's intrinsically fuzzy and that Maring of these Concepts and choosing the right affordances and the right obstruction layers um for that communication to be incredibly easy yet enable people to do complex things is hard and this is what we are trying to do with wordware as the engine to enable the right mix of the two let's talk a little bit more about no code I remember back when when I first got to Sequoia in 2018 I remember no code was the hottest thing ever and it was like you know we're not going to we're not going to program anymore um and you know obviously some no code companies have done extremely well like rol for example uh but by and large we still have you know we have software Engineers today and so so far that no code promise hasn't really come to fruition what's different now like what has changed um I think coming back to what I just said as well uh the Assembly Language is English and um you I kind of have this small insecurity when people call wordware a no code Tool uh because we have not yet reached um a ceiling with any of our clients so you can achieve absolutely everything we've created you know an ability to put in code execution blocks which if you want an escape Hutch and wordware is not fully capable of everything uh you still can do it um and in that way you know the document format of how to structure agents um and how to write them down one by one is still very similar to how code works we still have loops we still have conditional statements and we you know have calling other flows which really is function calling you know um so in a way we don't see ourselves as a no code Tool uh and we kind of believe that the word Crafters uh the word Artisans uh of the future are still coding it's just uh you know they are structuring the English in a very precise way uh to make sure that the prompt is populated in the right manner h word crafters and word oens I like it wordware Engineers you've heard it here for how are you using the English language then to make you know since you bristle at the no code term how are you making no code more codel like and maybe this is a good time to to just give a 30 second overview on you know the word we product and how people use it to build things sure so um by trying to bring that structure to the intrinsically fuzzy English language we've created an editor where you can use the similar Concepts to software engineering looping conditional statements functions calling functions um and marry it in that editor in that natural language um IDE uh in a way that people can construct agents something that we are not we are not coaching uh we go straight to agents and in my definition um agents are almost like they are still software they are still a little bit like software taking in inputs and out outputting outputs but some of the stages of what they do is fuzzy so so uh marrying that structure in the editor uh whatever you build there whatever you iterate on there you then have three different ways of deploying it you can deploy it as an API that will power your product or that AI button or that AI chatbot that is a little bit more complex than just doing a vanilla API code to CLA or open AI That's number one uh number two is you can deploy it as a workflow where you know the main uh part of the workflow is not like zap here it's more AI native uh and the Really the brain of the AI is a little bit more complex than couple prompts trunk together and the Third Way the third thing that we're building is the GitHub for AI let's say uh for people to share and what they've done and other people are able to then Fork it and use these things as components again marrying some concept from software engineering you need other people's components and libraries and you know you want to be building on top of the uh on the shoulders of the Giants of of these AI thinkers so again wordware engine editor way to actually create these agents and then three different ways to deploy I think one of the beauties of code is just its expressibility and its Precision you know you know exactly what you were telling the machine to do um and you were you know expressing it in some languages in the most precise way possible English is not like that to your point it's fuzzy and so as you are turning the English language more Cod like should I think about that as you know helping with the control ability and the stability of that or you know what is the I guess abstract thing you were doing with English to to make it more programmable yeah I think uh you're exactly correct we are trying to bring a little bit more structure it's not all the way because if you go all the way to a programming language you lose the fuzziness and you lose the power of it um but it's hard and right now most of the people you know we had this wave of Val evaluation software uh companies at some stage and what we've realized with bunch of companies that we work with is that they don't know like they don't have these data sets to use for eval and what we came up with is um that editor very quickly uh gets you to understand and have develop an intuition of what works and what doesn't work and for now this is the most important thing it's clicking run a 100 times quickly and making sure that what you've written here has enough structure in order to Output things on the right side that you want and in that way you know as long as you know what you're trying to achieve uh and this is very hard you know we have a lot of companies coming in and just saying AI predict weather I literally had a big customer say can you guys like predict weather for me and you know that's not the case you need a document where you outline what you're trying to do and you know even that document that has enough structure if you have done you know an intro those are the inputs you'll be you'll be playing around with images and PDFs and then you will manipulate it in this way and then you'll get some outputs and developing that intuition um is just enough structure for today soon maybe we'll be able to do better evolves but for now a lot of people really don't know at the moment when they start playing around with wordware they don't know what they are trying to achieve and one of our customers coin this term of speed of creativity with wordware is uh is higher so they learn what they are actually trying to do as they encounter problems with the underlying models and they realize Gemini 2.0 pro might be better and maybe Gemini can take huge PDFs um and Claud can kind of take PDFs but smaller and then dpt4 cannot take PDFs and you know they develop that understanding and that helps them to structurize their faults how do you instruct the machine to go from intent to outcome like let's say I'm a brilliant filmmaker and I want you know I want to use wordware to create the next the next uh hit um what do I do uh in wordwar in order to make that happen yeah so for now we focus on knowledge workers where that is a little bit easier I think using your example for figuring out how the future will be you know if we have let's say you know George Lucas playing around hypothetically playing around with gpt7 and trying to create Star Wars um he might type in just the prompt being like the two sentences uh and he would just say hey create a movie about wars between Star systems and that's just enough to give a model an ability to run on its own and this is actually not what you want you want to convey your cre ative Vision which for now in word is just knowledge work but soon it will be all work where it needs that human sprinkle that human taste and these are the things that I really value is like people say oh nobody will have a job whatsoever I don't agree with it at all I think the human taste and how you do things will matter even more and I use this George Lucas example because it's a little bit easier to understand how taste is influencing that but everywhere writing a good email is um is dependent on Good Taste figuring out I was just hiring for an executive assistant and like everyone in our company needs to show that taste and needs to show a little bit more conviction in the way that they do things so for executive assistant like she needed to choose a right restaurant for our offsite you know and that also has taste I don't want to trust nii with this um um so yeah that's kind of that sprinkle of human touch is very important so taste as the last Bastion of humanity I think so I think uh creativity and taste do you think do you think machines can learn human taste I think they can um but that's not the point there is an interesting analogy here um they put humans into an MRI machine and they've shown them two different pieces of art both of them were done by AI and uh we they told the people hey one is created by a human artist and another one is created by AI our brains work completely differently and our different parts of the brain fire when we assume human intent behind something so you know I can create a song and just like it will be a good song with sunno whatever and send it to my friend and he'll be like yeah it's a cool song but if I ingrain my intent and I'll create a song about our um you know skiing trip to shamoni and I'll make it funny and I will that intent will be there I'm pretty sure his brain will be firing in a completely different way giving him like giving him a completely different experience in that way does that make sense it makes a lot of sense yeah I love it I'm going to transition to talking about the next billion developers and you've alluded to this a few times in the conversation and I I really want to just pull on that thread so you started this you know this this conversation by saying you know there's a certain set of people in the world that know how to code uh but there's a different set of people in this world that have creativity and have ideas um do you think that set of people is larger is is that is that how we get to the next billion developers there's an interesting analogy here uh we have 30 million software engineers in the world and we have 750 million active users of Excel and now uh you will ask like how does wordware comp to Excel and what Excel did in the 80s to data analytics and numbers is what we are trying to do to AI so in the 80s you either had to have a team of data analytics um people or Engineers um or you're using a calculator and I would say the calculator equivalent here is the chart GPT that you every time you need to redo the conversation and you every time you need to instill your um own needs into it what wordware is trying to do is saying hey a lot of the things that you do are repeatable similar to excel um and you can encode your taste into it um with wordware um and I I believe that that taste will be important as as mentioned before and I think the next 500 million or a billion um users of AI might be calling them I don't know what will be the term hopefully it's wordware engineer but uh you know word Artisan or or whoever and the really important part here is that they need to know what the AI is supposed to do many many many times you know we are a horizontal tool and people come to us and they say hey what can I do with AI and I tend to explain it as if for now it's an intern but intern after University and you need to write out on a piece of P like piece of paper a couple things that you would want to do with the intern so you need to say hey um this is your job this is the you know title of of what you're trying to do here are some of the documents or inputs that you'll be working with um here are the data sources and here's the the output that I'm expecting of you and the important caveat here that people don't often understand is that the data sources has to be something that you trust you can't just say hey go and search the internet because often you you end up with things that you don't agree with and if the intern works on top of that that's a problem and another one is and this is very important is that you're going to trust the intern with this so you know if you want to send a thousand emails uh to every person that that that needs a response in your inbox with some people you just want trust an intern to do this um and this is how like AI Works uh right now so as long as you have a job right now that you say hey if I have an one intern I could easily explain it to them and a lot of our work is like this right now we often read an email go search and Dropbox go search notion and then we create a response that is essentially based on this database that we curate um then you can be using Ai and I think more and more this knowledge work is going to be automated and I think know in that way the next billion people are going to be um are going to need two things in what actions to happen and taste how do you want to do this and all of the rest will will feel like CEOs of the biggest Enterprise because we will have a thousand knowledge workers working beneath us and trying to actually execute on these two things what I heard just now was a lot of automation about knowledge work the I mean the thing that I'm most intrigued about within Ai and within generative AI is its generative capacity uh including the ability to create create you know you mentioned the George Lucas example but also to create you know new applications new marketplaces uh you know new products and so do you imagine do you see wordware primarily serving you know making the knowledge worker more productive or do you see it also uh assisting in kind of the creation of new products Services uh you know pieces of art um for now it's mostly about the productive work I would say um it's you know the AI engine is the AI heart of your product is wordware um currently we have not dipped into the generative UI part of things we're not lovable um we've actually used lovable to um you know rra our AI hard um for some of our customers and that has worked great I'm just so impressed with their product talk more about this uh lovable I think also sees themselves as you know enabling the next billion Developers you see you you have a similar Vision uh you know how do you think your view of how the world will go foots with with their view um and like why didn't you CH choose to make a you know that style of of a no code tool yeah because the way that I see the word developer is a little bit different they see the word developer as what developers do today which is you know a lot of SAS is a wrap around a database with some dashboard and ability to manipulate that data uh they are creating a much more personalized D dashboards you know and um a lot of people are going to create incredible vertical size based on lovable and I think that's incredible and the one thing that was missing through all of this is that they not only grabb the UI part they also grabed the database part which many people do not know how to manipulate and hence they unlocked a lot more use cases whatever but what we are trying to to say is that this part of like creating a generative U creating a UI on top of a database is not the future the future is to actually utilize this reasoning engine that an lmlm is in a productive Manner and we focus on that that that you know substance of AI at the beginning you know in the future like we might want to expose that engine as a in a UI maybe it's a chatboard maybe it's you know digesting some images Etc um but the real important part is the AI engine and yeah so if you think about an app as you know there's the UI there's the application logic and there's a database what you're saying is you know you really want to just you know knock it out of the park on the application logic so to speak yes and I think you know um the databases that we used to work on were um discret and and right now we are able to work on um a lot more data which is not structurized and this is the big big difference right now a lot of sasses right now will still work in a similar manner just the database is fuzzy and the database might be what you see uh every day and how the hell would you put that in a typical normal you know database um and I think working on top of that context is the really exciting part for me let's go back to this concept of word Artisans or wordware Engineers if everything goes right of you know what's your vision of what a wordware engineer looks like in 10 years oh that's a tough one um I've have been thinking a lot about what does work look like in 10 years for human beings and I was struggling with this at the beginning because um it's really hard to understand people's jobs even today and often I boil it down to um the software that they use they can talk a big game about strategy and you know I said the mission and in the end of the day I ask them do you do meetings do you do email do you do PowerPoint presentations do you work in Excel what exact or do you work in code and I just want to understand what does work look like in 10 years and like what are you really working with you know is it a hair like the movie Hair uh like interface when you just talk to the AI and it does a lot of work for you I think to be honest voice is kind of not the best modality to express that hence I kind of think that in its simplest form wordware is a document where you jot down your thoughts and you do it in a more structured way uh wordware a copilot AI is helping you throughout um structuring it and in the end of the day you behave like a CEO which sets the strategy intent and all of that on that piece of paper essentially on this blank canvas and you draw that Vision maybe it's even more than words it's just you know you generate this vision of how the how your own Enterprise works and you know I look at different things around us and I see furniture or shoes or whatever and I think there is taste ingrained into what kind of shoe would you like to make so in 10 years if somebody wants to be become a um a creator of the best brand of shoes it becomes about it that shoe becomes a luxury object which has ingrained taste and intent in it and then a bunch of things in the end will be um will happen on its own the really tough Parts even manufacturing it and so on will happen on its own but what's your job in the future is talking to other CEOs I think we will not humans don't want to lose that control so you will talk with other CEOs about maybe doing a partnership with your shoe brand and somebody else you have to be still critical about the intent of the other person and you have to instill taste and your own Creative Vision into that shoe do you think that you know do you think a billion people globally will be capable of programming a machine uh in English language the way you describe or in wordware documents the way you describe because it does require you know it's it's almost like pseudo coding and you know there is logic there's there's loops and things like that I guess maybe talk about today like do you need to be technical in order to use wordware like who is the ICP today and what is needed to move that ICP so that you can reach a billion developers yeah I think um right now what we've enabled is people who are somewhat technical CEOs technical PMS high up in the orc chart to ingrain their own like kind of think that they know what needs to happen and get there quicker you know so um MOX from instacart for example uh he is a founder and he spent four days just you know refining his idea in wordware instead of hiring a whole team but he is somewhat you know analytically minded and for now that's the case uh we did not want to make too much magic because the the models were not there um right now what we're doing is we're moving more into that blank canvas when you just describe the idea and we take care of guessing the right structure and you still will be able to you know in a very fine grained way edit it but you will start playing a lot more we'll probably use O3 to get you to the first draft of how that flow works and um when we kind of loop Back to the Future of how it will all look like and really whether we'll have 1 billion developers you know working in wordware it becomes a much bigger question here it becomes a question of like will a billion people want to do productive work like you know we just talked about the shoe how many people will have the drive to put out something to the world and they will want to express that Creative Vision maybe in you know post resource scarcity world most of the people want work but I think will still have the equivalent of billionaires and it will be about influence it will be be about taste and it will be about how you utilize your own resources and how do you multiply it to have the equivalent of future money and I went a little bit deep here but for what it's worth I think that I think the innate drive to create is like a deeply human drive and I think that exists in a post post capitalistic world I also have that opinion and I really believe in humans like I want them to succeed like somebody asked me one of our prospective employees asked me what like Philip in 10 years what do you want there to be like what what what what have you done and I want to save like the human Creative Vision I don't want everything to be AI I really have the pleasure when I go to an artisan shop on my holiday and I know that somebody put in the intent and put in the work and I want to interact with it and I want to interact with the story of it okay so today your ICP is the analytical career which is a little bit of a unicorn uh and over time as you kind of lower as the models get better as you iterate on your interface you'll lower the bar so it'll really be just more of the creative is is as your ideal user you're going to lower the bar of how analytical you need to be in order to use word yes but at the same time you know my use of the word creative is not to what most people associate it with right now I think a good creative is also you know using growth channels in the right manner they are creative about everything that they do in this new uncertain word of AI where everything is changing and you know I'm not thinking only about an artist that's painting on the canvas it's it's I I think creativity is um can basically show itself in so many different aspects of work let's talk about user interfaces and you know the future goys the gooes of the future um right before we filmed this podcast you made the analogy that you know Transformers or the new transistor maybe say a little bit more about that and what you think the new guey is going to be so I think the analogy here is that if um the llm is the well if Transformer is the the new transistor and it's being packaged as the model the model is kind of the main frame let's call it you know and then we took our sweet time to utilize the power of that main frame in a guey that's accessible by billions of people you know there has been really two big um spikes there the first was the desktop you know and apple came coming up with their guey um and the second one was mobile and um you know right now we are almost like exposing the numbers and the logic in a chat style thing and nobody has had a better idea we think that the document style is better for create like doing kind of more complex work um because often when you're trying to achieve something you just give it two sentences and the model just runs on its own and it's just enough the two sentences our lead investor recently said that the two sentences is just about enough for a model to hang itself on and you know you will get something completely different than what you actually wanted and this is a problem of like lovable ende devant of this world as well um but I basically think that uh there are better goys coming and you know whether they will be based on AR or um you know there will be an assistant that's listening to everything that we do I that was actually my first company um augmenting human memory of always on listening devices using gpt2 and BD I've been in this uh ah you've been in this since the gpt2 days I mean I my research wasn't into lstms which are the precursor to the Transformer architecture and I've been in this for a while I think nobody has yet delivered on this I want everything that I hear to be somewhere in a searchable database that also has the perfect context about me you know the way that I want to do things and I think those affordances and those like we called it goey but it's really the underlying like way of interacting with intelligence uh is is not going to be mainly chat I just I just don't believe it programmable documents do you think that is fundamentally what word looks like UI wise and call it 5 years um I think there is more and more magic in it and uh I would believe that I want people still to be able to do that fine grain work you know we've we've linked it with George Lucas doing the the movie you know in a way you almost want to firstly start with the high level thing the two sentence description and then zoom in and zoom in and zoom in and create you know modules which make the best scene that is 5 seconds and then combine them together in that way so what I would like wordware to be is uh to transcend uh obstruction layers and you know be able to zoom It All Out start with a sentence and have it run maybe see whether it's working in the right Manner and then as you seeing that some things are not doing the thing that you want them to do is to be able to zoom in and so and see maybe you know four senten of exactly what it's doing and what are the inputs to this what it's trying to do in the middle and what are the outputs you know that's kind of the most simple one level in and then you want to zoom in more and more and more as you def re redefine and reiterate on your idea of how this should be done how did you arrive at the current user interface I think it does feel really novel uh compared to how others are you know enabling AI Builders today how did you arrive at the current user interface was it more experimentation listening to users was it you philosophizing about you know what it should be I think uh currently the and currently and before the approach to creating these agents was a Blog based on a 2d canvas and once you know we I've been building agents for a long time you know I think you know March 2023 I put out the first article about how to build agent and me and Robert my co-founder we've been in this for a long time and the more the better the models got the prompting became more difficult because you can do more complex things with it so at some stage there was this movement of like the prompt is going away so on we actually really disagreed with it and that that idea has gone a little bit it's it's like you know we came back did a loop again and be like actually communicating your vision is really important and when we tried to communicate our vision which was a little bit ahead of what the models could have done at the time we started to not notice that the 2D canvas uh is just not enough like if you do a reflection Loop inside of a reflection Loop you run out of dimensions and we basically really like the way that code is structured code has an ability to uh Express very very complex uh Concepts in a way that is still like you can still manipulate it and understand it think about trying to structure the you know the whole Uber up with all of like everything in it on a 2d canvas it would become so cluttered and so messy you know you can do the big picture thing but not really the you know you you don't want Engineers to be interacting in that that way you want the engineers on the future wordware Engineers to be interactive with something that's easy to grasp the structure of very complex systems whereas the Uber app actually could probably be described in pseudo code uh and it seems like you're you know you're getting people closer to that Vision versus the 2D canvas um yes and I think you know the most important part here is that Uber has an agent equivalent and this is what we are trying to build you know if you want an agent to decide where is that person going and where are they starting their journey and whether they will accept uh that charge or you know you want to maybe make sure that the charge is right for that particular person there is an agent equivalent there and you know people are going like people can build that agent on wordware it's not like you're going to create that whole UI with with with with for Uber and I think you know probably Uber is the right obstruction layer you don't want to be ordering an Uber through a chatbot or through like a voice-based thing or you know but you might want an Uber to be uh to be ordered for you if you have a calendar invite so you know in a way that like for your personal use Uber is nice because you can click around and the agent will not always know but I was coming here and I wanted like a wayo actually wayo can't get that far yet but I wanted the wayo to be ordered and to be ordered perfectly when I need this and it's almost like a assistant personal assistant would do this for me and now that capability is open to everyone so we'll soon have these kind of ordinances and these kind of obstruction layers there I think that's a great note to end on should we end on a lightning round uh let's go okay one one or two sentence answers only uh okay first question what is your most hot take or contrarian take in AI not related to word wear or everything we just discussed uh pre-training was still going to matter and deep seek is a little blimp that people liked to uh people jumped on because people love a good drama and it was connected to China and actually it doesn't matter that much um okay I know I said lightning round but you you have to say more what what do you mean it doesn't matter that much I mean they utilize some cool techniques and the rest of the community is going to learn from that uh however you know like the fact that they like trained it for a little bit cheaper for like a lot cheaper does not involve all the experimentation that they did before that and you know I'm I don't know if I'm supposed to say that but I'm pretty sure they had access to the best nvidias as well for that experimentation and um it's not that novel like people jumped on it because they were like oh my God China is taking over the race and so on and Nvidia stock price like plummeted and I just think it's another place where some models were trained that were open sourced and it's not going to you know we're not going to remember it in like a like like a year or like even six months or maybe they will take over but the model doesn't really matter that much how you kind of work with that best model out there that's what matters that is a hot take indeed okay next question who's going to have the best Frontier Model next year o um I think open AI is always super bullish and they always promise a lot and then uh I was just on a talk with with uh Sam outman on the yci retreat and the O3 that the way that he pictured it sounded great but I think we both know that they overpromise a little bit a lot uh and I love anthropic uh I think their kind of vision and their kind of the way that they've created this is great but recently Gemini 2.0 pro with their abilities to you know ingest 6,000 pages of of PDF is really blowing my mind so end of the story is I have no clue this is a place where this is a place where it's super fragmented and people have zero loyalty pre-training is hitting a wall I think you know famous people including Ilia have been have included saying something to that extent recently agree or disagree uh disagree uh right now I think you know it's the intelligence of a model is linked logarithmically to the resources that is are needed to train it but you know doing a 2X of intelligence is in its on its own exponential like if I'm smarter tox than somebody else it doesn't mean I'll do 2x of the work it means that I'll find ways that probably mean I'm a 10x or even more favorite new AI app not wordware uh I would say I started to edit content because uh we need to explain and educate people a little bit more about both wordware and AI so the script is something that I've been I've been loving and I use granola every day uh and the newest model that I'm really impressed is the Gemini 2.0 pro um I really like it that is that's a hot take as well I haven't heard much of that from people I think it came out like four days ago so people have not been playing around with it their PDF capabilities are awesome what application or application category do you think will really go mainstream and hit this year I would love to see I'm personally very very involved with that call um AI having the context of your life and being able to you know basically make better decisions based on the context and you know I've rewind which you know I think it they are called Limitless right now I've ordered their pendant by the way it's been like a year and a half and I still don't have it uh I don't know send it to me you're listening please send it uh and I had to change a color cuz I they didn't have the color but I would love for there to be a provider which has a lot more context and can do the personal stuff for me uh don't you think that's Apple over time I was just about to say I think ideally that n421 model or whatever it's called of the AR glasses that they are trying to push out there which I think Facebook has taken over a little bit maybe we'll see early um stages of that and I think they're the only ones where the Privacy really like they have a good brand around privacy and two even if you're new AR glasses run out of battery it's still cool to be wearing a $5,000 you know uh piece of hardware and maybe does the ux but I don't know what's that ux and like a microphone so far failed um yeah single piece of content that an AI official AO should read or watch I would say all of the deep learning.ai resources everyone like we have bunch of candidates apply for jobs by the way we are hiring whatever I should be looking uh very very aggressively so come join wordware but uh The Deep learning. a resources are awesome and they explain everything from uh from the bottom layer all the way to the Practical uh layer of how to actually get it done um I also think if you don't understand the underlying technology go see free blue one brown an incredible Channel on YouTube and they explain everything super well um yeah I think wonderful your Lightning Run was full of hot takes I didn't even have to ask you for a specific hot take uh well philli thank you so much for coming on I really enjoyed chatting about you know how you see the world evolving from developers to word Artisans or word you know word uh wear engineers everything goes right and appreciate you sitting down to share your vision and your hot takes thank you for having me thank you [Music] [Music]

========================================

--- Video 22 ---
Video ID: Rp1uwO8T_cQ
URL: https://www.youtube.com/watch?v=Rp1uwO8T_cQ
Title: The near future of video creation: Josh Woodward, VP Google Labs | Training Data #ai #creative #tech
Published: 2025-03-21 12:30:25 UTC
Description:
What if you could say "make that red sweater blue" and change an entire film instantly?

Josh Woodward, VP Google Labs, shares how AI video tools are evolving beyond basic generation to full creative control, lowering barriers while raising possibilities for filmmakers everywhere.

Transcript Language: English (auto-generated)
I think generative video is kind of moved from this moment of almost possible to possible. Six months ago, year ago, few years ago, you had Will Smith eating, you know, past was a disaster. And then even last year, you had kind of these videos of like knives cutting off fingers and they were six fingers and you know, it was like that's where we were. Um, so I think physics, tons of progress, the ability to do photorealistic quality, uh, very huge progress. So, I was talking to a couple AI filmmakers this week, and what they're really interested in is exactly what you're saying. Character consistency, scene consistency, camera control. It's almost like we need to build an AI camera. You think of some of the cameras that are kind of filming us right now. This is sort of like decades of technology that's kind of been perfected for a certain sort of input output. And I think we're on the verge of kind of needing to create a new AI camera. And when you do that, you can generate infinite number of scenes. You can generate like, oh, you're wearing a red sweater now make it blue. And not just in that scene, but in like a whole 2-hour film. We kind of talk internally in the team about how do you kind of lower the bar and raise the ceiling. And what we think about that when we're building products is how do you make something more accessible? Yeah. Or how do you make like the pros take it and just blow, you know, the quality out of the water and make it incredible stuff. Um, so that's what we're seeing with video. It's kind of right at that point where both are happening.

========================================

--- Video 23 ---
Video ID: ppfWLkYteaA
URL: https://www.youtube.com/watch?v=ppfWLkYteaA
Title: AI can amplify human creativity - Josh Woodward, VP Google Labs | Training Data #ai #podcast #tech
Published: 2025-03-20 17:57:45 UTC
Description:
"Are you trying to replace and eliminate people, or amplify human creativity?"

Josh Woodward, VP at Google Labs, cuts to the heart of what matters in AI development today.

Transcript Language: English (auto-generated)
actually when I first started at Google it was like right as the iPhone moment was kind of Just Happening and taking taking hold you when Steve walked on stage in 2017 said this is the iPhone if you look at the App Store 3 years later which is roughly where we are in this AI Revolution the App Store in 29ish I went back and checked websites that have been shrunken down to fit on your phone y flashlight apps and fart apps these are like the highest top downloaded things that were happening so I think we're kind of in this stage where the real stuff is going to start to come out kind of this year next year the next year that's when you start to see the Ubers the airbnbs the instacart the things that really change kind of how you do stuff I'd encourage people listening to like really think about of course there the models and who's winning and the back and forth but like what are the values You're Building into your company because I think this is one of those moments where there's going to be like tools created that shape like follow on Generations I think it's really important people think about that and like are you trying to replace and eliminate people or are you trying to amplify human creativity I'm on the side of wanting to amplify human creativity but I think there's like there are these moments that happen in our Valley here where like things change and they change often for generations and they can change for good or bad and so I would just encourage people that are in spots where you're building and you have this incredible technology that's only getting smarter and faster and cheaper to put it to good use and think about the consequences is Downstream

========================================

--- Video 24 ---
Video ID: 3-wVLpHGstQ
URL: https://www.youtube.com/watch?v=3-wVLpHGstQ
Title: Josh Woodward: Google Labs is Rapidly Building AI Products from 0-to-1
Published: 2025-03-18 09:00:04 UTC
Description:
As VP of Google Labs, Josh Woodward leads teams exploring the frontiers of AI applications. He shares insights on their rapid development process, why today’s written prompts will become outdated and how AI is transforming everything from video generation to computer control. He reveals that 25% of Google’s code is now written by AI and explains why coding could see major leaps forward this year. He emphasizes the importance of taste, design and human values in building AI tools that will shape how future generations work and create.

Hosted by: Ravi Gupta and Sonya Huang, Sequoia Capital

Transcript Language: English (auto-generated)
what I found too Building Products over the years is it's very common everyone talks about product Market fit you'll know it when you see it and all that which is true but at least for me I've always felt in the first part of Building Products you iterate a lot on the product and sometimes you forget to iterate on the market and finding the right Market side is also just as important as the right product and you have to connect those two and so I think that in these early stage things with Mariner that's where we are it's like does is it possible for a computer to like an AI model to drive your computer yes that's a huge new capability is it accurate sometimes is it fast not at all yet like that's kind of where we are um in terms of the actual kind of use case or the capabilities and then it's about finding the right Market [Music] today we're excited to welcome Josh Woodward from Google Labs the team behind exciting Google AI launches like notebook LM and the computer use agent Mariner Google Labs is Google's experimental arm that's in charge of pioneering what's next and how we interact with technology by thinking about how the world might look like decades from now Josh is helping to reimagine human AI interaction from the provocative claim that writing prompts is already becoming archaic to the emergence of multimodal AI as a default user experience he shares insights on the rapid Innovation culture in Google Labs offers a glimpse of what's next in generative video and much more Josh thank you so much for joining me and Ry today we are excited to hear everything that you're doing over at Google Labs maybe first to start you mentioned a provocative topic to me uh on your way in here writing prompts is old fashed what do you mean by that okay so um thanks for having me uh I do think it's old fion we'll look back at this time from an enduser experience and say I can't believe we tried to write paragraph level prompts into these little boxes um so I kind of see it splitting a little bit right now on the one hand as a developer an AI engineer you should see some of the prompts that we're writing in Labs right now are these beautiful like multi-page prompts but I think for IND users they don't have time for that and you have to be almost like some sort of Whisperer to be able to unlock the models ability so we're seeing way more pull and traction and I kind of seeing this in other products in the industry too right now how can you bring your own assets maybe as a prompt drag in a PDF or an image sort of recombine things like that to sort of shortcut this giant paragraph writing so I think it's going to kind of divide I think as Engineers AI Engineers you'll keep writing long stuff but I think most people in the world we're probably in a phase that'll sort of fade out here pretty soon so the form of of the context will change right you know so you still have to get give the model something right but it might be that you can communicate it via picture or communicate it via like just look at this set of documents yeah your voice a video any of that these models love context so the context is not going to go away but we're making a lot of bets right now that the type of context and the way you deliver the context that's changing really fast right now I love it okay uh we're going to go deeper into the future of prompts and multi mulle models in this episode maybe before we do all that say a word on what is Google Labs you know what what's the mission and uh tell us a little bit more about how you sit where you sit with inside Google yeah so Google Labs if anyone's heard about it we had one a long time ago that went dormant for a while and this is kind of back about three years ago it got started it's really a collection of Builders we're trying to build new AI products that people love so they can be consumer products B2B products developer products it's all zero to one um it tends to attract an interesting mix of people maybe people who have been at Google a while but also a bunch of startup Founders and exf Founders and so we kind of mix these people together and we basically say what's the future of a certain area going to look like say the future of creativity or software development or entertainment and they go off in small little teams and they just start building and shipping and so that's how it operates and it sort of sits outside the big traditional Google gole product areas but we work a lot together but there's kind of an interesting interplay there and I think that's been part of what's been fun about it is you can kind of dip in maybe work with search or Chrome or other parts of Google but you also kind of have the space to explore and experiment and try to disrupt too and that's that's kind of what we're up to how do you create the culture inside of labs that you want right if you think about there's got to be a lot more failure presumably than there are in other parts there's got to be a different metric for Success than there is just the sheer scale of Google so what is the culture you're trying to create and how do you create it so we really pride ourselves in trying to be really fast moving as a culture so we'll go from an idea to end users hands 50 to 100 days um and that's something that we do all kinds of things to try to make that happen so speed matters a lot especially in kind of an AI platform shift moment the other thing is we think a lot about sort of big things start small and one of the things if you're in a place like Google you're surrounded by some products that have billions of people using them and people forget that all these things started with solving usually for one user and one paino and so for us we get really excited if we get like 10,000 weekly active users it's like you know we'll celebrate that that's a big moment when we're starting a new project and for a lot of our other kind of groups inside Google their dashboards don't count that low right I mean it's like so there's kind of this moment where you know the size of what we're trying to do is very small um it probably looks a lot like companies you all work with honestly from that uh perspective and I think the other thing we're trying to do is because we sit outside the big groups at Google we kind of have one foot in the outside world we do a lot of building and kind of co-creating with startups and others but also one foot inside Google Deep Mind and so we've got kind of a view of where the research Frontier is and more importantly where it's going and so we're often trying to take some of those capabilities in so we take a lot of Pride and sort of finding people who are very creative people who are almost like see themselves as underdogs um they have kind of a hustle to them and so we have this whole dock called labs in a nutshell and my favorite section in the dock is called who thrives in labs and there's like 16 or 17 bullets that just list them out um and that's kind of how we try to build the culture but you do have to normalize things like failure you have to think about things differently around promotion compensation all these things that you kind of would do in a company too you mentioned the Deep Mind links I think that is super cool what have you found is the kind of Ideal kind of product Builder Persona inside Labs is it somebody with a research background is it somebody with a who comes from a successful consumer product background is it you know is there the magical unicorn that's great at both research and products what type of person we take as many unicorns as we can find and we actually I found some uh which is great you do look for that kind of deep model expertise as well as kind of like a consumer sensibility in terms of those people exist they exist they're great too um if you can find them uh and we also kind of have found ways to kind of train or develop people so that's another thing we think a lot about is like how do you bring in often people that might not be the normal talent that you look for so like we're always in the interesting kind of zone of like who's undervalued who's kind of like really interesting but maybe not on paper but when you interact with them look at their GitHub history I mean there's all these different signals you can look at um but yeah that's kind of how we would think about it really cool how do you decide what projects to take on next is it is it bottom up top down how does that work yeah great question we kind of do um a little bit of a blend actually so at the top down side we're looking at what are the areas that are kind of on mission for Google that are strategic to Google because we sit inside it so we're thinking about ourselves in that broader context so that may be for example like what would the future of software development look like there's tens of thousands of software developers at Google and obviously this is an area that AI is clearly going to make a big change in so we'll be thinking about could we build things for other googlers but also externally how do we build things like that so we take that kind of top down view think of it as almost I'm from Oklahoma we like to fish a lot in the summer but like you're trying to figure out what's the right Pond to fish in so we put a lot of thought into those like ponds to fish in but then we let a lot of these teams often their four or five person teams come up with the right user problems to go try to solve and that's where we kind of meet in the middle and I think for a lot of other teams they might look at what we do it's a little chaotic you know we don't have like multi- quarter road maps like we're trying to survive to the next whatever 10,000 user Milestone and then try to grow it uh but I would say it's kind of that sort of blend what's one of the products that you guys have built that you're excited about now oh yeah so I guess if you've ever used um the Gemini API or AI Studio notebook LM or any of VO any of these things these are products that we've kind of worked on from Labs I mean maybe I'll talk about one that's maybe well better known and one that's coming up so the very excited about where notebook lm's going I think we've hit on something where you can bring your own sources into it and really AI just like grips into that stuff um and then you're able to kind of create things so a lot of people maybe have heard the podcasts that came out last year there's so much coming that follows this pattern um so watch this space uh they um there's just a lot you can do with that pattern and I think what's really interesting is it gives people a lot of control they feel like they're steering the AI we have this term on the team at actually one of the marketing people came up was like an AI joystick that you're kind of controlling it so that's interesting um I would say there's a lot of stuff coming right now we're very excited about vo um Google's imagery model and sort of video model and where those kind of come together so we've got really interesting products coming along in this space I think maybe we can talk about that some at some point but I think generative video is kind of moved from this moment of almost possible to possible and I think let talk about it now tell us yeah yeah well I think it's it's interesting these models are still huge to run like V2 takes hundreds of computers right so the the cost is very high but just like we've seen with the text based models like Gemini and even ones from open Ai and anthropic you know the cost is reduced like 97 times in the last year so if you kind of assume cost curves like that what you're going to see with these vo models what's kind of brand new say with V2 is it's really cracked really high quality and physics um so the motion the scenes the if you talk to a lot of these AI filmmakers they talk about what's your cherry pick rate which is a term for like how many times do you have to run it to pick out the things that's really good and what we're seeing with something like VI is the cherry pick rate is going down to like one time got what I want and so the instruction following the ability for the model to kind of adhere to what you want is is really cool so I think when you put that in tools um you're now able to convey ideas in a whole different way what do you think are the solved problems and the unsolved problems in AI video generation cuz I remember you know uh last year it was like you know even last year there were all these you know there was so much talk about you know generative video is you know a physics Simulator for example right right can kind of emulate physics and it's like that's amazing is the physics stuff solved do you think like what else is you know what's done and then what's to be solved yeah I would say physics is a hard thing to solve forever but it's close I would say it's close enough yeah but you're six months ago year ago few years ago you had Will Smith eating you know pasta was a disaster and then even last year you had kind of these videos of like knives cutting off fingers and there were six fingers you know it was like that's where we were um so I think physics tons of problem ress the ability to do photo realistic quality uh very huge progress the ability to kind of do jump scenes and jump cuts and different sort of camera controls that's really coming into almost solved there's paths to solve all this stuff um still going to solve the efficiency and serving cost I would say and probably still have to figure out a little bit more of like the application layer of this cuz I think this is another big opportunity as we've seen like a lot of other modalities with AI you get kind of the model layer you get kind of the tool layer and then the real value we think is in this application layer and so I think that's really interesting to rethink workflows around video and I think that's pretty wide open right now do you think the models are capable of you know even having video that is malleable at the application layer so for example if I want to have character consistency between scenes are the models even capable of that or I imagine you want model steerability in order to be able to kind of work with it at the application Level like what what is model Readiness um and what's required in order to be able to do magic at at the application yeah so I was talking to a couple AI filmmakers this week and what they're really interested in is exactly what you're saying character consistency scene consistency camera control it's almost like we need to build an AI camera you think of some of the cameras that are kind of filming us right now this is sort of like Decades of Technology that's kind of been perfected for a certain sort of input output and I think were on the verge of kind of needing to create a new AI camera and when you do that you can generate infinite number of scenes you can generate like oh you're wearing a red sweater now make it blue and not just in that scene but in like a whole 2hour film so there's all kinds of ways that we're starting to see these prototypes that we're working on too internally where this is this is here like it's coming um we're kind of entire I think things that used to either be too expensive or too Timely or it required a certain skill level um we kind of talk internally on the team about how do you kind of lower the bar and raise the ceiling and what we think about that when we're building products is how do you make something more accessible or how do you make like the pros take it and just blow you know the quality out of the water and making incredible stuff um so that's what we're seeing with video it's kind of right at that point where both are happening there was an interesting tweet from or post from Paul Graham recently on this idea I think of based on this pace of progress he's like you sort sort of want to be building things that kind of don't quite work yes and are way too expensive yes right because they're going to work yeah and their cost is going to come way down y right and so I would imagine that has applicability for you guys too particularly in video that's exactly how we do it yeah I mean right now I don't know off the top of my head but each video 8sec clip generated is obscenely expensive but we're basically building for a world where this is going to be like you're going to generate five at a time not even think about it one of the actual principles I've kind of learned just over the last few years working all this AI stuff is make sure your product is aligned to the models getting smarter cheaper faster and if your core product value prop can benefit from those Tailwinds you're in a good spot if any of those are not right question Your Existence like that would be my uh my summary takeway on that yeah I like that how far do you think we are from having uh economics of video generation that are you know right side up where where you know it costs less to to generate the thing than the economic value of of generating it yeah oh wow this is tough this is a prediction you're never really sure about I don't know but I would say one thing we're seeing just as we're modeling out a lot of costs because we're starting to put vo into some of our own tools that are coming out is we're probably going to need Innovation on the business model side in addition to just the product and the application layer and what I mean by that is you could our first thought was oh let's just make a subscription and then just charge per usage on top that might be a way to to do it another way to do it is when you talk to some of these creatives whether they're in Hollywood um or even these AI filmmakers that are popping up they're kind of like okay I want this output and I'm willing to pay this much and it's kind of a pay per output kind of which you've seen in other cases AI companies are starting to do some of this too but for sort of film and video that's it's a little bit how you'd think of doing a project if you were a producer but now you're kind of imagining it like the individual cre creative level which is kind of interesting so that's more like almost like an auction type model potentially so I think there's a lot to explore I think we're probably though you know the pace things are moving it's it's on the it's on the scale of like quarters I think where it starts to get interesting as opposed to like many many years um so that's yeah I think there's a path you talked about the pace of progress a couple times yeah do you think it's accelerating you have the a unique view in the Deep Mind and let's use that as a I don't know Harbinger for some of the others yeah yeah as a propy yeah what what where are we at are we accelerating are we you know on a crazy uh trajectory and maintaining the same one like yeah I'm interested yeah yeah um I keep thinking it will slow down and it's never slowed down in the last three years um so you know you think oh pre-training might be plateauing inference time compute a whole another Horizon opens up and I think there is so much um there's an author on the team we actually hired his named Steven Johnson he Co found in Notebook LM when we first brought him on and he talks about this notion of like there's adjacent posses he has this really interesting book on the history of innovation and I feel like right now it's like you walk into this room and there's all these doors that are opening up into these adjacent posses and there's not just like one room and one door it's like one room with like I don't it feels like 30 doors that that you can go explore so I think that's what it feels like on the inside I love that visual of the the rooms and then the adjacent posses I'm going to steal that and maybe take it and call it my own plasic VC over here um what do you think the future of video consumption looks like for us as consumers like am I still looking at Hollywood style feature films that are created by Hollywood Studios just done a lot more coste efficiently am I looking at a piece of content that's dynamically generated to what you know about me and it's only for me to watch am I like what what what do you think the future of is as so this is one of those that could go in spider in many different ways I would say I'd say some of the things we're excited about and what we see so I think the future of entertainment is way more steerable so right now you think about you sit on your couch like this and you maybe scroll through something or whatever you cast it on you bring it up on the TV so it's going to be way more steerable where you can kind of interject if you want and maybe take it certain ways we think that's one area we think another is personalization like you said if you think today about YouTube Tik Tok any of these algorithms that can kind of figure out this is what you're interested in imagine that I think way more extreme uh that could be kind of fine-tuned to sort of what you want to share with the model um I think the other bit is a lot of this I think is going to be generated on the fly so another theory we have is that just like there was a rise of kind of a Creator class couple whatever 10 15 years ago that powered YouTube and the rest there's going to be a shift or maybe it's a different set of people that we think of as like curators where you curate stuff and you work with the model to maybe create things and I think another loop in that is how you can remix all this and so that's another big part of what we see in the future of entertainment is that there'll be like oh I kind of like that but they'll make it more like this and if you think you know at some level the cost the time the skills required of this is literally maybe just like tapping a button or just describing it and you get kind of different versions that's kind of where we see some of this going it will be really interesting to see if like some of these same percentages hold like we know today that a lot of times certain percentage like 90 95% just consume from platforms and you have very small Creator class so like will that balance change um but I see a totally different ways you could think about content platforms that have some of these native controls um like for example will we expect uis that have a join button where you know today our uis maybe have a play pause whatever save bookmark something star heart it like will there be like new things where you join and they're like oh hey Sonia Ry what do you want to talk about you know what I mean and I think like that's totally possible we're building that in the notebook LM today uh so that you can imagine Play it Forward you've got avatars or humanik characters or not with lip animation voice cloning all that can come together in sort of new ways I think do you think movies and games start to blur yeah I think that's a real possibility yeah there's a whole interesting intersection that's happening right now between movies or video content games and sort of World building and 3D and it's really unclear to us right now where that's going to go but there's so many areas right now where we're seeing learnings from each and even down to some of the training techniques we're finding things like that yeah so actually that was going to be one of my questions like if you look at all the companies building generative video models right now some people are kind of going straight from the you know the pixel stream so to speak and some people are going from the 3D angle with with the idea that you know to really do video right you need to get 3D do you do you have an opinion on that yeah we've actually got bets on both sides right now I don't know I don't know yeah we're hedged we're hedged on this one so on the 3D side we have this project we got started where we basically said like take six pictures of a sneaker and create a 3D spin of it and we put that on search it's been really great and it's amazing how it fills in the details but I think what's interesting as we've been going down that path something like V2 shows up now you don't need six photos anymore you need like two or three and you can basically do like an entire product catalog like every product that's ever been indexed at Google just overnight sort of can create it so now you've got a 3D object basically of any object bookshelf chair whatever from any angle that you can pan tilt Zoom relight and now that's like an object that you can drop in anywhere so that's kind of the 3D angle from the video angle it's interesting or kind of the World building we had this little prototype we built we're like wouldn't it be cool if you could recreate landing on the moon for like every classroom and like give teachers a tool where they could put the kids in the like you know lunar module as it's coming down so we built this thing it's kind of terrifying actually because we also built a little side panel where you can inject problems where it's like oh no something's on fire in the back like simulate things we had a little fun with it but that was interesting cuz the models you could say like look right and it would actually fill in the details um and so you start to get this that's why it feels like it's kind of blurring and I guess why we're hedging on both sides right now yeah we're not sure 2025 everyone's talking about agents yes yeah computer agents yeah you just said it three times exactly proba being a VC again exactly I've been called a VC twice today um this is a very big insult uh can you talk to us about Google Mariner yeah yeah so Mariners one we put out in December last year this is a fun one actually because we started seeing this capability developing in the model we're trying to understand if you could let these models control your computer or your browser what would happen um good and bad um and so that was a good example of a project where we went from hey this capability is kind of showing up let's put it into right now it's a Chrome extension just because it was quick to build idea in people's hands 84 days uh very fast very a lot of memories made on that but I think what's interesting is you're seeing both across anthropic open AI obviously Google and a bunch of other startups in the space are all hitting on kind of the same idea that models are not just about maybe knowledge and information and synthesis and writing but they can do things and they can scroll they can type they can click they can not only do this in one browser in one session but like an infinite number in the background um so I think with Mariner what we're really trying to pursue is like of course there's the near-term thing of like can it complete tasks in your browser but the bigger thing is what's the future of human computer interaction look like when you have something like this kind of not just one of these things but basically like an infinite number uh kind of at your disposal and so that's what we're chasing with that project what do you think the ideal use cases are maybe even in the near term for Mariner because I I think all the demo videos I see not necessarily from Mariner specifically but with computer use more broadly or you know here have this agent go book a flight for me or go order a pizza on door dash for me right like that's nice but like I like doing those things yeah yeah yeah you're pretty good on those on your phone is one of my one of my uh Delights in life and so um what do you think are the the killer kind of consumer consumer use cases yeah well that's what's interesting it may not be consumer it may be Enterprise and one of the things we're seeing when we do all the user research right now on Mariner because we have an usted tester and people are playing with it and giving a lot of feedback is it's really these high toil activities toil is kind of an oldfashioned word that doesn't get used a lot but this is when people talk about it it's like this is what makes me grumpy and this thing is helping me solve it but what's interesting is a a lot more of those are showing up on the Enterprise side just to give you a couple examples from yesterday we were hearing from one of the teams and they're basically they have this co- browser use case so imagine you're in like call center somewhere some customer calls in they right now have this very complicated way the agent in the call center can like remotely take over your machine that's not working browse through things and do something for you they were like we would love to have Mariner do this um and that's like a way another one we heard which was kind of interesting was people they are like part of a sales team or something they have take a customer call then they've got all these next steps they need to do and they just want to fan that out and it's often updating different systems sys that are all probably I don't know some SAS subscriptions they're paying everywhere and they're just like the UI is clunky it takes a long time I just want to send Mariner do all this so these are the kinds of things that are kind of interesting that are just naturally coming up on the consumer side I don't know have you found one yet in your mind that you like because I we're we've got a few but I it's I'm curious I'm think I'm trying to think what the toil I have in my everyday life yeah talking to Ry uh I'm kidding I'm kidding talking to Ry the best part of my day want to appreciate that I think but I like the framework even if we don't have the exact use the framework of like what are the things that are the heavy lifting that you don't enjoy right throughout the day that take up time away and I do think that that was actually the same logic that yielded things like door Dash or instacart right um you see how I had to get insta card in there I'm just making sure that that was there um on the Enterprise side when you think about it yeah um how are you testing that are you testing that with existing you know uh customers are you testing that with Google Cloud customers like who are the Enterprises that you guys will actually like test things with yeah so in that case we kind of go across big and small so there will be some Cloud customers we have a lot of cloud customers who always want the latest and greatest give us that they have like Labs equivalents inside their companies right so those are awesome test beds we also work with a lot of startups um and I mean if there's others listening to this that are interested let like DM me let me know like cuz we're always trying to learn kind of from different sides of the market what I found too Building Products over the years is it's very common everyone talks about product Market fit you'll know it when you see it and all that which is true but at least for me I've always felt in the first part of Building Products you iterate a lot on the product and sometimes you forget to iterate on the market and finding the right Market side is also just as important as the right product and you have to connect those two and so I think that in these early stage things with Mariner that's where we are it's like does is it possible for a computer to like an AI model to drive your computer yes that's a huge new capability is it accurate sometimes is it fast not at all yet like that's kind of where we are um in terms of the actual kind of use case or the capabilities and then it's about finding the right Market but yeah to answer short it's kind of in these early days we do lots of stuff really quickly and what I kind of Coach our product managers on and other people on the team because we have engineers and uxers they all go to these sessions is like don't look at the dashboards it's too small numbers right now look at their eyes like look at the customer's eye and when you show them stuff do they light up or not you know what I mean and like that's kind of the signal you're following it's way more art than science at this stage can we go back for a second just to the context point because I was thinking about this V like you working at Google right and you talked about bringing your own you know um is there a world where where someone can just opt in of like Google knows a lot about me right already you know my searches my Gmail my calendar is there a world where you can just sort of opt in be like I don't want to bring it all now I just kind of want you to use what you got and make magic right is that something that could happen because Google's uniquely suited to be able to do something like that probably more so than anybody U is that something that you guys can play with in Labs or have a possibility for or is that not possible we do some more kind of internally with some of our own like data on the team right where like I've opted into a lot of things it's just like take it all like let's make good stuff um but I think you'll see some of that come through in the Gemini app too where you can link different things but I think it's actually an area that's like actively kind of being explored too of like what types of data is like the most interesting and the most useful and of course also the right controls where people feel like okay I'm not just giving it away yeah so I think that is an area though that we do experiment on um some but I'd say right now a lot of the experiments are more on our own stuff as we're trying to figure out you're going to have to tell us separately some of the things that you could have done now now that they know everything about you you know like what is the magic that can be created for you yeah I think certain things that immediately come to mind that are pretty powerful is you can you can see things like in my own data I feel like I have a second brain that is a true like there's always been this vision of a second brain and tools for thought and all this stuff and I feel like you can get pretty close to that and I think the Gemini model specifically is really good at long context the ability to have this like impressive short-term memory and so Gemini too that's an area we're really trying to exploit right now like how to use that on Mariner yeah similar question to what I asked on on vo uh when do you think we'll have computer use that is accurate enough and is fast enough to do some of these use cases you talks about yeah that's another one it's kind of hard to tell at the pace though right now I mean not just inside Google but what you're seeing from some of the other labs too they're on like about an every month or two rev so you can imagine just this year we're going to see four five six revs of each of these things right um again that's just what we know is happening um I think the areas that are a little bit trickier or harder right now is how the computer like finely or precisely navigates like the XY coordinates almost you almost want like a lat long of your screen and that's still kind of really interesting Jagged edges on that I would say the other big area I would say is like this it's more of a human thing like when do you want the human involved or not when do they want to be involved or not and kind of creating the right construct almost was like Hey I'm about to buy something oh no I want to know about that or I'm okay for $5 but nothing more than that do you know what I mean and so there's a whole bunch of almost like hardcore like HCI like research and like really going deep on the empathy of like how you set those controls that I don't think any of them including the Google Mariner one right now we don't have I mean we do certain very blunt things like don't buy anything don't consent to any toss you know like so there there's so like crude uh things right now that you can do but I think people are going to want a more fine grain way so these are some of the things that are I consider more unsolved again that principle just banking on the model is going to get smarter faster cheaper um and you're going to get four or five six seven revs this year um yeah okay I have a meta question yeah how come all of the research Labs converged on computer use at like as far as I can tell the same exact point in time was that an accident was that just all the technology happened to converge at the same time like what happened there it's a good question I mean this is I don't know the specifics there of each of the other labs but I would say you know when you read about the history of innovation and there's like all kinds of things on this there it's not uncommon that discoveries kind of around the same time and I think there's kind of a new paradigm now with these models and I think lots of people are seeing the potential in certain ways and I'm sure there's also I don't know people changing labs and other things that are cross-pollinating all these ideas too but it does feel like it's one of those is kind of how I'm interpreting it is like I think similar with coding right you saw there's already even the agent stuff right now there's lots of this stuff kind of bubbling um which makes it really fun but also keeps you on your toes right cuz this is kind of the underdog mindset here are you going to hire any other authors the reason I asked is I was thinking about I think Matt Ridley is the one who's written about some of these things about like adjacent Innovations you know you have Stephen Johnson maybe why did you hire stevenh Johnson how did that happen and are you going to think about other people that don't have obvious backgrounds that you would bring into Labs yeah yeah so um the quick story on Stephen was um the guy who kind of restarted Google Labs was a guy named clay bore who mut friend exactly and um he and I big fans we've basically read everything Sten had written and Sten was a very interesting guy because for like decades he's been in search of the perfect tool for thought and so clay clay cold emailed him and we were both subscribers to his substack we kind of messaged them and we're like we love you will you come work with us we can build the tool you've been wanting to build that's where it started actually and this was like I mean it was like summer 22 so like before any of the you know CH Chi moment or anything and stepen picked up the phone he was like yeah let's do it so he came in he was a visiting scholar the job ladder didn't exist uh I had to go figure out with our HR person how to create a role that he could take on so was very kind of unconventional in that way um and then the rest is kind of History obviously um I've read a bunch of Matt's books I don't know Matt he'd be awesome so if he's listening like he's listening come that's right that's right uh I would say we've done this quite a bit so we've actually brought in musicians I'm actually really we're trying to figure out right now like a like a visiting filmmaker that's cool um so it's kind of a model Stephen kind of pioneered it he was the first one that it's like how to bring in it's a big value in Labs of how do we co-create we don't want to just make stuff and throw it out there we actually want to co-create it with the people that are in the industry and what we find when we do that is you actually get Way Beyond the like oh that's cool toy AI feature you get into the workflow and if you're working with someone like stepen Johnson who's written you know dozen plus books there's a certain way he thinks about and almost like a respect for like the sources and the citations all that stuff comes through a notebook LM and we're doing similar stuff with music and video and IND and other stuff yeah is the goal to create ne new products that you can take from one to 100 to to a billion Standalone or is the goal to you know find product Market fit with things like notebook LM and then really fold them into the Google Mothership so to speak yeah it's interesting so when we first started I would say it was all about build something graduated so kind of a traditional incubator sort of model it's been interesting as it's gone along we've done that some cases like AI studio and the Gemini API we graduated and that's now in Deep Mind and they're kind of running with it um something like notebook LM we're just going to keep in Labs right now for the foreseeable future cuz it's kind of a different creature like it's only possible with Ai and a lot of the stuff we're working on now I mean we'll have to see how many of these we can put together that actually can kind of get escape velocity but we're really interested in turning them into businesses and making them sustainable and kind of you know that's been a lot of the the focus actually is like take big swings and that gets back to your point a lot of these won't work um because if you're just if they're all working you're not swinging big enough yeah so it's like trying to find that balance but that's definitely we start with kind of could we make this a business work backwards from that and if we end up graduating it that's still a good good outcome for us another good outcome is we stop it and was like cut the losses we did our 100 day Sprint or whatever move on to the next thing yeah you mentioned at the top of the episode that you try to do some top down thinking of you know what are the most interesting pools for us to be building in yeah what are your predictions on the most interesting pools to be building in for 2025 like where are you hiring um talents like where you where you sniffing around where are you co-creating with the the Deep Mind folks yeah yeah there's a lot happening with agents there's a lot happening with video some of the things we've talked about with computer use but I think about those ponds a little bit different I think about them we have this doc called Labs is a collection of Futures and it's 82 predictions about the future um which is always dangerous to make one prediction about the future let alone 82 but the thought experiment on the team where we got to this was imagine you're in a room like this the ceiling just opens up and this little capsule comes down we all jump in it and it slings us into the future it's 2028 you can get out you get five minutes look around write down everything and you're brought back to the present and then write what you saw and that's what this dock is is so what's the future of knowledge look like what's the future even though prompts are oldfashioned that's a pretty good prompt that you gave to the team tell you right now yeah yeah so that's you know we think about we think about it at that level at kind of a high level so say something like what's the future of knowledge going to look like we think it's going to be one piece of that prediction one of the 82 is that it's infinitely remixable and anything that comes in can be transformed and become anything on the way out if you believe that then you take certain bets and you build products kind of with that future in mind so that might be one of them but I think like going back to maybe some of the ones that a lot of people might be listening or building I do think we're kind of at the moment for video we're at the moment for very interesting agent stuff with the thinking and reasoning models and I think there's also maybe something kind of under the radar right now a little bit still think coding has major leaps we're going to see this year um and so those would be some of the ones that are top of mind for us are you guys doing work on coding out of labs too yeah we are we are so right now at Google 25% of all the codes written by AI yeah I saw that Jeff te yeah that's right that's right and that's up a lot in the sense of just how fast the progress is um this is an area that that I think there's kind of two approaches you could think about like how again think of lower the bar raise the ceiling right how do you make coding available for people who could never write code before massive opportunity you know like I've been coding my whole life I mean some of that well it's kind of interesting is some of the most interesting stuff happening here I don't know if any of you have played with like repits agent stuff really interesting right couple of weekends ago I'm with my fourth grade son we are struggling right now in our household to implement chores we created a chore tracking app 28 minutes 45 cents done we're daily active users and so it's a way to kind of get into software and a world of kind of software abundance that's really interesting um so we've got some stuff in that area uh we're also interested in how do you take a professional trained s programmer and make them like 10x to 100x and there's kind of I think interesting bets on both sides of that yeah what do you think is overhyped in AI right now oh that's an interesting question I wish we move beyond the chatbot interface a bit like that's one area that feels like we're kind of reusing that in a lot of places Google included um I'm also not sure there's still a lot I think of like people jamming AI into stuff like AI itself is a bit overhyped I wish we were a little more precise about how disruptive or like where to apply it and so I think again we're trying to think a lot about like workflows not just taking existing product in bolt on AI um so I think that's maybe a little there's a a race like you're seeing the first generation of AI put it in and it reminds me a lot actually when I first started at Google it was like right as the iPhone moment was kind of Just Happening and taking taking hold you when Steve walked on stage in 2007 said this is the iPhone if you look at the App Store three years later which is roughly where we are in this AI Revolution the App Store in 2009 is I went back and checked websites that have been shrunken down to fit on your phone flashlight apps and fart apps these were like the highest top downloaded things that were happening so I think we're kind of in this stage where the real stuff is going to start to come out kind of this year next year the next year that's when you start to see the Ubers the airbnbs the instacart the things that really change kind of how you do stuff and so that's that's kind of my thought on it all right then Sonia ask you the overhype question I'll ask you the uh under the radar underhyped question what some areas that deserve more attention within AI we talked about coding a little bit maybe just one other thought on that is I think if you can get code models uh that can kind of write code and self-correct and self-heal and migrate and do all this stuff it just makes you think the pace is fast now that totally changes the curve so I think that's a huge I still think it's underhyped like it's hyped a lot by the way um but I think as hyped as it is it could be hyped more that's one um um I don't think we fully internalized the notion of like what does long context or like infinite context mean it gets to some of your personalization questions potentially but it also gets to some of the stuff we were talking about around how can you make things like a mariner literally just keep going like um and so uh that whole notion of long context I mean you'll you see a lot from Google but we're investing a lot in that because we think that's a strategic lever um that's important uh especially as you get more agentic chain together kind of workflows um maybe another one I think there's there's not enough talk about taste and like I think if you believe the value is going to be in the application layer if you believe there's going to be some percentage of AI slop if you can just see a few of these Trends and I think there's going to be a value in Good Taste and good design and it doesn't mean it has to be human created necessarily although I think there's going to be high value on that too as like human crafted content becomes more Artisan um but I think that's another one I would say I think maybe related to that it's like veracity and Truth um and sort of what is real like these are things that I think are going to become way more important than they already are today I think the context Point within there I like really firmly agree with on like what can happen with you um your infinite context point because if you think about the relationship in your life where you have like the most context shared context it's probably with your spouse right and if you think about that what ends up happening is you can communicate with your spouse literally with just like like the flick of an eye right and all of a sudden they know exactly what you mean they know it's time to leave the party whatever it might be that's right right and you think about that's the aspiration for what can happen with infinite shared context we know that's the ceiling exactly right and so you think about you're like think about how far away that is from now where you're like typing things in about what it is in your point of like well hold on there's all these different ways you can communicate it and can get to know you better if it has memory and so I I think there's so much gold in there of it just being able to keep going right but giving it the right context and whatever it needs you think of any company that you all back or even Google like what's one of the most painful things is when a long-term employee leaves CU all that context walks out the door so I think it's exactly right whether it's a personal relationship or a work relationship yeah okay we're going to wrap with a a rapid fire round you ready yeah sounds good okay favorite new AI app I mentioned it earlier I'm having a lot of fun with repet love it the new agent thing and on the phone I think they're doing some really interesting stuff there you know one of our partners Andrew Reed is known for slinging like creating these amazing memes and sending around it's now so easy to create an app he just creates these all the time and sends them to me um they're they're really good yeah we have this concept of like disposable software you you use it once and you kind of throw it out after you're done with it so yeah okay what application or application categor do you think is going to really break out this year video okay uh recommended piece of content or reading for for AI people oo that's an interesting one um you know this one's not a traditional AI pick because I think probably a lot of the listeners here I was going to say over the break I I read a lot and one of the books I picked up was actually it's the Lego story and it's the history of Lego and it's on its third generation of family ownership um I'd recommend that one it's a really interesting uh yeah here's why though there's a pivotal moment in the company's history where they had 260 products and maybe for a lot of Founders that are listening you can imagine your company could go in like all these different ways you're trying to figure it out and the grandfather the CEO at the time basically identified like the little building blocks this is it and he bet the company on it and he bought these incredibly expensive machines and so I think it's like an incred I like to read biographies a lot and this was one that really stood out Josh has an Inc incredible taste in books and he has this wonderful reading list that he's been kind enough to share with me oh no way that's really wonderfully curated it has this very good formatting as to when it's something you really got to read versus not and so uh you should to all the listeners you should take Josh's suggestions seriously I actually really want a great AI reading app that's like my wish list app what would in part because I have terrible memory but out of out of everything I've ever read or listen to which I think is a different set of things than all the books on the planet like there's all these things that are kind of on the tip of my tongue and ideas that connect but you know they're all kind of in an abyss and they're all pretty inaccessible to me and and so something that surfaces some of those thoughts and ideas that I've had things that I've read you know that next layer of thought I have from reflecting on two different things that I've read and the connections probably across them yeah it's a good idea I think even within that like just the hard copy version the Kindle version and the audiobook version being like you know seamlessly intertwined like you just the most basic level you know so that you can continuously pay attention to something that you like and then we can get to the version that you said yeah request for startup okay uh pre-training hitting a wall agree or disagree o maybe lean agree I think there's still stuff to squeeze out there but I think a lot of the the focus has shifted yeah Nvidia long or short I don't give stock advice Index Fund would do you ever uh sit with Demis and be like look as someone between us we won a Nobel Prize do you ever start with that you know because you know that feels like something that's true you know between the two of you there's one Nobel Prize it's all one directional it's Den John jumper those are the people that won the Nobel Prize not Josh Woodward yeah uh okay any other contrarian takes an AI any other contrarian takes I I guess maybe I'll leave it with this I think we are kind of one thing is like what a time to be alive and building because I feel like there's this window where there's like so many adjacent possibles opening up I think the second would just be like I'd encourage people listening to like really think about of course there's the models and who's winning and the back and forth but like what are the values You're Building into your company cuz I think this is one of those moments where there's going to be like tools created that shape like follow on Generations I think it's really important people think about that and like are you trying to replace and eliminate people or are you trying to amplify human creativity I mean there's like one that's like you know going immediately comes to mind when I'm thinking a video for example I'm on the side of wanting to amplify human creativity but I think there's like there are these moments that happen in our Valley here where like things change and they change often for generations and they can change for good or bad and so I would just encourage people that are in spots where you're building and you have this incredible technology that's only getting smarter and faster and cheaper to put it to good use and think about the consequences Downstream thank you so much Josh for joining us we love this conversation yeah thanks again [Music] [Music]

========================================

--- Video 25 ---
Video ID: eXK-_yyQDMM
URL: https://www.youtube.com/watch?v=eXK-_yyQDMM
Title: How AI Breakout Harvey is Transforming Legal Services, with CEO Winston Weinberg
Published: 2025-03-11 15:31:10 UTC
Description:
Harvey CEO Winston Weinberg explains why success in legal AI requires more than just model capabilities—it demands deep process expertise that doesn’t exist online. He shares how Harvey balances rapid product development with earning trust from law firms through hyper-personalized demos and deep industry expertise. The discussion covers Harvey’s approach to product development—expanding specialized capabilities then collapsing them into unified workflows—and why focusing on complex work like international mergers creates the most defensible position in legal AI.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

Transcript Language: English (auto-generated)
Something that's really important in professional services is prestige and trust, right? The reason prestige is so important is because trust is the most important thing in professional services, right? And so the reason we went after the larger firms is if you earn the trust of a few of those firms, the rest of them will trust you and the rest of the firms downstream will definitely trust you, right? And their clients will trust you, right? So I think that something you know something that we thought about doing in the beginning was well just go straight to enterprise right and there are a bunch of problems with that but one of the main reasons is there's just no reason for them to trust you right that you can actually build these systems [Music] Greetings. Today on training data, we have Winston Weinberg, co-founder and CEO of Harvey. Harvey has occupied a special place in the AI ecosystem over the last two or three years, becoming the canonical example of what it means to build an application layer company on top of the foundation models. A couple years ago, when these companies were derided as rappers on top of GPT models, Winston and his co-founder Gabe realized that this is where a lot of the value would actually be created, dealing with the messiness of real world problems, not replacing human beings, but giving them superpowers. And so over the last couple of years, as much of the industry has taken a tech out approach to build their business, Winston and the team at Harvey have taken a customerback approach and now stand position to revolutionize the legal industry, which is $400 billion in the US alone, the same size as the global cloud markets. We hope you enjoy. Winston, welcome to Training Data. Thanks for joining us on the show. Thanks for having me. All right. So, you have built you and your co-founder Gabe and your team have built a company that has become to many people sort of the canonical application layer AI business or sort of the defining application layer AI business. But when you started back in July of 2022, yeah, end of July 2022. So when you start in July of 2022, pre-Chat GPT nonetheless, um this wasn't really a thing. And as soon as it became a thing, people looked at this whole category and they said, "Eh, these are just rappers on top of foundation models. There's no value to be built here." So they're immediately sort of derided. So I guess the question is as a starting point, what gave you the conviction that this was the right business to go build and that there would be value between foundation models and customers? Yeah, I think the simplest answer to this is quite literally that these industries are incredibly messy, right? Um, and so I think that the largest misconception with kind of the the GPT rapper companies was a lot of what you were starting to do didn't have like a massive delta between what the foundation models provided and what your small company could do, right? So one of the largest advantages that we had in the beginning was how much emphasis our product had on citations. like literally just it was very important in the legal industry to be able to site like line by line and the accuracy of those citations, right? There were a bunch of other things as well, but like from day one that was something we put resources into. If that's all our company was, if we just said, "Okay, we're going to be a citation company, right?" Then yeah, I think you're just a GPT rapper and it's going to go away. But if the ambition of the company is to actually partner with an industry to completely transform it, this is a trillion dollar industry. It's incredibly messy. There are tons of different data sets. There's tons of ways that people work. There's tons of different specialized workflows, etc. Um, and so I think over time it became clearer and clear that these models weren't going to just automate entire industries away, right? They were going to change and like fundamentally serve as kind of like the groundwork for changing an entire industry. But I mean, one way to even look at this is when a new feature comes out, we usually think of that feature as this is a great piece that we can put into part of a workflow. So an example of this is like OpenAI will release deep research, right? And my first reaction to that is, oh my god, there's all of these capital markets use cases, right? For like studying markets, looking at all of this data that's on the internet, etc. It's not everything a capital markets attorney does, but you could put that into a piece of a hundredstep process in your product and it makes that process better. And so the best way to think of this is literally whenever I see either the foundation models can do something better than they could before or completely new feature. It just like unlocks part of the TAM or part of the actual process that you can build on top. Let's pull this thread a little bit. So, at time zero, you got the model down here, you got the customers up here, and you got Harvey doing citations in the middle for for a month or so. Yeah. Let's um what came next? Kind of walk us through some of the major leaps in the product or some of the major value creation opportunities you've seized upon along the way. Yeah. So the best way to kind of describe how we've been building the product and this is I think one of the things that we've gotten the most right and it has also been the hardest to manage which is and and from an org perspective and from a kind of like how which pieces do you focus on etc. And and the piece here is at all times you have to basically expand the product and then collapse it back, right? And so what I mean by this is actually if all of these models work perfectly and humans were perfect at communicating with each other, the best interface is literally email. Like that's it. Like there is no interface. It's literally just email and it's perfect. It has all of your contacts. It can read your mind, etc. I mean it could just be a neural link. Yeah, sure. That's perfect. Yeah, that's even better, right? Um but that's not how it works, right? And I don't think that we're just going to randomly oneshot be able to do that, right? And even if we could do that on the model side, we can't do it on the human workflow side. In other words, like the example I give a lot is even if the models could do, you know, somehow chain all the steps together to do a large merger like the Activision and Microsoft merger, the user can't send that to the model and say merge please and then it just like does all those steps, right? There's all of these communication sides on the UI side as well. But okay, so holding that still, going back to the, you know, you have to constantly expand and collapse the product when you're expanding. What I mean by this is the chat UI doesn't work for every use case, right? And it doesn't work for every use case right now. I don't think it will for the future, right? And so an example of this is if you were trying to build something that does really good case law research, right? There's multiple steps to that. You want to build a system that is very good at doing retrieval over all the cases. You want to build a system that is really good at comparing contrasting all the cases. You want to build a system that is really good at synthesizing the facts in your case to all of the case law, etc. Right? And if you're doing that, the best way to do it is you expand your product by building specific vertical, you could call it agentic systems, whatever you want that do that thing from start to finish, right? And then you collapse them all together. So what does this end up looking like? We will take a bunch of use cases that are very high value. We'll build out a specific workflow to do that use case and then we will chain them together so that you can complete a task from start to finish. Right? And so the difficult piece of that is if you're trying to sell something that is seats, you have to sell something that is applicable to as many users as possible. And so you have to balance are you building something that is really good for a securities attorney or are you building something that's really good for all of the attorneys, right? And so that's kind of the collapse part where you want to build these specific workflows, agentic workflows, whatever you want to call it, whatever terminology it works, and then you want to combine them into the same service level on the product, right? And so what this looks like eventually is you upload a share purchase agreement onto Harvey, right? And we have the user may might not see this but there's tons of different workflows that are like extract the reps and warranties from that or summarize it or whatever it is right and we've built them all separately and you can use those UIs and actually do that workflow separately or when you just upload an SPA Harvey says would you like to run any of those workflows and that's the collapse version right so you're building these specific solutions and then you collapse them back in how much of the magic do you think is in the models themselves versus what I what I think you just described as you know an agentic system or a cognitive architecture. Yeah. Um and you know how much of is it is it the workflow that's hard? Is it training things together that's hard? Is it the combination of all of it that's hard? So I think like I would categorize this in three areas. So the all you need to do for kind of like every single workflow is basically what does the user want? What is the intent right? Um after that it's so what is what do they want? What's the intent? How do you extract that intent out of them? The second piece is what context do we need? And the third piece is is this right. Right? And so my point is there are different systems that work really well for different versions of those patterns. Right? So routing like using the models to actually like do predictions and routing is really helpful for the first one. Right? And so that's the example of you know either follow-up questions is really important there. routing uh someone's query to the like particular task you think they want to do is very good. Um and so there's kind of like an orchestration element there, right? Context is okay, do you have predefined systems that will search for your internal documents that are relevant to the question and then your external documents that are relevant to the question? A lot of that work is retrieval, right? like most of what you're building there is retrieval and then routing to make sure that you're accessing the external documents when you need to and the internal documents when you need to as well. And then the third one is is this right? Right. So going back to my citations example which is is silly but it's it's actually really important. Um I think there also a lot of the work can be done by the models but you have to make sure that the models are very good at checking for certain things. So let me give you an example of this. uh there's a thing in legal that is what is market right and the models don't know what market is and there's various versions of what market is there's market for a particular private equity firm like what terms they are used to you know doing on an LBO or a dealer in a sideter etc there are terms that are across all of private equity and then there are just kind of like general M&A terms right and the models don't have access to this data right and so on That third piece, a lot of it is can you build a system that is very good at retrieving all of those different data sets when needed and comparing them against each other. And I'm sure it helps if you have all of those different folks as customers. Yes. Yeah. So I mean that I think is goes to another piece which is the biggest problems that we have and I think I mean is a very interesting problem and I don't think the models are just going to solve this is the process data for a lot of these tasks doesn't exist on the internet right so the process data for how do you do disclosure schedules or what is market right those are not things that are just kind of like on Reddit somewhere right um and so what we do is we actually hire domain experts who sit down and say these are the steps that I would take right and then you just chain the models on top of that right and or if there's a gap between what the model can do then you do fine-tuning right but the best way to do fine-tuning and post- training is task specific it's not here there was a a large kind of ethos maybe in the legal industry and and a bunch of other industries that somehow if you got all of the legal documents and then you just trained a model on than that it would do law, right? Um and and that's kind of like saying read all the, you know, your case load and all of the textbooks in law school and then just put you into like a profession and you somehow know how to do it. And that's not how it works. So much of it is like you actually have to learn how to do the different steps, right? And then the other side of that is not only is the process difficult, but the evaluation is really difficult, right? So you have to hire kind of like mid-level folks to do evaluation for a lot of these things because if the junior folks could do evaluation, they would be mid-level or they'd be senior, right? Like that's the reality. And a lot of what you do in a law firm or professional services is you're actually evaluating the work of junior folks, right? It's incredibly expensive. I'd argue that maybe 20 to 30% of the revenue of these places is doing that. I want to go back to this concept of expanding and collapsing the surface, which I love. Um, what is your view of the ideal end state for how, you know, a lawyer should be interacting with Harvey? Is it kind of that email chat interface and it's just merge merge company A and B and we're good or I don't I don't think that we will get there anytime soon. And that is not for me to say that I'm not bullish on the foundation models getting better. I mean I you know this I am incredibly bullish on the foundation model is getting better and we have designed the company kind of without a constant push as a driving force. Um I think the biggest problem with that is that doesn't allow the user to have enough kind of e exercise enough judgment on the workflow itself. Right? So, I think one thing that when people are talking about agents, they're talking about these tasks that are like quite simple and don't have massively high like economic value, when we're talking about building workflows and putting kind of agents in those, we're talking tasks that cost hundreds of thousands of dollars, right? I mean, one reason the the legal industry is is so good for LLMs is if you can kind of think of this is is the industry textbased and then how valuable is a token, right? And a token is incredibly valuable in legal and professional services. If you look at like a merger agreement, like a 50-page merger agreement, the token in there, like how each piece of a word is worth so much money if you think about how much it costs to produce. So this is all to say I think the end state is you keep building these agents and workflows and then you chain them together as much as you can right and that makes it so that your UI actually looks the same but your suggestion and your routing model gets better and better your orchestration level model gets better so you can kind of think of this maybe like let's go to the law firm I think that we're building out the specialized associates that can do different tasks But it's also incredibly important that you have the partner or the managing partner operating model as well. And I think that as the models allow us to build more and more of these specialized kind of specific associates, you also have to put all of this effort into actually having the orchestration layer that pulls all that together, right? And so I think our UI actually looks similar in the sense that it it does look like a kind of like a text window, right? But the ability for a user to upload a bunch of documents and for just to suggest what you do or the ability to say this is what you did last time. Would you like to do it again? Things like that improve kind of like a colleague that's just like really good at knowing what you want to do. On that note, are you selling software or are you selling work? Is this legal software or is this AI lawyers? Yeah. So, so far I mean software um I think that the best way like most of our clients in the beginning were law firms and professional service providers like PWC. Um we now have a lot of enterprise customers as well and the way that the product is evolving is you can kind of think of it as actually two products. One is a productivity suite, right? And that is lawyers in the loop at all times etc. Um and you know the ROI on that is is incredibly helpful. It saves you tons of hours a week, etc. Right? The other side of that is you're building these workflows that do part of the work from start to finish, right? And that is closer to selling the work. The way that we're actually approaching this is we are building those with law firms and helping them get more business, right? So we will get these revenue split agreements with law firms or for professional services and we will combine their domain expertise with our tech and then they go out and sell it to their clients, right? Um and so we are transitioning from just a seatbased company to actually selling the work as well. Uh and I think we'll see a lot of that this year. What and what are the ripple effects of that kind of like inside the building and outside the building? How do you how do you manage that as a company and what change does that require of your customers? Yeah, so managing it internally is all about taking bets. Um because it's it's you know if you're building software that is and every single feature that you're adding is good for your entire user base, you've kind of like reducing your chance of failure, right? because every time you add something to the software, it is hopefully uh increasing the value that you get from every single one of the users. If you focus on building one of these specific workflows and it doesn't work, you've b like it's zero and one like they it either works or it doesn't, right? And it's for a specific use case. The value of it is really high, but if you can't sell it to people, then you're in trouble, right? Um, and so the best way that we've handled this internally is we've actually let the law firms do a lot of the discovery for us, right? So we will do joint projects with a large company and their law firm, right? And so we will use them as design partners to make sure that this is something that is repeatable. You can actually do it, right? Like the technology is there to do it. And number three is appetite. So there is a large problem in legal where a lot of kind of what you need is like compliance checks or you're doing the work for insurance etc. And so the the third piece is also really important where you have to make sure that the in-house team is actually okay with AI handling that use case. Right? So that's kind of how we handle it internally where we spend tons of time on discovery, right? Tons of time talking to customers and we have a lot of design partners for the early stages of that. One more piece about how we handle it internally as well. In order to generalize, what we're doing is we're building like AI patterns. So here are the 15 types of actions in the legal industry that we need, right? So research like case law research is a huge one. Uh regulatory research is another one. Uh clause extraction is another one, right? And so we'll put tons of effort into those like kind of generic or widespread horizontal things and then we'll chain them together kind of like a Ford factory line. So those are the two ways. Uh so on the GTM side it's discovery. On the product side, it it really is kind of building that Ford factory line of of AI uh patterns. Externally, I think this will have a very large impact. So, we're working with a firm that is almost a hundred years old and they became famous from literally advising I think it was King Edward the something on the abdication of the throne. So in other words, these firms are really old. Like some of these firms are like hundreds of years old and they have had the same business model for a hundred years, right? And they're actually willing to work with us and change some of their business model, right? And like take a bet on this and explore it, right? And so as I I think most people know, a lot of professional services and especially legal has been the billable hour forever, right? And so the problem with that is if you're selling them efficiency software, there's only so many hours in the day, right? unless you come up with a software that invents the 25th hour of the day, which by the way would be the best thing you could do to sell to professional services. But if you can't do that, um, efficiency is is a hard cell unless you can convince them that there are ways for them to actually transform their business, right? So, what we're starting with is let's help you take a bunch of these workflows that you normally do at not a very good cost or a loss, right? um and let's turn those into software and help you kind of spread it out and get more market share in areas that you don't normally have. So, I'll give you a pretty good example of this. In private equity, one of the ways that law firms go and kind of get new private equity business is they'll do things like side compliance or kind of like lower-end work and they'll do that at a loss so that eventually, you know, the private equity firm will pay them for the LBO or or whatever the big M&A deal is or something like that. And so we'll build them software to help them get that work and not operate at a loss to get it, charge for a flat fee, etc. Uh, and then take some of the cost. Is the tech ready? Like how much I guess legal work can already be automated with today's models and if you were to freeze model development. Yeah. You know, how how much can be automated? Yeah. Yeah, I mean I think that's a good question in terms of and especially the second one of I guess like how there's the reasoning ability of the models and then the capability re of the models, right? And if you froze the models reasoning ability, I think we'd be fine actually. Um, so if we somehow kind of like froze the ability for the models to process data, um, that doesn't mean that they know which process to actually like analyze that data, but just their ability to ration over it, um, to make rational decisions over it. We'd be in a really good spot. So in other words, I think we're at like small percentage points right now of like legal and professional services, but if you paused, we would increase pretty high even if the models didn't get better, right? Because I think the reasoning ability of these models is is is there actually I think the bigger problem is like evaluation, improving process um and collecting more data too. The the data isn't there. Is model development going to freeze here? Um, I I don't think so. No. Um, I mean, I I think we're we're seeing a pretty large evidence of if you throw more compute at the model and you let it make more and more reasoning steps, it just gets better and better and it improves time. Yeah. Exactly. Right. Um, and it's interesting if we if we go all the way back to 2022 in the early 2022, the thing that Gabe and I had done was we were doing a bunch we got access to GBD3. This is public by the at the time. This is like early 2022. And we the thing that we found was we were just doing chain of thought prompts before anyone was thinking about chain of thought prompts. And the way that we actually started was we were doing that over a bunch of kind of like legal questions and we cold emailed the general counsel of OpenAI and sent him that. And he basically, his name's Jason Quan. Um, and he basically responded, "Oh my god, I had no idea these models were this good at legal." And I think the main reason people weren't looking at it is because they were just doing one model call over a set with like one static prompt and just calling it a day. and they weren't doing what are the actual process steps like they weren't telling the model these are the steps that you need to take in order to answer this question right and so my point is that's what we saw in like early 2022 and that's like the direction that these models are going right um and I think that it's really good for legal and for kind of these these industries where you have to have these complex decision-m processes for two reasons one the model get better at that that's just increases the ability of them. But two is that process data doesn't exist. Like I said before, like it it's not online. Like how to book a flight is online, right? Uh there are easy ways to train that. It's harder to do how do you do an LBL? Well, and you and you guys you now deservedly so have an unfair advantage around the process data because you have some of the very best law firms in the world as customers of Harvey, which I think was a contrarian strategic decision that you and Gabe made a couple years ago and you made it with high conviction. And and I I remember a time when there were lots of law firms who wanted to come work with Harvey and you basically said no so you could focus on some of these big prestigious firms. Um can you say a couple words on what gave you the conviction that that was the right strategy? And then maybe more importantly once you determined that that was the strategy, how in the world did you get them to trust you? Yeah. like this is a very very scary new world and and and you managed to earn their trust. So how did you do that? Yeah. So I think okay so the reason for doing it um there was a a GTM reason for it and a product reason. From the product side the bet was that the models were going to get better and that you want to build systems that the the next generation model cannot do right and so you want to go after the incredibly complex international merger type work. Right? you want to do the very complex work and build systems for that because it is the most defensible by far. Right? So that was kind of the the product side of it. Um from the GTM side of it, I think that something that's really important in professional services is prestige and trust, right? The reason prestige is so important is because trust is the most important thing in professional services, right? And so the reason we went after the larger firms is if you earn the trust of a few of those firms, the rest of them will trust you and the rest of the firms downstream will definitely trust you, right? And their clients will trust you, right? So I think that's something, you know, something that we thought about doing in the beginning was well just go straight to enterprise, right? And there are a bunch of problems with that, but one of the main reasons is there's just no reason for them to trust you, right? That you can actually build these systems, right? Uh, how did we do it? Um, I we did a bunch of things that do not scale at all. Like in in all honesty, um, I think that one of the things that we did really well is I don't think there is any excuse for someone who is building an AI product and trying to sell to not do hyperpersonalized demos. Like there is no excuse. I think it used to be really important to do that. Now it's paramount and it is so easy to do this, right? Um, and so one of the things that we did in the beginning was whenever I would demo to a partner, I would try to use something that they recently worked on. Um, and then the other thing too is lawyers are argumentative, like very argumentative. Uh, and I mean that in the best way. So just let them fight with the model. Yeah, I'm serious. And I mean that in the best way. So I would sometimes say, you know, was this a good argument? Um, and how would you improve it? And if they were really bored on the demo and then you say that they are reading every single word that comes out of Harvey, like every single word. And you know, it wasn't always the perfect response, but I think it engaged them in a way that they've just never been engaged with software before at all. Right. Um, and and we found I mean, one thing that's that's interesting is, you know, a lot of the older partners at at firms sometimes we might be their first AI product that they've used, right? And so it's really important to actually like show them, you know, kind of the basics as well, not just exactly what, you know, is is special about your product, too. How have you elicited the behavior change out of your customers and kind of taught them to use AI? Yeah, this has been really hard. Um, I think that it starts with product by far. So when I was talking earlier about the expand and collapse, the most important piece of the collapse is to make it so you don't have the blank page problem, right? Like that is by far the most important thing. So that when you go onto the landing page of Harvey, there are all of these buttons that you can click that will help you get started, right? And that is super important. And now we actually have it to a point where you put in how many years you've been practicing, what kind of lawyer you are, things like that, and it will change what that screen looks like when you start. Right? So that's been super helpful because if you make it so that those very specific systems are exactly what they do day-to-day, it is really easy to get them to start and if they start using it, they'll be creative. They won't be creative right off the start. It's hard to get people to do that, right? Um so on the product side, I think that's the most important piece is just making it very personalized and making it so the time to value is really short, like as fast as possible and then they'll go out and explore. on the CS side, we've hired a lot of lawyers. Um, and I think that's what been super helpful is you need to hire domain experts who can say and and we're doing the same too in tax and these other areas. You need domain experts who can come in and say this is how I would use it and this is, you know, I've been doing this for six years. I was in the same shoes you were in. Um, I know exactly what work you do. Talk about the trust thing. Hallucinations. I think if you look at where AI has worked really well, it's where, you know, where hallucinations are a feature like in the creative industries, right? Hallucinations are a feature. In your industry, I'm assuming hallucinations are an absolute bug. And so, what do you do to make, you know, those argument that lawyers really trust? Yeah. Really trust what the model was saying. So, I'd actually push back on the second piece a little bit in a lot of what lawyers do is creative actually, like a lot of it. Um, so like I'll give you an example for this. Litigation would not exist if there were just 10 really simple rules and everyone knew which fact pattern fell into the boundaries of those rules. Like there would be no litigation, right? And so I I was a litigator. I mean transactional is also incredibly creative, but litigation I think is just a better example because what you're trying to do is I have these facts. Here are somewhat of the rules of the game. Let's figure it out, right? And that's actually incredibly creative. Like very creative. Um so but still who like accuracy is more important than than creativity to be clear here. Um and so on the accuracy piece so even given that creativity is actually super useful in some instances and we try to make it so that our product doesn't like get rid of that aspect. Um on the accuracy side there are a lot of things you can do to improve it for one. Um but for two the main thing is law firms are hierarchical. So you like a junior associate if they get a task they do the first draft of it and then a second year reviews that and then a fifth year might review that and then the partner reviews that and it goes out right um and so actually the you know minimal viable quality of your output can be lower for a law firm than it can be for an in-house team actually. So selling to the law firms was also helpful in the beginning because so much of the work gets reviewed, right? And so you aren't selling them something where they kind of click it and forget it, right? They're actually in the loop at all times. When you're selling to an in-house team, it is better to sell them the specialized versions of tasks that they can kind of see an insanely high accuracy level on. Um, and the more specialized you are building, the higher accuracy you can get because it's easier to fine-tune. It's easier to do evaluation because there are less steps and the surface area is just smaller. Let's talk about the lawyer of the future and the law firm of the future. Foundation models are going to keep getting better. You guys are going to keep doing valuable stuff on top of those models. What does the job of a lawyer end up being? What does a law firm end up looking like? Let's go like five, 10 years out. Yeah. So, I'll start with the job of a lawyer because I I just care a lot about this. Um I think that it goes back to actually what it used to be. Um so 50 years ago the role of the lawyer was an adviser and a lot of what they did is look around corners give a different angle of advice etc. And and we've actually seen this evolve to a degree where the CLO chief legal officer title is like pretty new and really it's like most of the CLOs's that I have met they are part of the business like they are business drivers. it is not just the no person, right? Um, and I think that that is going to happen. So, I think what's going to happen over time is kind of this like lower-end work is going to get somewhat commoditized and automated. Um, but then the highle strategic work is actually going to be more valuable, right? And so this is really good for a young lawyer. It's fantastic because you know most people they go to law school and their goal is not to sit in a data room or do discovery or doc labeling for 10 years and then maybe go to trial once. That's not what they want to do, right? They want to give advice to clients. That is why you want to be a lawyer. They want to help people, right? Uh they want to help people or they want to win. One or the other, right? But sometimes both. Um that's what you want to do. Uh, and it very much is it's like close to like professional athletes where they want to be the best at their craft and you get better by having more hands-on strategic experience than you do just sitting in a data room forever, right? And so I think that experience will be really good. Um, the law firm I think will be I think there will be a lot of transitions in how law firms operate. So I mean I now am a client like I use a lot of law firms, right? And one thing that I've always got annoyed with is the really high billable hours for very low-end work, like looking at, you know, whether this change of control clause was triggered or not, things like that, right? Um, and as a customer, that can be annoying, right? But actually for the high-end strategy of like, how should I go about buying this business? How should I think about restructuring this part of my org? Things like that, I would pay more than you pay the best lawyers on Earth right now. like the delta between a junior associate and the best partner on earth is like three or 4x, right? Which actually doesn't make sense to me. Um, and so I think what will end up happening is there will be a lot of fixed fees for kind of the part of a transaction, the part of a litigation that is somewhat more commoditized, but the insight, the value, like strategic advice, looking around corners, things like that, I would argue you can charge more for that. I know part of your mission is related to providing better access to justice. What does that mean to you and and kind of draw the line for how Harvey helps us get there? So, a little bit of background info on this. The average price of a lawyer in the United States is $352 an hour. So, almost no one can afford a lawyer, right? Um, and I think there's a bunch of arguments about, you know, how much latent demand is there for lawyers, etc. The reality is there is a massive population of folks that do not have access to our justice system, right? Like either way you slice it, that is the case. Um, and even if you had every single lawyer work 20 hours a week on access to justice still wouldn't close that gap still literally in in without any anything else being fixed, right? Um and so I do think that we are going to have a large transition to lawyers using AI um to actually help and increase access to justice. Um there are a bunch of things on kind of like the regulatory uh side that prevent a lot of this but I think a lot of those will change. So the the best example of this is um there are kind of two conflicting rules here. It is unauthorized practice of law, right? Um so businesses cannot give legal advice. someone who has not passed the bar actually cannot give legal advice etc. Um and the second one is you cannot make an equity investment into a law firm unless you are a lawyer, right? Uh and so that has basically cut out any sort of traditional financing in the legal sphere, right? Um Utah, Arizona, and some now some other states are also thinking about getting rid of these rules or creating sandboxes to kind of experiment with that and it's been going really well. Um, I do think that there is going to be a lot of change in this in the next couple of years and I think it will be amazing. Like there there really is I think that if you can figure out the way to make sure that people are getting the same quality of legal advice with AI and maybe a lawyer in the loop depending on kind of how that that pans out. This will be incredible for folks. And the my last piece on this is one of the things that I think people don't think about a lot is most people don't know when their rights have been violated. They do not know, right? And so like I I think like we take for granted that we have all been like, you know, in in this kind of like bubble have been super well educated. We know exactly what when something happens, we know whether we have legal recourse or not to some degree. A lot of folks don't, right? And so they don't know if a position a person in authority is doing something that is illegal or not. Right? This is a huge problem in housing, huge problem um in kind of like collecting unpaid fees for things that shouldn't even be a fee, etc. Right? So I think that's an area where you can do a lot of not just providing the services but a lot of education um at scale that you couldn't do before. Can we zoom out to the AI market more broadly? Yeah. OpenAI was lucky enough to partner with you in the early days. Um what what do you make of the recent developments that are happening in the AI ecosystem and what do you think are the developments that are most interesting for you? Yeah. Um I mean costs going down are always good. Um I I think like one one thing to think about here is there are so many use cases that we have gotten to work 70% of the time, right? Um, and the Oer models have been incredibly helpful for us because not even just the O series models themselves, but it unlocked kind of like our product strategy for the next 6 months to a year, right? Where there were so many systems that I don't think we thought that we would be able to build in the next year if it was just the GPT series models, right? And I think that the Oer has massively kind of changed that for us. And so our product roadmap has drastically changed because of that. What's an example of something that you couldn't do before that now you can do? So examples of this are like things that you need to do multi-step reasoning for and pulling from many sources at once. Right? So the thing that the O series models is really good at is orchestrating a plan and also executing on that plan. Right. So if you build a bunch of systems that are really good at basically extracting information from Edgar, then extracting information from case law, then extracting information from all your internal documents, the missing piece for us was what do you do once you extract all that information, right? Like once you have all of those different pieces, how do you combine them into the correct work product? That's what the O series models allow us to do. If Sonia had a magic wand and she were to wave her magic wand and you and Sam Albin all of a sudden switch places and you're now CEO of OpenAI and he's now CEO of Harvey. What would you do different at OpenAI and what might he do different at Harvey? Yeah. Well, at Harvey he'd raise more money. He is pretty good at that. He's really good at that. Um maybe I'll start with what I think OpenAI has done an incredible job of and I would maybe even double down on it more. I think that they have done an incredible job of capturing the consumer zeitgeist. So I am not from Silicon Valley and I don't have I mean now I have tons of colleagues here and tons of friends but I didn't beforehand right and almost none of my friends know what any AI tool is other than chatbt that's it right. Um, and I the thing that I think I would do differently and to be honest I think they are going this route anyway is just like put way more effort into productionizing that for consumers as much as possible. Right? So and and that's not just model performance at all. Right? So I think that still one of the biggest problems with chatbt and just kind of like all of these tools in general is they're looking so much on the performance from the model side and not at how do you make the experience easier for the user, right? How do you make it so that it extracts more information from the user? How do you help the user figure out what it can do and what it can't do? How do you help the user figure out how it combines with different pieces of information, etc. So, I think on on this side, it would just be putting more and more effort into understanding the consumer behaviors and how they use AI right now and making that easier for them. What you guys have been partner with OpenAI from the very beginning. what has it been like and how has it evolved over time as it's gone from kind of undiscovered to being you know the center of the universe in many ways? Yeah, I mean so I think the thing that we have kept up that has been really awesome for the engineers um at at Harvey but but also at OpenAI I think is we have always been working on things that I think a lot of companies aren't. And so what I mean by this is OpenAI would give us models all the time and say it performs way better on all of their benchmarks and we would respond and say sorry but it doesn't on ours like it's actually not better for us. Um and I do think that the reason our relationship has been so strong is we keep saying okay here's a model that can do XYZ. Uh I would like it to do XYZ and the rest of the alphabet too. Right? And that's what we're trying to do. Um, and so I think we've actually helped them a lot too with at least like applied use cases and how they think about post training and how they think about what are some of the things that companies like are really pushing to try to do, right? Um, and so I think that has been really really good for the relationship overall. Do you have any hot takes on Microsoft Open AI? Um, yeah. Um, one of the hardest things about being an application layer company is you have to bet on model providers. Um, and it is not like we build our systems so that you could kind of just pull out one model and replace it for another, but you could do pieces of that, right? So like we don't just use one model for the entire system. You'll build a piece that's really good at this and then a piece that's really good at this and then you'll chain them all together, right? Um, but the reality is, you know, who's in the lead changes so often. Um, and different models are good at different things, right? And you have another problem too where all of your customers or most of them I said the partners probably aren't using a bunch of AI tools, but the associates for sure are and the associates are using all these different AI tools, right? And so what I'm trying to say is we I think have done a good job where we work with all the model providers and we are constantly testing out models with all of them. And I think that Microsoft, which has, you know, such a massive customer base, they have to figure that out too, right? They have to figure out, oh wow, we have customers that think that Claude is better at certain things. What do we do about that, right? I mean, they can't really do anything about that, but they're like, you know, Mistral or whatever it is, right? And so I think that I think that that relationship is evolving as it naturally would. If the opportunity for Harvey is to revolutionize a trillion dollar industry and provide better access to justice, what is the threat? What are the biggest threats to Harvey? Um, I think not moving fast enough. I mean, I I I tell this to my team a lot. Um, and I think it's becoming very obvious in the past couple of months, too, that we're really living in a time when all of your timelines are compressed. I would argue this is in all of human history the most compressed timeline in terms of what you can change in the world, right? Um, and I think that you have to move so incredibly fast in order to keep up with that. And the speed is compounding in a couple different ways. If you are constantly moving fast, you are required to constantly test everything around you. Pay attention to every single every single part of your industry. How your c every single customer is using your product, how your subprocessors are changing their models, everything, right? And if you do not do that fast enough, you'll make a bunch of mistakes, I think. And you I mean you constantly will make mistakes, right, by moving very quickly. But the scariest mistake is you move too slowly and you miss a massive thing. Right? So there is part of a feature that someone releases something and you have steps 12 through 13 complete on that feature. But that 13th step you just cannot get the models to do it and somebody releases something that unlocks that you need to put that in the product immediately right because you need to start testing it. You need to see actually did it solve it etc. And if you aren't moving quickly and you just say, "Ah, that's something new. We'll try it out eventually." I think it's a huge problem. Speaking of moving fast, good segue into our lightning round. Since starting this company two and a half, maybe three years ago, how many days have you taken off? It's a loaded question. Uh, I have not taken a day off. I probably should though to be to be in in all honesty. I think like I I've definitely a little bit worn the you know in like full transparency I've worn the badge of honor of like you know don't take any time off obsession etc. And I think on on one side I actually very strongly believe in that. So going back to my point about the timelines being incredibly compressed you need to be obsessed right like you massively need to be obsessed. Um but I do also think the other side of that is like you need to transition how you are a leader like that needs to change. Um and you know our company last year we started the year with around 40 people. We have 260 right now and you just need to change how you are spending your time right um and I think that I've definitely learned how to spend my time differently. And there's also stuff I've held on to and I actually like deeply believe in. So, one of the things I I deeply believe in is I actually think you should do part for a little bit of every single job at your startup for a little bit. Like I actually I did I do it too much and I did it for too long. But it is all most of my hiring mistakes have been I didn't understand what the role did like at all. And I a bunch of people told me what it was, but you just it doesn't help you hire if you don't understand what the actual role is. Um, and so that one I I feel really strongly about, but at the same time, I also took way too long to hire. Um, and I probably I was probably doing too many low-level things for too long, right? Um, and so it's a a combination of both of those. For those 260 people who work at Harvey today, what is the best thing about working at Harvey and what is the worst thing about working at Harvey? Yeah. Um I think the best thing is every day something new is happening right um from the product side from the GTM side uh you'll see you know a bunch of changes with the model providers that you want to integrate very quickly it is ex like it is stimulating above all else like for sure um and I think we're seeing like a lot of impact too and it it changes too and I think that's if you ask folks that have been here for a year or a year and a half and there aren't tons that have been for a year and a half. Um the thing that I think is most fun for them is how much our market has changed, right? Where I mean we used to it it was brutal in the beginning for kind of how you do your sales process, the requirements people had and things like that. And now most of our customers like they partnering with us, right? And they're letting us do their onboarding. They're doing all of these things where it really has seemed like we're kind of this wave of all of us together. Doing it together. Yeah. Doing it together. And that that changed a lot. It there was a lot of push back in the beginning. Like a massive amount of push and there's still some, but I think that has changed pretty drastically. Um the worst thing is the expectations are really high. Um, and you know, we had a really good year last year and I think we we had our offsite recently and I went up to the offsite and I basically said, "Hey, we did a really, you know, we had a great year last year. This year needs to be like significantly better and we need to raise the bar." Um, and you know, I think that's not always what people want to hear. Sometimes people are like, "Wow, we did such a great job last year. Now I take it easy." Yeah, let's take it easy. Let's just do the or do the same as we did last year. and then it'll be fine, right? And the reality is, again, going back to those timelines being so compressed, you can't do that. And I think that the the main thing I I ask people is basically, look, like I don't know what your goal is. I don't know if your goal is to make the, you know, a large industry change to learn as much as you can, make money, whatever it is, your options to do that in the next decade is the best it will ever be. like it it just is right. What you will make more impact than you ever will have the chance to in your life. How have you changed as a CEO and what prior have you updated the most? Yeah. Um uh hopefully I've changed somewhat. Um I I think the so the the prior that I've updated the most is teach not do. Like I'm I'm bad at that. um because I I kind of want things done so quickly. I have a massive problem of whenever I start seeing friction, I just go, "Okay, I'm going to go do it." Right? And and that's actually really really bad. Um I think it it makes it so other folks can't learn, it makes it so that it is kind of like too top down, right? And it's something I'm working on a lot and I think I've gotten better at it hopefully. Uh you can ask my direct reports. I'm not sure if I have, but I I think I have. And I think that that's the area that I want to keep getting much better at is taking kind of, you know, slowing down in some instances and actually like setting oursel up to scale. Um, instead of everything being like go fix that, fix that, fix that, fix that. Um, yeah. Who's a better athlete, you or your co-founder Gabe? Oh, Gabe is a much better athlete than I am. I mean, it's it's really unfair. So, he he played professional soccer. Um, and he is just a much much better athlete. But he had a knee injury recently and I will say I do a little bit like we live together and every morning like I'll get up to go to go to the gym and I definitely slam the door like a little bit loudly just so he knows that I'm going to the gym and he can't quite yet. I got it like we we met each other uh before this and we and we were we like became best friends and had no plans of doing a startup. Um, and he was just like always a better athlete than me. And so this is my like revenge a little bit. It's not gonna last long. He's healing right now. And so he's probably right now working out and getting better. All right, last question. I'm going to steal the last question from Guy Ros. I don't know if you ever listen to his uh his show How I Built This, but he has the same last question every time, which is how much of your success has been luck and how much of your success has been skill? Um, it depends how you define luck. Um we have been in a place where we have the options to apply skill and we have the leverage to apply skill and if you apply it correctly enough times to actually have a large impact from that. And so the luck is the timing. The luck is do you actually have the option to make a difference to make an an impact. And actually I think you were we were kind of asking earlier about what have I learned as a as a CEO. One of the things that I I think I've actually tripled down on, quadrupled down on is young talent, like by far. Um, and that goes back to giving them the luck or the opportunity to actually try something they've never done before. It works out really well and like they don't get it right every time and but I don't either, right? But you adjust really fast and their ability to adjust I think is better than a lot of folks that have been doing this for a long time. And so maybe the the best way to phrase that is their skill is their ability to adjust to luck and seize luck opportunities more than anything else. I like it Winston. Thank you so much. Yeah, thank you. [Music] [Music]

========================================

--- Video 26 ---
Video ID: huR0Oa2odxA
URL: https://www.youtube.com/watch?v=huR0Oa2odxA
Title: The AI Product Going Viral With Doctors: OpenEvidence, with CEO Daniel Nadler
Published: 2025-03-04 10:00:54 UTC
Description:
OpenEvidence is transforming how doctors access medical knowledge at the point of care, from the biggest medical establishments to small practices serving rural communities. Founder Daniel Nadler explains his team’s insight that training smaller, specialized AI models on peer-reviewed literature outperforms large general models for medical applications. He discusses how making the platform freely available to all physicians led to widespread organic adoption and strategic partnerships with publishers like the New England Journal of Medicine. In an industry where organizations move glacially, 10-20% of all U.S. doctors began using OpenEvidence overnight to find information buried deep in the long tail of new medical studies, to validate edge cases and improve diagnoses. Nadler emphasizes the importance of accuracy and transparency in AI healthcare applications.

Hosted by: Pat Grady, Sequoia Capital

00:00 - Introduction 
02:05 - Doctors are consumers 
08:40 -  The firehose of medical knowledge
18:08 - What is the evidence?
24:57 - What does “open” mean?
31:41 - How’d you build it?
38:18 - What about hallucinations?
45:34 - What’s changed about starting an AI company?
49:17 - Hire for neuroplasticity
55:33 - Lightning Round

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 27 ---
Video ID: bNEvJYzoa8A
URL: https://www.youtube.com/watch?v=bNEvJYzoa8A
Title: OpenAI’s Deep Research Team on Why Reinforcement Learning is the Future for AI Agents
Published: 2025-02-25 10:00:42 UTC
Description:
OpenAI’s Isa Fulford and Josh Tobin discuss how the company’s newest agent, Deep Research, represents a breakthrough in AI research capabilities by training models end-to-end rather than using hand-coded operational graphs. The product leads explain how high-quality training data and the o3 model’s reasoning abilities enable adaptable research strategies, and why OpenAI thinks Deep Research will capture a meaningful percentage of knowledge work. Key product decisions that build transparency and trust include citations and clarification flows. By compressing hours of work into minutes, Deep Research transforms what’s possible for many business and consumer use cases.

Hosted by: Sonya Huang and Lauren Reeder, Sequoia Capital

00:00 - Introduction 
01:45 - What is Deep Research?
06:22 - Surprising use cases
13:02 - End-to-end training
20:49 - Deep Research and Operator
23:47 - Where to go from here?
28:45 - Lightning round

Transcript Language: English (auto-generated)
a lesson that I've seen people learn over and over again in this field is like you know we we think that we can do things that are smarter than what the models do by writing it ourselves but as the field progresses the models come up with um better solutions to things than humans do the like probably like number one lesson of machine learning is like you get what you optimize for and so um if you if you're able to set up the system such that you can optimize directly for the outcome that you're looking for um the results are going to be much much better than if you sort of try to glue together models that are not optimized end end for for the the tasks that you're trying to have them do so my like like long-term guidance is that um you know I think like reinforcement learning um tuning on top of models is probably going to be a critical part of how the most powerful agents get built [Music] we're excited to welcome Issa fulford and Josh Tobin who lead the Deep research product at openai Deep research launched 3 weeks ago and has quickly become a hit product used by many Tech luminaries like the cisin for everything from industry analysis to medical research to birthday party planning deep research was trained using endtoend reinforcement learning on hard browsing and reasoning tasks and is the second product in a series of agent lunches from openai with the first being operator we talked to ISA and Josh about everything from Deep researchers use cases to how the technology Works under the hood to what we should expect in future agent lunches from open AI Issa and Josh welcome to the show thank you thank you so much for joining us excited to be here thank you for having us um so maybe let's start with like what is deep research tell us about the origin stories and what this product is doing so deep research is a agent that is able to search many online websites and it can create very comprehensive reports um it can do tasks that would take humans many hours to complete and it's in chat gbt and it takes like 5 to 30 minutes to to answer you and so it's able to do much more in-depth research and answer your questions with much more detail um and specific sources than regular chat gbt response would be able to do it's one of the first agents that we've released so we released operator um pretty recently as well and so um deep research is the second agent and you know we'll release many more in um in future what's the origin story behind deep research like when did you choose to do this what was the inspiration and how many people work on it like what what did it take to bring this to poition good question this is before my time so to hear yeah so I think um maybe around a year ago we were seeing a lot of success internally with um this new reasoning Paradigm and training uh models to think before responding and we were focusing a lot on um Math and Science domains but I think that that the other thing that this kind of new reasoning model um regime unlocks is the ability to do longer Horizon tasks that involve like AG gench kind of you know abilities and so we thought you know a lot of people do tasks that require a lot of online research or a lot of external context and that involves a lot of reasoning and discriminating between sources and you have to be quite creative to do those kinds of things and I think we finally had models or a way of training models um that would allow us to to be able to um tackle some of those tasks so we decided to try and start training models to um do first browsing tasks so using like the same methods that we used to train reasoning models but on more real world tasks was it your idea and Josh how did you get involved at first uh it was like me and Yos ptil um who as our opening ey is working on a a similar um project that will be released at some point which we very excited about um and we we built an original um and then also with Thomas Dimson who's one of those people who just is an amazing engineer like will dive into anything and just you know get loads of things on so it was very fun yeah and I I joined more recently I uh rejoined opening I about six months ago um uh from my startup I was uh out opening ey in the early days and um was looking around the projects um when I rejoined and got very interested in some of our HMT efforts including this one and uh and got involved through that amazing well tell us a little about who you built it for yeah I mean it's it's really for anyone who does knowledge work as part of their um as part of their day-to-day job or really as part of their life um so we're seeing uh a lot of the usage come from people using it for work um doing things like uh you know research as part of their jobs um for uh you know understanding U markets companies uh real estat a lot of scientific research medical I think we've seen a lot of U medical examples as well um and and one of the things we're really excited about as well is um you this this style of like uh I just need to go out and spend many hours doing something that um you know where I have to do a bunch of web searches and Cate a bunch of information is not just a work thing but it's also um useful for shopping and uh travel as well so we're excited for the the plus launch so that more people will be able to try deep research and maybe we'll see some new use cases as well it's definitely one of the products I've used the most over the last couple weeks it's been amazing using it for work for work definitely also for fun like what are you using it for oh for me oh my goodness um so I was thinking about buying a new car and I was trying to figure out when the next model was going to be released for the car and there's all these speculative blog posts like there's patterns from the manufacturer and so I asked deep research can you break down all the gossip about this car and then all of the facts about what they've done what this automakers in before and it put together an amazing report that told me maybe wait a couple months but this year like in the next few months it should come out yeah like one of the things that's really cool about it is it's it's uh like it's not just for going Broad and Gathering all of the information about a source but it's also really good at finding like very obscure like weird facts on the internet um like if you have something very specific you want to know that you might might not just turn up in the first page of search results it's good at that kind of thing too so that's cool what are some of the surprising use cases that you've seen o I think the thing I've been most surprised by is how many people are using it for coding yeah um which wasn't really a use case i' considered but I've seen a lot of people on Twitter and in various places where we get feedback using it for coding and code search and also for finding the latest documentation on a certain package or something and helping them write a script or something so yeah I'm like I'm kind of embarrassed that we didn't think of that as a use case because it's like you know for chbt users it seems so obvious but um it's I know it's impressive how well it works how do you think the balance of business versus individual use case will will evolve over time like you mentioned the plus launch that's happening you know in a year's time or two years time would you guess this is mostly a business tool or mostly a consumer tool I would say hopefully both uh I think it's a pretty General um capability which and I think it's something that we do both in work and in personal life so yeah I'm excited about both I think the the magic of it is like um it just saves people a lot of time um you know if there's uh something that might have taken you hours or in some cases we've heard like days um people can just put it in here and get you know 90% of what they would have come out up with on their own um and so yeah I I tend to think there's like there's more tasks like that in uh business than there are in personal but I mean I think for sure it's going to be part of people's lives and both yeah it's really become the majority of my uses for CHC I just always pick deep research rather than normal so what are you saying in terms of consumer use cases and what are you excited about I think a lot of shopping travel recommendations um I've personally used the model a lot I've been using it for months to do these kinds of things um we were in Japan for the for the launch of deep research so it was very helpful and finding restaurants with very specific requirements and finding things that I wouldn't have like necessarily found yeah and I found it like when you have um something it's like the kind of thing where you know if you're shopping maybe for something expensive or you're planning a trip that uh is special or you want to spend a lot of uh uh that you're you want to spend a lot of time thinking about it's like for me you know I might go and spend hours and hours like trying to read everything on the internet about this one this product that I'm interested in buying um like scouring all the reviews and uh the forums and stuff like that and deep research can put together um kind of like something like that very quickly and so it's it's really useful for that that kind of thing the model is also very good at instruction following so if you have a query with many different parts or many different questions so if you don't you want the information about the product but you also want comparisons to all other products and you also want um information information about reviews from you know Reddit or something like that you can give loads of different requirements and it will do all of them for you yeah another another uh tip is like just ask it to format it in in a table it will usually do that anyway but it's uh like if you it's really helpful to have like a table with a bunch of citations and things like that um for all the categories of things that you want to research yeah there are also some features that hopefully we get into the product at some point but the model is able to the underlying model is able to embed images so it can find images of the products um and it's also this is not a consumer use case but it's able to create graphs as well and then embed those in its response so hopefully that will come to chbt soon as well nerdy cons use case yeah yeah well and speaking of nerdy consumer use cases uh also like personal personalized education is a really interesting use case like if there's if there's a topic that you've been meaning to learn about um you know if you uh uh need to brush up on your your biology or uh or you know you want to learn about like um like like some some world event it's um it's really good at you know put put in all the information about um what you feel like you don't understand what aspects of it you wanted to go do research on it and it'll put together a nice report for you one of my friends is considering starting a cpg company and he's been using it so much to find similar products to see if specific um names are already you know the domains already taken Market sizing like all of these different things um so that's been that's been fun to he'll share the reports with me and I'll read them so it's been pretty fun to see another like fun use case is that it's really good at finding like a single like obscure fact on the internet like if there's like a uh you know like a an obscure TV show or something um that you want to you know to like find like one particular episode of of or something like that it'll go and it'll go deep and uh find the like one reference to it on the web oh yeah my my brother's friend's dad had this very specific fact um it was about some Austrian General who was in power during a certain a death of someone during a battle like a very Niche question and apparently chat gbt had previously answered it wrong and he was very sure that it was wrong so he went to the public library and found a record and found that it was wrong and so then um deep research was able to get it right so we sent it to him and he was he was excited um what is the rough mental model for you know what deep research is excellent at today and uh you know where should people be using the O Series of models where should where should they be using deep research what deep research really excels at is if you have a sort of detailed description of what you want and in order to get the best possible answer requires reading a lot of the internet um if you have kind of like more of a a vag question um it'll help you kind of clarify what you want but it's I mean it's it's really at its best when there's like a specific set of information that you're looking for and I think it's it's very good at synthesizing information at encounters it's very good at finding um specific like hard to find information um but it's maybe less and it can make kind of some new insights I guess from what it from what it encounters but I don't think it's NE it's not making new scientific discoveries yet um and then I think using the O Series model for for me if I'm asking for something to do with coding usually it doesn't require knowledge outside of what the model already knows from it like pre-training so you would usually use 01 Pro or o1 for coding or3 mini high and so deep research are a great example of where some of the new product directions for open AI are going I'm curious how to the extent you can share how does it work the model that powers deep research um is a fine-tune version of 03 which is our most advanced um reasoning model and we specifically trained it on um hard browsing tasks that we collected as well as um reason other reasoning tasks and so it also has access to a browsing tool and python tool so through training um end to end on those tasks it learned like strategies to to solve them um and the resulting resulting models good at online search and Analysis yeah and like intuitively the way you can think about it is um you make this sort of this request ideally a detailed request about what you want the model thinks hard about that um it searches for information it pulls that information and it reads it um it understands how it relates to that request and then decides um what to search for next in order to get kind of closer to the final answer that you want um and it's trained to do a good job of pulling together um all of those all that information into a nice tidy report um with citations that point back the original information that I found yeah I think what's new about deep research as an agent at capability is that because we have the ability to train end to end there are a lot of things that uh that you have to do in the process of doing research that you couldn't really predict beforehand so I don't think it's possible to write some kind of language model program or script that would be as flexible as what the model's able to learn through training where where it's actually reacting to live web information and based on something it sees it has to make it change its strategy and um things like that so we actually see it doing pretty creative searches um you you can read the The Chain of Thought summary and I'm sure you can see sometimes it it's very um very smart about how it how it comes up with the next thing to look for so John Carlson had a tweet that went somewhat viral you know how much of the magic of deep research is you know real time access to web content and how much of the magic is in kind of Chain of Thought uh can can you maybe shed some light on that um I think it's definitely a combination I think you can see that because there are other sear products that don't um necessarily they weren't trained ENT and so um won't be as flexible in responding to um you responding to information and accounts won't be as creative about how to solve specific problems because they weren't specifically trained for that purpose um so it's definitely a combination I mean it's a fine tun version of 03 O3 is a very smart and Powerful Model A lot of the analysis capability um is also from the underlying 03 model training um but so I think it's definitely a combination before open AI um was working at a startup and uh we were uh dabbling and building agents um kind of the way that I see most people describe building agents on on uh on the internet um which is essentially you know you uh you construct this graph of operations and some of the nodes in that graph are language models um and so you can the language model can decide what to do next but the overarching logic of the you know sequence of steps that H uh that happen is defined by a human and um what we found is that it's really it's like powerful way of building things to get quickly to a prototype but um it falls down pretty quickly in the real world because it's very hard to anticipate all the scenarios um that the model might face and think about all the different branches of the path that you might want to take um in addition to that the um models often are not the best decision makers um at nodes and that graph because they weren't trained to do to make those decisions they were trained to do things that look similar to that um and so I think the the uh the the thing that's really powerful about this model um is that it's trained directly end to end to solve the kinds of tasks that uh that users are using it to solve so you don't have to set up a graph or make those node like decisions in the back on the architecture on the back end it's all driven by the model itself yeah can you say more about this because you know it seems like that's one of the very opinionated decisions that you've made and clearly it's worked um there's so many companies that are building on your API um kind of prompting uh to you know to you know solve specific tasks for specific users do you think all a lot of those applications would be better served by kind of having you know trained models end to end for their specific workflows I think if you have a very specific workflow that is quite predictable it makes a lot of sense to do something um like just described but if you have something that um has a lot of edge cases or um it needs to be be quite flexible then I think something similar to deep research is probably a better approach yeah I I think like the guidance I give people is um the the one thing that you don't want to bake into the model is like kind of hard and fast rules um like if you have you know a database that you don't want the model to touch or something like that it's it's better to encode that in in human written logic but I think it's kind of like a a lesson that I've seen people learn over and over again in this field is like um you know we we think that we can do things that are smarter than what the models do by writing it ourselves but uh in reality like usually the mo like as the field progresses the model come up with um better solutions to things than humans do and um and uh also like you know um the like probably like number one lesson of machine learning is like you get what you optimize for and so um if you if you're able to set up the system such that you can optimize directly for the outcome that you're looking for um the results are going to be much much better than if you sort of try to glue together models that are not optimized end to end for for the the tasks that you're trying to have them do so my like like long-term guidance is that um you know I think like reinforcement learning um tuning on top of models is probably going to be a critical part of how the most powerful agents get built what were the biggest technical challenges along the way to to making this work I well I mean maybe I can say as like an observer um rather than someone who was involved in this from the beginning but it seems like kind of one of the uh the things that um that Isa and the rest of the team worked really really hard on and was kind of like one of the hidden keys to success was like um making really high quality data sets um it's uh you know another one of those like age-old lessons in machine learning that people keep relearning but the the quality of the data that you put into the model is is probably the biggest determining factor in the quality of the model that you get on the other side and then have someone like um Edward Edward son who's other um person who works on the project who just any data set he will optimize so that's a secret to success find your Edward yes great great machine learning uh uh model training how do you make sure that it's right yeah so that's obviously a core part of this model and product is that we want it to be users to be able to trust the outputs so part of that is we have um citations and so um users are able to see where the model is um citing information from and we during training that's something that we actually like try and make sure is um correct but it's still possible for the model to make mistakes or hallucinate or trust a source that maybe isn't the most um trust worthy source of information so that's definitely an active area where we're want to continue improving the model how should we think about this together with you know 03 and operator and other different releases like does this use operator do do these all build on top of each other or are they all kind of a series of different applications of o03 uh today these are pretty disconnected um but you can kind of um you can imagine kind of where we're going with this uh right which is like um the the ultimate agent that um people will have access to at some point in the future should be able to do um you know not just web search or using a computer or any of the other types of actions that you'd want like kind of a a human assistant to do but should be able to fuse all these things in a more natural way any other design decisions that you know you've taken that are maybe not obvious at first glance I think one of them is the the clarification flow so if you've used deep research the model will ask you questions before starting its research and usually chat gbt maybe will ask you a question at the end of its response but it usually doesn't have such um uh that kind of behavior upfront um and that was intentional because um you will get the best response from the research model if the prompt is very well specified and detailed and think that it's not the natural user Behavior to give all of the information in um the first prompt so we wanted to make sure that if you're going to wait five minutes 30 minutes that your response is as detailed and satisfactory so um we added this additional step to make sure that the user provides all the detail that we would need and I've actually seen a bunch of of people on Twitter are saying that they have this flow or that they will talk to 01 or 01 Pro or to help um make their prompt more detailed and then once they're happy with the prompt then they'll send it to deep research which is interesting um so people are finding their own own workflow so how to use this so there's been three different deep research products launched in the last few months tell us a little about what makes you guys special and how we should think about it and they're all called Deep research right they're research yeah not a lot of naming creativity in this field um I I think people should uh should try all them for themselves and get a feel I think uh I think the the difference in like quality um I think they they all have pros and cons but I think the the difference will be clear um but like what that comes down to is just the way that this model was built um and the the um sort of the effort that went into um constructing the data sets and then the the engine that we have with the O Series mod models um which allows us to just um optimize models uh to make things that are like really smart and really high quality we had the 01 team on the podcast last year and we were joking that open I is not that good at naming things I will say this is your best named product deep researches at least it describes what it does I guess yeah so I'm curious to hear a little about where you want to go from here you have deep research today what do you think it looks like a year from now and what maybe are complimentary things you want to build along the way well excited to expand the data sources that the model has access to we've trained a model that's generally very good at browsing public information but um it should also be able to to search private data as well and then I think just pushing the capabilities um further so it could be better at browsing it could be better at analysis and then and then thinking about how this fits into our agent road map more broadly um like I think the the recipe here is um something that's going to scale to a pretty wide range of use cases um things that are uh can surprise people how well they work um but this idea of you take a a state-of-the-art reasoning model you give it access to the same tools um that that humans can use to uh to do their jobs or to go about their daily lives and then you optimize directly for the kinds of outcomes that um that you're looking that uh you want the agent to be able to do um that recipe there's like really nothing stopping that recipe from scaling to more and more complex tasks um so I feel like yeah AGI is like an operational problem now um and uh I think yeah um a lot of things to come in that general formula I'm so Sam had a pretty striking quote of deep research will take over a single DIN percentage of all economically viable tasks valuable tasks in the world how should we think about that I think of it as like um it's deep research is not capable of uh doing all of what you do um but it is capable of saving you like hours or sometimes in some cases days at a time um and so I I think like uh what we're hopefully relatively close to is um deep research in the agents that we build next and the Agents that we build on top of it um giving you you know 1 5 10 25% of your time back depending on the type of work that you do I mean I think you've already automated 80% of what I do so it's definitely on the higher end for me we just need to start writing checks I guess yeah are there entire job categories that you think are kind of more at risk is the wrong word but like more in the in the strike zone for what deep research is exceptional so for example I'm thinking Consulting uh but like Are there specific categories that you think are more in strikes on yeah I used to be consultant I don't think any jobs are at risk like I I don't really think of this as like a labor replacement kind of thing um at all like it's uh but for these types of knowledge work jobs where like where you are spending a lot of your time kind of looking through information and making conclusions I I think it's uh it's going to give people superpowers yeah I'm very excited about a lot of the medical use cases just the ability to um find all of the literature or all of the recent cases um for a certain condition I think I've already seen a lot of doctors posting about this or like they've reached out to us and said oh we used it for this thing we used it to help find um a clinical trial for this patient or something like that so just people who are already so busy just saving some time or um it's maybe something that they wouldn't have had time to do so and now they they are able to have that information for them yeah and I think the like the impact of that is like maybe a little bit more profound than it sounds on the surface right it's not just like it's not just like uh you know getting 5% of your time back but it's um the the type of that might have taken you four hours or eight hours to do um now you can do for you know um a chat gbt subscription and 5 minutes and so like what types of things would you do if you had infinite time that now maybe you can do like many many copies of so like you know you should should you do uh research on every single possible startup that you could invest in instead of just the ones that you have time to meet with things like that or on the consumer side one thing that I'm thinking of is you know the the working mom that's too busy to plan a birthday party for the for her like now it's now it's doable so it's I agree with you it's way more important than 5% of your time it's all the things you couldn't do before exactly what does this change about education and the way we should learn and you know what what will you be teaching your kids now that we're in a world of agents in deep research yeah education's been like one of the top few things that people use it for um I think it's I mean this is true for atrib generally it's it's like a if uh like learning things by talking to um an AI system that is able to like personalize uh the information that it gives you based on what you tell it or uh maybe in the future what it knows about you um feels like a much more efficient way to learn and a much more engaging way to learn than uh like reading textbooks we have some lightning round questions all right okay your favorite deep research use case I'll say yeah like personalized education just like learning about anything I want to learn about i' I've already mentioned this but I think a lot of the personal stories that people have shared about finding information about a diagnosis that they received or someone in that family received have been really great to see okay we saw a few application categories breakout last year so for example coding being being an obvious one what application categories do you think will break out this year I mean clearly agents agents I was gonna say too okay 2025 is the year of the agent I think so yeah and then how do you think about what piece of content that you should recommend people reading to read to learn more about agents or where the state of AI is going could be an author too training data this podcast not I think it's it's like um it's so hard to keep up with the state-of-the-art in AI um I think the like the general advice I have for people is like um pick one or two subtopics that you're really interested in and go like cu cre a list of people who are we think are saying interesting things about it and like how to find those one or two things you're interested in um maybe actually that's a good deep research use case like you know go go uh go use it to F like go deep on things that you want to learn more about this this is a bit old now but I think a few years ago I watched the I think it's called like foundations of RL or something like this from pel and it's um it's it's a few years old but I think that it was a good introduction to reinforcement Landing so yeah would definitely second any any content by uh Peter AAL my grad school adviser yeah oh yeah yeah okay reinforcement learning is it you know it kind of went through a peak and then felt like it was in a little bit of adrum again is speaking again is is that the right read on what's happening with RL it's so back yeah so back yeah why why now because everything else is working like uh I think if you um maybe people have been following the field for a while we'll remember the Y laon cake analog analogy if you're building a cake um then most of the cake is the cake and then there's a little bit of frosting and then there's a few cherries on top and the analogy was that like unsupervised learning there's the cake supervised learning is the frosting and uh reinforcement learning is the cherries on top when we in the field were working on reinforcement learning back in you know 2015 2016 it's kind of like I think uh uh Yan lon's analogy which I think in retrospect is probably correct is that we were like trying to add the cherries before we had the cake um but now we have language models that are pre-trained on massive amounts of data and are incredibly capable um we know how to uh how to you know do uh supervised fine tuning on those language models to make them good at instruction following and like generally doing the things that people want them to do and so now that that works really well it's like very ripe to uh to tune those models for any kind of use case that you can define a reward function for great okay so from this lightning we got agents will be you know the breakout category in 2025 and reinforcement learning is so back I love it um thank you guys so much for joining us we we love this conversation congratulations on an incredible product and we can't wait to see what comes with it thank you thank you thank you [Music] [Music] oh [Music]

========================================

--- Video 28 ---
Video ID: rqgCXcd9U1U
URL: https://www.youtube.com/watch?v=rqgCXcd9U1U
Title: AI, Security and the New World Order ft. Palo Alto Networks’s Nikesh Arora
Published: 2025-02-18 10:01:05 UTC
Description:
Palo Alto Networks’s CEO Nikesh Arora dispels DeepSeek hype by detailing all of the guardrails enterprises need to have in place to give AI agents “arms and legs.” No matter the model, deploying applications for precision-use cases means superimposing better controls. Arora emphasizes that the real challenge isn’t just blocking threats but matching the accelerated pace of AI-powered attacks, requiring a fundamental shift from prevention-focused to real-time detection and response systems. CISOs are risk managers, but legacy companies competing with more risk-tolerant startups need to move quickly and embrace change. 

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

Transcript Language: English (auto-generated)
like I I have a principle that I was joke even on our All Hands say I've never met a person who comes to work to screw up I W up morning let's go sunshine it's time to go to work let's me see how badly I can do today everybody walks in with the right attitude it's something that happens at work that we create that causes the unintended outcomes it's not the person who walks in if you found the right person with the right domain knowledge the right intelligence and the right attitude then the rest is upon us [Music] today on training data we have a very special episode with nikesh Aurora the CEO of palala networks since joining paloalto in 2018 Nash has built it into the largest and most valuable cyber security company in the world with 70,000 customers and more than $120 billion of market value prior to paloalto Nikes spent a decade at Google as the chief business officer as the company Grew From three billion to about $65 billion in revenue nikesh is an extraordinary CEO with a inquisitive mind and a wonderful sense of what is happening in the world of AI thanks to being in the center of it with palato and all of their customers please join us for a wide ranging conversation about AI its impact on security and what excellent leadership looks like we hope you enjoy the cash thank you for joining us on training data so we emailed you and asked you if you join us on the show and your response was and I quote as long as we can talk about deep seek and the New World Order let's start there tell us more I Pat you know we all have our interpretation of AI and there's a bunch of us trying to figure out and rationalize this some sort of mental framework so like everybody else I've had my own and I think from my perspective what we've seen in the last 12 months has been phenomenal and we have people trying to build effectively a brain right of some sort a brain with immense capacity to remember everything to process everything and do patent recognition which is kind of like you know my my interpretation of an nlm now that brain because being trained in data that's out there is acceptible to reaching the wrong conclusions depending on data is using to train itself so this is not a secret and we hear of that in various contexts of hallucination or not having the right answer because I've never seen it before and that's fine you can call it the early brain but at some point in time these things are going to become very smart f as smart as upad then it would take several more years to hit Sonia stage exactly right so at that point in time I think we all have to start getting a little worried so the question is how much money does it take to build this this brain one and two how can all of us use it effectively I think we can all use it effectively today in certain use cases we've seen out there whether it's you know creative use cases or search use cases or you know data aggregation use cases or data regurgitation use cases at some point in time you're got to take this brain and give your arms and legs and let it do stuff that's where things start getting dangerous and we've seen examples where people gave these brains the right to do stuff too soon too early where they started giving free cars or you know refunding airline tickets which is not a good idea because that is their version of hallucinating and giving stuff away but you know on the other hand uh people are sitting in cars where these brains have arms and legs are driving us around with drivers so there are examples where there are Precision use cases which are narrow and Tas specific where we are letting these things get access to it so I'm sorry for a long Preamble but the whole notion of the new world order was you know and I won't I don't have an opinion on whether cost $6 million or more my opinion is that if somebody built a brain cheaply and made it available cheaply it just expands the opportunity for a lot of these startups a lot of people to try and deploy that brain to do various tasks and that to me is is a major shift in what has been sort of the main state of this industry where we all T we need to spend a lot of money to build amazing models and looks like there could be task specific models which could be built a lot cheaper and you mentioned some of the hallucinations and the attempts to jailbreak these models or prompt inject these models there's a report that came out a few days ago about deep seek R1 that said 50 out of 50 prompt injections worked so basically 100% success rate on attacking the model is that a deep seek thing is that an open source thing do you have a perspective on what the implications of that might be maybe it's not as simple as six million bucks get you the same thing you get out of open AI well the question is which one do you want right and like at this at the end of the day every model is putting a bunch of guard rails around it these models are all raw if they're in the Raw they have they're in the Raw they've you remember the early versions of CH gbd and Gemini I think it was even called vertex what it called that it was called something else before Gemini and those things had the opportunity for us to prompt inject as well so they were versions of these models which had to have guard rails bu built around them and those things that's what it cost money to do is build guardrails and the guard rails initially were skin deep as you know we've heard of these phenomenal stories in the early days where were able to jailbreak them and get around them and get models to start doing crazy stuff so I think we will see more and more guard rails more and more simpler attempts being blocked I think there are still sophisticated things that can be done to these models even the more in your mind more expensive models have loopholes or have side doors which can be used to attack them to some degree have we've seen that happen in the past so yes perhaps deep seek is not as guard rail as it is and perhaps it was built cheaply but in the the end of the day whichever model it is when you deploy it for a Precision use case and give it arms and legs it doesn't matter what guard rails the model comes it you will have to superimpose better guard rails and controls around it this is where People Like Us come in where we say it doesn't matter what you got I'm still going to put a all to firewall and put a straight jacket around this and make it only to spond to task specific stuff I.E if the model is designed to improve your manufacturing process you can talk to it about you know rewriting Shakespeare let's talk about that for a minute what's in scope and what's out of scope for palala networks as it relates to securing AI well from our perspective look we're seeing two interesting use cases one we're seeing a lot of people who are employees who are kids who are users using AI in some way shape or form to augment their day job and you can call it augment for now and maybe it'll it'll creep up and do more more of your day job but it's being used as human augmentation for now right because we're not getting it control I'm not telling an AI agent go write me a paper for my class or I'm not telling any other agent to write me a Blog and possibly one of these days they will for now it's being us for human augmentation and the general fear in the Enterprises my employees are taking to a PRI data and putting it sub model and it'll be used for training and over time you know it'll get either out of copyright or get stolen or it'll be become part of general knowledge based netor PRI data so we have a use case where we can intercept data which has been used by employees or models or AI that's been used by employees and provide visibility to Enterprises and provide control so they can stop employees from going in using AI models or AI apps uh with without any controls that's kind of one use case which is kind of interesting we see a lot of companies who want their employees to use AI but they want them to be able to do in the control fashion other more interesting new cases I haven't found a company which is not experimenting with some sort of AI project whether it's a simple as a customer service chat part it seems to be the most popular example or some sort of workflow automation capability which is another example to the extreme where people are using it to perhaps slowly Ed giving it control over certain you know Control Systems which may not be Mission critical but they're experimenting there in all these scenarios the biggest fear is the model runs them up the model gives the wrong answer or the model takes control or somebody hijacks the model all those are scenarios which customers are vary about which is kind of like understandable in that scenario we have a product which is effectively we you know formally call it the a firewall the a firewall which inspects anything going in anything going out of the model it'll make sure the model doesn't have back doors nobody can access it the data is not being sent out of the model somewhere else you can run it on Prem you can run it in your you know protected Cloud instead so those are kind of the two use cases we're seeing the behavior of the model is the responsibility of the people generating the model M our job is to make sure the model doesn't get hijacked doesn't get intercepted doesn't get taken over or manipulated so that you know people lose control off their quote unquote AI brain can you say a little bit more about you know what are the real threats from AI versus the perceived or the hypothetical risks like I remember back when when self-driving cars were still you know a little bit of a pipe dream and everyone was saying we're going to have these adver serial you know images QR codes in the rows that are going to make the cars you know become weapons and you know things are going to go crazy and that ends up being like very academic theoretical risk it feels like there's some of that happening in LM lands like what what are what do you think are the madeup academic risks and what are the very real risks you think um are G to you know where AI is actually going to really help the bad guys and we have to protect ourselves well look there is the these two scenarios right a scenario where the bad guys are just going to use the llm to attack us faster right because are you seeing that happen already it is already happening there's a critical security incident or vulnerability in a product you can go to certain jailbreak models or open source models out there which will give you a recommendation on how to exploit the CV because 3,000 models of huging face so you can pick a model which hasn't been given guardrails or given any morals effectively in the context of a brain and saying hey here's a cve what are the five steps you take to protect it and you know what are the five steps that bad guys could use to attack it so he says oh by the way watch out for these five things bad guys could do so there are models out there that can actually give you a recipe to figure out how to exploit a CV or you can actually tell it I try to attack a customer with option A I tried option b none of them work because it give this return response and it says hey how about you try option C so there's a whole bunch of ways that these models can be used very they're very helpful right now right so they're going to try and solve your problem and there's a risk that actually not just a risk it's actually true right now and what that does it reduces your your meantime to attack an Exel trade data or meantime to breach which means the only way to solve that problem is to be as Nimble as effective and as quick as the bad guys are which sort of like you know it's a it's a as I always say it's kind of a disbalanced problem they have to be right once we have to be right 100% of the time which means they might need this sliver of data to attack you we need the entire Corpus of Enterprise logs and Enterprise data from every it system to be able to understand where there be there may be an almost almost activity which is being driven by AI so that mean right now ai is at the margin more helpful for the bad guys than it is for the good guys well it depends can we can always sell our book and tell you if you're deployed or xim product we can be as equally effective and equally helpful and Tau the bad guys but yes you know they're not fully deployed not everybody has it so yes there's a possibility that it just has made the ability to attack much faster for the bad guys right and that's kind of a real threat it's not a perceived threat and I think if you if you play the movie forward and say let's abstract ourselves from this today and this is version one or version two of AI you know in five years from now everything will be happening in a real-time basis everything every bad actor or bad llm agent would be able to attack an Enterprise infrastructure which not fully secured and there'll be agents running around your infrastructure trying to make sure that every loophole every door every window is locked and constantly monitored so you can imagine the battle of the agents on either side uh I don't think it's infeasible it's possible but to get there there's going to be serious upheaval required of the Enterprise data that exists in the company which by the way is not unlike the fact that to get effective AI for organizations we're going to have to have a lot of good data to automate or manage or you know run businesses so I think that's kind of where we're going to end up in terms of the other part Sonia you asked about the proceed versus real threat look the think about it this way you know let's assume and we all I think I don't know I haven't heard you guys talk about this but I'm guessing you agree that at some point in time these models get smarter and smarter and they'll be more and more capable so let's assume that's going to happen they get very capable this person is equivalent of a PhD researcher from PI your favorite University and can do drug disc Discovery now you've trained it you given all the data that exist in Enterprise it's all proprietary it's all the drug data for Alzheimer's Parkinson's you pick your favorite you know research project that you want to do and you ask the model or this brain to give you an antidote to V medication could be amazing for society and the question is in the wrong hands this trained brain could also be be asked to make a virus to create that situation right create a bioweapon it's possible this this brain has no guardrails you've trained all the data it has all the knowledge that you need to have then the question is can I make sure that this brain cannot be taken over by the wrong people that it falls in bad hands so just for fun if you were supreme ruler of the universe and you had a magic want yes and you could determine exactly what regulation was going to apply of this hypothetical what sort of Regulation would you craft you know pad this is an interesting debate and you I had a debate about this with a very very smart uh person who's involved in some of regulatory aspects of this look at the end of this there will be two versions I think one version is critical systems where be before giving AI Control of critical systems you'll have to go through a serious certification Discovery process with some of the US government right you cannot give the control systems to AI for shipping routes and running cargo containers which can crack and burn or controls the entire electrical grid of the United States he can't give it to any other model because you need to have controls in place and need to be able to have a you know conversation around what the fallbacks are and what the control so I think there will be a set of classified activities which will need some degree of consultation some degree of certification validation it's kind of like you know FDA does drug approval so there'll be some version of you know AI approval which can have critical irreversible impact if you give control to Ai and that'll have to be some sort of certification mechanism and I think where it is not as fatal where it's not as critical perhaps you'll have some degree of self- responsibility you know you make a bad car people have a problem with it you're responsible not every car goes through inspection process but there is a tremendous amount of accountability to the car companies that is don't have seat belts that don't comply with regulations that they're responsible for the bad outcomes which this way if you deploy AI in a bad way in your company and give it arms the legs in Patrol then you're responsible there'll be some degree of because it is impossible for any reg authority to create an inspection system of this amount of computer data which can get it right every time so there will have to be self- policing and self accountability in there just to way it exist in today in many Industries NES do you think AI Labs get nationalized uh in this you know your version of supreme ruler of the universe yeah Labs get nationalized I don't think so I think the problem is if you know given that we're living hypothetical if there is it is true that a new model can be produced at a lot lower cost right which is in the single digit millions or tens of millions and the AI lab could be anywhere it would be impossible to find discover and you know control so what's stopping somebody from and and part of like these challing these regular concepts are very dangerous on a global basis today which we live in effectively a world with no borders even though I know that we have whole different conversation around borders but conceptually what's stopping somebody from deploying 50 million dollar in a server cluster in a country which has lack regulation Vis this stuff and me building it there or somebody building it there so I don't think that the idea that yes of course if it's a $500 billion a cluster that needs is needed to build the world's superb brain and AGI yeah you can find a way of you know maintaining some degree of uh oversight perhaps on it but if the answer is this 20 million bucks and I can build a world-class model which is really smart then I think all bets are all let's say I am not necessarily A ciso maybe a CEO let's say I'm a corporate executive of some sort and I see the potential for AI so I'm excited about trying to use AI but I'm very scared I'm very scared because I think that when my people use AI they're just increasing the attack surface and making us more vulnerable and I'm also scared because I think there are bad guys out there who are going to weaponize AI against us and sneak in in ways that they might not have been able to sneak in before what what would your advice be you know top three things that you would advise this person to do what what can people do to get the benefits of AI without exposing themselves to unnecessary risk I I know I think that that would be uh ill informed fear in my mind okay I think uh they're perfectly fine use cases which I'm sure you can inate and Sonia can and and a lot of people can where you can run a model in a constraint on Prem or dedicated Cloud cluster which cannot be intercepted cannot be manipulated in the end that model is only useful if you put your own data into it and if all you do is have the model generate responses which you're not letting it give it any control you can run experiments you can look at what the model produces and compare that to other things you can do AB testing and say wow the model says this and my best researcher says this and you can run experiments understand the power of AI without giving her any control so I think that's why the fear is a bit mislay because it's not doing anything it's just trying to give you the outcome and you can see if it's faster better and both are possible or one is possible Right some things happen faster some things happen even better so I think running AB testing being able to tested is easily possible in today's world without having any fear I think it's even possible for letting employees experiment with it in a way that it is not manageable I think where it starts to get more interesting not dangerous perhaps is when you start letting AI act on your behalf right in whichever capacity and that's where I think any any person not just CEOs anybody would have to go through a rigorous amount of testing to see how it reacts in various circumstances because depending what you're giving control to it could have a significant impact to whatever product service business that you're running that's where I think it becomes more interesting but I think for now running experiments running models which are cannot be hijacked or manipulated models that won run a mock it's all possible today I think you know it would be irresponsible for companies to not experiment well put because I don't think that this thing's going away yeah you may not know exactly how to get to the future but you know if you do nothing you're going to get left behind I learned about Chad GPT on a flight to India I was going there to go speak at my alum matter and I read about this thing I was sitting at Dubai airport not doing anything with two hours I kept playing with it and I rote my entire speech I went and said you're about to witness the biggest technological Revolution now I just say it before Jensen H this is the iPhone moment but more important he's got a bigger mouth and he's a supreme commander of AI so we'll let him we'll attribute that quote to him but that's okay and I felt it was a seminal moment and I came back and I said you know what first things first I have no idea about this thing so I called a bunch of my team like what do you guys know what we mean a nothing all like you know bunch of the important uninformed but we were important so we were but we had an opinion so the first thing I did is I put them all into a like a training room I invited everyone from Thomas gan's team to you know Nat Garmin now his team or bunch of start like just brain dump on us and we did that for two days a month we get people to brain dump we had a bunch of our people go come with ideas we had 70 ideas people w't execute cut them down to seven we started playing with with it h I ran everything every possible problem that you could run with vertx AI or with the first model of Chad gbt or the first model of claw we tried everything we ran everything through we had models running with AI we had models running with semantic search we were training with all kinds of data we learned we learned what is useful we learned what is not useful now it's doing some things by itself which we had to go Jerry rig but you know we are partially informed which is better than being totally ignorant what was the biggest surprise from those learnings the biggest surprise well you know the early version of this thing was pattern recognition was Data summarization was I'll call it infinite memory right once you train it with some data it's never going to forget it now there are use cases where I have 50 people solving the same problem and depending on who answers your phone they're going to solve it differently in this case I improved the general level of awareness and knowledge for my entire team playing get it to tell you the answer and then work from there so it did set lift the average intelligence of the average average capability of the teams and I think as it gets better and better it's going to shorten the time to answers and I mean at the end I want to expose allot of this stuff to our customers right so they can go solve this problem so my problem is if you don't start learning when every startup is learning eventually the startups do your business right you've seen that n technological Revolution that we've run into whether it's a cloud Mobility the internet we saw it every time and every time there was these large now we can call them Legacy but large businesses with dominant market share with every asset at their behest which they could have deployed and nobody should have seen the lighter day who was competing with them with the new technology but for some bizarre reason every time you turned around there was a Travis there was a you know Chad Hurley at YouTube and there was a Larry Page and there was Mark Zuckerberg and there was an y on us so the challenge is that if we don't go Embrace this as early as we can and learn while everybody else is learning we run the risk that we're late and then we goe law of unintended consequences maybe on that note one thing I'd love to understand is it seems like the biggest platform companies in security are kind of formed around these platform shifts like you know the firewall identity as a perimeter you know maybe the cloud uh and and cspm like do you think AI is a new platform shift opportunity from a security point of view and and do you think a new security platform company which could be you guys emerges or is this very much you know uh is similar kind of set of of uh tools is going to is going to serve the AI first world I think AI has the opportunity to turn Security on its head and the reason I say that is that the security is a needle and haast stack problem right because you don't worry about it until you have to worry about it so you to suddenly wake up and get really really smart very very quickly because Something's Happened in your infrastructure and it's just impossible to go from 0 to, like overnight because somebody calls holy there's somebody in our infrastructure they've exfiltrated sunm or they in the midst of exfiltrating data and traditional security has been I'd say 95% at the border or on prevention and 5% on detection and immediation right you buy a firewall it inspects everything that's coming in you block a bunch of stuff you buy some sort of you know remote access endpoint agent you buy an endpoint xdr capability and that all works because there's a lot of prevention that happens in that process but the problem in in breaches is it's not what you prevent it's what you let in and there's things like zero day attacks which have never been seen before so you haven't seen it very often you can't prevent it and the only way you figure out all that stuff is you ingest a lot of data you look at it and look for anomalous Behavior right you can't rely on security signature so if you're going to look at anomalous Behavior you need to be able to inest all day you got to look at pattern recognition say does this happen like this every time and say well I don't know but looks like something's different happening so I think this whole notion of doing pattern recognition ingesting a lot of data analyzing it on the Fly and looking for things is easily possible with col it machine learning call it Ai call it whatever you want to call it but I think that's the only way we can do this at realtime speed what about what it means to be a security team a ceso a security practitioner in this new world um and you know we we get 20 pitches a week right now for like the AI powered sock analyst or the AI sock uh what what is your vision for you know what you have one so you send them our way say you know try let's you on how that evolves and what the end state is uh for for kind of humans and security like in security uh at a very first principal level we sell two things we sell a sensor with Senses at the edge of your perimeter whatever the perimeter is whether it's your laptop whether it's your application is a customer accessing your bank account that's the parameter right the parameter is the edge of your techn technological footprint that's the parameter so we all sell sensors that sit at the parameter and inspect it's like having like a digital security card at the parameter we all sell parameters and we protected parameters we inspected parameters we blocked parameters right and then we and then what happens that somebody WR in the bad app somebody in the back door somebody's in the side door by mistake then people enter through there so we that's we sell sensors we we protect parameters and then we analyze data in the back end to look for any vulnerability that you might have been created by the infrastructure that you have that vulnerability could be exploitable in the future so we look for potential exploits and that's that's going I do which means and the reason I tell you story is that which means if I want to sell and AI powered anything I need to be at points of data collection in your Enterprise because AI requires data so again this is the old adage right I am in the best place to collect all this data and analyze it and of course that doesn't mean anything because in history people who are in the best place got knocked off their knees and somebody else came and built something better because they were lazy sitting on their hunches now the only thing is we don't want to be lazy we don't want to sit on our hunches we're out there hustling as fast as we can not as Nimble as possibly in you know a startup but we're Nimble enough as a company we've done 27 products which are in the magic coordin to the right so we're not shy but I think every security company every security startup is going to walk to every customer say I can build this for you the customer great how do we start says well let me go deploy a bunch of sensors around the parameter so I can collect the data holy I already got a bunch of you guys in the industry you got sensors out there then what do you want me to do then give me all your data I'm like wait a minute you want all my data who are you again so I think that's kind of that's kind of the risk you run into is this is a large data problem and large data problems are harder to solve as a startup not to say it's not being done there are people out there raising $500 milon but not everybody can do that in security fair enough do you think security teams are comfortable giving uh arms and legs I think so to speak uh like agentic oh no I think they're petrified do you think that flips at some point and and when well I think most of security TS aren asked right I mean way mode wouldn't exist if they out the security guy or Tesla F ASD possible would not exist some security guys are you crazy you're giving the car control to your car all kinds of bad things can happen right yeah so from a security perspective these all bad things happen look and in Security leaders are for the most part Risk Managers right they're they're Risk Managers they're trying to understand what the business need is and how do I deliver the business need with the least amount of risk possible Right the safest room in the world is one with no windows and no doors but it's not very useful so you got to let doors and windows be created which means you're managing risk so security people are Risk Managers they sit down with the business understand what potential risk does it cause they'll give you some ideas as to how to make sure that you protect against that risk and they'll set up a whole bunch of safeguards let say you know you know gate one gate two gate three if it doesn't stop gets stopped here get stopped here get stopped here and then off the races so I think security people will allow the arms and legs have to because that's kind of the crying need of the hour I think the question will be what kind of security tools do we have in place to create those protections that customers can comfortably go ahead and use these capabilities but that's been true with every technology our uh our part Jim gets and for any listener who's not familiar with Jim Jim has been involved with Palo since formation uh and and Jim Jim has a creative mind one of the uh one of the things that Jim mentioned was after you came in about seven years ago as CEO The Innovation engine and paloalto really started to pick up and I think we see that today also with how quickly you guys have pounced on AI I guess the question maybe two questions question one if you had to R yourself if you had to reap paloo on a ility nimbleness ability to respond to market conditions I know you're a tough grader so you can't give yourself an A+ how would you grade yourself and then question number two you do have all the advantages of scale and data and distribution and being at those points where you need to collect the information to do whatever detection remediation you need to do um but it's hard to get a big organization to move fast enough to respond to the market so question one how would you gr yourself question to how do you drive agility at this scale like just practically speaking what do you do to make that happen you know uh let's go first step first I'd give us a seven or seven and a half on scale of 10 in terms of agility because uh we have about 15,000 people um 5 6,000 people in our product s so there's a lot of stuff a lot of complexity a lot of Legacy stuff that has to be brought along a lot of stuff that has to be and died to make sure that these things works and you know I think part of the challenge you know is that you have a s base of 70,000 customers right any tweak you make which impacts 70,000 customers brings their infrastructure down you lose your license to operate right so it's not like we can innovate throw at the wall and see what sticks and go with that and ignore the other stuff so we have a serious responsibility and making sure stuff that we build that we put in line has to keep performing and not bring down anyone infrastructure because they the best secur is inline we have to be able to watch what's going on inline security has the property that if it doesn't behave it can impact your infrastructure so we have a very high responsibility from an availability perspective and not disrupting our customers that we have to apply a higher Precision standard as it relates to inline Securities from that perspective I think 7 a half is not a bad place to be see probably were three or four seven years ago as an industry I don't say p Alo as an industry with three or four and I say the industry's moved its agility if I look at some of the newer players they're moving faster they're not sitting back anymore because they see the playbook for the future is not where you let other people sort of come by in the new swim Lane say oh nice to see you you know congratulations great job now it's like holy how they get there we're going to go Chas them down so I think the industry Dynamics have changed uh in terms of how do you drive agility you as you know possibly you know from talking to Jim and from talking to us that we have no sort of qualms about going and finding people who are doing it amazingly well and embracing them and saying you got this figured out let's go do it together we'll run for fast right sometimes companies get trapped in this idea that I have so many resources I can take them down they don't understand there's a team of 00 motivated people funded by people like you are out there running at sort of you know light speed who build an amazing product which are going to then if they get traction then they your competition on every day in the market and they get better and better and stronger so the question is when's the right time to say oh let's embrace them they've got less resources but still manag to kick our ass let's go make them part of our team we've done that 19 times as you've seen so we're not not we're not shy about embracing Innovation if it doesn't come for and having said that I was looking at it the other day I think more than half of our products are made in Palo Alto right not acquired so it's not like you know we only have one strategy we do both because in some cases building on our platform is a lot easier from a go to market and deployment perspective than buying something and spending time integrating it so we've gone to the point of scale that it's more important for us to innovate on our platform then just go out there and willly nearly try and look at the fastest innovator and try and stick them onto our Tech platforms so I think from that perspective it's that constant balance what do you buy what do you build how do you Embrace somebody else doing it better and then you got to be nimble and say you know what I am going to get some stuff wrong the question is when you get punched in the face how quickly you recover right don't let them count to 10 so that's kind of like how you maintain agility and then and the only other thing is I call it Relentless inspection Relentless inspection of your go to market capabilities of your deals Rel what form does that take like what's the sort of thing you might do in one of those Relentless inspection conversations that somebody else might not do well I give an example you know um for the longest times I kept seeing us doing really well in certain things and our teams would create all these incentive programs to drive more Behavior to get people to sell and I have my sales leaders tell me that everybody has an account plan I said these things look like very interesting things I should take a look at so one fine day this is possibly an year and a half ago we having a tough quarter I said great here's what we're going to do we're going start from customer number one and keep going they show me your account plan so what does that mean send them five slight say fill those five slides and show up on a zoom call have to show up and explain those account PLS to me and I'll fast forward I probably been to 750 of them so far in the company I did about 15 yesterday and it's it's like Tater now there are 500 people dialed in from across the company wow they all get to watch you salesperson can dial and watch an account review and process because for me it's it's it's basically them learning how to do it and we go through it and say who's the person who's the buyer does they understand the product what did you pitch how do you sell it did you sell it did you talk about this did you not talk about this why you talk about this and by the way the best thing for our teams is if you feel that this is doesn't look as robust as we'd like it to me you know B Jenkins our president many of our product leaders people get involved we're not there to we're not just rly inspecting we're actually assisting and like you'll be surprised if people spit up their laptops are open like people are pinging people LinkedIn texting people saying hey do you know this person this company I don't think our plan is robust enough we don't know the right people let's go so that's kind of like Grassroots I I I have a I have a little um whiteboard in my office where I write down things which I want my team to remember and the second thing written on it it says sales is a math problem true so which people people find hard to understand like look if you have the best product in the market and you now are able to win and generate billions of dollars of tcv a year then the question is why are you losing it's not the product because there are people buying the product is working it's not that nobody's will to buy it it's not that lots of people are will to buy it what happened in that process where you were selling that you didn't win somebody else did let's go inspect it yeah and sometimes you'll find some product things that you need to fix many times you'll just find execution errors let's keep going on this thread of driving performance set of people because one of the other things that Jim said was that you have sort of an exceptional ability to recruit and retain really exceptional people and that you you sort of Drive followership in a way that's unique like you're pretty hard on people and you demand a lot from them yeah I went to I went to speak yesterday uh for for a person who I worked with at Google her name is Lexi Reese and she actually ran for Sen and she has a startup now and she introduced me by saying I didn't quite enjoy my time when I was to work for you but I'm a better birth I learned a lot so I'm like well I'll take it whichever you give it to me but anyway sorry well and Jim said you balance that with a very nice human approach where people know that you care about them and you'll go to Bath for them you know in the right situation so I'm just curious what you've now been an executive in a variety of contexts and you've been successful every time um you know Google is a consumer business Palo Alto is an Enterprise business you know you kind of grow up around around marketing and sales you become more of a product person and so you've got this Diversified set of experiences um from that I'm an Enterprise person you're a full-fledged Enterprise person now sure I guess what leadership principles or what leadership techniques are sort of context independent that worked at each step in your journey and are there other things that are kind of pal offo specific but I'm mostly curious like what are the sort of core principles of your approach to Leading people not a lot of pal specific right because at the end of the day my senior executives are not writing product documents right they're analyzing strategy analyzing go to market they're understanding now of course do I have experts s security of course we could survive without that you know so here zook our founder Le CL told you product officer some the other product leaders that they have they're very smart guys they understand what our products exist in this industry they act as sounding board sometimes I'll challenge them at so there's there's no getting this done right without the right domain knowledge so you have to have do but I think outside of that fact that there's domain knowledge you still have to have the right people like as is possibly understood that I don't suffer for it because I can fix a lot of things I can't fix in others uh and if somebody doesn't get it like for sure and um you have to make sure you surround yourself with smart people but you find them keep them because the next question becomes you know what is the attitude these people bring to the table as long as they are willing to learn and humble and they understand they're part of a team all systems go like I I have a principle that I was joke even on all hands that says I've never met a person who comes to work to screw up I wake up in the morning let's go sunshine it's time to go to work let me see how badly I can do today everybody walks in with the right attitude yeah right it's something that happens at word that we create that causes the unintended outcomes it's not the person who walks in if you found the right person with the right domain knowledge right intelligence and the right attitude then the rest is upon us and then the question is how does management create the environment that people can Thrive and it's not just about you know happy gol lucky environment like I always say there are three jobs that I we have as Leaders one is we have to identify the North Star people have to know which monum we're going to climb you get the best climbers to find out you told the which one they're all on eight different mountains around you holy what happened they're all in different places so my job is to make sure I identify the mountain I fight I argue I debate you know I I kol whatever needs to happen to make sure we have a plan of record where we're going but ample degree of input but the end somebody's got to make a decision then the next thing is is it achievable can we write a plan to make it happen because the last thing you want is people come say I get it you wanted to build this but he gave me you know one pickaxe and one shovel and two people right and he you want me me to go dig the Platinum mine takes a lot more than that so the next question is is it a feasible plan to get it done and are you resourcing it right right and very often companies you look at our industry right A lot of people had the right ideas when I came to B Alto it's fny I joke sometimes I didn't do anything different we had a cloud security acquisition we had done we had an xdr acquisition we had done we had the idea of building a Sim we just hadn't resourced it we hadn't written the plan for it he kind of knew what we wanted to do but he haven't sat down debated argued so what does the future look like and we hadn't we hadn't written the plan we hadn't resource we have mon the resources we need it so you you don't have a plan you have an idea ideas are not good enough you got to have a plan and an North Star you have to resourcing that you can sort of execute it a plan the third job as management is to keep communicating it and weeding out things that block the execution of plan whatever it is whether it's a the person is's not doing it whether it's a resource that's not available whether it's a contract that's start working you it's blogging and tackling and sort of making way for your team so they can go and execute behind you so if you follow those principles to find the right people around you and you know don't suffer fools and have a good time while doing it uh Sometimes some people do get more scrutiny than the others but it's good for their for their career and good for their character it's amazing let me ask one more question on sort of management leadership and then maybe we can go back to some AI topics so um you mentioned the 19 companies that you guys have acquired in the last six seven years maybe two questions on that one at the moment when you're pulling the trigger what goes through your mind like how do you when it's time to make the go noo decision how do you decide that an acquisition is is actually an acquisition you want to make and maybe second question one of the things our partner Jim mentioned was that pretty much all the founders who've joined forces with palet and networks have stuck around the majority of them so yeah so what do you do postacquisition to actually keep them around and maybe it's the same thing you were just talking about with leadership generally a little more that like um I think before we get to decide it's an acquisition we want to make we spend enough time trying to understand you know is it even worth engaging with the company for a few hours or a few days right um and we have some principles I don't like buying number two or three it's a lot of Founders a lot of companies say you know what well you know what the first one's a billion the third one's 300 million let's just take the third one we got enough resources we'll spit and shine it it's going to make it br knew it be worth a billion dollars well there's a reason to trades a 300 million not a billion first of all which means they possibly have some gaps which the customers have identified and you haven't two you didn't actually take out the biggest player in the market it still going to be four steps ahead of you so now all you've done is taken a Nimble startup is is number three and possibly made it slower despite all your love and attention to it you like say okay well let me like bring me along but so now You' suddenly slowed down number three you've enhanced the the opportunity for number one or two so you said then say okay and a lot of times you joke about it saying I wish this this competitor would be bought by somebody in competition because they slow down so so we make sure we're only look one and two at best and sometimes they're neck to neck sometimes they're chosen two different parts and we did that in the browser space you know we're very happy with the acquisition of talent I think string really well that in DPM with dig actually in our industry what has happened since bat and Sonia is that at first people looked at me say what the hell this guy's buying these companies I don't know what his plans are and now we actually have a slide we keep track of once we buy something in a category that category becomes hot so people think we know something we've done a good job of about that but like I think the principle is you got to make sure you're buying the right sort of right player in the market then you got to make sure that you can convince the founders that they believe a Better Together story than the go alone story right there's no sell and dump because not going to happen we're not going to take the asset because when you're buying you're basically buying a Northstar an execution plan and a team that executes but usually they're a third or 40% down their Journey it takes more than 7even years to build a great product usually these things are three years out three or four years out they haven't fully matured into a full product that's going to win in the market so you need them around and you need the team around and still once we figured out this is the right company the right attitude you know we can actually make it work and there are now given our scale there are some technical considerations to be have to rewrite the stack which takes longer is this a complimentary area that can just run on our stack easily is this something you've never done before in which case it doesn't matter with tax on so a lot of those considerations from an integration and time to Market perspective but let's assume all those hurdles have been surmounted and we're actually engaged to the company I have a rule I walk in and tell my team and I I I live it I say treat them on day one as they're part of your team right because if they're going to work with you they're going to remember every interaction and very often I find many companies the most well-intentioned companies you start treating it as acquirer and acquired you know I live I come from a country which used which was acquired or ruled and I don't like this idea there's no like one rules the other it's like you we're part of the team and the day this deal gets signed we all be brought on the same page we all be trying to drive the same stock price and the same business forward so for that six weeks that you're in that discussion phase how does it why is it important that you're the acquirer and you're the acquired so so let's assume we do that we like a company then I send off the Finance and Accounting guys and legal guys to doe diligence and I tell this founder and their team saying your job now the next six weeks is to build a joint product plan and a joint or chart at the end of six weeks if you don't like the product plan or I don't like it if you don't like the orc chart I don't like it there's no deal this is leared behavior the first two three times we didn't do that and we discovered we spend the next six months arguing about what the product should be and who should be the boss this doesn't work this is a bad idea ideas of like hey buddy you want the money you can have the money it's my house I'm going to paint it yellow you don't like the color tell me now you can get somebody else to painted pink no problem so that has an amazing cleansing property because you're making a decision with all the facts in front of you saying if I get part of be part of PA Alto this is going to be the product strategy it could be yours it don't doesn't have to be mine I'm not smart enough so but we have a joint product strategy the not store and we have a joint plan of execution and then very often Pat just goes back to Jim's comment most often the founders we have bought companies from become the senior vice presidents of our company running their business our people work for them which I think is unique in the market very often you'll find there was an acquirer SVP who ran crypto or blockchain or pick your favorite I'm using non-security terms to keep it you know protect the innocent but you say oh since I'm responsible for this these people are going to work for me I'm like wait a minute you had all the resources you lost to them we're not going to have them go work for you maybe you can learn a few things so we did that a bunch of times and in some cases our teams worked for them really well in some cases our teams left to just F so I think those are some of the things that allow us to make these amazing Founders can work here and actually drive more value for us collectively Nikes I want to ask you about some of the chess that's now happening on the AI stage because I think you've played the you've played the chess game so flawlessly in the security market and you know you have so clearly emerged the winner the security the the AI Space by contrast feels just white hot competitive Hunger Games right now I'm curious your view I think it's a lot clearer than that I think it's just it's just it's just it's just not clear to the naked eye but I think it's a lot clearer but going on say more tell more yeah yeah tell us more like if you think about the state of maturity of AI you know there are two extremes we'll call them the very precise the alpha go type situations which you know Demis and Google built together which are I'd say fine-tune models which are designed for drug Discovery or the you know the bio Pharma field and there you see that they did a really good job they focused on the trained right data they hopefully treat them Ms in such a way that that can actually become a useful thing for society so you have that which is highly tuned AI models very TOS specific or category specific and then you have the generic ones and the generic ones are you know the rage today between the clouds and and the M draws and the Geminis and the open eyes of the world and those are large they are all encompassing all knowledgeable but we saw this movie before right we saw this movie in Search and as a Google you know then he had Vertical Search because the large Google search could not do as good a job of local search so he had local search the couldn't do a good job of product search so he had product search in Amazon so how can you be amazing at everything in this space when you couldn't do it in the last few technological Evolution so I think over time we're going to have to figure out what the distinction between you know general purpose large scale I know everything I'm I can do everything model versus models that are fine-tuned for tasks and I don't believe that all the perfect information in the world exists in open domain that you can go out and build it without you know specialization which means you are going to need specialized proprietary data to build these models and I don't know how you share data between glal mline and nardis and fiser and say I can build the best drug Discovery Model in the world because I have perfect information right so that's a question that remains to be seen so I think over time you'll see a bifurcation from an Enterprise use case and in our business in the Enterprise side you need Precision I can't afford to be wrong you know a wrong turn by a Tesla is going to kill somebody a wrong you know block bipo Alto is going to bring somebody's infrastructure down or or a wrong permission is going to let a bad actor in so I don't have that tolerance that consumer models can have because they have low consequence so I think High consequence High consequence applications require a lot better Model A lot more training and more precise domain data I think that's going to become a sort of thing of its own I everything we're seeing today is general purpose models and eventually they'll be like and I don't know the answer was that general purpose models become task specific evolved models or there's a new category of task speciic evolved models which are built more in the sort of genre of the alphao version now on the on the general models I think the people who can deploy them against exist existing consumer properties are sitting pretty right because it creates more retention more continued monetization of space so whether Google can deploy a whole bunch of AI against it three plus billion users across multiple Properties or Mark Zuckerberg can do it Vis Facebook and C billion users across Instagram you know WhatsApp and and Facebook that's that's cool I think Sam's done a great job in building a consumer Direct business on the subscription side which he continues to drive very well and that's becomes sort of sort of his his Moe now because no other model has built a subscription based consumer model so I think you're seeing the general purpose models being built by existing large consumer properties you're seeing a new consumer property emerge Visa you know open AI I think the Enterprise use case is still early because we haven't seen the mission critical applications be developed because of the lack of you know great training data so anyway that's what I think but let me ask you one of the one of the things that's not on your LinkedIn profile is prior to prior to sof thing prior to Google prior to T-Mobile if I had my fact straight you were an award-winning equity research analyst covering Telecom and I I believe that one of your claims to fame was calling the internet bubble and the bursting of the internet bubble I still have that note a cell note which I wrote in November 99 not bad so are we in an AI bubble lightning doesn't strike twice how would I know uh look again the for the number of times I've heard it's different this time you know we could all be very rich so but there are some things which are different right if you look at where the AI inflation has happened the equity markets it's still in the plumbing and the plumbing is real right it's not like people are driving the plumbing up without substance because you're selling four times or 10 times more chips than you sold two years ago so they real revenues that underpin that now clearly people are projecting that into a trajectory which I don't understand and you know every day you see a new development you tell me is Target the future or is deep seek the future and I don't mean with all its negative connotations I mean as a concept are we going to have cheaper models being built for large scale application with limited specialization or are we going to have a super model in the context of AI uh which is going to be expensive but be a to do everything amazingly you tell me the answer and I'll tell you mine son should we head into lightning round or do you have more questions great lightning round okay you just bought a cricket team why yes you know there's a bunch of us who are together not just me there's 10 of us including your partner Jim get um we're all failed cricketers we're all sports aspirants so they were part of that that's uh there's part of the passion which says wow I can be associated with the sport at the highest level without having the talent it's just kind of interesting that's one reason for it now you couldn't have a bunch of us buy it who are business savvy and say where is there is there a business model here or not and if you look at it the only thing that's left in streaming that is linear is Sport M no longer News television movies nothing is linear the only thing is linear is Sport you want to watch it when it's happening pretty much when it's done you know the score and you lose the interest to watch that that event right the post post event viewership is a lot lower in than live viewership every other is the other way around every other streaming content is the other way around the post launch viewership is higher than launch viewership whether it's movies or television or any podcast or any video streaming to do hopefully this one right so it's the only linear sport out there uh is being bit up C is the second most watch sport in the world uh IPL is is the biggest franchise hundreds is the next best thing it's in the country which is the home of cricket and then have you follow the same philosophy did I told you about my startups you could to buy something buy the best so we bought LS which is the home of cricket it's going to be fun love it what did Cricket teach you about life or leadership it's a team sport it doesn't matter how good you are if the other 10 people suck it doesn't matter it teaches you that right you can have a bad day and you can still win because you participated with the rest of 10 people so it teaches teach you about life it teaches you about business you're wearing a Pebble Beach pullover I hear you want a proam recently what's your handicap my handicap's nine n CGA and it's a combination of the best Pros that I could find luck and a few misplaced good shots Nvidia 118 bucks a share 2.9 trillion doll market cap 39 times earnings earnings are growing about 150% year over year long or short I don't understand it if you were Jensen would you be making the same moves look Jensen has played a very long game I think he's built a phenomenal franchise I think what he's done is like no less than what Elon has done for electric cars I think you know he took something that she built for gaming thought about it understood the large need for compute and put all his energy in t behind he's been the longest serving CEO in the world right so you can't take it away from him just can't even trivialize we have to talk about him with tons of respect what he's done is amazing he has a vision and he's taking it Beyond just the chips because he's slowly building an ecosystem saying my chips work with a lot of other things together so I don't think from a long-term perspective you can argue that AI is not going to be relevant I don't think you can argue from a long-term perspective that we will be constantly doing some form of development which requires more and more compute like in the history of mankind compute and bandwidth and memory have never shrunk yeah so it's not about to start now I think he's sitting a phenomenal asset you know is it a 3 trillion asset trillion asset today I don't know it'll be a three trillion dollar asset in 10 years possibly more what CEO do you admire most you know I have a collection of CEOs I admire traits that CEOs exhibit it's very hard to have one Idol like because one Idol and has the prop property that they could disappoint but if you admire certain things certain people do you know you learn a lot from that aspect of it and if you have the same circumstance you might do the same thing you different circumstance you might do a different thing I mean I admire elon's creativity and what he's done for the world I would want to work for him but I admire what he's done it's amazing like can I always joke with my team I'm saying would you go on a rocket to Mars built by the guys around you people I don't think so right but imagine you's got a bunch of people who build a rocket and people go up in that thing that's amazing but you sit in the car which has got no driver in it and so people have done it so at what's I did to micros of like he took somebody nobody believed he could turn this around a$3 trillion company and he did it's amazing right so like at Tim Cook you know Steve Jobs is a hard act to follow and Tim's g a phenomenal job in taking that amazing company and maintaining it down the middle and constantly innovating what can Mark Zuckerberg recently right he's taking that thing around and turn around now you the fact there are certain things that they done which I respect is amazing that doesn't mean anything about the rest of their lives and I don't need to worry about it which CEO is executing the best in AI right now for all the conversation around Sam I think what he's done is amazing right I mean before Chad gbd came about we weren't talking about AI right and before Chad gbd came out you know you think Google didn't know about AI I knew about a when I was at Google do you think Google didn't have a self-driving car then they did know D think sat didn't know what a means he did but look at what's happening right now you can't you can't you can't run into a want SP the words AI so Sam has created the next sort the impetus for the next technological Revolution that's the way Steve Jobs did it with the iPhone and the fact that was a straight face you can go out there and get people to commit to spending half a trillion dollars in building infrastructure and every I think the mag 7 every one of the CEOs is spending way more money in building computer and data centers because nobody wants to be left behind I just sounds on a your job executing AI now you know history is is hard and business is hard and we don't know that means that you'll be the winner in the future but damn has he done a great job in getting us to where we are yes I think that's it thanks ni cash [Music] [Music]

========================================

--- Video 29 ---
Video ID: sKdottg6iIA
URL: https://www.youtube.com/watch?v=sKdottg6iIA
Title: Vector Databases and the Data Structure of AI ft. MongoDB’s Sahir Azam
Published: 2025-02-13 10:00:57 UTC
Description:
MongoDB product leader Sahir Azam explains how vector databases have evolved from semantic search to become the essential memory and state layer for AI applications. He describes his view of how AI is transforming software development generally, and how combining vectors, graphs and traditional data structures enables high-quality retrieval needed for mission-critical enterprise AI use cases. Drawing from MongoDB's successful cloud transformation, Azam shares his vision for democratizing AI development by making sophisticated capabilities accessible to mainstream developers through integrated tools and abstractions.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

Transcript Language: English (auto-generated)
in a world of probabilistic software you know the measure of quality is about that kind of Last Mile how do you get to 99.99 X you know sort of quality and so will the domain of sort of quality engineering that we typically associate with manufacturing kind of apply to software and and that really got me thinking in terms of you're not going to be able to necessarily get a deterministic result like you would with a traditional application talking to a traditional database so therefore the quality of your embedding models how you construct your rag architectures mer it with the real time view of what's happening in the transactions in your business that's what's going to get you that high quality retrieval and result and unless it's high quality in a world where it's probabilistic I don't see it going after mission critical use cases in a conservative Enterprise and that that is a problem space we're very focused on right now [Music] today we're excited to welcome sahiram who leads product and growth at mongod DB sahir was one of the Architects behind Mongo's successful transformation from on Prem to the cloud and he's now helping to steer Mongo's evolution in AI first world Mongo's journey into Vector databases began with semantic search for e-commerce but it's evolved into something a lot more fundamental becoming the memory and the state layer for AI applications we're excited to get sahar's take on the past and the future of vector databases and what shape infrastructure itself will take in A Brave New World of AI agents and applications and unlimited software creation so here welcome to the show we're so excited to have you here thanks Sonia I'm super excited to be here uh we're going to dig into everything from Vector databases to embeddings to knowledge grafts and much much more on this episode I'd love to just start with the big picture question uh and maybe your hot take is AI going to change database Market uh that's an interesting question I think the the related and probably more interesting question is whether it's going to change software development in applications and I think that it really is you know I think we're seeing AI power generative AI powered applications address a set of use cases that traditional kind of deterministic software hasn't been able to go after and I know I've read some stuff from seoa around the idea of like you know Services as software Etc in that whole space and we firmly see that in terms of our early adoption of what we're seeing in the market and so that in turn changes a fundamental way we will interact with software it changes the way business logic of applications will evolve over time with things like agents and that all has underlying implications on how the database layer will need to transform as well can we can we poke on that for just a minute so I'm curious since you guys are operating at a layer will you see a lot of what's being developed um what what are people developing today that they could could not have developed a few years ago before these capabilities emerged yeah I think on one hand one Trend we're seeing is certainly that it's much more easy and efficient to create software than there ever was before so you know the fact that there will be more software in the world means that there will that will have implications in terms of data persistent storage and processing so that's kind of one you know sort of related piece but in terms of use cases I think the fact that we can now interact with computers in completely different ways beond on just the Classic web and mobile applications that we're all used to you know the more interactive experiences I think the blending of the physical and virtual worlds in ways that I don't think we can you know we've really seen yet obviously there's a big Trend around how AI impacts robotics you know there's a great blog I read from I think it was from Lang chain the other day around sort of ambient agents and sort of you know reacting to signals without necessarily intentional Human Action I think we're at the verly early stages of that top layer of sort of human computer interaction fundamentally changing and I think that can now tackle a whole bunch of use cases in terms of improving the productivity of our personal lives our professional lives and go after you know fundamental productivity that I don't think traditional software has gone after I think that's the biggest kind of meta change that I think this all has the potential to go do you have a favorite example either one that either a customer obviously you love all of your customers but do you have a favorite use case either that you've seen a uh one of your customers build or a favorite use case that you yourself use yeah I would say generally we're seeing more and like most things we see more sophisticated Advanced use cases tend to come up first in you know more risk tolerant sort of faster moving startups but for that reason I'll pick a couple Enterprise use cases that have kind of captured our imagination one is we worked with a large automaker in Europe and you know they have huge fleets of cars globally they have a bunch of first and third party you know mechanics and maintenance sites people whether they're dealers or other sites where people go to get TR you know help when their cars are having issues and you know the common problem if I hear something funny with my car like how do I go diagnosis means that you typically go in mechanic who has expertise has to go kind of Tinker around figure out what it is and then go through a manual to figure out what the remediation steps is or what parts they have to order to fix it y we work with them to actually identify an audio embedding model that can allow them to record with a phone the corpus of and semantically match up with a corpus of sounds that are typical problems that are known problems with you know their their their cars or any cars which shrinks down the actual diagnosis time you know by typically could take hours if it was a tricky diagnosis to something that could take now seconds it's almost like Shazam for car diagnostic and then on the other side of it instead of looking through PDFs or you know physical manuals on what the approved remediation steps are to fix it now it's sort of a natural language interface to say okay this is is the the issue that we match to what should I do next in terms of fixing the problem and you know that's all about unstructured data semantic meaning of the information both and the problem with that car and if you extrapolate the business case of that though across thousands of dealerships or hundreds of you know different models and iterations of cars like that's millions of dollars of potential savings for them and a better better customer experience and you know consumer sentiment around their brand and so that was kind of definitely one cool one another one uh you know in a more very heavily regulated industry worked with Novo Nordisk uh you know one of the largest pH Pharmaceuticals you know obviously getting a drug approved is a highly um scrutinized process and so there's this idea of a clinical study report that pharmaceutical companies have to fill out which typically takes a lot of manual effort to write and structure and review and you know kind of get approved they were basically able to use a large language model again train that against all their approved drugs all the process they do manually and now they can get that initial draft of a CSR as they call it within a few minutes and so it shrinks a lot of just the initial drafting Cycles the quality of that initial draft is higher than what they typically see if it was you know manually done and so again like you can draw a pretty quick line towards true dollar Roi savings on use cases that are you know not necessarily even bleeding edge in some aspects of what we're seeing in the in the sort of early stage ecosystem but are being applied in a context and scale in industries that obviously have big implications for you know for them and for their customers y so now that the shape of these applications is changing um and you know they're multimodal as as you said they're they're agentic they're ambient agentic what does that mean for the database layer and if you wouldn't mind just giving us the the 101 today of like the the role that databases play for software as we know it today deterministic software and what role do you see database is playing in this kind of new evolving market for AI applications is it good news or bad news uh we're excited so two reasons one you know the point I lightly brought up earlier which is if there's more software in the world which I think generative AI will just make it easier to create more types of software experiences I think that in general is a Tailwind for any data persistence infrastructure you know technology it doesn't necessarily mean that mongodb or any other particular vendor is automatically going to be the beneficiary there's a lot of execution that goes into making sure we're technologically and you know for our partnership and ecosystems well set up for that which is where I spend a lot of my time but in general like more software means more data and needs for persistence and of that information so that's a very macro sort of I think Tailwind that we're definitely definitely excited about I think the shift from relatively simplistic gen use cases often times where you're just interacting via chat with an llm doesn't necessarily need very Advanced kind of data persistence but as Enterprises need to ground the results of their AI applications to proprietary information or to control the result set so the retrieval is of high quality now there needs to be a lot of interaction with these foundational models and their underlying information about how they run their business and a lot of that is not necessarily publicly trainable information on the internet and so whether that's you know Advanced or simplistic rag workflows whether that's fine-tuning different approaches around post training there this I think there will be more need to interact with an Enterprise's data and foundational models over time especially as these models become lower latency and so they interact more with the real-time business data that's being generated in an organization and that's really what we're seeing in the most advanced companies right now is they're they're building really sophisticated ways to control the output of these llms based on the use case that they're trying to drive towards and merging it with the operational data that drives their application or their business and so I think we're still early days in that in terms of where I think that can go but I really do fundamentally believe the databases will get to get much better at high quality retrieval in particular of on structured data because you know when I look at all these embedding models and just what we can do with probabilistic software it takes the value out of 70% of the world's data unstructured data and makes it applicable to applications in a way that just really wasn't possible before and I think that's the real opportunity what's the Devil's Advocate answer to that so for example I'm thinking of Jensen at our first AI S I think you were at that a he said something like every pixel is going to be generated not rendered and I think of rendered as you know retrieved from database somewhere uh like what what is the devil's advocate point of view to the you know is it good or bad for databases as generative AI takes off yeah I think the Devil's Advocate view to me is less about whether there is a database somewhere behind the scenes more about where is that abstraction and is that something that's a choice of the application developer building that application is or is it abstracted behind some level API or is that a choice that an llm makes in terms of as it autog generates software or Auto renders that environment where does it choose to persist that data yeah but at the end of the day you know we like to joke internally an AI application is still an application you still need to persist transaction safely to make sure People's Bank balances are accurate you still need you know the ability to search information based on text keywords not only on the semantic meaning and so I view all these generative AI needs from the data layer as ADD not necessarily substitutive to the needs of a traditional application and you know one of the reasons people love today is the developer experience right if you fast forward the clock and you know maybe there's x00 million human human software developers but there's trillions of call it agentic developers what makes a good Agent developer experience like why would an agent choose to use as its database if that even makes does that make sense as the question yeah I think it does and it's something you know we think a lot about sort of how the nature of software development will change and I think one of the things as we move from more simplistic generative AI kind of powered applications to more advanced ones with more you know agent-- driven business logic state will be more necessary because now you're coordinating you know a more complicated workflow where you need to be able to track the results of a particular piece of a transaction and coordinate that and all of that requires storing that somewhere and you know manipulating and updating it over time so I think in general things are becoming more state full in generative AI applications over time which is a drag of of data and database you know consumption uh overall you know in terms of where things are going now I think in terms of the abstraction you know I think the question is if you know developer experience is the thing that you know makes any technology really accessible today for human developers does that same value proposition hold for AI and I think what we're seeing even if you look Beyond just the database space think of you know the adoption we're seeing of some of these uh call it AI platform as a service type companies you know look at the the adoption of things like versel vzero or you see things like repet or whatnot I think we're seeing that at least with early AI generated software there's a preference for great developer experience Allah higher levels of abstraction so um I think it's too early to be definitive on that but you know I think we're seeing some promising signs speaking of higher levels of abstraction um I forget who had this oneliner somebody had a good oneliner which was you know English is the ultimate layer of abstraction right and at the limit you will just be able to describe and plain English you know what product requirements you have and a foundation model will spit out the code required to you know build whatever application you want to build first off do you believe in that as a future State and then secondly is that great news for because there's just going to be so much more software and most of it's going to need a database sitting beneath it or is that bad news for because it neuters some of that development experience that is a good Advantage for you do you see that playing out and what does that mean for yeah I think I think for databases in general I feel pretty confident it's a absolutely a Tailwind I think mongodb specifically one of the advantages we have is that our data model is really well tuned to managing structured data semi-structured data and now with embeddings unstructured data so I think we have some fundamental architectural advantages We Believe even more of an uh you know prevalent and important in AI as you're representing all these forms of data regardless of whether the software above it that's interacting with it is human generated or machine generated so to speak now that being said we're certainly not resting on our Laurels that that's going to happen without us being really intentional about it so we are working with you know the whole ecosystem of AI Frameworks and model providers to make sure that we are well integrated whether it's inference players or you know Dev Frameworks Etc to make sure that just like JavaScript and Web 2.0 and Cloud were big Tailwinds and are big drivers of our business that the modern Stacks that are being used to generate these applications to be is well integrated as a default in so I think there's a lot of work happening there we're also focused on this idea of you know what is the equivalent of you know quality training or even SEO for llms meaning you know if you go scrape the internet to train a code assistant on any technology is is that necessarily what the best practices are probably not but there's no standard way for you know an a vendor or a technology expert behind a particular area to submit the canonical training data for Quality mongodb code for example and so we're working with some of the labs on methodologies around that we're doing things just even without involvement to test sort of you know what we can be doing to create you know data sets that allow for the quality of the outputs of these systems to be reliable you know last thing we want is some going and saying I want to use mongodb help me generate some code for some functionality and it's you know not high quality performs poorly and so there's various facets of this that I think are very intentional efforts to make sure that our technology fits well as things evolve over the next few years yeah so actually to that point I think there's been a lot of chatter and increasing chatter that you know we're we're hitting a wall uh in terms of just public data globally available mhm there's a lot of data still left in private Enterprise data you guys sit in the middle of of a lot of it um I'm curious how you think about uh your role in kind of that you know as as the market of all sources next leg of finding that next uh trillion tokens worth of training data um do you see yourselves you know being a training data provider for your customers do you see yourselves partnering with the labs um are your customers mostly you know looking to use their data in for uh for for rag or are they looking at also training models on on the data they have in your systems yeah definitely you know I think just to be clear any of the data that we manage on behalf of our customers is owned by our customers so you know we're certainly not taking that data and you know training any models that are outside of what that customer wants us to train or use for rag so I think that definitely is where more of our focus is and you know we see a variety of different things very simplistic kind of use cases where people are just using you know core operational data stored in mongodb or metadata as part of their kind of rag workflows we're seeing obviously a lot of vector adoptions are fastest screwing new product areas they try to merge you know metadata transactional data and semantic search sort of together into a single sort of system for more quality uh retrieval kind of use cases which is sort of I think where the Market's going and then we see instances where people want to use the data they have in mongodb and other systems to either fine-tune or straight up train smaller models that are specific to a particular use case and I don't believe that there'll be kind of one particular modality that suits every single you know use case I think there's going to be a plethora of different things that customers will begin to optimize for their latency requirements or performance requirements so I think you have the most fascinating seat to what's happening in the vector database Market we constantly pull our portfolio on what their AI stack is and consistently has been the number one vendor that everyone uses for Vector databases so I think you have the deepest and most interesting perspective on this um maybe like from from the 20,000 foot view it seems like people View are using llms as you know they have World Knowledge up to some pre-training cut off date but beyond that you need Rag and you need Vector databases in order to supplement knowledge uh to provide you know specific domain knowledge uh you know almost as in like information retrieval like knowledge Source but if I look at Vector databases they kind of came from the semantic search world and you know e-commerce and things like that and so that's that's that's a very different world so how how do you think about you know what are people using Vector databases for today is it a technology of the past that's being improperly shoehorned into this information retrieval use case or is it the ideal data structure um to kind of be the the knowledge uh infrastructure for llms like how do you think this all plays out yeah kind a quick question on that of course did I'm aware of Mongo's Vector database because of generative Ai and seeing people use it for generative AI sure did you guys have a vector database pre generative AI we started because of of a more classic classic now semantic seuse case so a few years ago one of the things we noticed were that you know many of our customers would use Monga Tob as an operational data or any operational data and side by side with it have an adverted index kind of search engine for full Tex kind of lexical search and our customers were basically like why do I have to copy data between these systems to run two different databases just to get the search results I want to empower my application with and so being focused on developer experience and simplicity where like this seems like an obvious problem for us to go after and so we started there with our um search product to really just simplify it so a developer interfaces with one database but really it has different modalities of indexing and storage that can serve you know allp type queries as well as full Tech search queries um some of our e-commerce Advanced e-commerce customers were the ones then saying okay that's great but I want to start to do semantic similarity search and blend full teex lexical search alongside it search because that's what's going to give me higher quality search results and that's where we started getting pulled into building the vector capabilities into our engine and for us it's you know one of the things we we're always trying to do is remove the need for customers to have multiple systems so when we say we add this capability it's a lot of it goes to how do we integrate it in an elegant way to our data model how do we extend our query language so it's very easy for a developer to just feel like it's not a separate system they're just interacting with it as part of their application development and so we were down that line and then obviously you know the world explodes post chat GPT and you know we were like all right you know this is going to be even more relevant than we thought and so we poured the gas on things accelerated things expanded the strategy to be well integrated into a whole bunch of new Frameworks you know working a lot more closely with the AI Labs because it is to Sonia your point it's it's certainly a different use case to leverage you know vector embeddings or even just metadata or transactional data integrate to rag than just a pure semantic search use case but as we look at our most advanced customers now in 2025 they're actually seeing that the integration of all those modalities is really important because you need to filter based on metadata you know about your your unstructured data your what whatever it is you're you're building an application around there are times when you need to sort by keywords and relevance ranking like a more traditional search engine and then you need to understand and extract semantic meaning from vector ined and there's a whole bunch of things around how to improve the quality of that and only then can their overall application get the percentage quality predictability for especially for large Enterprise to trust putting something in front of their customers especially in a regulated industry and so that's turned out to be a real advantage to have all of those in a single system because otherwise it requires a whole bunch of what you know I call kind of rag gymnastics to try to tie all these things together which is possible but it puts a whole huge burden around the development cycle what happens in app code and frankly you need to be a pretty sophisticated team to figure that out on your own and so we're trying to democratize that all by making it just much simpler for the average application developer H how do you think about Vector versus graph are they substitutes are they complement what are the trade-offs because we see Vector uh vector- based rag we also see graph rag yeah yeah I I every week goes by and there's some new sort of approach to higher quality retrieval is kind of what I think everyone's sort of trying to chase I think they're complimentary you know like there are reasons why you want uh graph relationships because that's an augmentation of understanding that you may not be able to just infer by the vector embeddings themselves so we view that as additive just like pre-filtering based on some sort of metadata you know about your your unstructured data and embeddings is additive and improves the quality of results and so I do view these modalities as very complimentary you know our goal is to just make it simple to combine all of those for a developer so they don't need to have their graph representations of their objects in a one style of database their metadata and another database their transactional data and a relational database and then have to have a separate Vector search database and try to you know rationalize all that which is kind of what happens we're trying to just make that dead simple is it fair to simplistically think about you know in an agentic system the llm as the brain and the database whether it's a vector database or uh or suet of those as the uh the memory is that is it brain and memory is that the right abstraction the right mental model I think that's definitely one way to think about it because absolutely you need to persist memory and state especially when you have agents that are having more complex workflows and need to drive interaction across multiple endpoints not necessarily a single foundational llm with a One-Shot call so you need to persist more of that state I view them as sort of two pieces of an emerging architecture you know you've got obviously compute storage networking is sort of the under Primitives but now there's this whole set of use cases that foundational llms can go after that are more probabilistic in nature that can automate tasks that knowledge workers would typically have to do manually and which is super powerful but then that needs to store its state and be grounded and interact with the transaction that the application is driving and the other information that's either semi-structured or structured and those things together come to create a great application experience and end user experience it's not in either or I think it's it's complimentary in a really powerful way which will only become more important as llms become lower latency and faster where now you can really use what's happening in a real world setting to augment the results of an llm in much closer to real time than today where it's you know it's just a very different interaction speed so you're saying the database is not only the memory for the llm but it's a reflection of world State yeah like you llm needs to interact with World State exactly well I think that rough framing is consistent with what we've talked about internally which you know if you think about the bottom as raw infrastructure compute Network and storage you think about the top as the application you've got all this stuff in the middle and for anything that is deterministic you're going to be better off with Vector database graph database relational database nosql database kind of the traditional database world for anything that's more probabilistic you want something that looks like an llm the functionality that gives you is a little bit of human computer interaction and a little bit of reasoning right which is complimentary to what you get from this part of the world but I want to take it one step further because it sounds like we pretty similar view on this default architecture of the future or kind of this emerging pattern if you take it one step further does that imply that the mental model investors should have for the API portion of anthropic or open AI or the other Foundation model companies is meaning they're occupying a similar lay in the stack they both reside on top of the public clouds they both reside beneath the application layer is is a good frame of reference for what the API businesses of open Ai and anthropic would or should or could become over time yeah I think it's an interesting proxy because it you know you sometimes read like okay the llm is the new operating system I that never felt logical to me in terms of how application capability and functionality should look maybe I'm wrong things you know are changing so fast these days but um but what we see is really these are side by-side complimentary components that drive and serve the business logic and interaction layer of the application above yeah and there's a whole bunch of use cases obviously that large language models can now reason about and provide human Interac around that weren't possible before that's the amazing powerful aspect of them but it doesn't in you know in any architecture we've seen supplant the need to have deterministic outputs from structure data to manage transactions and search and all the other data components it's it's really complimentary and I think it's still early days I think you know SEO has done a great job sort of writing about as well like we don't know what the real Next Generation business models and applications are yet today I think we're still seeing the the early years of it and that's what's fun to be able to see all these different early stage companies or these Enterprise use cases that I highlighted earlier even then you know I think there's a lot more to come yeah all hypotheses at the moment yes I mean speaking of hypotheses there's all these hypotheses about you know what model architectures are going to Leap Frog and you know what the next model architectures are going to be I'm curious your hypotheses on the database side so we went you know we went from nothing to Vector databases pretty quickly it seems like do you think we're going to Leap Frog to a new type of data structure for for AI for these AI systems or do you think this is kind of the the the ideal architecture yeah I think the the fundamental data architecture at least as far as you know vectors are concerned seem to be strong Primitives that seem to hold and where I think we're still trying to figure out how we extract all the possibility there now if something else comes along you know certainly open-minded to it but I think it is a primitive in my mind you know I think there was a question in the Market at some point of like all right is it only is a vector database a whole new segment in the market or new is that going to replace core databases we view it as a primitive like you know if you want to manage unstructured data the combination of the ability to index and you know Vector embeddings combined with high quality embedding models that can represent the meaning of that unstructured data is sort of a new primitive just like you know text indexes or B indexes and databases Etc so we view it as a foundational element I I don't see that going away I think the how you create high quality results from that data and how you have high quality vector embeddings or how you augment that with other information there's a whole lot of of evolution happening there right now and I don't think that's by any means settled I see so the data structure the data storage that's you know vectors and and the way you store them seems pretty sound and the thing that's you know yet to be optimized is how do you go from all these vectors to like ultimately meaning and um yeah and I'm not saying there aren aren't going to be optimizations or room for Innovation and how that can be more efficient more performant more cost- effective there's plenty uh you know always in the database space happening there so I'm not trying to make a statement that that there's certainly Innovation going on there but I think the more interesting thing is when you're in a world of probabilistic software and um I Heard a really interesting take on this from uh Ben Thompson uh for who writes try where he kind of said in a world of probabilistic software you know the measure of quality is about that kind of Last Mile how do you get to 99.99 X you know sort of quality and so will the domain of sort of quality engineering that we typically associate with manufacturing kind of apply to software and and that really got me thinking in terms of you're not going to be able to necessarily get a DET terministic result like you would with a traditional application talking to a traditional database so therefore the quality of your embedding models how you construct your rag architectures merge it with the realtime view of what's happening in the transactions in your business that's what's going to get you that high quality retrieval and result and unless it's high quality in a world where it's probabilistic I don't see it going after mission critical use cases in a conservative Enterprise and that that is a problem space we're very focused on right now how do you think all the innovation in the reasoning model side interplays with what's happening uh in your in your corner of the world yeah I think um in terms of you know whenever there's reasoning memory comes into place long running logic you know I think then when how reasoning plays into more advanced agentic workflows all of that needs State as I kind of mentioned earlier so at a very loose level I think database are going to be more important to that than just a one shot simple C you know answer engine from an answer from an llm so I think that's the kind of meta Trend as an end user I'm fascinated by these types of reasoning models I mean I am definitely a uh I know this is very not exactly novel in the last couple weeks but Google's uh Gemini deep research and the product experience around that I think is amazing so like I think like there's a lot that can be done there in terms of the user experiences and the types of use cases that applications can build off of that at least the first wave of of LMS that we saw haven't been able to really Drive in terms of adoption very different direction so one of the things about your background that people uh who are listening might not be aware of is is that you sort of like architected and led the transformation of mongod DB from being a traditional on- Prim enterprise software business to being a cloud native consumption based business um with which is now most of mongodb and um I think any transformation of that magnitude is really hard to pull off and you guys did it at reasonable scale and of course now the company has you know billions of Revenue scale the reason I'm the reason I'm harping on this a little bit is I think there are probably a lot of Enterprises or even a lot of start startups who are currently faced with a similar challenge where they need to undergo a transformation of their business and yours was an on-prem to CL transformation which not a lot of companies got right the one we're looking at now is sort of a non AI to AI transformation and so so the question is what made that work for maybe just say a little bit about sure the nature of the transformation what made that work for you guys and do you have any advice for people who are looking at an AI transformation of some sort now yeah I I appreciate you bringing that up and and certainly you know we're very lucky and fortunate that you know we were able to make this pretty Monumental shift in terms of the the business model the product strategy of the company and and certainly by all means it required a lot of different people doing a lot of different things uh to make that happen but I think one important piece I want to key off is you you're using the word kind of business transformation yeah that is really important because I think for a lot of companies that have tried to drive this type of a transition they just view it as okay this is a new skew a new product that's all I have to worry about but I think you know certainly I took it as a sort of business transformation is the goal here and therefore we made sure that every functional leader in the organization one understood that they had a really important part of that transformation and we also accountable for working you know to think about in a consumption based Cloud first model how customer success changes how our financial model changes how you can name any single function how did you guys get Buy in in the early days when the thing that generates all the revenue was not this right like how did you get people to care yeah absolutely so one definitely having strong top down support you know like it was very clear to the company that launching Atlas making this transition was a super critical you know business priority you know there's nothing that that gets around the fact that you need that level of top- down consistency you know that included empowering me as sort of the person to help drive that and so when I went knocking on one of my peers you know doors in a particular function I said hey I really think we need to you know fund some headcount here to think about the cloud side of the business that you know I had the sort of ability to kind of drive that level of influence but I think what's important about that is we didn't treat it as this sort of separate mini Buu that's isolated from The Core Business we wanted every functional leader to feel like they were part of that transition and it wasn't some competing thing for uh you know and they were going to lose some sort of you know part of the the function they rant so I think that was a really important thing certainly it meant a lot more you know shuttle diplomacy for me versus direct Authority but that was critical to bring the whole company along for that transition as opposed to it just being a starved new business initiative in a corner which you see sometimes yep uh start to happen certainly you know in terms of the sales organization the revenue functions in particular it took a lot of one just really rolling up sleeves and being a seller meaning being in the early deals learning what's objections are coming up whether that's a product objection we had to go build on the road map or whether it was just an enablement issue or a positioning or messaging exercise or pricing thing so really really taking a mindset of like all right I'm you know our team the product team launching this is going to be side by side with the sellers and the SAS in every single one of the first deals and I'm going to remember in our smaller New York office at the time I used to make the rounds every you know every evening and be like all right what's happening with this deal what help do you need like where are we on this what are you hearing and that got a lot of sort of one all right the sales team isn't just being asked by some stranger to do something CU it's important like I was trying to show that I'm with in them in it with them and then certainly you have to drive incentives around it when something's working and people know how to drive Revenue a certain way in any function there's going to be so much inertia around that already because you know the software business is still a growth business for us so we had to be very intentional about putting spiffs heavy emphasis on enablement inspection and accountability to make sure enough momentum got built in the new business until we could kind of neutralize it because ultimately we're we're about customer choice we don't want to artificially push a customer that's on Prem to the cloud if they're not ready that's largely out of our control but in the beginning we needed the sales team to get a lot of attention on something that they felt was not necessarily the needle mover until we got a certain level of momentum yeah yeah interesting the lessons I heard for anybody going through an AI transformation is a lot of top- down support which I imagine requires a lot of conviction that this is where the future is going um fully integrated not some project sitting off in a corner getting stared for resources but actually part of the Core Business and holistic transformation it's not a skew it's a wholesale reinvention of the business in a lot of ways right and some of the most important things were not technology decisions it was you know business model transition it's sales enablement to sell to a different segment of the buyer in the organization different buyer within the organization that we were traditionally so almost every function had to change in pretty fundamental ways and I think sometimes outsized amount of our time went to those things that you wouldn't think were were needed to change that much or that would be easier versus you know what you assume to be the hard part which is how do you deliver a highly reliable Cloud database that's by no means easy yeah but you know that's the part I think everyone gravitates to but it's all these other things around the different functions that drive the business and making sure all those line up in a in a coherent way that a lot of attention went to I also think one of the analogies to draw on tell me this is just you know I'm off in L La Land but you were you and in our conversations you were really focused on driving the developer experience uh through that period of transition and you know the developer was going to choose uh the database um for this kind of new new mode of operating um it feels like to me for companies going through the AI transition right now right now it still is developer developer developer to your point developers are choosing AI tools eventually if we have trillions of Agents running around it might be the agent experience that's the thing to to Really prioritize yeah know especially if agents are the ones who are going to be driving a lot of the business logic without necessarily custom development Happening by the organization I could see that I think you know often times from the outside I get the question of like how did go from Enterprise to plg and I always sort of like winse at that you know I think to me those things are absolutely complimentary and more have to do with where a customer is in their adoption Journey or what style of organization they are whether they're a you know technical founder-led fast-moving startup that doesn't want to necessarily engage with sales in the beginning of their Journey or whether it's a large Enterprise that's never going to show up via a self-service type Channel and so you know we spent a lot of time thinking about the whole system holistically and trying to map that to how the users and the buyers actually want to engage with us as a company and so I think a lot of that is what has been behind the cloud transition sort of success is not trying to be too philosophical of saying you know credit card C business C you know customers are the right ones and Enterprise sales no way I mean there it's neither or you know both of them have to be cohesively integrated to reach the global scale of customers that we have at at this stage should we wrap with some uh AI rapid fire questions all right sounds good good okay first one favorite new AI app ah all right I I mentioned that I'm definitely a Gemini deep research fan so that I got that I mentioned I think um that and also perplexity for me they're not new by any definition you know in my mind run counter to the you know okay thin thin AI rappers aren't really sustainable because I see a lot of product craft and I know Gemini obviously has a deep model you know uh training behind it but just the product craft is what I think is really interesting like the way you know perplexity makes the user experience the design sense for example is really great as an end user so I don't think it's so simple that you know AI models are suddenly going to make software go away I think there's a lot around adoption and understanding your user having great design sense and there'll be a version of that as we go to other interactive modalities as well even if it isn't visual so I think that's kind of one thing in terms of what's new to me I don't know how new this product is but somebody last week turned me on to uh snipp I'm a uh s snpd I'm a big uh podcast listener okay and it's a great example of an application that I think is woven AI really well through the user experience so um it like subcribed all your podcasts it like Auto summarizes it allows it surfaces up some of the key insights in readable form or in a shortened version allows you to take kind of notes we need this we've been looking for this okay I just found out about it last week and um I am loving learning how to use it well love it who do you admire most in the world of AI that's a tough one I mean certainly I think some of the just researchers that see the future and are probably have a sense of where things are really going every time I listen to them on this podcast or read you know some of their writing you know I feel like really excited about the future and you know the typical names there so I think that cohort of of people is always inspirational to me um you know I think it's fun to listen to the large company CEOs kind of uh mudsling a little bit about whether their applications are just systems of record or who's going to win the agent race and all of that so I think you know there's it's interesting to see the Battle of Titans happening in terms of who are going to really be the incumbents that can survive and thrive versus the ones you know that uh that may not make the transition so without naming names I'd say those are the two most interesting cohorts of of leaders that I tend to to listen to fair enough okay agree or disagree every developer will become an AI uh agree you know I think that traditional machine learning is you know was typically specialized in a centralized ml or data science team and applied to probably a subset of the use cases that could it could potentially add value to what we're seeing though with generative AI being integrated into applications whether it's Greenfield or to an existing application is it's the average full stack or application developers that are the ones that are responsible for that so you know really democratizing that capability across the organization is something we're trying to do and so if I had to give a simple answer I would agree with it wonderful soah here thank you so much for joining us today I think you have super fun you have really profound uh thesis on how AI is going to change not just databases but software and technology and the way we interact with technology as a whole and how that ripples uh over to the Daily based market so thank you for taking the time to share your thoughts absolutely thank you and Havey happy to be here and you know we'll see if any of these thoughts actually hold water things are moving so fast awesome thank you [Music] [Music]

========================================

--- Video 30 ---
Video ID: xMrBgC-bKUU
URL: https://www.youtube.com/watch?v=xMrBgC-bKUU
Title: Using AI to Empower Creators fr Roblox Studio Head Stef Corazza
Published: 2025-02-04 10:00:57 UTC
Description:
Stef Corazza leads generative AI development at Roblox after previously building Adobe’s 3D and AR platforms. His technical expertise, combined with Roblox’s unique relationship with its users, has led to the infusion of AI into its creation tools. Roblox has assembled the world’s largest multimodal dataset. Stef previews the Roblox Assistant and the company’s new 3D foundation model, while emphasizing the importance of maintaining positive experiences and civility on the platform. 

Hosted by: Konstantine Buhler and Sonya Huang, Sequoia Capital

Transcript Language: English (auto-generated)
we have this unique um Synergy and collaboration with the community where basically we we told the community hey uh give us access to your data to train AI we're going to make the best AI companion the best AI assistant that we can and that uh assistant goes back into studio and is free right so we're not making money off your data we're actually helping you create more and so we found uh the overwhelming majority uh of the creators in our community uh gave us permission to use their data for training and that's why I was mentioning earlier we have uh not only one of the largest data set in the world but also the most [Music] multimodal welcome to training data today we we have an amazing guest in Steph Kaza he leads generative AI at ROBLOX one of the largest gaming platforms on the planet Roblox has 79 million daily active users and they have a crater economy that pays out hundreds of millions to creators because of this Roblox is uniquely positioned to transform how games are made and played with AI Steph is a Founder at heart he started mixo a pioneering AI company for character animation and was acquired by Adobe Roblox brought them in to revolutionize how games are made with AI at their own platform under step's leadership Roblox is pushing the boundaries of AI and gaming from their groundbreaking AI assistant which lets you generate games with simple natural language all the way through their 3D Foundation model Technologies welcome staff to training data today we get to talk about games in particular we're talking about uh AI at roox Roblox is one of the largest gaming universes on the planet and I say universe instead of platform platforms a pretty overused term and really Roblox is created much more than a platform it's a place where you can create it's a place where you can play where you can meet new friends and it's all done online virtually um now we are technologists but we're also investors and I want to spend a moment on how remarkable of a business rollbox is it's obviously an amazing technology and we'll get to there but the business is uh exceptional you've got a $29 billion market cap company with over 35 billion of run rate Revenue uh an amazing stat here is they're really building an economy it's not just a selfish company it's a company that actually has produced over 800 million uh for their creators for the people actually building on Roblox uh over the course of a year 70 billion hours of gameplay on Roblox per year 70 billion uh and they're able to deliver cash operating profit so $600 million operating profit that's because they've T catered to a huge audience 75 79 million daily active users um over time they've actually shifted up in age and you've got 46 million of those daily active users are actually now over the age of 13 uh 3.8 million daily active voice users and the numbers just continue to grow on this amazing business and so Steph we're so excited to have you here today uh we could not have asked for a better person in the category of AI gaming from machine learning to computer vision to biomedical engineering you have a pretty impressive technical background an amazing journey from what was initially uh biomedical to a generative AI efforts and we were hoping that you could kick it off by just telling us how you got here how did you get to becoming the head of generative AI when you started off as an engineer in a very different field many years ago uh and thank you for the great introduction about roblo it's really like an amazing example of a compounding effect and we are every day we are mesmerized ourselves to about the success my journey uh started uh at Stanford uh about 20 years ago when I uh Came From Italy as part of my Exchange program and I was focusing on uh uh computer vision machine learning for the measurement of human motion and and so you you have basically two markets there one is the biomedical that you mention and the other one is animation and so we were basically at the at the boundary between the two and at some point I realized that there was a much bigger opportunity in the animation space and so I basically work on like uh video base motion capture and animation creation uh solution that then led to uh the spin-off of mixim the company that I started in 2008 get later go acquired by by Adobe in 2015 and it's still today one of the most used uh uh machine learning services in the industry to rig and animate characters uh and so um after uh few great years at Adobe actually seven of them I help build the the 3D offering there uh including products that my team built like uh Adobe Stager and and Adobe arrow for AR and then we acquire legor rithmic and so we build the full uh 3D portfolio and then after that I was really passionate about gen I was uh working with the CTO uh of adobe trying to figure out completely new ways to to generate things and that's when basically Roblox reached out and I had you know breakfast a few times with Dave and we talk about it and I really realized that potential the Gen I had was really finding in the gaming space and specifically in in Roblox being a platform where so much data exists you know 15 million experiences every day every year are played I found basically Roblox be the place where this could really Blossom with a massive impact uh worldwide so before we get into the AI components can you tell us a little bit more about the platform you're you're an engineer by background and so scale is incredibly exciting to you I would imagine the sheer scale at ROBLOX is pretty mind-boggling I think to anyone especially users they're obviously platforms that have more daily actives the Facebooks of the worlds there are platforms that have uh more monthly actives Etc but it's very rare to have the amount of bandwidth and compute and graphics and everything in one place maybe you can share a little bit about what that means to have 15 million daily sessions and uh 79 million monthly actives and uh a little bit about the stats and what that means technically yeah we always like to talk about the the daily active which is the 79 million that you mentioned but the the monthly active that I think we don't communicate uh to uh the the outside world is even more staggering being in the several hundreds of millions right so it's it's a massive community that is growing very healthy and pretty fast and sometimes people uh ask us you know where are those games coming from that people play right some of some of those games are are now like worldwide franchises with like tens of millions of concurrent uh players and then sometimes I give this number which is also like a reminder of the scale so every day we have roughly 90,000 experiences in Games published on roox Wow and so that gives you the scale of uh the the human creativity if you want and how much really this is becoming more and more of a creation uh platform and and gain development platform that has like incredible uh numbers in terms of scale of creation H and also like it's an economy on its own as you mentioned you know we we paid out you know $800 million to our creators and so people have jobs people buying houses people have like companies they are now you know we have game studios that have like more than 100 people some of them are VC funded so it's basically creating its own like uh creation economy uh and tiada and so that I think is very humbly and at the same time has incredible potential because the uniqueness I think of of Roblox is that we are one of the most vertically integrated companies on on the Planet you know we we own our own data center we have many data centers around the world uh where we own like the hardware but then we also own the app that distributes all these games and the players and all the services like uh video chat and live chat and chat translation so on but then we also have the creation tool which is a Rob Studio that I have the privilege of of leading with my team uh and then also we basically have all the services for Creations that they go with it so it's all the way from like the bear metal CPU GPU cluster all the way to the creation tools uh with the with the big difference that um we only charge uh our we only basically take take uh some some Revenue ourselves when our creators make money right we make money when they make money there's no upfront fee to to get into the game there's no upfront fee for the tool the tool is free uh a lot of services are free and so there's really very little friction to start using Roblox and if your game has one user or a million users you don't have to worry about anything we scale it for you we pay for all those like uh CPU GPU instances storage in the cloud and everything and and it's completely opaque to us the Creator so I think that's the uniqueness and and as part of the fully vertical integration we are also able to um subsidize AI right so we are one of the few uh companies out there uh with a full a full-fledged uh U AI assistant offering for game development for code creation material texture asset everything and it's all free uh to the creators I'd love to get into that maybe can you just walk us through today what is the experience of creating a game like who are the typical creators on your platform is it a high school student is it a professional game developer and what types of games are they creating uh that's a real question uh so we have uh several million creators uh on a monthly basis on the platform so uh big numbers there uh as well and usually uh I think the average age is in like the mid 20s uh it's a little bit older than our uh player demographics of course um and typically the majority of them are uh doing like World building they're like uh building stuff they're artists they're making games and then we have about uh 30 40% they actually are coding right and then of course there's an overlap between the two audiences but basically roughly uh this is what uh we're seeing and it it's used for the most uh different uh Creations you can imagine from like uh natural disaster simulations to to learning uh to the cloud more classic like a gaming experience events concerts uh fashion design uh we are seeing like new type of experience popping up on a on a daily basis which is which is very fascinating what's your favorite game uh I've been playing lately uh racing Empire quite a bit it's a I like car games so that's a that's a good one that I really like Steph I was actually surprised to hear that you said 30 to 40% of the developers are actually coding um as opposed to World building why is that and maybe that kind of parlays into what you've created with assistant uh yeah so uh there's just a it's a skill that is harder to master and to get into right A lot of people just like go uh from players and they want to create something and so they start like building the world and that is probably more intuitive uh than than writing code where you have to understand this like high level uh constructs and apply them uh to get interactivity and so that is one of the things we wanted to tackle with assistant we wanted to basically remove that friction of having to learn uh to code and to learn like programming language in order to create interactivity and so that was like one of the initial Inspirations and that's why you know code assist was the first uh uh feature that we released uh and then now uh this was like in March uh 2023 which feels like a two decades ago and now you know a year and a half later we basically have a full um uh a full basically assistant that has uh game development capabilities they go from writing code autoc completing code explaining code debugging code uh applying scripts to uh parts and and objects in your scene uh so that's all the coding stuff then we have uh documentation right people ask how do I do XYZ and usually they have to browse through Dev forums and and uh and documents on the internet instead Now assistant can basically summarize those um information for them and then the third aspect is uh creation of assets so we wrote out a material generator we wrote out a texture generator which was a lot more complex where you can basically texture any 3D object uh with very uh quite good Fidelity resolution just from a text prompt and so all this together is what we call assistant as with an umbrella name and basically it's allowing now um to create entire like simple uh games from scratch just by typing natural language in the future there will be more like multimodal input through images but basically right now we have like simple games that people are making also as a test where like they're only using assistant uh and so you can imagine you can make that that game on your phone uh you know with a microphone you just speak to it and then a system will just generate the word will create the forest will'll create uh the enemy the boss that you have to fight and then also add all the game mechanics uh uh to it everything uh automated are the games created with assistance like how good are they if you had to give a score out of 10 the games created from coding versus what what people are doing right now with AI versus where it's going I mean when you if you can code of course you get to another level of sophistication in terms of the the complication of the game play and and all the nuances that make like a game fun to play so of course we're not there uh the examples that that I see are mostly like we do game gems and you know we spend like two hours to make a game with assistant and uh what can come out is pretty incredible um but we haven't had you know uh in in the community I'm sure they're going to take it to the next level the goal is not to exclusiv use assistant the goal is to basically combine assistant and learn skills through assistant so maybe the beginning the first script a system will write it for you and will attach it to a part then you know where the script should go and then you know you know how to I don't know make uh uh some Obby platform move up and down you learn on the way so we we see assistant as a companion that it shows you by doing what how you make a game and then over time people really develop skills that otherwise it's hard to uh to acquire Stefano have you seen that the type of development has changed as in the assistant not only has language like the documentation you described it does also have code it even has images I saw that it's we'll get into the technical specifications in a little bit but as a user you have language you have code you have images have you seen the behavior change for how people develop in Roblox as in tactically have you seen the number of people actually coding going up I would have guessed that outside looking in because the barriers to entry of coding have gone down but does that just mean that more people are developing in general and the ratio actually stay the same uh that's a great question so what we have seen we have measured uh the productivity of people that use assistant versus not and so we have found that people that use assistant uh create 180% more code wow and so the individuals are a lot more productive that's benchmarks to people who already wrote code or does that also say just more oh wow okay two two of of coders one using assistant and and code assist code assist is the uh you know suggest uh code and an assistant I creat from scratch but they all kind of integrate into a similar uh uh user workflow and then if you're looking uh the same uh cohort comparison for material generator uh creators that use material generator create 60% more materials so also on the art front there's more productivity and then if you look at the final goal which is how much they publish right because publishing the game is the ultimate goal the lift is about 30% so uh people that use assistant publish 30% more than people that are now using it and and remember this is now still in mostly um it's going to get out of beta soon I can I can give you the the EXA uh day but you know it's going to be relatively soon and so we're going to see even even a broader uh adoption of course there and impact one more question on this on Sonia's quality question like what about usage so they're publishing 30% more do you have any kpis that actually give you a sense of if the assistant games the hours spent the robu spent on them whatever the kpis might be if they are also that 30% lift yeah that's a great question so our number one uh kpi right now is around retention and so we are seeing like a week of a week retention of people using assistant there much higher than any other features that we roll out and also we are seeing the overtime that uh retention in the long run increases quite a bit so we have people are using assistant with Studio we are seeing significant lift in the overall retention of studio and then of course we see a high retention assistance so the number of daily users has been growing organically and very steady we don't do any marketing of course uh on this and and it's free which is I think the best marketing uh you know like a it's not it's not cheap let me tell you that right I think Roblox is very generous on that but we have this unique um Synergy and collaboration with the community where basically we we told the community hey uh give us access to your data to train AI we're going to make the best AI companion the best assistant that we can and that uh assistant goes back into studio and is free right so we're not making money off your data we actually helping you create more and so we found uh the overwhelming majority uh of the creators in our community uh gave us permission to use their data for training and that's why I was mentioning earlier we have uh not only one of the largest data set in the world but also the most multimodal uh if you want because we have we have code we have images we have 3D assets we have audio video all that is part of of a gaming experience and also uh with the interactivity the is the glue for all of that and with the analytics on the usage right so it's a very uh very powerful data set the we are very you know treating with a lot of respect and and a lot of uh uh you know uh the best practices to to keep that data of course uh uh very uh secure but at the same time allows us to really harvest the value and ultimately what we are doing is we are teaching AI game development which basically that's what we're doing right we're not teaching how to make an image we're not teaching how to write code we are teaching game development that's the ultimate goal and so it's gonna all these tools they at the beginning will feel a little bit uh hey this is a tool to make a material this is to make the text this is to write code we are already seeing that they are converging we already started that process of converging uh some of those like um lower level Tools in into larger ones where basically the uh AI is actually learning how to develop a game as opposed to just how to do like a small task I love the vision of the AI learning how to develop a game and the question I have is if you break up game development into its component parts which parts do you think are most likely to be taken over by a I well in the in the near term versus which what do you think humans will be uniquely good at for a while uh that's a great question so we don't see AI as taking over by the way I know I think we we made with the best parallel that we made uh in in at RDC uh like a couple of weeks ago was AI is your dishwasher right like nobody wants to wash the dishes right and so we are really focusing on task especially with the last release of of assistant actions we are focusing on the task that you don't want to do right washing the dishes doing the laundry so we have introduce among the assistant capabilities a new capability uh where assistant can basically make large scale modifications to the data model Give an example um I have made a beautiful open world game with a huge Forest this Forest has 100,000 trees now all of a sudden I want these trees to actually follow the SE and you know get more leaves more yellow because fall is coming the amount of work that will take me uh to implement that will be huge assistant can do that with three lines of text I can say select all the trees or I can say select all the uh pine trees actually those don't become yellow so that'll be the WR one sorry uh select all the trees other than the pine trees and make the leaves yellow right I can just give these three liners assistant can go can select you know the 57,000 trees there are no pine trees it can change the color of the leavs uh for me in just a few seconds so that's the kind of task that we are seeing a lot of value and honestly this was the feature that the community loved the most of all the ey features we released in studio in the last two years this was by a landslide the one that got the the highest score and appreciation for the community because again it was the dishwasher we are not replacing your talent which we believe is that you're replac but we actually help you the Tas that you don't want to do I am always so impressed by roblox's emphasis genuine emphasis on community and teaching you've said this in the past few minutes you've talked about Community quite a bit about teaching how to code and how to create games and really uplifting the entire Community frankly and I just want to say that this is very much true all the way to the core of the business I got to follow my really good friend Craig Sherman to board meetings in 2017 and 2018 at ROBLOX and even behind closed doors you know years ago this was always the focus it wasn't the the banality that a lot of board meetings on monetization and financials it was Community it was uplifting Community teaching them how to code teaching everyone how to use Roblox in a way that actually benefits themselves in their own learning and this dishwasher analogy sounds very consistent with the ethos of your business um how impressive how did you implement it it sounds like a segment anything type of uh algorithm was it a segmentation approach or this particular feature how did you do it uh that's a great question so we basically have found a way to um give assistant so assistant is really good uh generating code right as you know L is based on llms and uh some of them actually we are supporting as open source project like star coder and we train with our own data so over time uh assistant is getting better and better generating code some of the code instead of running uh runtime you can actually run it uh um edit time in studio studio has a command bar where you can just execute some of that code and so uh assistant actions basically is creating code that is executed directly in studio at Ed the time uh and because we integrated in studio has full awareness of the data model of your scene and so it knows oh this is a tree this is a car it knows what you did and has fully aware full awareness of that and so we have combined the ability to generate code with the ability to create uh using the same code uh Lua commands in studio and the awareness of the data model those three things coming together have basically unleashed the power of the llm onto like a data model manipulation and is that do you have these things labeled or is it dynamically determining that this is a car uh there's a good amount of inference that just happens uh cool yeah how do you imagine the creation experience evolves let's say 10 years from now uh how do you see assistant deing so when we when we met about roughly two years ago and we said okay how how is the I going to impact creation and where should we start uh the one paradigm shift that we envision that we thought was going to happen in the industry not just the Roblox was a shift between uh control on on the fine control on the creation uh to uh capturing intent right and so I I spent quite a bit of time in the Photoshop uh uh land where basically there you give control to the color of the individual pixel of an image even though most people don't need it but some might right and so there it's all about 100% control non destructive workflows but 100% control on the actual artifact that you're generating and and then we are now moving to towards a complete different generation of tools where uh the tool is successful and can produce good outcome only uh as long as it can capture the intent from the user and so all the all the digital tools that we have seen in the last 30 years it's all about surfacing more control to a user and then the user will will figure out how to how to use that control right instead we are migrating from control to capturing intent and so we're going to see uh probably quite a big uh change and there's going to be you know a thousand startups trying to do in different ways and there will be new uh ux Paradigm popping up but we can see audio as an input we can see high level uh gesture uh not just you know with your hands but also with mouse and keyboards so things that are just providing an input into what you want to do we are seeing multimodal input I can describe a word better if I can type and I can provide maybe a concept art and maybe some high level sketching on top of it right so this is very different from having the phenomenally granular control uh but at the same time the velocity is like two orders of magnitude uh faster the challenge in all this is that for you know for casual creator or people that are you know have limited amount of time to spend on it uh it probably what AI is providing it's already good enough uh to share on Tik Tok or or to or to make an experience on Roblox and invite your friends over but for people they want to spend a lot more time you still have to provide that fine control right and so the challenge is how do I make how do you make a tool that is really good at capturing all the initial intent but at the same time allow the real pros to iterate with the same level of control of the traditional tools so that's a little bit the the challenge there I think a lot of companies like like we are facing in in studio uh a lot other companies are also facing we have a really fun episode of training data with uh with this team that created a company called dust and one of the founders Gabriel Hubert talks a lot about rasterization versus vectorization and this is definitely your language as a graphics person they also were Stanford computer vision type folks and it seems like that transformation has happened uh and the vectorization kind of what you're describing like you can you can expand it to a great deal based on intent you can shrink it to very small and almost like fractals go down into more and more detail uh when do you think we will get there what do you think we'll get to the point where you can say hey this is the attent and then you can go in and and and a fractal level of detail change each pixel also based out ontent but on a much smaller scale uh yeah that's a a million dollar question so what we're trying to do is uh allow assistant first to be able to do to perform those operations right so uh the samei that gives me the like the rough uh you know slot machine type of input you know I I throw in some text an image I I pull the lever let's see what we get right we want basically a system to also be able to go in and do the fine grain change so hey only the trees that are above you know five feet can you just change the color of that right so uh and then can you take the texture of the tree and then open up and I'm going to like paint over it right and uh and so we we want to allow AI to already be able to go beyond like one shot and allow for iteration with We Believe iteration is is a fundamental uh um way people create and so we want to make sure the AI can support that from the get-go and then we will always have a fallback uh with some tools and maybe there you know it's going to be more like a progressive disclosure where we don't throw it in front of every user right but only the users that want to go deeper then you know we can basically pull the curtain and then allow them to to go a little bit deeper in some cases honestly we would just like interrupt with our other tools uh there are things that only in blender you can do or only Photoshop or sou painter uh you know Studio uh won't become you know this uh crazy place where you can do everything but it's hard to do anything right so we are very I think committed to uh um Studio being good at what it does and then it's okay to have you know for really the the pros that want to go deep to have a great interoperability with external tools we talked a lot about Studio but actually the thing that we are the most excited is taking all this AI goodness and bring it to inexperienced creation and so we think that's going to be the next Frontier so two aspects in the industry we are at the very beginning and we are curious to we don't know what's how it's going to play out and we are super curious using AI for substantially different gameplay uh like uh you know companies like ego. live are experimenting on that and then taking all the ability to create the AI has Unleashed and bring it to inexperience creation so I'm there I'm playing my my game on Roblox now I want to modify this level I want with my friends create something new that we can play together those are like completely different type of of creation experiences that we're going to see be Unleashed by AI so right now we are incubating in studio we're making solid robust we're making sure the output is high quality but we are very excited to bring you know those apis to in experience and we think that's going to be the the real impact I'd love to dig more into that maybe starting with just the ingame the gameplay experience like how do you think how do you think from the players perspective these games will be different uh I think they can be uh we we basically no effort or only very little effort from the developer uh these games can be more personalized and also they can be always different right like if you if you play the same game uh but there's an llm and is aware of your past uh uh you know all all the you know all the things that you have done in in past sessions and then you come back then you can make the game different more interesting can change the challenge can spin a little bit the story right so all these things when you have like a um a very smart you know AI back end uh who keeps track of what's happening in the history and and knows also who you are it can really morph the game onto something that is really more enjoyable for you specifically so we think that would be a big opportunity there's a lot of companies they now doing experiments uh and we are very excited to basically provide the eye as a platform and then let them experiment with different game plays that sounds Stefano technically very hard and I remember even even for the simplest type of of Roblox gaming which there there there isn't really a simple type you're generating digital worlds continuously like it's actually a very very heavy lift you mentioned having your own data centers right low latency being able to play with people internationally across the world how do you think about this new level of complexity this AI inference uh complexity for gameplay as in generating the world varying it on the Fly what will have to change from an infrastructure perspective that's a great question so I think the first step TOS that will be uh NPC so non-playable characters that you hook up to an llm uh that doesn't need uh massive changes in right that's Prett for and I think that's going to be the first rev and we're GNA see what kind of impact uh that that creates then we have seen other experiences where they actually want to create the whole world and of course there are some challenges there and let me tell you I think what I think is the biggest challenge is actually moderation because our Roblox safety is our number one product and we want to keep the the the platform safe and we want people to connect with civility uh and and when you allow people to create anything of course the bar goes uh you know a bit higher and so you know we have we we open this Pandora box but we also have to build build the guard rail so things stay you know stay positive and and uh and and we have you know everybody can have a positive experience on on the platform so I think that is more of a of a challenge than like a latency infrastructure because we can you you know CDN we can cash things uh we can pregenerate some of the content we can use a level of detail so there's a lot of things that the gaming industry has developed in order to basically stream uh we are using streaming as well right in the platform so uh we can basically in real time generate and and stream more assets I think the challenge will be more like if you have now the control of the game that you're are playing how do you make sure that everybody else has a great experience I think that's going to be the challenge both on the moderation side and also like in making it fun right G we we have infinite respect for game developers because they know how to make things fun and know every player player you know has mastered uh the same skills of like making games fun for the last you know 20 years right so uh it's like how do you give this freedom to create while you somehow control the game play and the story so it stays compelling so these are all things that uh we are very eager to learn from the community honestly and I think that's the beauty of being a platform we don't need to have any opinion about it and we don't need to uh figure it out ourselves we we just provide uh the apis to the community and the community with the infinite creativity it has will we figure it out so I've heard NPCs as that first stepping stone in in game AI frequently what do you think is going to be the next stepping stone uh for how you know game mechanics or game playay might change because I imagine you're not going to go from NPCs to entire world right like is is there another kind of what's the second stepping Zone uh I've seen like games where um they limit the creation to one specific item so for example uh you there's a very popular game on Roblox could build a boat uh you go into this game you're building a boat and then you're selling that boat and then you're racing other players and so uh you don't create the word the word is is pretty defined by by the uh developer the of the experience uh but you basically allow that constraint creativity as in this is these are the materials that you can use to build the boat and this is the size that has to be and then you can go crazy and build whatever thing you want and because robock is such a physics sandbox right we have our Dynamics further Dynamics and so we can actually see what you create and like simulate the wind and you you're going to see the outcome of what you created and so I think experiences like that right now are a little bit more difficult without Ai and AI I think can can really um Power those up quite a bit where you can create your race C you can create your Bo your airplanes or like some elements of the game play uh the u based on what you create you can have you know an advantage uh in the game but you're not completely changing the game itself constraint creativity that's a great answer exactly I have two follow-ups from that the first is you just mentioned physics engines and there's been a lot of enthusiasm for years now around pins and these neural physics engines Etc is that now in the physics engine at robox if you can disclose are you using neural networks as part of the physics engines as part of the AER you mentioned aerodynamics right like you guys at the point where you're estimating navig Stokes uh because it's cheap enough because you can use neural networks uh or or is that maybe in the future and then my second question is about NPCs would you guys ever allow a world or a game that is purely NPCs isn't something you watch as opposed to something that you you participate in uh that's a great question so on the physics uh side uh I I think the use of neural networks for physics is not being proven as effective I think there is more like there's an infinite amount of like uh real world approximations that you can use that are like very computationally efficient and I think that's more important than than how good you're approximating the word uh and so on that on that front uh I'm not sure um NE networks are going to provide that much value uh honestly um especially when you have like already implemented all the basic functionalities and people are already using them successfully so especially for a purely digital world I mean if you're trying to if there's a noisy real world I imagine it's different from like roox world where everything can work in the physics estimations is that fair yeah that's that's fair so maybe I I will say uh that that's all I I I can say it's it's basically maybe I think at the moment is still not fully proven as a as a part and then on the on the NPC front um you know if you think about a game where all the players are NPCs and you're just watching it to me it sounds like TV right yeah so people watch this all the time and you know uh these NPCs can be super smart and and do super fun things can race can do whatever they they they like and maybe it's just fun to watch so definitely we are seeing such a huge community of people that just watch other people play games and so those players could be be llms in the future and still generate great entertainment and fun so I will not exclude that I think at the beginning we're going to find a hybrid model um like which is like you know NPCs existed where people will inject NPCs in the game to just make it more interesting and have people that you know you also can populate games that just launched they don't have you know a thousand concurrent uh players yet and so you can basically populate huge word with with characters they are interesting to talk to um without having to have you know the the real uh people there uh right at the beginning so I think there's going to be a lot of uh potential on that front for sure what about for user generated content like how do you imagine ugc changes uh with another generative AI is really coming into the experience uh that's a great question we have seen some of this with the Gen features that we release for Avatar creation so Avatar creation is another huge area of creat ity uh there's a massive community in Roblox they uh work on like making assets and clothing and accessories for avatars and a lot of people make make a living doing that and so we have found that there were a lot more people like always that had amazing creative ideas but may not have the tools to like do 3D modeling for example right 3D modeling is a very narrow specific skill they would have to learn you know blender or or Maya and and so no all of them were up for for the challenge but they had a great idea about what output to create and so there we experimented with uh using uh images as an input or text as an input and generate avatars and we found that it substantially democratize uh how many avatars could be created and is this is an area uh we launched like an early beta of our Avatar Auto setup and we are doubling down that we're going to do a lot more uh in the coming months that you're going to see but basically we want to allow people to create avatars clothing accessories with just multimodal easy input uh again there's more people that have they have great ideas than than people that can actually execute on those and so we want to basically uh remove those barriers and Rel leverage AI for for that type of creative expression as well do you think that generative AI blurs the line between what it means to be a user versus a Creator like do you imagine those two become kind of the same thing and I imagine you think of them distinctly today oh for sure yeah and I mean think about you know what happened with the music uh uh scene right the the boundary between the composer and and and the um people that could play an instrument and people and the audience were just so set at the beginning and then you know I think karaoke like completely blur that right and then now everybody can create things on garage band and now with the eye people can just type lyrics and the genre and then a full song is created so there like the boundary got completely burned I think in in the audio and music space pH and you know for game development is going to take a little bit longer because just more complex as a type of content um but I I think it's a you know just entropy only only increases and so we're going to see that for sure Stefan you mentioned that you guys have an absurd amount of data uh you guys have an insane amount of video data I think roox is still the number one VR application in the world by a lot like uh including on VR headsets and then uh also obviously personal computers all sorts of devices Etc ton of audio data ton of textual data like kind of everything data and so you kind you mentioned this I've been kind of walked by it but obviously Sonia and I are very curious with all that training data yes the name of the show uh are you guys going to have a world model I think I know the answer to this I think it might even be announced already depending on when this airs are you going to have a role model what's it going to look like what will be its boundaries and where do you go from there yeah that's a great question so yeah we do have an enormous amount of data and I think the challenge for us is less about Gathering data and more like being able to use it uh because there's there's a lot that goes into from you know the raow data to uh be able to to train uh an llm or or AI in general so that is where like the work is focusing we have announced uh just a couple of weeks ago that robox is working on a 3D foundational model model uh our intent is to open source that uh and basically what what it does is uh it will allow uh the digital synthesis of uh scenes and and word uh from multimodal input so that that's the goal and I think also we can go a little bit beyond that as I said before our goal is basically to teach game development to Ai and so uh it's not just about word creation I think that will be the first Maybe area that we that we can really attack in a meaningful way but then there's going to be all the interactivity of that word the ability for things to move band doors to open CS to run around right and then the next level will be uh all the interactive gameplay and so we are seeing the data and we also uh can see from the data what works what is fun uh for the for the user and so we will be able to guide AI to not just make games but actually make games that are fun uh because you can see which games get more traction or like which specific levels get played more and so they could be in the inference aspects of of quality of the experience not just like hey I'm just going to generate whatever you ask me but I can also like if I can pick I can pick things that like more fun to engage with and so uh again that one is going to be um you know it's going to take take a while to figure out but you know we we have the data uh that they can support it I think it's more about on us to be able to um pick up you know there's going to be a lot of like unsupervised learning that we have to do and so we have to pick up a we have to find ways to pick up signals of intent from our creators that then we can use in an unsupervised way to to classify things and then figure out what AI should be learning from I imagine because of the physics engine he has such a competitive Advantage with object placement object interaction like the struggle with a lot of training data for these video and Role Models is like they're two- dimensional and there's no concept of I mean maybe you can infer from it the concept of what is three dimensional on how objects interact but is that something you guys are leaning on particularly heavily I'd imagine it's a a big advantage to the the three model you create yeah it's a huge Advantage for getting also consistency over time and and spal consistency if you actually have the full 3d model and if you have a full 3d scene you know then you can anchor uh uh the the spatial uh and the temporal coherence in a much stronger way than if you're estimating uh right those things from from a 2d I mean the classic example is uh if you're if you're trying to uh stylize a video and you use just video to video you know you're going to see drifts and artifacts and everything then you know with control R net and and the depth map right which is like a you know two and a half D approach uh this video to video uh type of stylization became a little bit more reliable and and more consistent temporally but you still like don't have a mean you have basically the situation where like if a character is looking at you and then you stylize their face and then they look away and then they look back they are like a different person right we have all seen those examples and so the only way I think to overcome the is to actually train with 3D data and the industry went so fast so far with like just 2D right and is now making magic with 2D and I think there's still like the work to incorporate 3D information into those into those algorithms still has to come and I think it's going to bring to frion this like temporal and and special coherence that right now we don't have or that it's so hard to you know to generate out of 2D data right because it just it's not reach enough just to make sure I understand what you just said do you think that 2D is going to lead to a dead end and you have to start over from 3D or do you think that 2D can kind of get there with scale as well I think if the goal is to operate on a single image 2D has done fantastic and so if you need to text a character or like stylize an image all these things is fantastic if you want to actually have uh video so if you add the bisy time on the axis and now you are also maybe moving the camera so the camera is moving and then you know there's like 2,000 frames ahead of you they have to be consistent at that point if you're not using 3D data you're at the disadvantage and you're going to be dealing with drift and yes you can enforce it in many possible way but the problem of 2D data is that has occlusions right so if I if you're looking at me and my arm is behind my back uh and then at some point my arm pops out you have nothing to enforce consistency of the look of my hand because you just have not seen it before right so all these problem don't exist if you're operating on a 3D representation of your scene because even the data they are not seeing from the camera it's there and it's available so I think the industry has gone like you know in a amazing I made amazing progress in trying to cope with that which is a fundamental lack of data and then if that data is actually used and can be incorporated in the say a stable diffusion then we're going to see a much better outcome uh Steph in our homework for this episode we heard that you're passionate about neural rendering can you tell us what that is uh yeah I don't know where you're at that but yes it's true I'm very passionate because I have a strong belief the uh neural networks will uh change the way we're going to like stylize games and and make a games visually a lot more compelling than they are today uh and so for if you look at the history of of game devel velopment the creation of assets and the style of the game were always tied together you know if you if you're making like a Super Mario and and and that's the style of the game you can just like on the flip of a coin and say okay make it look like a Call of Duty it would just not work you will have to redo all the assets from scratch but if you're using neural rendering or or generative rendering techniques uh you're able uh with some text description and maybe some reference images to restyle your game in real time and then you you leave the geometry where it is so it's physically consistent right uh uh your physics capsule is in the same place but the visual look can be very different and this can be used also to make games completely photo realistic even though you're using you know some low resolution meeses and textures right you can use this as a final pass like you know in games you are you know typically Bloom at the end to make things look really cool and so it could be like a final you know a generative rendering pass make it photo realistic or stylize it in this specific way that I like and you can restyle your game um you know in a way that actually doesn't add any extra assets that you have to download doesn't change the assets it's really like a compute layer uh the appens at the end right now is very um compute intensive but you know looking at the speed at which things have moved you can imagine that in the future people developers can just add this filter to their game describe the style give a reference image and then have the the game look beautiful without changing you know a single uh geometry in the assets wow so I I'm a believer that in five years this will be the way games will be built and it is going to run also at least on the high-end uh phones and um yeah we're very excited about it Steph does it ever become the end Gamers Choice the way it's rendered as in I put my skin or Style into whatever game I'm playing a good question I think if the if the developer of the game allows that freedom to to the player why not right I think it's going to be an artistic Choice hey you can play my game and you can make in any style you want and again you can still play with people that will visually see a different game but the game is still consistent all the physics all the game play still still hold um Stephano thank you so much for joining us today we learned an incredible amount uh certainly about the scale and complexity of robloxs not just from technology but also the scale of impact the 799 million daily actives hundreds of millions of monthly actives the amount of technical depth that's necessary to get there we learned about how you're using AI as dish dishwashers to empower your customers and and developers and Gamers uh and not just in the assistant but also in the code assistant and the material generator and we learned a little bit about what you think the future's going to look like uh we might be using infrastructure for guard rails to make sure that the civility of roadblocks is preserved we might be watching some NPC television and we certainly will have a a 3D World created by robloxs uh can't wait to live in that Oasis thank you so much for having me it was a pleasure thank you [Music] [Music]

========================================

--- Video 31 ---
Video ID: 6CMCkeSU9FI
URL: https://www.youtube.com/watch?v=6CMCkeSU9FI
Title: From AlphaGo to AGI ft ReflectionAI Founder Ioannis Antonoglou
Published: 2025-01-28 10:00:07 UTC
Description:
Ioannis Antonoglou, founding engineer at DeepMind and co-founder of ReflectionAI, has seen the triumphs of reinforcement learning firsthand. From AlphaGo to AlphaZero and MuZero, Ioannis has built the most powerful agents in the world. Ioannis breaks down key moments in AlphaGo's game against Lee Sodol (Moves 37 and 78), the importance of self-play and the impact of scale, reliability, planning and in-context learning as core factors that will unlock the next level of progress in AI.

Hosted by: Stephanie Zhan and Sonya Huang, Sequoia Capital

Transcript Language: English (auto-generated)
go is a complex game and there's there was always a bit of worry about whether alha go was truly as good as we believed so we actually had the conviction that know deep reinforced learning is the answer based on everything that we could measure and everything we could see but that's uh the thing about these systems is that they're not like classic computers where you just like know that they always produce the same answer they're like stochastic uh they are creative and they have like some PL Sports they hallucinate like similarly to how like model LM cinate so you need to just like really push them and just like see exactly where they break and uh the only way you could actually do that is by having like the best humans playing against [Music] them today we're excited to welcome Giannis an a researcher and engineer who has contributed to some of the most significant breakthroughs in AI as a founding engineer at Deep Mind jannis played a crucial role in developing alphago which made history by defeating go world champion leol he later co-led the development of mzero which pushed the boundaries even further by mastering multiple games autonomously now as he embarks in his latest Venture with reflection he's focused on building the Next Generation of AI agents we're excited to talk to Giannis about the Breakthrough moments in AI history that he's witnessed firstand from alphago's famous move 37 to his perspective today on what's next for the combination of reinforcement learning and large language models on the way to AGI Yannis thank you so much for joining us today thank you so much for having me Yannis you have an incredible background having worked at deepmind as a founding engineer for over a decade um starting with some of the most notable projects that have really defined the industry Deep Mind quite notably um created this notion of building AI within games to start can you share a little bit more about why deep mine chose to start with games at the time yeah so tip mine was the first company to truly embrace the concept of artificial general intelligence or AGI from the outset they had Grand opitions aiming to build systems that would much or exit you intelligence so the big question was and still is how do you build AGI and more importantly how do you measure intelligence in a way that allows for Meaningful research and performance improvements so the idea of using video games as a testing ground came naturally to Deep Mind Founders it was uh demisis and Shane leg because Demis had a background in the gaming industry and Shane's PhD thesis defined AGI as a system that could learn to complete any task video games provided a controll yet complex environment where this ideas could be explored and tested and to what extent you mentioned games are they provide a very controlled environment to what extent are games representative or not of the real world like if you have a result in games do you think that generalizes naturally to the real world or not so I mean I guess games have indeed been valuable for developing AI uh and you actually have like a few examples of that so you can see that poo for example which is currently being used in rhf was developed using openi gy and Moko and Ari and similarly we have like MCTS which was developed which stands from monteal research and was developed through board games like Pam and go but at the same time games have like a number of limitations so the real world is a messy is unbounded and uh it's it's a much much tougher nut to crack than even the most complex games so even though they just like gives you uh an interesting desp to develop new new ideas is definitely uh limiting and it doesn't really capture all the complexity of the real world okay interesting though so but a lot of the techniques and algorithms that you've developed in a game environment TPO um Etc these are used in the real world yeah so po was actually like exactly what uh chpd used for rlf and um so MCTS uh it's used in Museo and uh Museo has been used in like in the real world in things like uh uh you know compression video compression for YouTube um in it was part of the sell driving uh system at Tesla uh at some time uh uh and it was also like used for developing a a pilot like that was completely uh controlled by nii so yeah I mean you can see methods like that being used in the world to solve real problems so interesting Yannis um I remember back in 2017 when alphago the movie came out and it featured the incredible game of alphago against Le at all can you take us back to that moment in time and maybe the years leading up to it as you're building alphao how was alphao specifically chosen as the game to focus on so I think like games you've always been a benchmark for AI research so like uh before go you have chess and uh chess was like a major Milestone with like IBM deep blue defeating G Kasparov in the late 90s and um I mean even though you know chess and go are completely different games and go is definitely different past there is a like games have always been acted as test BS for like the development especially board games for the development of like new AI methods uh actually even going back to the earliest days of AI research uh touring and Shannon they both worked on their own versions of like chess bots so now the thing about like go is that um it's a much harder problem than like chess and the reason for that is because it's almost closely impossible to Define an valuation uh method a heuristic so in chess you can just like take a look at the board you can count the number of Pawns that like each side has you can see what the ranks of these pawns are and then you can just like make some you can draw some conclusions on like who is winning and why but like in go there's there's nothing like that like it's mostly human intuition and if you ask like a go you know professional player like how they know whether whether a position is a good one or a bad one they will say that like you know after having played the game for so long they can just like see it in their gut but like this is a better position than the other one so now it's actually a question of how do you encode the feeling in your gut into like an AI system right so this is exactly the reason why solving go was considered the Holy Grail of AIR research for a long time and it was a challenge that seemed almost impossible but at the same time it was like Within Reach people felt that like you know they could actually get it cracked and this is exactly what alphago did uh back in 2016 um and it kind of like showcase uh two new methods which is like deep learning and reinforcement learning because back in 2015 in 2016 like now we kind of think of deep learning and reinforc learning as mature Technologies but like back then we're kind of like literally like making the they're taking their first steps and they were kind of like the new kid in the block um and most people were kind of like really skeptical about them like everyone thought that de planing was a was another AI fat that uh would just like one last the test of time so yeah I mean Alpha go was chosen because it was like clear to show J uh that you actually have like the most the most performant agent in the world you could actually evaluate it you can have it play with other humans and at the same time it was Within Reach given like the latest developments in deep learning and foral learning I remember reading that there's more configurations of of the go board than than Adams in the universe by by many other than magnitude and that blew me away because I grew up playing go and it felt like such a you know it's a very simple Ru in terms of the rules uh but um I see why it was the Holy Growl um maybe can you explain how alpha go worked uh technically maybe explain it to me like I'm a fifth grader because that is that is effectively my my level of sophistication understanding these things but how did it work um and you mentioned that both reinforcement learning and deep learning were involved I'd love to peel that back a little bit yeah absolutely so um alha go has two deep new netw so like t a neural network is a function that like takes something as an input and produce something as an output and it's literally like a black box we don't really know exactly how it does it just like know that you can actually if you train it on enough data it'll just like learn the Mapp it will learn the function like from input to the output space so alphao actually had access to two deep networks the policy Network and the value Network and the policy Network suggested the most promising move so it will just take a look at a Current Port position and just like say okay you know based on the current position this is the list of moves that I would recommend you just like consider playing and it also had access to the volum will just take a look at like a board position I just like give you winning probability like what are your chances of actually winning the game starting from this position this is exactly the gut feeling like uh it had like its own gut feeling on like whether the position is a good one or a bad one so once you have access to these two networks then you can actually like play in your imagination in a number of games you can consider like the most promising moves then you can consider your opponent's most promising moves and then you can just like evaluate each mve like the volue network and then you know you can use a method called minmax what that says is that I want to win the game but I also like know that my opponent wants to win the game so I want just like pick a move that will maximize my chances of uh winning knowing that like my opponent will try to maximize their chance of winning so if you actually like do that and simulate a bunch of moves then you can just like get the optimal action and you know the way to just like do this imagination this planning this search uh in the most efficient way is by using a a treets method called multical Treet so MCTS so whenever people talk about uh MCTS they literally just like mean this euristic of how do I you know how do I choose which Futures to consider so that like I can make informed decisions the role for reinforce learning and TP learning and building Alpha go was that uh Alpha go first of all was a success of reinforcing learning and TP learning uh because like this is exactly the two methods that powered off go and uh the policy network was initially trained on uh a large set of human games so you had like many games played by uh human professionals and you just like consider every position and you consider the move they took at this position and then you have like deepbrook that tries to predict uh this move then once you have the policy Network you need to somehow find a way to just like obtain a value Network so we did it in two ways first we just took the policy Network and we had it play against itself and we used reinforcement learning to uh to improve it to improve the the blank strength of model so we use a technique called policy gradient so what policy gradient does is that it just like looks at the game and then it looks at the outcome this is the simplest version kind of like of Po it looks at the outcome of the game and for all the moves that led to a win they just like say great you know just increase the probability of choosing this move and for all the moves that led to a loss it says great now decrease the probability of like this move being selected in the future and if you do that like you know for many games and for long enough then you just like get an improved policy now once you have this improved policy you can just generate a new data set of games where like the policy plays against itself and then you have like a huge amount of games where for each position you know who the final winner was so then you can take this network you can take another Network a value Network and have it predict the outcome of the game based on a on the current position so what the network learn is that if I start at this position and I play under my current policy on average this is the player who wins like it's either a black player or their white player so uh this is the first version of like a value Network and you can just like use it within Al go by combining it with the police Network and what were some of the biggest challenges in building this and how did you overcome them yeah so Alo was not just the r challenge but was mostly I'd say an engineering Marvel it was uh the early versions run on 1,00 CPUs and 176 gpus and the version that played against listed old used 48 tpus so like tpus for like the first accelerator custom accelerators and this were like uh these accelerators were like really primitive back the end because literally it was like the first version right like now the later accelerators are much much better and much more stable so the system had to be highly optimized to minimize latency maximize trut we had to build large scale infrastructure uh for training these networks and it was a massive Endeavor just required a lot of coordinated effort from many Talent individuals working on different aspects of the project but uh you know I just like walked you through a number of steps just like obtain the policy Network and the value Network and each of these steps had to just be Implement at the at the limits of like what was a available and what was uh possible back then in terms of scale and it had to be implemented in a way where people could just like think everything it they could just like try the resist ideas fast and get results results fast so yeah lots of people uh scale in you know at levels that hadn't been implemented before and it's kind of like working at the Forefront of what was cap what was possible back then I love your highlight of it being a research Marvel and an engineering Marvel and I remember you sharing one time that part of the reason this project came about also was because Google had tpus that they needed to they needed a test customer for and that was the spark this alpha alpha go project so that's pretty incredible how much conviction did the Deep Mind team have that this is going to work you mentioned that you know at the time deep learning reinforcement learning were still Rel atively novel but deep mind was very much founded with that belief but did you guys think that you were going to be able to have kind of these super human level results beating the top go player in the world like was it a crazy idea and maybe it'll work or did the team have conviction like this is going to work yeah so at say Tech the team had a cautious optimism so one of alpha Go's Le lead developers AA H uh he is a strong amateur go player and he had been working on goal for like a decade before Al go happened and we also had like a lead report of a computer game of computer players and you could see that alago was significantly stronger than anything that had come before but go is a complex game and there's there was always a bit of worry about whether Alpha go was truly as good as we believed so we actually had the conviction that you know deep reinforce learning is the answer based on everything that we could measure and everything we could see but that's uh the thing about these systems is that you know the they're not like classic computers where you just like know that they always produce the same answer they're like stochastic uh they're are creative uh so and they they they all have like the the they have like some blind Sports they hallucinate like similarly to how like model LMS cinate so you need to just like really push them and just like see exactly where they break and uh the only way you could actually do that is by having like the best humans playing against them um move 37 can can you tell us what that was it was such a Monumental move and I think everyone watching it at the time uh it was and least at all maybe primarily was confused by that move um what was going on in your head when that happened so yeah I mean uh move 37 uh yeah in game two against L was literally just a spectacular moment in the sense that kind of showed case to the world that Alpha go has creativity and it demonstrated that AI could come up with straty that even top human players hadn't considered so at first like I still remember that like we thought that alphao made an error so that uh it actually like hallucinated he did something that like it didn't mean to do but then turned out to be a brilliant a conventional move that underscore that the system had a deep understanding of the game that the system actually had like U creativity it could think of things that like people hadn't thought off before I want to take us to another key move in the game I think it was in game four at this point I was rooting for a leag because I was like a poor guy needs to win a game um move 78 I think I think uh alphao made a mistake and Leisa don't know this is it um I guess what was the weakness there uh that Lee found during the game yeah exactly so I mean Lista do fix in game four was literally a testament to human Ingenuity like move move 78 was unexpected and called Al go of guard initially Al go like based on its evaluations Mis interpreted as a mistake and thought that it was actually like winning so that's why it didn't respond appropriately and you know this kind of highlighted a blind spot in the system so the game show that like while uh systems like afo are extremely powerful at the same time they still have vulnerabilities and there were like still areas where you could uh further improve it but how do you go about improving something like that do you do you need to show it a lot more data of you know kind of that type of human engine move or or how do you go about fixing and and patching those those blind so yeah I mean it's actually interesting that that uh by the end of uh the games with like lcid all which just like put together a benchmark where you're just kind of like uh trying to quantify and just have a way of measuring U the mistakes that like afo makes and you know these SC blind spots let's say and then we just write a number of approaches to just like improve the algorithm so that we can you know solve these issues um and what happened is that actually the most effective way of getting rid of them was just like do what we were doing just like at a at higher scale and better so just like uh change the the architecture of the model we just like switch to a deep breast net with uh two output heads and we also like uh uh we just had a bigger Network trend on more data uh then just like mve to Alpha zero and better algorithms and that kind of like made it so that we didn't have any hallucinations anymore so in a way just like scale data you know things that uh are always kind of the the the the well-known recipe in the field of AI is exactly what sold it in RJ to with scale and data how much did higher quality data or maybe specifically data from great professional players the best professional players make a meaningful difference or was it just any data no for us What mattered was that the we kind of like solved it using um selfplay yeah so we actually had access to the most uh competent go player in the world and we just like used it to generate the best quality games and then we just trained on these games so I guess like you know the we didn't need to have like human experts because you had like a n experts in house it wasn't human huh interesting amazing well I'd love to move on to the progression from alpha go to Alpha zero and um you talked a little bit about this notion of selfplay just now Alpha zero was powerful because it learned how to play the G game from scratch um entirely from self-play without any human intervention can you share more about how that worked and and why that was important so Alpha zero was a game changer because it land entirely from scratch through S play without any human data and this was like a major Rel from alphao because like alphao as I said relied heavily on human exper gains so two things happened first of all alpha alpha zero managed to simplify the training process and also like showed that AI just like get from zero to superh Human Performance just purely by playing against itself and that allowed it to just be applicable to a whole range of like new domains that were Out Of Reach because like there wasn't there weren't enough like human data for it but um I think like the the more the more important thing is that we just we just saw that Alpha zero also solved all the issues that like Alpha go had in terms of hallucinations in terms of uh uh you know BL spots and robustness so like Alpha Z was like a better method Just You full star and you you explained kind of how alpha go worked to to a fifth grader um what would you tell the fifth grader would be the key difference um technically that you that You' implemented with Alpha zero so Alpha zero just like Alo uses a policy Network and value Network along with modales so in that respect it's exactly the same as Alo so the key difference is in training Alpha zero starts with random weights and lears by playing games against itself and uh by playing games against itself it iteratively improves its performance but the main idea behind Alpha zero is that whenever you take a set of Weights a set of uh policy and value Nets and then you just combine them with search then you just like end up with a better playing uh better player uh you just like increase your performance you just like become a stronger player so what that meant is that we can actually use this mechanism to improve the the model policy the role policy so this is what we call in the in reinforcement learning a policy Improvement operator whenever you can just like take an existing policy and then do something some magic and then just like uh come up with like a better policy and then you can just like take this policy and distill it back to the initial policy and they just repeat this process then you have like a reinforced learning algorithm and I think like you know this is exactly what people are trying to do uh today with like uh you know two star or like uh you you know synthetic data this exactly the idea of like how can I take a policy do something with it planning search um compute whatever it is and derive a better policy which I can then imitate and just like kind of distill back to the original policy so this is exactly what Alpha zero is doing it uses it MCTS sech to produce a better policy then it takes his trajectories it trains this policy and value Network on the new better trajectories and it repeats this process until converges to the you know to an expert level U goal player that's fascinating and counterintuitive that kind of like starting without the the the weights that you would have from from you know professional level players is actually a better starting place the epitome of um AI agents and games have achieved I think via muzero which is the progression even from alpha zero itself and it's also where you became one of the co-leads or one of the leads of of the game um Alpha zero was obviously impressive because of selfplay but it also needed to be told the environment's Dynamics um or the rules of the game and musero takes us to the next level um without needing to be told the rules of the game and and it mastered quite a few different games um go chess and and many others can you share a little bit about how muzero worked and um uh why was this particular particularly meaningful absolutely so Alpha Z you know as you said was a massive success uh in games like chess go uh Shi so uh in games where we actually had access to the game rules where we actually had access to a perfect simulator simulator of the world but like uh these two lands on the perfect simulator made it challenging to apply to real world problems uh and real world problems are often messy and they lack clear rules and it's really hard to just like write the preference simulate of them so that's exactly what Museo tried to solve so M Masters the games of course like go chess and SOI but it also like Masters more visually challenging games or games that like are hard go like Atari and it does that without having access to the simulator just like learns how to build an internal simulator of the world and then just use this internal simulator in a way similar to what Alpha Zer was doing so it hases that by using model based reinforcement learning where what that means is that you can just take a number of trajectories generated by an agent and then try and you know learn a model learn a prediction model of how the Vault works so this is actually like quite similar to what methods like Sora are trying to do now where they just like take uh YouTube videos and they try just like learn a world model by just trying to predict based on starting from one frame what's going to happen in the future frames so new tries to do exactly that but it it does it in a way different from you know generative models in the sense that it tries to only model things that matter for uh solving the reinforc learning problem so he tries to uh to to predict what the rewards going to be in the future what's the value of like future States what's the value of like future uh what's the policy for like future States so only things that you need within your MCTS but you know the fundamentals kind of like remain the same so how do you just like learn a model based on trajectories and then once you have this model you can just combine to search and um you know get super human performance so it if of course like you can always decouple the two problems and have like the model been trained separately from you know data out in the wild and then just like combine that with new zero and we just found that back then given the limitations of like our models and the smaller sizes kind of like made more sense to just like keep those two together and only have the model predict things that matter for uh planning instead of just like try to model everything because you're kind of hitting the limits of what the capacity of the model uh could uh could take so interesting is it right to assume that that not only Sora you know takes the same approach but maybe other world models or other robotics Foundation models yeah so anything that tries to just like build a model of how the world works and then just like use that uh for planning it's within you know new zero like methods so yeah you can just like uh train it on YouTube videos you can train it on like uh the inputs coming from like robots you can train it on you know any any environment you can even think of like CL language models as a form of models of like text so like they they the model text but the the thing about text is that like the model is a bit trivial like you you don't need to just there aren't many artifacts happening when you're trying to predict what the next world is going to be right so have you seen the ideas behind muzero kind of be used um outside gameplay or or in in messy real world environments so yeah I mean so as I've said um Alpha zero your quite General methods and they were like uh there's a number of scientific communities um in chemistry so there's Alpha cam in Quantum Computing some people tried to use Alpha zero in optimization where they just like adopted Alpha zero because uh it was really powerful in really doing planning and like solving this optimization uh problems at the same time uero was incorporated in a version of like Tesla sell driving system it was kind of reported in their AI day and um it was all Al uh used and I think it's currently being used uh within YouTube as a custom cation algorithm but I think you know we it's it's early days and uh takes time for like these new technologies to be fully adopted um from by by the industry we'd love to talk a little bit more about reinforcement learning in agents um you alluded earlier to the fact that reinforcement learning and deep learning back in 2015 were new inent ideas they really you know grew in popularity 2017 2018 2019 onwards um and then they were overshadowed by llms um uh largely because of um uh the GPT and everything else that came out but now reinforcement learning is back why do you think that is the case yeah I mean first of all llms and multimodel models have indeed brought incredible progress to AI so these models are exceptionally powerful and can perform some truly impessive tasks uh but they have like some fundamental um limit ations and one of them is the availability of like human data uh people just keep talking about the data wall and what happens once you run out of like high quality data and this exactly where reinforc learning signs so reinforc learning excels because it doesn't rely Sol on pre-existing human data instead reinforcement learning uses experience generated by the agent itself to improve its performance so this self-generate experience allows dreamforce learning to learn and adapt and to even adapt to scenarios where human data is scarce or like non-existent so if you define the reward the reinforcement learning problem in the right setting in the in the right way you can literally effectively exchange compute for intelligence you can just like get to a point similar to where we were with alao where we just like the the moment we threw more computer at it like we made the networks bigger we just like you know used more games we just literally got a better player and it was uh deterministic you always get a better player so I guess this is this is exactly where we want to be with like the synthetic data pipelines currently we have that with um you know the scaling clause in llms that if you have like more data and bigger models then you get like a you know you can predict that there's going to be an improvement in performance but you know once you run out of like uh human data how do you just keep going and synthetic data is like the answer to that and the the only way uh that you know you can actually get high call during Force learning uh high quality data to just like improve your model is like via some form of reinforcement learning and just like leaving I'm just like keeping reinforce learning as a really kind of blanket term here where I just like Define it as anything that learns through trial and error how do you think uh reinforcement learning is is being brought into the kind of like LM world and and you mentioned qar earlier um like I guess in in a close form game you have like a pretty clearly defined policy and value function how does that work uh in like a messy kind of real world environment or the llm world so I mean I guess like uh there are two different types of like messy real world right like there is the if you try to just like build a controller or something that's a really messy environment and then if you if you operate in the digital space so personally I believe that uh digal AGI which is happen much earlier than you know robotics AGI and there isas reason for that exactly that you have control over the environment and the environment like computer is like the digital world so even though it's like messy and noisy um it's still contained it's not like the the real kind of flying World in that sense so now in terms of um how do you bring like reinforcement learning so reinforcement learning is uh there we you used to say in deep mind that you have like the the problem and you have the solution and the problem setting of reinforcement learning is how do I take a model how do I take a policy and generate synthetic data or like I I I I learn I find a way to improve this policy by interacting with the environment by a trial and error and this like the reinforced larning problem setting right and then there's like the solution space where you have um value functions and have like U reinforced learning methods so I think that there's a lot of uh inspiration to draw from like classical reinforcement learning methods that were developed in the past decade but have just adopt you have to adjust them to the to the new world of llms so methods like you start try to do that by just taking the idea that if I have a policy and then I do planning I consider possible future scenarios and then I have a way to evaluate which one is better then I can just like take the best ones and then ask the model to imitate this better ones and this is like a a way of improving the policy so in the classic RL framework you you do that uh by using a policy and a value Network in the new world you'll just do that by asking your uh by having reward model or asking your uh your llm to just like uh give you feedback on uh of an output it gave you so interesting um you also talked a little bit about synthetic data earlier I think some folks are very bullish on synthetic data and some folks more skeptical um I also believe that synthetic data is more useful in some domains where um outcomes and successes perhaps deterministic can you share a little bit about your perspective on the role of synthetic data and how bullish you are on it yeah I mean i s like synthetic data is something like we have to solve one way or another so it's not about like whether you know you're bullish or not it's kind of it's an obstacle but we have just find a way around like uh we will run out of data like you know we there is so much data like humans can produce and also like it's important that this system start taking actions they start learning from their own mistakes um so we need to just find a way to make like synthetic data work um now what people have done is that they've tried like the most um I guess like naive approach where you just like take the models they produce something and you try to just like train on that and um of course like you know they've seen that there's mod mode collapsing and uh this just like doesn't work out of the box but you know new methods never work out of the box just like uh need need to invest in it and just like take your time and you know really kind of think of what's the best way of um of doing it so I'm really optimistic that we'll just definitely find ways to improve these models and I think that like actually there there is a number of of methods out there like um the two star and the equivalence that just you know in the new world where people don't really say their research breakthroughs uh the way they used to is probably hidden behind like some company Trade Secrets I'm going to ask about reasoning and you know novel scientific discoveries uh do you think that that can kind of naturally come out of just scaling llms if you have enough data or do you think that kind of like the ability to reason and you know come up with net new ideas requires kind of doing reinforcement learning and and uh you know deeper compute at inference time so I think like you need reinforc learn to get bit reasoning because the distribution of like it's it's it's also about the distribution of data right like you you have like a you have a lot of data out in the wild in the internet but uh at the same time you don't always have like the right type of data so you don't have the data where like someone reasons and they just like explain the reasoning in detailed you have some of it you have like uh and it's incredible that like the meth the the the models have actually managed to uh to pick it up and just imidate it but uh if you want to just like improve on that uh capability then you need to do that through reinforcement learning you need to just like uh show the model how this kind of emerging capability can f them be improved by just like have it generating the data interact with the environment you know just tell it when it's doing something right and when it's not doing something right so yeah I think like reforce learning is definitely part of uh the answer for that Alpha go Alpha zero and mu zero are the most powerful agents we've ever built can you share a little bit about how some of the lessons and learnings unlocked from and that are relevant to how we're pursuing building AI agents today yeah so I think like alphao and M you know have um they've actually fundamentally transformed our approach to AI agents because they highlight the the the importance of planning and scale in my opinion that um if you actually look at the charts of like different uh models and how they scale you can see that like alphao and Alpha zero were like kind of really ahead of their time like they were kind of outliers you had like uh this um the sces of like how compute scaled and then you have like Alpha zero like somewhere standing on its own so it show that like if you can scale and you can really push on that then you can get like incredible incredible results at the same time you know it also showed that you don't have just only train you can also like you know have better performance during inference during uh test during evaluation but just like uh using planning and I think that this is something that we start seeing more and more um in the in the near future or like this methods would just like start thinking more like planning more before they're just making any decisions so I'd say that like uh this is more of the heritage of alpha go and Alpha Zer and U zero it's the the the the basic principles and the basic principles are of that scale matters planning matters uh these methods can uh really solve problems that we thought that are insanely complex or like you know beyond what we can solve on our own um similar problems with the ones that we actually observed today with these large language models are things that we saw back then like back in 2016 we actually saw that these models can hallucinate or that like at the same time they're also creative that they will just come up with solutions that uh we hadn't thought of but they can also like have blind spots or like hallucinate or uh be susceptible to kind of like addressal doxs which I guess like everyone knows now that this NE Network suffer from so I think that like these are the the the main kind of um lessons uh drawn from this line of work what do you think are the biggest open questions from this line of work uh for the field Daner going forward so the main question is we had like Alpha and we just like Ms to have like this insanely robust and reliable systems that will just always play go and at the you know the highest possible kind of level and they'll just like achieve consistently they will just like be top of the leaderboard will just like never lose again so half a Go Master actually like played against 60 people um in online matches and just like Lally won in every single one of them so there's like no there there was like this patters were like conet robust reliable and I think like this is exactly what we're missing now with this llm Bas stents sometimes they get it sometimes they don't you cannot trust them they we just like uh you know you have like some amazing demos but like you know they happen once every two times even or like once every 10 times you have like something amazing and the remaining nine they just lost their way and didn't do anything so I think like what we need to do is just find a way to just make these llm based agents equally robust uh to the ones that we had with alha go and muo and Alpha zero this is like the the new open question of like how do you actually do that we'd love to move into some of your thoughts on the broader ecosystem today um you've touched on a few really core problems that people are working on right now um one the the data wall um problem that will hit eventually perhaps by 2028 or so um as some folks predict another being the idea of planning um um as an area that AI agents need to get better at and then um uh you know a third idea that you just described was around robustness and reliability can you share a little bit about maybe some of these areas that you think the whole field needs to solve um that that you are most excited about to help us unlock this vision of really getting to the AI agents that we want yeah I mean I just like also add another one to the list so I think like another major uh another major um challenge is like how do we improve the in Contex learning capabilities of these models or like you know how do they how do you make sure that like these systems can learn uh on the Fly and how they can adapt to new context like quickly so this is like another thing that I think it's going to be really important it's going to happen the next uh few year couple of years actually so Yiannis what's the term that you used for that in context learning oh in context learning yeah in context learning yeah so it's the idea that A system can actually learn how to do a new task with like few short prompting uh like it kind of like sees a few examples and on the Fly uh it kind of like lears how to adapt the new environment it learns how to use the the the the new tools that were provided to it or like it's kind of like lens it's it's not just all the knowledge it is stored in sus but like it can also like acquire new Lo knowledge by just like interacting with the real world interacting with the environment so I think that this is like another um place where uh there is a lot of work happening at the moment and uh going to have like amazing progress in the in the next couple of years and I'm really excited about that so yeah I mean uh to rec up I think like planning is important um you know in context learning is important and you know relability so the best way to achieve reliability is just like ensure that this models somehow know how to return from their mistakes so if they just like made a mistake somewhere they can just like see that and they're like okay you know I made a mistake I'll just like correct for it the way that uh humans you know make mistakes all the time but like we you know you can correct for them so these are like the three areas which uh I'm really I'm really excited to see progress on now that you've kind of embarked on your own entrepreneurial Journey uh how do you think about the areas where startups can compete against the the big research labs and like um how do you kind of motivate yourself for that for that Journey yeah I mean uh it's a compet like a it's a new world for me but at the same time it's not that new because when I joined deep mind it was literally a startup so and I was like literally in the first St G emploees so I actually like saw that uh firstand but you know one of the benefits of like working for a startup is that uh you know the agility and the focus so everyone really cares everyone just moves really fast and there's like a clear focus on what we want to build so the building is like what's the most important important kind of motivation for people like just like building and I think like this is one of the big advantages that like startups have over more established businesses at the same time you know it's easier to just like to adapt to new findings new technologies uh you're not kind of like tied to you know some pre-existing Solutions or like U some products that you know you don't want to deprecate because like you know they bring a lot of Revenue to you while if you're a startup you know you have like no such chains you can just like move fast and you know be Innovative and uh just you know break conventions um and at the same time just like allows you to leverage like open source resources things that are out of touch for like the big labs and uh yeah and you don't have like the red tape that like big places tend to have I love the term that you use sometimes you honest main quest versus side quest yeah it's the idea of like having a main focus like you know in big places in big labs they have like many different uh projects that like people are working on and uh it usually happens that they have like the main quest the main you know thing that like everyone is working on and there's like many multiple like smaller side quests that uh the idea is just like feed into the the bigger Quest but like usually they don't get as much uh they don't get like as many resources or like as many as much Focus from like the leadership so yeah they they tend to yeah the trophy um in the broader field what are some of the most defining projects that you admire the most and maybe who are some of the most influential researchers that you admire the most yeah absolutely so so I actually like started my AI research Journey back in 2012 and I've actually like seen some Milestones so I just like I give a list of like what I think are like the main Milestones like in AI in the past like 12 years I've been around so the first one I say it's like Alex net this is uh the the first paper that kind of like showed that deep learning is the is the answer I mean back then it didn't feel like it it just like felt like you know a kind of curiosity but like now I think that most people are convinced that like deep learning is part of the answer then it was uh tqn I had the pleasure to actually work on tqn and just like see it firstand how it started um it was actually developed by a friend of mine Vlad me vlat me and it was like the first system that showed that you can actually combine de learning with reinforced learning to to to achieve super to achieve Human Performance or like super human performance in really complex environments then um it was Alpha go again I was like really uh likey to just like walk on that and it showed that you know scale and planning are really important ingredients and if you just like do that right then you get huge success in a incredibly complex environment um Al fold another one uh this is Again by Deep Mind it show that like these methods are not just like things that you can use to solve games but they have um they they actually will make this Vault a better place they'll just like ensure that healthc care is improved that uh scientific discoveries uh are being real realized uh that we'll just like make sure this world is a better place by using AI then uh just uh it kind of like brought AI to uh everyone just like made it accessible uh to the broad audience like everyone knows what AI is now it's uh it has made my life of explaining my job much easier so um and finally CHP to 4 and I think that uh yeah probably CHP to 4 is like the latest kind of big advancement uh in AI because it kind of like showed that you know AR special general intelligence is a matter of years it's within reach um yeah we we are getting there like I think that you know many most people now believe that we like a few years away from like Ai and yeah that's uh that's because of like the incredible breakthrough that uh gp4 was uh now in terms of like some people I really admire uh before I forget so i' say fast like David silver he's uh he was my PhD supervisor he was my Metro at Deep Mind uh he's an incredibly researcher uh he walked he led Al go and Al zero and you know he is you know he he has a guilding dedication to the field of uh reinforce learning and he's you know probably the one of the smartest people or maybe the smartest person I know and amazing guy um in amazing rece learning uh engineer and the second one I'd say is uh Elia s and you know he was a co-founder at open I had the opportunity to work with him just a little bit in the really early days of alphao but um I think like his commitment to scaling I methods and pushing the boundaries of what the systems can achieve is remarkable and you know he kind of made sure that like G 3 and G 4 happen so um yeah immense respect towards him thank you for sharing that let's close out with some rapid fire questions and maybe first what do you think will be the next big milestones in AI let's say in the next one five and 10 years so I think like uh the next five to 10 years um the world will be a different place I actually really believe that I think that um in the next few years we'll see models becoming powerful and reliable agents that can actually independently execute tasks and uh I think that AI agents will be massively adopted across Industries especially in science and Healthcare so in that sense I'm really excited on what's Happ coming what's coming uh in Ai and you know what I'm most exciting about excited about is um a AI agents uh systems can actually like do TKS uh for you and you know this is exactly what we're building at Perfection and what year do you think we'll pass the 50% threshold on S bench so I think we're are one to three years away from the 50% threshold for S agents in 3 to five years from achieving 90% so the reason is while progress is amazing I think like we still need um reliable agent to hit this milestones and uh it's really when it comes to research it's like hard to make precise uh predictions when do you think we'll hit the data wall for scaling llms and do you think all the research in RL is mature enough to to keep up our slope of progress or do you think there will be a bit of a lull um as we try to figure out what happens when we hit the wall so I think like uh the W you know based on like what I've read I think like we have at least one more year uh for text just like before we hit the wall and then we have like these extra modalities which might actually buy us uh maybe a year extra and I think uh we are in a really good place just like start uh using synthetic data uh so in the next few years we'll just like figure out the synthetic data problem so I think that we won't really hit a wall just like we'll hit the wall but like no one realized it because we have like new methods in place do you think llm will have their Alpha go moment um and if so when I think like llms had their Alpha go moment with the initial release of chpt where they showed G the power and the progress made over the past decade I feel like what they hadn't had yet is their Alpha zero mod and uh that's the moment where more compute directly translates to increased intelligence without human intervision and I think like this breakthrough is still on the horizon when you think that will happen um I think it's going Happ the next five years wow amazing Giannis thank you so much for joining us and taking us through the awesome history of alpha go Alpha zero muzero your own journey through deep mind and then many of the core research problems that the whole industry is tackling today around data and building for reliability robustness and planning and in context learning um were really excited for the future that you're helping us build and um uh that you're pushing forward the in the field as well so thank you so much yianis thank you so much for having me [Music] [Music]

========================================

--- Video 32 ---
Video ID: ZfyzMsVKwXU
URL: https://www.youtube.com/watch?v=ZfyzMsVKwXU
Title: Nubank ft. David Vélez: An Outsider Upends the Brazilian Banking System
Published: 2025-01-23 10:00:37 UTC
Description:
When Nubank started 10 years ago, a few big banks in Brazil had a stranglehold on the largest economy in Latin America: they controlled nearly all the market share, and imposed some of the highest fees and worst banking terms in the world. David Vélez was an unlikely character to challenge the system: an outsider from Colombia and Costa Rica with a Stanford MBA, David was working at Sequoia with the goal of investing in Latin American companies. When the realization struck that they couldn’t find any companies they wanted to invest in, David set out to start one himself. What followed is a literal David vs. Goliath story of epic proportions. David and co-founders Cristina Junqueira and Edward Wible explain how Nubank survived competitors' attempts to crush them, and became the largest Latin American neobank, with over 100 million customers across three countries.

Host: Roelof Botha
Featuring: David Vélez, Cristina Junqueira, Edward Wible, Doug Leone

Transcript Language: English (auto-generated)
it was our end of year celebration and I had to speak to entire company part of our culture was treating everybody like an owner and a partner and that means being very transparent with our employees the the natural thing to do was to tell everybody don't worry everything is fine we're all good let's celebrate let's have some drinks we're going to figure it out the reality is we had no idea how we were going to figure this out what was consistent with our values was to say this is real we're GNA work really hard over the weekend and figure out what to do but right now I don't know how we're going to solve it welcome to Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world's most remarkable companies I'm your host and the managing partner of sequa capital rof buam in 2013 David vales set out to upend the Brazilian banking system where big Banks charged among the highest fees and interest rates in the world his was to democratize financial services for those with no good options new bank is the story of an underdog team determined to disrupt an oligopoly in a geography most would have bet against by delivering on a consumer experience unimaginable to most Brazilians David and his co-founders faced Crucible moments as they navigated a highly regulated industry persevered as competitors conspired to shut them down all while cultivating rabid customer [Music] love my name is David bis and I'm the founder and CEO of new bank I was born in Colombia and I lived in Colombia until I was eight Colombia in the 80s and early 90s was going through a very tumultuous time lot of violence drug cartels and so my family decided to leave the country and and was lucky enough to end up in Costa Rica my dad has 11 siblings and they're all entrepreneurs they all started their own business so I grew up working with my dad in his Factory he had a Boton Factory I spent a couple of Summers working there saving a little bit of money there was definitely an entrepreneurial ethos inside the family that supported being Your Own Boss there was this sense of freedom and autonomy that entrepreneurship and having your own business brought I looked up to Stanford and Silicon Valley as the place where the big companies got started and where some of the best entrepreneurs operated and so my own dream was to try to make it somehow to Stanford nowbody from my school in Costa Rica had go to Stanford but that was a dream of mine of trying to figure out how to end it up there and and was lucky to be able to go to Stanford and studied engineering there uh did my undergraduate at Stanford worked in finance in New York for a couple of years in financial services and private equity and then went back to business school at Stanford and I was ready to use my two years at Business School to finally start a business I felt a bit disappointed that during undergrad and even after undergrad I hadn't decided to go on my own and I wanted to use business school to do exactly that have two years to focus on figuring out what I was going to do but a couple of weeks into the first quarter of business school a friend of mine from my class told me that seoa was staring to look at Latin America and Doug Leon wanted to meet me we were looking for someone to help lead our efforts in Latin America we had just expanded to India and China and as we looked around the globe and you think of the word brick Brazil was a key part of that my name is Doug Leone I am a partner at SEO Capital the natural place to look was at Harvard and Stanford and so I interviewed the seconde students at Harvard and at Stanford they were terrific young men and women but I just couldn't tell them AP part they they look like the same single type of person and so I I had given up on that channel the following year my now son-in-law David George said there is a a lion a tiger who has just started a business school and so I got to De I co-called him I invited him to Sequoia there are people you interview where you look at you watch it's 20 minutes 30 minutes and Mike meritz my partner calls him half pages meaning you have a half page of notes and that's all you can come up with well the conversation with David ran way over he was a great communicator The Experience he had the Investments he made and the Investments he didn't make the ability to articulate the issue the drive he was not only a half pager he was a two-pager or three- pager and as I was going down the stairs of the seoa office by the time I get into my car I already had a email from Michael morit telling me David come back I want to meet you so I went back and met Mike and spent another hour with him and it was a phenomenal conversation and then ended up with an offer from Soria to join them as a part-time interm to help them figure out if we should open an office in Brazil and seoa Latin America basically he worked full-time in business school and he worked full-time at Sequoia we used to figure out when his classes were for the two days classes he didn't have classes we would fly down to Brazil fly right back time zones didn't matter I mean he was working day and night and then as soon as he graduated we flew to Brazil and we opened a shared office think of a we were kind of office and we started getting rolling and I would fly to Brazil once twice a month and we'd look at companies and we made a couple Investments but through those travels we learned there wasn't a lot of original technology in Brazil every tech company started with we are the soand so of Brazil you pick your favorite us company we are the Uber of Brazil we're the doores of Brazil and so it was a lot of me to investing and why we liked the founders the vibrancy that we saw in Far Lima Street where lots of young people would meet with their laptops open sharing ideas we were questioning whether the Market was going to be a thriving market and we had a seminal trip where we invited another partner Jim gets to join us we wanted a fresh pair of eyes and at the end of that trip it was fairly obvious based on what we saw and on a conversation we had Among Us that we probably were not going to open a Brazil office we came back to Sequoia we had the conversation and now we had to communicate to David valz it was a very tough moment I remember it was right around my birthday in 2012 in October that doc was supposed to come and visit with a number of people of SEO and he called me and said David you know what this is not going to happen it doesn't make sense to open a full office in Brazil why don't you come back to zako and let's figure out what to do and initially was a big shock uh because there was a big change of plans I've been working for two years to set up this office in Brazil it was going to be a great move for Brazil and for Latin America to have somebody like seoa investing in the region for the entire ecosystem but the reality is that a lot of the startups that we were seeing at that moment they were just not that interesting it was a very sudden call from Doc who told me we have decided not to do it anymore I was very straight with him no sugar coating anything he has a career he's a young man a man with respect and we told him one we would hire you in California all day long you want to come work in California you have a job offer the proposal was why don't you move back to California David and you help us do growth Equity internationally maybe you'll get to invest in Latin America but also you look at Asia and other parts of the world but already at that point I I was very clear that I did not want to invest anymore and I did not want to be in California he told us s he believes in Latin America for the next 50 years that ended up being the window that created the opportunity for me to finally become an entrepreneur it was a very honest point of view which allowed me to not waste any time doing something that ultimately was not going to be successful it made it easier for me to finally pursue what I really wanted to pursue which was starting my own business from scratch I spent about two months this was the end of 2012 figuring out what to do I was enamored with financial services I had spent a lot of my career in financial services and I had also felt the pain of financial services when I moved to Brazil I had to open a s called bank account and it was a horrible experience I had to go to one of the biggest branches in Far Lima which is like the center of financial services in Latin America and I was so surprised about how hard it was to get a simple bank account I had to go into this banking branch that had bulletproof doors there were a lot of armed guards that asked me to leave the branch and leave my backpack in a locker and then walk back in and wait 45 minutes for a branch manager to throw a bunch of paperwork at me and then start this process of 5 months trying to open up a simple bank account and there was so much anxiety and frustration and pure rage about how hard it was to get a simple bank account to then pay some of the highest fees and interest rates in the world that I didn't understand how it was possible that Brazilians were putting up with this how isn't anybody competing with these big Banks and offering better Solutions and I started talking to my friends in the space and my Brazilian friends and said you know what yeah these banks are horrible but you know what all the other banks are the same there are no any other Alternatives if you complain to your bank they're going to say well where else are you going to go and uh when you combined that consumer pain with everything that I had seen at seoa around smartphone penetration and Brazil becoming the social media capital of the universe as the W J called it there was an opportunity of reimagining a new Financial Services brand and a new bank that was fully digital that would pull the consumer in front and center and would create an incredible opportunity so after spending those two months digging in in this opportunity I got really excited about doing this and decided that I was going to focus on this idea of new bank which was a consumer obsessed digitally native bank for Brazilians so I remember going back to seoa with the idea once I had it laid out I had a deck specifically telling the story that there was this opportunity to disrupt the single biggest industry in Latin America so this was as big as it gets in terms of market cap and and Market size and so this definitely got seo's attention because it was not just that me too it was fundamentally rethinking an entire industry but I remember a conversation with ruoff when I sh him on the story and also in very critical candid feedback I remember drof telling me well David very interesting story there could be a really opportunity but you're not Brazilian you're Colombian you're not native speaking Portuguese you have never worked for a Brazilian bank you have never worked for a Brazilian credit card iser and you want to do credit cards you never done credit and you want to go do credit you don't have a local network of regulators you want to build a technology company but you're not a a computer scientist I remember this conversation well David was going up against five entrenched Banks some of the most powerful and influential companies in Brazil these Banks were operating essentially as an all lopoly and they would do everything in their power to stop a newcomer David's mountain of obstacles was steep the fact that he was an outsider would make the challenge even more daunting once he gave me that list I thought his conclusion was going to be therefore you should not do it and that was not it his conclusion was therefore you got to go and find a team that is very complimentary to you in nature and they're going to be feeling all of these different gaps and your single most important job right now is going to be building that team choosing the right team is a crucible moment because if you choose the right first people and they're A+ people they will bring like people along if you bring people that look I don't want to label people AB BC but for the point if you bring B people you're never going to recover for that b people don't bring a people you have to bring five brether you have to continue to average up the quality and the skill set of the people through the first 50 or 100 highers because you know for sure when you go from 100 to a th000 or 5,000 it is difficult to move the average up in a caliber people so it is not just important it is Paramount that the first few hires are nothing short of excellent after that meeting I was very laser focused in finding two co-founders that were going to be filling the biggest gaps that I had as a Founder which was first I was an outsider it's great to be an outsider but to execute I need an Insider that understood the banks from inside out that understood The Regulators that had the network that I didn't have and then the second big gap that I had was I wanted to build a technology company not a bank this meant technology had to be at the Forefront of our strategy so clearly we needed very strong technologists deid set out to find a winning team of co-founders but when he returned to seoa with his selections both raised eyebrows so both co-founders were conted in a number of different ways I was very young to begin with I was 30 years old I was barely out of business school I knew nothing about the Venture Capital industry I'm Christina Juna and I'm one of the co-founders and currently Chief growth officer at new bank so before meeting the VA had spent 5 years working for the largest income in bank here in Brazil and most of the time I was actually running the largest piece of their credit card business and I remember being blown away by the fact that nobody even talked about competitors and the customers perspective was actually very very low in the hardare of of priorities for them I remember spending the best part of my last year trying to change that from within I worked for for a long time in a project to design products that that would actually be interesting to people so that we could have much more of a pull type of demand in opposition of this push system and and after a year of work and and getting very close to starting a pilot that was completely shut down so I resigned then the and I met on a Thursday evening he told me how he was thinking about the financial system how he thought about technology and design and data dramatically changing how we do things that that just sounded like music to my ears but I I had no experience with tech companies whatso ever I had never worked outside Brazil and I had just met deid so he didn't have a lot of references and and he got some references on me like by talking to some people that I had worked with there were mixed reviews Christina who had worked on one of these big bangs truth be known her references were mixed and it was a tough call it was a 5545 call so there was a specific point of one of her former bosss that said if Christina was a state of matter between liquid solid and gas she would be gas because she's the type of people that expand completely inside a room and fills every single empty space I wouldn't take no for an answer I would challenge a lot what was being done in many different ways in a big company that's a negative because everybody feels threatened by that type of presence people feel like stick to your lane do your job don't get into my turf and so that was a net negative but as a entrepreneur that was a huge positive because there was so much work that needed to be done that we needed somebody that could expand and take a lot of space if that was possible and Chris is an athlete she's a DACA athlete she can do everything she could do marketing she can do consumer she can do customer service she can do product she can do Finance is one of these that you ask her of do she can do and she will expand to do it so Christina's pick was contrarian in that sense Ed was a contrarian pick in that he was not the traditional technology CTO or technology co-founder of course people looking at my CV and not seeing any Tech operating experience on it were were naturally skeptical my name is Edward wble I was a co-founder of new bank I was the CTO for a number of years and today I am a software engineer at new bank Edward had been an associate at Francisco Partners which is a tech private Equity Firm it has the word Tech in it but associate and a private Equity Firm isn't part of the spec it was clear when I met him that he was very smart but between very smart and lots of experience there's an abyss starting a a startup is risky taking risky bets on unproven Talent is even riskier and the multiplication of these things it just sounded imprudent unwise unadvisable but I think that what devid saw from the time we spent together was probably hunger just like extreme motivation to find a way to prove or to establish that that sort of a career was going to work for me to jump from a career like private Equity to operating in a startup environment was not necessarily a long-term dream of but it became increasingly clear as I was working in in a private Equity setting that I didn't really want to be managing a diversified portfolio of bets like I wanted to just completely fall in love with one of them and I wanted it to be mine Ed was a hacker he loved to code he loved technology he had an incredible work ethic and he's the type of people that we call in s new bank a human learning machine if on Friday he doesn't know how to do something by Monday he had just read 10 books and he had downloaded so much information that he will know how to do it but I remember that Edward as a co-founder almost got fired during our first board meeting the very first board meeting after having raised the seed we had a slide with the team and Ed appeared as co-founder with his resume and dog immediately pushed back on that profile and I had to say hey dog by the way Eddie's right here next to me I don't know if you knew that so so Ed always tells that he almost got fired in his first poor bidding cuz he was not a traditional pick as a co-founder for me it sort of didn't matter right like I was in it to prove and to prove to myself more than anybody that this could be my new career so the fact that somebody said something or other was was very secondary but I can only imagine how hard that would have been for deid to maintain the conviction that he was making the Choice when a lot of people around him were saying how risky that must be but he trusted me to execute on the plan we built together and really buy into what we needed to achieve the deal that we made with d and Edward fine we'd love to have you join us but we had a gentleman at Sequoia Capital called Bill Korn Bill had run engineering at Google for many years and we agreed that we would have two-way Bill and Edward or four-way conversation with David and I and just have a look at the recommendation that Edward was making on how to build the platform dog asked Bill corn to Shadow ad to grill ad to ask a lot of questions on aded along the path and every time that bill grilled aded Bill ended up being confident with some of the decisions that we were making everything Edward recommended was approved by Bill Bill confirmed these are absolutely the right choices and the more these meetings we had the more we felt confident about Edward and quite frankly the less we needed Bill Korn and the more we started being extremely confident on the choices that the V can make his sniffer was terrific and so the first two hires which are critical in these companies were A+ moves there's no other way to say it they were terrific terrific choices and that's what the started to show us and me at sooya that he had real talent for understanding what the company need finding people and leaning the right way in choosing the right people ready to begin work on new bank the trio of Founders settled into their first office it was a dingy house in a Suburban neighborhood and Chris found a way to actually let me live upstairs in the house I definitely have memories of people arriving at the house and saying this isn't what I expected I thought you were going to be creating a bank that would be competing with the likes of the the giant legendary banks in Brazil we ended up with Engineers who were not super preoccupied with perks and Comforts and the bells and whistles we we got folks that were anxious to prove that there's a better way to build systems there's a better way to make a bank like this can't be the best that we can do so by around April and May in 2013 we had raised our seat around from SEO and CK and Christina Edward and then a few early Engineers were on board and then we had our specific goal of we need to launch our first MVP our first product which is going to be a credit card fully managed by the smartphone app there were many reasons why we decided to start with a credit card the main one was actually because it was one of the few products that we could do without being a regulated entity Brazil had strict laws around foreign back companies providing banking services getting a bank license to offer checking and savings accounts was nearly impossible for a startup credit cards which were unregulated would be the company's way in or so the team thought we set up a timeline of around 12 months to have the first products up and running the first customers up and run however a couple of months into the entire process a new regulation in financial services in Brazil came in that regulated issuing credit cards they created this new framework called the payment institution and now credit card issuers would be payment institutions and for that they would need to be regulated they would need to be authorized by the Central Bank of Brazil and the new regulation basically said that we had to be up and running by April 2014 and if we were not up and R by April 2014 we were going to first need to ask for a license to then start operating and that meant effectively waiting two years to get that license that would have been the death because we were not going to be able to raise more Capital we only have the seed that seed would have been exhausted and that effectively meant a life or dead decision either we were operating four months earlier than we expected or we were done Crucible moments that bring these kinds of constraints can actually have benefits they demand focus and creativity they force you to operate at a higher level than you thought possible when you're given an impossible type of deadline I have no doubt there are constant doubts in the ability to meet that deadline but first it takes the courage to say that we can do it it takes the foresight to come up with plans where you question everything and you everything from first principles and you just go go go I don't think anybody got terrified I think everybody got crystallized everybody got motivated everybody got even more focused there was no debate it was easier to debate the initial timeline when we thought we had until fall 2014 now there was no debate we had to do it every day counted every day counted we couldn't afford to miss a day here a day there because those things add up and they would certainly put us out of the safe Zone in terms of the Finish Line we were always asking why why why if somebody told us not that cannot be doable or we cannot do it in two weeks we would ask why five or six times till we found the real bottleneck there was on one end the engineering team all hands on deck building the Integrations and the system that we needed to build Ed was leading a lot of the charge he was working 20 hour days he would take a couple of naps up in the second floor and then come back down with everybody else in terms of technical decision making the regulation helped a lot with prioritization and trade-offs and just decision-making what is the absolute minimum that needs to work in order for us to claim active operations does it absolutely need to be in and if not it's out so some of the decisions became very simple and maybe they weren't the ones that I really wanted to pick but it helped us to find that alignment and that Clarity and to Galvanize the engineering team and getting people just really fired up about this sort of Do or Die moment Chris and I were not coding so we were trying to remove all the different bottlenecks and the single biggest bottleneck was integration with MasterCard as a credit card iser and MasterCard has timelines that are generally designed for larger organizations that have 12 24 months to get something done we didn't have 12 or 24 months to get anything done so this meant that we had to go to MasterCard and every time they told us well I was going to get 30 days to get back on you we said no no no no no we need an answer in 7 days just to give you a specific example there was specific process of approval where we needed to get a yes from MasterCard in about 15 days to be on track on our timeline and three out of those 15 days was sending assigned paper via the mail to MasterCards headquarters in Belgium so we started realizing that it's going to be faster for us to actually fly to Belgium and hand delivered that paperwork that was going to save us 48 hours I'm like I'm taking this to Belgium like I'll get on a plane I'll show up there and I'll hand deliver the envelope they were going to fend that like what you and the massacre guys were like no you're not allowed to I'm like are you telling me that you're okay receiving a random FedEx guy with an envelope at your Center but if I show up there with the same envelope you won't take me they're like yes that's exactly what we're saying like how you know how can that possibly be true they didn't have a process to be able to process act piece of paper and anybody receiving that so ultimately they said no no no hold on don't do that it's okay just send it via FedEx and as soon as we receive it there are a couple things here that we can move so that we match your timeline long story short I didn't go to Belgium but this was the level of restlessness that we needed to take on to be able to get to that Finish Line every single day mattered and we were just willing to do anything and everything that that required so that was just the very first of many lessons on why he pays off off for you to aim really high and be aggressive but you know what we got through it we made it just weeks before the new regulation would have forced new bank to shutter the company launched to the public the first transactions with a new Bank credit card were made on April 1st 2014 but the purple card designed to stand out in a sea of gray and silver ones didn't immediately catch on initially we actually were a little bit disappointed about the level of of interest in the market I remember that we expected that there was going to be a lot of interest among the University students community and we actually went to the universities and we talked to students and nobody had any interest in getting a new credit card they all had the credit cards of their parents and why would I want a new credit card and where are the miles and I already have miles so they wanted rewards and our credit card had no rewards initially so in the first couple of months there was not as much interest as we expected but then after a few months there was one publication a very nich online publication that gave glowing reviews about the entire process of getting a card without fees all in the smartphone and that describe the experience of being magical and that day we get around 5,000 customers and then the following day we got like 10,000 customers and then by the end of that month we were getting 10 20 30 40,000 customers because we were getting so much interest we had to create a weight list because we didn't have the team we didn't have the customer service to say yes to everybody and that weight list created even more interest this very oddl looking purple credit card became this aspirational product that everybody wanted and not everybody could have so we start growing from there to 100,000 a million customers we had always decided that the foundational value was going to be we wanted customer to love us fanatically so the entire experience the customer service everything was designed to get customers to love the product which was insane considering the emotion that Banks elicited in consumers in Brazil at that moment was hate or rage or anxiety or frustration it was the opposite emotion we wanted custumers to love us fanatically let me tell you about the culture of new bank there was one month that we noticed that customers had become suddenly more profitable and we went looking why did it come more profitable and the reason they became more profitable is we forgot to send them a letter that payment was due so a number of customers had missed their payments thereby own us more money so they became more profitable so what should we do I can tell you in most cases the banks would say wow we should stop sending letters and make every customer more profitable at new bank we did the exact opposite we sent a letter apologizing to the customers we had left out apologizing that we neglected to send them a letter reminding them and we reversed their charges I can tell you we're probably the only financial services company in the world that would have done that I will also tell you that those customers that got that letter with an apologies of not getting the original letter under which we were no obligation to send and seeing the charges reverse are probably customers for life but that is the culture of new bank that is the culture of the vi valz internally externally how we treat employees and how we treat customers and that is a unique Advantage for this company and it shows you the character of the man new Bank's customer loyalty would prove critical when just a few years later another regulatory disaster struck this time the proposed change was the result of lobbying efforts by established Banks looking to smother the new competition in 2016 we had a pretty big scare with a potential regulatory change I remember on a Friday morning I wake up and I realized that suddenly there was going to be a change in regulation one more change in regulation where the credit card in Brazil we had 27 days to pay the merchants so if somebody used the credit card to buy a TV we as a credit card iser have 27 days to pay that Merchant for that TV This legal threat was something that would change paying the merchant 27 days later to paying the merchant one day later and when we made the calculations that meant that we're going to need billions of working capital overnight that was really a life or death moment for us sure we were a little bit of a bigger company than we were in 2013 when we started from scratch but we were still very much uh a startup we were still printing losses we were still living off of our own camp far from being break even and it would certainly put us out of business or Force us into fles selling the company to somebody else I remember it was our end of year celebration and I had to speak to entire company part of our culture was around treating everybody like an owner and a partner and that means being very transparent with our employees so in a way this culture value was being tested here the natural thing to do was to tell everybody don't worry everything is fine we're all good let's celebrate let's have some drinks we're going to figure it out the reality is we had no idea how we were going to figure this out what was consistent with our values was to say this is real we're going to work really hard over the weekend and figure out what to do but right now I don't know how we're going to solve it of course after the first few minutes in which we're freaked out we came into problem solving mode and we were working the regulators and working the press and talking to different stakeholders to educate people on the the catastrophic effect that that regulation would mean if it were to take place we says listen this will end the little competition there is for these five big Banks because this will require so much capital for Earnie entrance that is going to make it impossible for any startup to enter this space this is going to be end of competition in Brazil this is going to be end of new bank the five big banks that own 85% of the market they're going to just get stronger and stronger and stronger at some point news broke out that newbank could go out of business because till then we were trying to work the Press work The Regulators but off the records we didn't want to scare people in such way but at some point somebody just broke that and it was just all over the place and the bank could go out of business what we saw was heartwarming what we saw was like customers taking social media taking the internet to stand by us and to to ask for the executive branch and especially the central bank to not let that happen because this was the first competition that they had seen in years in decades in the financial system and for the first time they were being well treated by a company and they absolutely did not want that to go away tens of thousand of consumers telling the Central Bank of Brazil you cannot do this change you cannot end new bank you got to support competition very organic bottom up Grassroots mobilization of consumer saying stop don't do this this doesn't make any sense it was in a way a beautiful moment of people coming together to Stand By Us by around noon of Monday we get a call from the president of the Central Bank of Brazil telling us just come over let's talk so we go to the central bank and sit down with the president of the Central Bank and he clearly sees us coming into the room with a lot of like Challenge and preoccupation and concern in our faces and the first thing he tells Christina and Isaac relax this is not happening we're not making this change this is not going to happen and we were like wo amazing great we're saved all good and so we were able to go back to the office and in a big old hands we tell the company relax the change is not going to happen and it was a great Crucible moment because it was once again a moment of survival where this threat made us Focus this threat made us clarify what we needed to do we stayed true to our values we treated our employees as partners even though it was hard and it was amazing to see these consumers these millions of consumers becoming ultimately our biggest Defenders doing something that was great for Consumer meant our biggest defense against any natural changes in the market environment so it was a very foundational moment for all of us this moment marked a turning point in the company's relationship with Regulators from inception there had been a question of how new bank would be viewed by the government we did not know if Regulators were viewers as champions of consumers that can bring the price of banking service is down or our enemies the disruptors of the five or six large companies to which The Regulators might have been tied were pleasantly surprised that it was the former from that point on we embarked on a strategy to stay very close to Regulators to apprise them what we're doing because they seem to be huge fans of new bank mostly because we don't want to get Crosswire with them there's no reason why we should now work closely with Regulators to make sure we do everything right and we can run as fast as possible and help consumers all over the world bring down the cost of banking we wanted to be the kid in the front row of the class that has all the answers and that gets an A+ in every test I think that's very different from a lot of other startups in other spaces that are regulated because a lot of entrepreneurs want to operate a business in a space that regulated doing everything possible without being regulated and almost going against the regulator we took the opposite approach of that we said we're going to be regulated but let's excel at being regulated let's see regulation as an area where we can develop comparative advantage working closely with Regulators in 2017 new bank received its banking license allowing the company to expand from credit cards to debit and savings account accounts the scale and scope of their ambition continued to expand shortly after becoming a multi-product company new bank became a multinational one today we have regulated licenses in Brazil in Mexico in Colombia we maintain very good relationship with regulator that makes us influential since they know that we stand for consumers on one end and that we get a pluses on the other they listen to us they want to hear what we think they want to hear what we expect and it's a situation of partnership where we are partner with a regulator and not a serome game after expanding to two International markets Mexico in 2019 and Colombia in 2020 new bank began to consider going public we knew that becoming a public company was in our future because we certainly weren't going to sell the company to to any other player so that was for sure So eventually the only possible track was becoming public we decided it was time to go public one to shore up the balance sheet and second to make sure the world knew we real we were safe we're a place you can park your money the IPO for us was a financing event and a branding event branding to Wall Street and branding to our customer base we also knew something else we knew that this is always a very important moment in a life of a company and we knew that we needed to do this our way meaning we needed to bring our customers with us we've always been such a customer oriented company we've always felt like customers were such an important piece of this they were the center of everything they were the reason why we were doing this in the first place so it wouldn't make sense for us to go public without having our customers play a role in it new bank decided to launch a directed share program called new socios the program would allow existing customers to buy shares in the IPO and also award shares to new customers who started banking with the company so we designed a program to allow many millions of customers to become our partners we knew that this was going to bring some trade-offs especially on the time side because we needed to take the time to build the infrastructure that would allow for that we had to make sure that the Investments platform was ready to have customers a investing and B investing in the new bank IPO on the platform on the day combined with all the other things that have to go right for the IPO there's no way to really get that dialed and experiment your way gradually into it it's sort of a big bang Thing by nature and that makes even good Engineers nervous right even confident Engineers nervous the company had planned to go public in September 2021 but due to the intricacies of the platform needed to support noos the company failed to meet this timeline the September IPO started getting pushed out first to October then to November and now we're in the month of December and we were told the systems were in pretty good shape but were they perfect well we knew it would take another quarter to get them exactly to where we wanted I remember there being a sort of go no go moment when we were planning the IPO and negotiating around how many months can we afford to delay versus at one point has all the work that's gone into the IPO sort of getting stale and we're sort of exiting the window that was planned for and everything because of systems I was a proponent of pushing very hard even at the expense of this program I was willing to scrap the program to get the company public the markets are fickle and they're subject to overnight change we've seen overnight change in the markets we have lived through 08 and so I was eager to get the company public not because I saw something happen in January but I sensed that things could change at any time for whatever reason one of the things that we talked about was like listen the stars are aligned the market is there there's a window there's no major political disruption in Latin America we're doing well investors are except like God knows when this is going to happen again so we knew it was good timing and that we should take that opportunity because that window may not open open again the team decided to move forward with the IPO just before the year's end but they refused to scrap the newus program they bet that their systems would work I think that's a common compromise is that you sort of end up going to market with the product you have maybe not the product you wish you had given other constraints we were just willing to do that because otherwise it wouldn't be the same it wouldn't be us we wouldn't be true to what we stand for so sure it made a lot of people nervous he made a lot of people really question our judgment but we believe it was the right thing to do we decided the systems were good enough and we planed a stake on the ground that one way or another we were going to go public in 2021 and by the way the program we install for the stock buyers worked and the systems didn't crash we got right under the wire we raised a few billion dollars the IPO was well received the stock ran up immediately after the IPO and then they crash it new banin public on December 9th 2021 but the Euphoria quickly wore off in January US Stocks tumbled plunging the economy into a bare Market that would continue through October 2022 the funny thing about the moment of our IPO is that I usually tell people that it was one of those Indiana Jones moments in which there's this big door like closing just SL fting by right before it closes and that was us that was us becoming public at the end of 2021 because this big crash came and the window was closed we did get lucky in going out before and that after and I think we got lucky in being well funded and able to be on the front foot at a time when many other companies were distracted and stressed about financial concerns that they hadn't managed to resolve before the markets fell apart the crash and thankfully we were not credible we had money in the bank but then we had to deal with a stock price that every company had to deal with that went from a high level of 12 or $13 a share to $3 to $4 a share and what that does to morale and everything it does affect the morale of people and we had to endure that which we did in flying colors because while the stock was really a three or four we kept announcing one better quarter than the other and as I told to V it's only a matter of time until Wall Street wakes up and says look at this Gem of a company they're beating earnings and they're increasing guidance and so let's keep on doing what we do and the stock will take care of itself and so far it has the stock is close to $15 a share the market cap is about $775 billion and I happen to think that the future for years to come is very bright for this company I'm a mama three and I'm pregnant with my fourth child and coincidentally my first child I tell people that she's new bank's twin because she was born the month that we launched the company my second child was born when we launched Mexico and my third child was born right after Dio I rang the bell I was 8 months pregnant in New York and I'm pregnant now so let's see what the next big thing is going to be behind in product and strategy that we've executed lies actually a very simple Insight which is people want to have business with companies that treat them well and we should treat people like we would want to be treated by others and so we just kind of executed that Vision through the past 10 years it's been amazing to see the speed at which everything has happened putting us in a position today a decade after as the most valuable financial services company in Latin America if anybody would have told me 10 years ago that we would have reached that level I would have thought it was really impossible new bank began with a big vision and big purpose of challenging effectively the largest and most powerful companies in Latin America it was a very low probability Journey we had to work over several years against a lot of skepticism and a lot of the negativity around what we were trying to do but as we think back again over a decade on the big thesis of new bank this is a global thesis this is not a Latin American thesis this is not a Brazilian thesis Financial Services globally is the single biggest industry yet to be disrupted technology companies have only really been able to make a little dent in this market size there is over6 trillion dollar in in value there are several billion consumers that haven't been banked across the world and another several billion customers that are completely overpaying in terms of fees and interest expense and they're just not being treated well by their Banks and so what we feel very good about we've done over the past decade we are feel very humbled and energized and hungry around the next decade and taking this model really internationally into very more countries so we are very excited about being able to take this journey and and just feel that we're just at the beginning we're really not at the end at all we're just at the beginning of this great opportunity that we have ahead this has been Crucible moments a podcast from sequa [Music] Capital Crucible moments is produced by the Epic stories and Vox creative podcast teams along with sequa capital special thanks to David vales Christina junier Edward wble and Doug Leone for sharing their stories [Music]

========================================

--- Video 33 ---
Video ID: 7rrLY1bwQ6g
URL: https://www.youtube.com/watch?v=7rrLY1bwQ6g
Title: Turning Graph AI into ROI ft Kumo’s Hema Raghavan
Published: 2025-01-21 10:00:07 UTC
Description:
Hema Raghavan is co-founder of Kumo, a company that makes graph neural networks accessible to enterprises by connecting to their relational data stored in Snowflake and Databricks. Hema talks about how running GNNs on GPUs has led to breakthroughs in performance as well as the query language Kumo developed to help companies predict future data points. Although approachable for non-technical users, the product provides full control for data scientists who use Kumo to automate time-consuming feature engineering pipelines.

Hosted by: Konstantine Buhler and Sonya Huang, Sequoia Capital

Transcript Language: English (auto-generated)
if you have your data laid out as relational tables a Kuma just sucks it in so we just you just specify and you know through connectors uh tell Kuma what your schema is and then you can just start writing predictive queries so the graph is abstracted away but if you have someone like a data scientist who loves tweaking the neural network paros in case Constantine is be interested guilty exactly you can look under the hood and uh the analogy we always use is we'll give you the self-driving car but if you want to look under the hood or if you want to drive stick we'll let you drive stick [Music] we have a brilliant guest today on trading data welcome HMA ragavan co-founder and head of engineering at Kumo AI heo brings Decades of experience leading AI initiatives at LinkedIn she came up with the people you may know technology and other core features that Leverage The Power of graph learning her journey in AI predates many of the Technologies we all take for granted today she was working on NLP before Bert was even a thing with Kumo HMA and her team are revolutionizing how companies harness AI by making Advanced graph neural networks these neural networks let you do auto onl Automated machine learning on any platform from Snowflake to data bricks Kumo's Innovative approach allows companies to leverage their existing data warehouses in order to build sophisticated AI models faster cheaper easier you don't require the Deep expertise in graph learning or maintaining complex features you can just go straight to business value welcome HMA to training data today we have the amazing HMA ragavan you are building Kumo AI which is automl on the data warehouse using Advanced neural networks and graph neural networks automl was incredibly promising a few years ago it was a major Trend in the last wave of AI five six years ago it went through a little bit of a trough of disillusionment a lot of the ml players uh receded from uh from the Forefront and companies started to store their features and in future databases and the like why are you focusing on automl what's different about Kumo okay so there's automl and then there's autom ML on gpus and I think that's the big difference for Kumo doai and let me give you a little bit of an example from my own career so I started in NLP and when we would build systems back in the early 2000s to answer a question like when did Marco Polo land in Asia we would be encoding features like Marco Polo is the subject of the sentence and it's going to be the subject of the answer and all of that so we had to know a lot about language about linguistic structure and so on and then the GPU Revolution came that enabled neural networks to come at the Forefront of this technology we don't write features like that anymore those intermediate layers in a neural network really learn the parts of speech the named entities all of those properties of language it's the same in other classes of problems so in uh the class the automl that was happening maybe a decade ago we were looking at CPU based models so think of logistic regression think of XG boost svms and so on and all automl did then was parallelize what a data scientist would have done which was a lot of hand computed features and that required you to be you had to write code to think like a data scientist so you were trying to get the machines to think like humans whereas here what we're doing is we've uh we use graph neural network so it's a neural network technology and you can think of to GNN as a superset of uh CNN which is used for images or a sequence model which is used for languages gnn's you know allow for arbitrary structure and the gnn's are learning all of the features that you would normally use for prediction predictive problems so Kumo sits in the space of predictive Ai and we're really bringing Transformer technology to predictive AI problems can you say you mentioned graph neural network and you gave a great explanation can you explain it to me like I'm 5 years old because that might be where my level of understanding is like our graph neural networks's good for any class of problem is it good for you know you came from LinkedIn where you were working on you know the the social graph of LinkedIn is it is it good for specific types of of domains that's a great question Sonia so let's say you're going to put this podcast episode out and you know it's going to be on some uh video streaming site and we want you know uh to recommend the relevant podcasts for you know users of that video streaming site YouTube YouTube to be explicit someone's watching this on YouTube you want to recommend someone to watch it or not exactly so user logs in and uh you you not only have the content of the this podcast episode but you also have what you might have watched in the past so you can think of that the records of what you watched in the past is sitting in a views table uh collaborative filtering era exactly exactly but the difference with collaborative filtering is it's just looking at views Y how can we take you know the view data so the view data's Network so coming to Sonia's question right there's there's a podcast episode there's all the users who are watching it so you got it in terms of uh you know you have a bir directional graph the users and the podcasts but then you have the organization you have seoa Capital you have the channels from seoa Capital you have other metadata that you may have in so all of that can lend itself naturally to a graph and start thinking about links across these you know nodes of a graph and effectively what a graph neural network is learning is let's look at what Sonia watched in the past it seems like she uh really likes Ai Ai and baby shark videos at this moment okay so a Ai and baby shark okay but uh and then uh the neural network also learns that Constantine likes Ai and what would it be for you probably AI That's that might Ai and um like history that's great so that's Ai and history right so the neural network can learn that there's an overlap between both of you on the AI pieces of cont content you both engage a lot with seoa uh content and it's learning across this network right but the next time Sonia watches a baby shark video we don't want to be recommending that to Constantine right so how do you take that content that you engage with the view data The Click data that you're engaging with and learn across all of these edges think of clicks views every all your behavioral signal that you engage with with entities in this world as a graph and how do we learn across that graph yeah so you don't have to be a social network to have a graph everyone uh almost every Enterprise I know has a graph fch has graphs because they have customers they have transactions they have related data think of uh uh you know one of your delivery services they have uh uh the inventory the suppliers the the the means of transportation you have so they're all sitting as tables they're all sitting as entities and they're all linked across each other learning lets you learn across that's a pretty key Insight the tables before we go there that was a very smart 5-year-old I think that you have a 5-year-old I have an eight-year-old eighty old and a 12year old a 12y old okay well they're very very smart if they understood that explanation like what would be to Sonia's point if you were if you were five and you we're going to say graph learning versus any other type of machine learning what's the difference ah graph learning versus machine learning uh easy fast I think those would be the two things just you know uh low code I think that would be the key about Kumo that it learns all the weights it learns all the features and discovers them over time is that fair to say EXA exactly and my 8-year-old doesn't know machine learning but if you were going to write uh uh the uh you know a classifier the old school way yes you'd be writing uh features that say Okay users in this platform uh uh we need to look at click-through rate data for the last three months and six months and eight months for every single video and we discover that Sonia has a preference for data that's for videos that are evergreen so six Monon Windows really m matter for Sonia so imagine all of that code being written as features graph neural networks eliminate all of that code do you think that means Feature Feature engineering goes away as a discipline or what happens to it I think feature engineering goes away and that's not a bad thing as such because uh prior to uh Kumo I was at LinkedIn for uh almost seven uh close to eight years and data scientists love finding opportunities for the business to make value right and it doesn't mean that feature engineering is the place where you spend that you know that that's the uh the time well spent you would much rather try out end different models on end different parts of the app or whatever your business is and and drive value so trying out models in different parts of your application is where a data scientist needs to spend time we started this episode you you said you know automl on gpus is different from automl yes and so what about gpus specifically makes what you are describing possible like was it even possible to do this on on a CPU or is it just is it faster now or what's different than that you're doing on gpus yeah that that's a great question so uh it's definitely possible it's much slower right so it's very similar to what neural networks brought to the text and image image uh spaces in that we can scale these models to you know uh large amounts of data and uh while these models existed before the GPU Revolution it's we can actually take an entire Enterprises like fintech data and learn graph neural networks for them yeah in the previous era of automl so much of the juice in the performance came out of ensembles so you do these logistic regressions or you do do these svms or what have you and then you'd Ensemble them together frankly in the kaggle era which was how I first met your co-founder Yuri and the data science era of of kaggle and the like always the ensembles won even in the Netflix prize back in the day it was the ensembles that won and there was something to the fact that these ensembles are just tons of little algorithms chained together and what is a neural network but tons of little algorithms chained together I mean you could consider it billions of sigmoids or billions of logistic regressions and really the way I see graph neural networks is you're able to discover the the features and The Ensemble that you chain together to actually optimize towards the solution so it's the to me graphs are the most General data type yeah and a graph neural network is the most General you kind of you mentioned it's a generalization where even a Transformer is a subset of this generalization the most General type of algorithm that can do some learning yeah absolutely and uh as you mentioned ensembles something that struck me was try maintaining that in production you have n different feature generation pipelines and an ensemble and I've seen a world where you'd have one frontend engineer change how we were logging the view data yeah and everything either needed to change or some you know one pipeline breaks and it's all done and it's it's mess to debug so you uh so graphs give you a simple elegant uh framework to get get at the same outcome it also reminds me a lot more of our brain yes right our brain kind we think operates like a like a graph and is forming and pruning connections more like a graph even more so than a more structured neural network yeah and so have you ever have you guys experimented or or thought about that as an analogy and and any ideas of of the pros and cons of that analogy I think uh it's very similar to uh uh the way I think about it is let let's go back to that video watching example right and with if I think of Sonia as a note in uh in a graph and uh what these neural network algorithms are really good at is learning these embedding representations right and on this big graph which has so with her preference for baby shark and uh uh AI or her household's preference exactly makes more sense that that checks out H your embedding Vector would be pretty close to uh both AI so you're close to Constantine but you're also close in uh ukian space or you know in some big end dimensional space to uh the all the baby shark loving folks right and Bas we're basically learning these representations so people or these all the entities in the graph like even seoa capital in that case becomes a you know uh a representation so in that sense it's the idea is very similar but what gnn's do and uh is allow for arbitrary structure and that's where I think it's a lot closer to to the human brain but I because I don't think the human brain is wired as a linear SE quence or as a GD as an images yeah could you say a word about how it works under the hood like how are you able to let's say you go and work with I don't know a food delivery service how how does it actually work for you to go and kind of you know automatically learn this graph representation and how are you training models on that or pick YouTube given people might be watching it there and we're talking about AI baby shark in history already exactly so um so there's two pieces Mr Kum uh historically uh graph learning has been uh uh restricted to I want to say phds in graph learning yeah because it was it's not easy to view the world as a graph people think in terms of uh relational data that's the most common data layout in companies and that's largely because of uh uh the analytics Revolution that proceeded did the AI Revolution so everyone thinks in terms of relational data but really relational data and graphs are have a onetoone mapping because you have data laid out in tables usually an entity a primary key in a table and then you have all these relationships primary key foreign key relationships which encode uh uh the edges in a graph yep so that that automatic construction from a table layout to a graph layout is one of the Innovations inside C the other bit is we've uh invented a language called predictive query language and the language is allows you to specify any machine learning problem in a few lines that looks very much like SQL so think of sequel with the predict Clause so we've created this very simple abstraction layer on top of relational data warehouses there's already a universe of people who's who are writing SQL queries and we've created a language that appeals or you know is one on that resonates with them in some sense so that's one of the Innovations of ko the other one is running these uh graph neural networks so once you go from relational to graph just running graph Neal networks at scale and that again is something that has not been easy to do there are few companies in the world that uh can do it and it usually takes a huge infrastructure team to build that out and because graphs inherently like unlike databases where you can think of some logical partitioning grph it's not a matrix all entangled in so how do you split it across uh different machines with limited memory and so on so that's uh so all of these bits coming together makes Kumo uh easy to use but that's it so when we go to a company uh like you know a YouTube like company yeah we'll often uh talk to a data science team that is looking to get faster uh return on investment in AI yep and uh but then Kumo becomes really easy to do because if you have your data laid out as relational tables a Kumo just it in so we just you just specify in you know through connectors uh tell Kuma what your schema is and then you can just start writing predictive queries so the graph is abstracted away but if you have someone like a data scientist who loves tweaking the neural network parameters in case Constantine isct guilty exactly you can look under the hood and uh the analogy we always use is is we'll give you the self-driving car but if you want to look under the hood or if you want to drive stick we'll let you drive stick so so concretely in the YouTube example yeah historically if I was an analytics at YouTube and I'm watching this video I can look and say hey query all AI there would be some some tagging or some system to understand all AI historically let's see what the trends are over time that's quering the past yes what you're saying is once you have this in this database in the structure you're able to predict how people are going to watch AI videos in the next several weeks how many are going to watch baby shark videos uh how much are they going to spend what is going to be their monetization what are their ads what else can you do with this so you can say uh is this user going to churn for example yep right and then you can say what's the most relevant video that I want to show this user in order to retain them on my platform right so I want to drive value for my business uh given the past videos that they've watched what's the next uh video to watch and so on and we can also do demand forecasting so uh we have customers in uh in fact we have uh in in the healthcare sector and they use scho to forecast uh demand so that they're they're well stocked on their emergency room right so the applications of using Kumo uh go from consumer to Healthcare to fintech where fintech we see uh applications in fraud for example MH uh just uh is this user's Behavior suspicious should we flag the user so on and so forth so think of any question which says how much I love the use of query the future how much uh is this event going to happen uh is uh uh uh uh what's the next best action for this user from an action space those are all the kinds of questions that Kumo can help answer and I love that you you said analyst because uh Kumo aims to be as automl as you want it to be and you know we but we also have a python interface so you know you want to be a neural network expert you can go all in it's the brain the brain it's the analytical brain out of the applications you discussed just now I would imagine you know there's such classical ml problems that you discuss each of them probably has a five person fraud team and a 15 person demand forecasting team what do those ml people think when you know when Kumo is pitching the company like I walk me through that spiritual journey and and are you actually able to Dem to to get results out of the box that are better than a 15% personam maintaining it can can do I'm okay with it as long as they don't have VC prediction so I uh for a lot of the companies we work with uh the data scientist is excited about Kumo um as I mentioned writing those feature engineering pipelines comes with maintenance jobs to maintain those pipelines and that's not where they want to spend their time data scientists in most companies are incentivized with Direct business impact so did I push that atct model out uh this quarter did it drive uh x% Revenue so a lot of our customers will come to us and say you know what I signed up for x% Revenue MH but I'm only onethird of the way there can you guys you know uh help us accelerate and we do a 4-week uh po so you you know we and within 4 weeks we'll uh almost always I'm trying to think of a case when we've not shown value but and I can't remember one but we've uh we've always shown value within those four weeks wow yeah so you convert them into into Believers into Believers yeah and it's about where you want to spend your time right so I think once they get Hands-On product uh um many times people will come in and say oh but feature engineer ing is where I spend all my time right and how can you say that I don't have to do it manually anymore but uh we'll remind them we'll remind them of the NLP journey and then we'll also remind them once they get H Hands-On keyboard with the product and they realize that the journey in Kumo it's not completely automated away right because we say a data scientist knows their business well so if you're going to uh Define CH prediction for your business Maybe on YouTube uh activity around in the last 30 days is a good predictor of churn so you know you want to bring your events table with a 30day window schema the actual structure that you the schema or the window right so because these are all queries and these are all parameters in the queries or you could play with 90day or 365 Day activities so these are all queries you can write five of these queries and say oh really on my system the best predictor of churn is behavior in a 365 day window and I didn't even know that because I spending all my time looking somewhere else so the data scientist spends a lot more time finding the relevant tables in their organization that are going to you know bring value and then finding those uh the right query formulation or right the right business formulation in this case you know for example churn what's the right definition of churn for my business and once they see that actually they really they realize that this is a lot more fun than what I was doing before totally Yeah you mentioned tables structured data schema that naturally leads me to think about Snowflake and data bricks a lot of companies have spent the last 5 years heavily investing in their their data warehouses uh how how do you work with the daa warehouses okay that that's a great question so um um at the outset we have we started as a purely SAS company you know uh emulating a lot of you know the the principles from the snowflake architecture looking at uh their success stories uh one thing we realized is that data scientists though they need to see value on their own problem because they're so kpi or business impact focused showing them value on uh a cattle data set doesn't really count mhm so the easiest way to uh show value is of course when they can connect to their own data but connecting to your own data on a SAS product means you go through a huge Security review through the company which can in some many organizations can take you know a couple of months so that uh uh we wanted to reduce that fraction and we started partnering with the warehouse houses to think about uh deployment models where compute can be closer to the data and uh we have a deployment with snowflake which is part we use a combination of what is called snow park container services and really Kumo can deploy as a container in snowflakes compute pool so from a data scientist point of view we're also a native app in Snowflake so you a data scientist in uh an organization let's say YouTube can go in and uh click install snowflake so it's like an app on your iPhone it gets installed and then they can start writing those predictive queries and looking for value and often times the security team is completely okay with it because there's no data leaving the ecosystem we have a very similar deployment model with uh data brecks uh though in that case we manage the GPU compute and but data residency stays completely inside data brecks and that uh also let us so we started that from the point of view of uh you know get getting letting data scientists get hands on keyboard with you know uh with K more quickly but we also realize that freed us up a lot to not have to think about security compliance governance and let the data warehouses as they're already building all of the tools and technology for you know uh for management of of data let it stay there let it be managed there but KOMO just uh you know talks to data directly sitting inside the warehouse so you talked about uh relational versus CFT data and relational data is kind of how many of our brains have been taught to think yes uh we think about things in spreadsheets oftentimes we might go down and say uh if we have a series of AI videos you have them as rows and then you have some descriptors of them as columns but really what you start to see things as graphs which I did frankly back in the day around Yuri's time as a as a professor um I think everything starts you can start to see everything as a graph yes it's the most General data type and when you start to see things as graphs it's actually kind of how our brain thinks yes hey here's a video and that has a pointed uh characteristic that's some other part of the graph which is connected to another part how do you ingest all of this relational data which is the way that the world has been run for the way computers have been run for 50 years and put them into a graph structure sounds like a very heavy lift and doing that inside of snowflake and data bricks is probably pretty hard uh I want to say that's part of the magic of KOMO right and that's that was the uh friction that prevented graph learning from taking off and it staying within and uh you know uh the big companies yes exactly the the few people who could hire these individuals but really it is a question of we have a UNIF a unified schema that the you know uh that's the graph schema and looking at uh the relational schema we're able to identify what the entities are so in that YouTube example it's a video ID it's a user ID it may be a channel ID and so on and often those are primary keys and then it's a lot of I want to say sequel like code that runs under the hood and uh it could or I want to see spark like code that runs under the hood that converts this data to uh the graph format I see I see it makes sense and uh and beyond that uh once we get to the graph there's an edge index so we store all of the edges in a very com you know a proprietary and compressed uh format and then we distribute out the nodes okay because now because we realized that edes gpus distribute them to gpus or to to CPUs because we wanted to keep costs low so we keep uh the we only reserve the gpus for training right so when we are doing the learning but we store the edges in what we call the graph engine and then we have a column store where we store the features so we can bring in arbitrary features that represent the users right so uh everything about Sonia that we can infer we we're not constrained by memory that just horizontally scales everything on the CPU machine horizontally SC scales and we're only using the gpus for message passing cool yeah amazing in fact yeah so that uh that uh that has also reduced costs for our customers and they're often surprised that we can run graph learning at the scale that we do um at the costs that we do you've made the comparison to large language models a couple times yeah um I'd be remiss not to ask like what are the connections between your graph world and the llm world and and are there you know are there synergies between the two absolutely what a great question uh and so many synergies so let's take the example of this podcast which will get uh generated it's going to get transcribed by an llm it's going to uh it's uh uh so you have all of the summaries it you you uh you have all of the the semantic information that will come from the large language models right a graph neural network can actually take all of those features that you know uh the semantic representation that uh is inferred for this particular video as a node feature and what the GNN is learning is it's learning across all of the interactions that one may now let me uh give you another example and uh this uh uh we have a demo of this uh up on our website or on our LinkedIn Channel but uh an example would be a lot of people think of the llm Revolution as creating chat Bots okay so let's say you come to a clothing store and you uh are searching for yellow summer dresses so you search yellow summer dresses all the time yes and you're not a logged logged in user and uh uh the llm is going to probably get you a really good set of uh things that look like yellow summer dresses but if you were a logged in user and we knew all of that information about the the kind of interactions that you'd had in the past we can actually use Kumo's predictions to inform the llm so think of rag and think of Kumo predictions as uh feeding a rag algorithm to ground its truth to be closer to what is personalized so you can do that as well so there is the bringing in features but then there is also uh they're complimentary because Kumo brings you all of that personalization based on all that behavioral data that the app has which the llm doesn't take into consideration you mentioned Rag and we've talked about graphs graph rag is having a moment yes in AI right now in general yes thoughts on graph rag which is different from our from our approach of Kumo but thoughts on graph graph Rag and then also how it differs from using a graph neural network to do certain inferences tied to yeah some sort of rag yeah so graph rag is a lot s uh closer to uh what we just talked about but many organizations may have knowledge graphs um and that's another entire field of studies in in graphs uh think medical domains for example you have uh all of your uh Insurance codes and uh how the insurance codes connect with each other you have symptoms you have all of that there there's a lot of knowledge uh bases sitting out there with interconnected nodes graph frag allows you to ground your llm output in uh the the uh answers that are answered from these kind of knowledge graphs right so instead of going to a search and index so you can think of rag as going to a search index a knowledge graph a recommender system like Kumo and uh uh bring making the llm output more uh uh or hallucinate less I want to ask about explainable AI one of the things we've been discussing in Prior episodes of the show is you know these llms will we ever be able to understand how they think um and I remember the anthropic results were were really interesting um how do you think about explainability as it comes to Kumo's models that's such a great question Sonia because for the kinds of problems we work with and the customers that we worked with we this was another area we had to um actually develop a solution because we have customers in Insurance in healthc care and they often need to understand why a recommended output was recommended to them yes you want to know that we didn't uh the model didn't over rotate on uh you know uh race color ethnicity and so on and so forth right and uh so it became table Stakes for us to actually solve this problem and uh at Kumo we've uh innovated by actually uh developing an algorithm which after the uh the training part of the algorithm looks at the graph and looks at the gradient algorithm and can come down at the table level to say these were the tables that were were used these are the columns that were used and uh we have some early results that show that we can even come at an instance level and predict you know here's an instance the score why Sonia was recommended that video is high because of these uh specific features wow so uh it was stable Stakes just given the domain we were going into yeah yeah so we talked a lot about Ai and graph learning uh you have been in the AI space for a long time HMA can you tell us a little bit about what you developed at LinkedIn and specifically what AI growth was at LinkedIn maybe if you can how graph neural networks helped there yeah and the types of challenges that you dealt with really really large scale operationalizing AI yeah that's a great question um so I joined LinkedIn uh just a couple of years after the IPO and AI was making its way into various products um I joined the the growth team and the team I Le first led was the people you may know team and people you may know is all about graphs it's about uh uh large scale graphs and about uh uh and the amazing thing about LinkedIn was how closely tied people you may know because it's social network was to our core uh consumer metrics M so I was I had come to LinkedIn as an AI researcher and I suddenly found myself uh responsible for uh one of its core kpis which was sessions and monthly active users and by then I'd also started uh owning notifications which was a huge part of uh the growth uh ecosystem at uh LinkedIn and and along the way we had to uh start operationalizing uh uh Ai and before mlops became a word we were actually thinking about hey how do you from when the from the time when you deploy a model how do you measure how do you AB test and then how do you maintain a model in production we would see models degrade in production we would see uh uh why is that by the way why did you see that so frequently if the if the graph wasn't losing nodes or edges why would it degrade over time because depending on your business problem and the kind of behavior yeah Behavior change right uh people behave on LinkedIn in the New Year very differently uh from summer break right so creating those pipelines which do auto training yeah those all those all became very important and then uh when you're talking about scal that was an interesting problem as well because I started at LinkedIn when we were I think about 400 million members and then it it was rapidly growing so that's when we had to start thinking about infrastructure and the fact that uh uh the CPU based algorithms uh we can't just keep horizontally scaling them so what would be more efficient ways to run AI models in production um graph Neal networks now at LinkedIn and I want to say uh there's been an amazing team that took it Forward after I left as well it took them about four to five years to build in many many many engineers and but now it Powers everything from the ads to the feed to uh uh jobs and so on this they published a paper yeah recently about it so you guys the founders there's three founders at Kumo you were senior AI leads at at LinkedIn at Airbnb at Pinterest all those are massive scale also really sophisticated yeah that can be I think intimidating for smaller companies that have problems and say wait this is a champagne problem this is what the hyperscalers with hundreds of millions of users have and we don't have nearly the same problem is that true and what kinds of companies are not a good fit for graph learning that's a great question so um I think there are two things uh the first question is a is the reason why we ended up inventing predictive query language because what we needed to do was create a platform that was super easy to use right and many of the other companies had few data scientists and large number of potential Avenues where they wanted to bring AI so often times what we would get is hey we would love to be a LinkedIn an Airbnb or a Pinterest but we can't put so many people to it so giving them that uh easy to use interface and giving them that managed infrastructure at scale actually lets us get in but that said when is Kumo not a fit Kumo is not a fit if you're so early on that you haven't figured out your data landscape so sometimes we'll talk to customers who are super excited about Kumo but they haven't figured out how to measure the value of AI or sometimes we'll talk to customers and they're still in spreadsheets and they're moving to uh one of the warehouses so we'll say you know get the data layout uh settled it's like building a city right you've got to have your roads in the foundation first and then the vehicles come on it so so we'll wait and some and customers come back you know in a year or so but there's no category or type of problem it's more a data sophistication exactly I see yeah but you don't have to be so far along the sophistication curve you don't have to be an Airbnb LinkedIn yeah it's table Stakes you know certain kpis that can be optimized by an algorithm yeah like you can quantify certain things a and then B you have access to that data and something that you can plug into like a data warehouse yeah and the way I'd look at uh the evolu of an organization in data is often that U they'll you of course have to know what uh your product Market fit is after that you start figuring out your data ecosystem you build your data ecosystem for analytics because now you've built a product you've got to start measuring what that product is doing and what is starting what the behavior is and that's when leaders usually start thinking about AI which is is okay now I know how to query the past uh but I now need to start bringing in AI to instrument the change that I need in the in the ecosystem I'd love to close with some questions about your V vision for the future M maybe you know you've been in AI for a long time you mentioned you were in NLP before before Bert was a thing um what are you most excited about in in AI most broadly um I think I'm most excited with very broadly about the productivity gains it's giving all of us uh I mean Kumo is one part of it but just how we write documents or how we uh uh or think about health right like if Health improves productivity improves for example if you're just using one of those uh uh uh Health apps that do monitoring but nud you to for Behavioral change that is better health is better productivity so what I'm most excited about is how we're going to evolve as a human race with all of this productivity Gams what about technically what what features or approaches or algorithms or venues you think are going to be most interesting I've always found the big Innovations come at the intersection of hardware and software and I think while gpus were invented for Graphics there there's probably something more that H has to happen on the you know processor side so that you can scale these graph neural networks or neural networks further make U models uh maybe less expensive so um I'm looking forward to that technically what about your vision for the future of Kumo like what can we expect to come out of the product in the in the future so in terms of Kumo's Vision I'm actually really excited about the kinds of apps people are going to build with Kumo we're starting to see people Plum Kumo with Lang chain and pine cone and put together apps like the one we talked about like the you know a chat agent that uh recommends for you the uh the yellow summer dresses right so I'm very excited about the the top layer of applications that are going to get built on top of Kumo and what that's going to power so hey Mo one of the star features about you you're incredibly technically deep but you also are really good at culture if you talk to anyone at Kumo there's basically no regrettable turn ever and you guys hire some of the best phds in the world in ml and certainly in graph ml how do you do that what have you done to make the kuma culture exceptional and to have so much retention within your team I was at a leadership training once and we had to think about what our true north was and I realized that and the true north uh concept defines your true north which is a value as who you are but what you know the kinds of problems you solve either in your work and what you bring to the table and for me it's always uh about empowering people to do more than what they think they can so it's common to empower people to do what they to get to full potential but it's those aha moments like wow I built this so you hire a smart team you get them I think good managers step away but have their eye on how the team is operating and you get people to innovate get people to own what they're building so just see that Vision so you want to be able to hire people whether it was LinkedIn where the value was economic opportunity or at Kumo where the value is about uh uh building a you AI platform that makes AI so easy to use it's about when you bring smart people together that rally together on the same value that's when magic happens and my job is to just let the magic happen heyo why is that passion around relational data you could have you could have done we've talked about this sometimes in the past you could have used graph graph learning as a different type of architecture to do language models or you could have done graph learning to do any sort of any sort of AI right once you've figured out a scale how to do the generalization why can't you do the specifics by taking this big marble block and carving away all the nodes and edges until you get to a superior architecture you decided to do it on relational data why is that relational data usually is not the most exciting thing in the world for most people yeah but it is your life passion because nobody else was doing it and there's so much data in relational format and that was such a pain in our past jobs so uh I I feel like there's magic happening in the core area of NLP where you know I'm happy to see that Revolution and all the investment that's happening there people money and so on but there's this whole workload that's out there a whole set of data scientists that uh work on those workloads how do we bring that magic to them so it's really about uh you uh the opportunity and the past pain that each of us saw in our previous jobs okay I have one last question for the young constantines out there who are watching this episode on YouTube uh what advice do you have for you know aspiring AI Engineers who want to really make a dent in the field in the future oh she has a specific yellow dress recommendation I would actually say tools come and go languages come and go I know there's a lot about learning Python and uh you know uh taking the class on uh the latest deep learning but I would say don't skip your probability and uh a linear algebra classes because when it whatever method has been there in the last several decades it's always come down to core linear algebra and probability so don't skip those classes great and mine is there's a lot of graph enthusiasts out there yeah when do graph neural networks come main stage in the AI Revolution best guess for timeline I think we're getting there I was at a conference called kdd recently it's one of the big biggest data mining conferences a lot of academics lot of Industry folks and more than half the papers were ra neutral networks so I think we're sitting at that explosion it's it's going to happen thank you ha this was fantastic thank you Sonia and thank you Constantine it was lovely being here [Music] [Music]

========================================

--- Video 34 ---
Video ID: S_osYy3tsf8
URL: https://www.youtube.com/watch?v=S_osYy3tsf8
Title: Turning Academic Open Source into Startup Success ft Databricks Founder Ion Stoica
Published: 2025-01-14 10:00:22 UTC
Description:
Berkeley professor Ion Stoica, co-founder of Databricks and Anyscale, transformed the open source projects Spark and Ray into successful AI infrastructure companies. He talks about what mattered most for Databricks' success—the focus on making Spark win and making Databricks the best place to run Spark. He highlights the importance of striking key partnerships—the Microsoft partnership in particular that accelerated Databricks' growth and contributed to Spark's dominance among data scientists and AI engineers. He also shares his perspective on finding new problems to work on, which holds lessons for aspiring founders and builders: 1) building systems in new areas that, if widely adopted, put you in the best position to understand the new problem space, and 2) focusing on a problem that is more important tomorrow than today.

Hosted by: Stephanie Zhan and Sonya Huang, Sequoia Capital

Transcript Language: English (auto-generated)
in general this was our approach we are going to be aggressive also about Partnerships even though the partners could compete and overlap because you have to trust yourself that at least when it comes to spark you can build the best products who are you know kind of saying internally well you know if someone else is building a better product for spark then way we deserve to lose right so that's kind of always was the confidence that we can build the best product for spark and eventually um if spark wins we are going to [Music] win hi everyone welcome to training data today we're excited to welcome Yan stoa professor of computer science at UC Berkeley and co-founder of both data bricks in any scale he has a uniquely exceptional career as a leading professor and founder of companies at truly legendary scale today we dig into questions like data bricks positioning in AI how research projects like spark and Ray have led to the founding of data bricks in any scale how he ties his research projects closely to Industry from day one new projects out of his lab like VM mgpt LM CIS and vuna and what research Fields he's thinking about next Yan thank you so much for joining us today we're really excited to have you on the Pod to kick things off um we'd love to hear a little bit about where data bricks aspires to fit in the overall ecosystem especially with some of the recent launches what are you personally most excited about but thanks for having me here so um I think with with data breaks always we wanted to um provide the platform an inter platform um which helps our customers get most of the value uh out of their data uh and one of the best ways today to get value out of the data it's using this kind of new development in AI uh including lar language model and uh and everything else and the one thing is that to note that this was all always our vision from day one actually Spark um was created one of the main reason to create it m when build it it was to solve um to speed up uh classical machine learning algorithms right um and to scale them up yeah right and so in some sense for us it's full circle we started with with machine learning it's not classic machine learning and right now we are back and doing more and more AI uh uh because to take advantage and to create value out of the data so you mentioned that you know you founded around you know enabling classical machine learning how do you see the current AI moment as same and how do you see it as different um and I'm curious what are the specific things you're doing for this moment in time like the Mosaic acquisition things like that yeah so so certainly the momentum around AI it's it's on a different level today right it just look at the Investments out there right uh that should tell the you know um big part of the story um so I think that um obviously is like um what what the way we are looking at is that uh taking advantage and being successful is AI um is not is not easy right it's like the AI ecosystem is growing in complexity it's not just a simple call to a model right you need you have so many techniques now like Rag and raft and right to improve the accuracy of your application using AI right uh obviously everyone is excited about AI because it solve so many problems and there are so many you know generate so many headlines uh but but still it's it's not what you want you want a product of AI a lot of things still we are seeing today are fantastic demos right and demos are inspirational and when people see demos what you know whatever Char GPT Sol this kind of problems Olympics mat problem it's very easy to think that wow it doing that is going to do everything right so but going from demo to production it's a like I said it's a big step a demo means that you need to find an instance at least an instance to be really impressive right in production so it's like is there there exist such instance right but when you go from the demo to the product you when you have a product it has to work for all cages right so that's kind of the big gap so that's why where the effort is to improve the accuracy improve reliability of course eliminate hallucinations as much as you can uh and really also to find where it provides the best value you know for because you know you can apply AI to 1,000 use cases but which are the use cases which are going to provide you the biggest value I think that's what we are also trying to help our customers so to navigate that how to successfully apply AI to their product to their to improve their services to their business in that vein one of the most interesting things that I thought came with the datab brecks AI launch was the new open general purpose llm created by datab brex Deep s um what was the reasoning behind training your own models open- sourcing them and what do you think are some of the best use cases for that model yeah so I think the main if you look at like our main Market it's Enterprise and Enterprise customers um they do have uh a lot of concerns about data privacy confidentiality and obviously they kind of one control it's not only that they want auditability right they want to be able to audit um what data is used what results have been uh have been used and what decision have been made with what data the decision have been made and um so um that's why in in general the Enterprises everything being equal migrate towards the open source models we can host you know on their machines or in their VPC and so forth um so it's it's one of the reason for releasing the the braks is to help our customers and then the customers say you know in many of these cases they start from this model and then fine tune with their own data and uh um to optimize for their particular use case right so and it's again so our our Enterprise customers um they want in you know open source models they want kind of control as much visibility as they can uh um uh they want privacy and confidentiality especially in the you know in the recent light of you know of the brides which are widely published um the other thing is that you know the DNA of data bricks was has been open source it's it's not only spark but Delta and ml flow and and and many others yeah the other thing I thought was fascinating about deep Rex's its excellent programming abilities what do you think has made it such a capable code model especially even compared to something like code Lama 70b look I think it's about obviously it's about data and how you train it and um it's one thing I think you know we you know one of the major data breaks uh Advantage is Mosaic right we have also the entire infrastructure for training which we have not not only about the data but the entire infrastructure um of training and fine-tuning and that is make it much e easier and most cost effective to optimize uh models for different use cases and obviously um the co-pilot programmability um it's it's one of the very important use cases in this uh in Enterprises because software Engineers are still very expensive yes you know so making the people you have um and it's not it's not only about that it's like hiring top software engineering is very difficult yeah right um if you are a large company like maybe I don't know you know 4G or something like that um and so making those people productive it's it's it's extremely important and critical for their business you mentioned Mosaic as a key part of the strategy I guess what do you think are your most important chess pieces in this kind of AI Battleground um I imagine mic is one of them um and you know are most of your Enterprise customers looking to train their own models uh and and and how does you know how does Mosaic and your other Acquisitions fit in with your customer needs yeah so I think there are if it's like um there are a few Enterprises who would like which so it's again it's pre trining and then it's fine-tuning and using the using the model in your own on your own hard in your own VPC on your own machines you you know um you rent it you own um so and you may expect there are few which are doing pre-training but there are still a few doing one to pretaining on their data you have enough data a lot of them they want to do fine tuning right because look if you are an Enterprise right uh and a company right and you want to improve your business right what do you have what do you have and which other don't the think you have which other don't it's the data the data about your business about your users right right so therefore because this is something you have and other do not you want to take advantage of that right so how do you take advantage of that again there are many ways and you try different ways to do it right one is about fine tuning you use a data and you have an open source model you fine tune on that right um the other one to use uh rack right and and and like that um and but and and so you you are going to do any of those but then you want to do like like I said in your own VPC right to preserve security to have you know 104 security perimeter perimeter you want to do it in order to um protect confidentiality of your users and um obviously right now with gdpr the California consumer Privacy Act and so forth there all of Regulation and the the number of Regulation is going to increase so the the fact that you can having an open source models you can find tun you can use drag in your own VPC it's very compelling value proposition and are you finding that most of your customers want to go that approach versus you know open Ai and the Very powerful uh closed Source models on the market I think Enterprises and still we are still early on and um there are many Enterprises obviously using open AI for different use cages different applic different applications and um I'm sure open Ai and Microsoft aure will come with new products uh to provide better confidentiality better better better uh security um but at the end of the day what I was saying is that everything being equal right as a Enterprise I will prefer more control mhm security and strategic right it's like it's it's it's less kind of locking or something like that right so um so I think that's what what what what we are saying so if as as as open source models are going to catch up with uh the proprietary models in particular in the use cases which matter for the Enterprises it doesn't need to be you know perfect right for all the part use cases you just need to to be very competitive or it matters um then you know Enterprises will prefer uh these Solutions on which they have you know more control and more secure and how far away do you think we are from that moment like do you think we're there today we're all else is equal or when do you think we cross that dire in terms of the open source versus proprietary yeah open source being you know on par all else being equal for the core use cases um so you have a lot of use cases you know it's again it's the open source plus the data and you right now the application are more complex it's not just a call to a large language model you have this kind of you know what it called you know you compose you have you know application which build from diff many components and um that's what it called compound AI um and um so actually it it turns out that you know if you can um you know you you can build uh an applications for uh doing you know uh recommendation or something like that or or you know for programming for a for a particular copil or for a particular Tool uh you can actually do better than um uh even something like open AI on the latest at GPT because you have more data and the other things about about uh you know data breaks which um I think there also announcement about that is that um with a kind of unity catalog and and things like that you have access and you know also about the structure of the data which helps you tremendously to improve the accuracy of your applications right so it's not only the model right it's everything else you have around it and the quality of the data you you fed it in so I think that's uh it's it's it's a data stupid like you would say right like at the end of the day it sounds like control and security are two primary areas of of um things that enterprises really care about that you've noticed and and data bricks obviously has a tremendous Advantage with having access to data as well to help these compan use control and security for the customers exactly um how what about some other uh factors have you noticed that they also care about is how much does cost matter how much does diversity of other models matter I think um obviously cost is important and um it's like this right first of all you know you want to you know it's the initially the important thing is about the value yeah right can you provide the value right is like so that's the first thing and at that stage cost is not as important right uh and um and in in here actually in some of these early stages where people also try to use the most powerful model like open Ai and like that but once you cross that and you have a use case which um uh you conclude that is good add value to your business now you want to scale it up right and now you are talking about having more control control and more security and all of these things protect confidentiality of of the data privacy of your users and and so forth and now basically people consider how they are going to deploy it and this is where um having multiple choices and having like you said more control security it matters and where open source models and platforms like data breaks are um are very valuable right and again it's it's it's it's it's also all the other components which are together in data bre like I mentioned like like Unity catalog and everything else to increase the value of your applications yeah super interesting I'd love to talk about compound AI systems I think you guys probably coined or popularized the term and it seems like that's a lot of what the industry is latching on to now maybe for our audience can you explain what is a compound AI system and you know what are Enterprises thinking about when they're building these yeah so a compound system is it's basically consists of multiple components multiple calls to light language models or agents and um it's very much you can think about when you write a program you have multiple components you have different function procedures to do different things and then you then put them together to to create the program um uh the same is very similar here right you can can use um maybe one model um to pass the data to ex to extract the data you can use and then you can use for instance depending of the prompt you may use a model you know if the prompt is about Mass problems right it's like you can use one model if the prompt it's about maybe about uh programming you may use a different model right um and and then you may you may use for instance about formatting the the the result right that's another one you may use that for instance now more and more you are talking about agents and with agents you have to you know you call external uh services or functions like search or you can use a calculator and things like that so now you have different models you can do a better job and there are small models actually to take the prompt and convert it to a function call right so that's kind of what it is but the way to think about is like this the conceptual is like you write a program has different components and make it easier to develop um and deploy and manage the same thing you want to apply to AI applications so like a collection of smaller components that work together uh where the you know the some of the parts is is uh greater than than the one one monolith that you're placing I'd love to dive a little bit into the data Brick story quickly which I think is uh an incredible legendary journey over the last decade um but with a lot of nuances that I think maybe many folks don't yet understand from today looks like you were building the right company the right at the right place at the right time but the actual Nuance is that data bricks was originally started uh originally for data scientists it happened to uh cater well to machine learning workloads because of all the work that you were doing in data and over time you made the right strategic decisions to actually really grow with the AI Market can you share a little bit of some of the learnings and journey that got you to where data bricks was today yeah I I I I do think that um very happy to I do think that um a lot of a lot of it's also about being the right time the right place and things like that it's being lucky or I think that's all all of this is true um and there are many things needs to go right to be successful some of the things you control some of the things you do not control um when we started indeed um we are focusing to build uh a product a cloud product hostage product for data scientist right we have this kind of uh notebook and um and we have provided hostage spark and and and we we targeted data scientist and um we targeted data scientist um it was um one of the reason we targeted the data scientist because again we all have like I mentioned spark early on Al also targeting machine learning uh application machine learning workloads and at that time there were not many data scientists it was like 2013 however when you look around uh most of the universities already have data science uh programs right degrees you're starting to offer data science degrees and you're saying okay we are it seems a good you know pass forward um a good market and um I remember you're looking at LinkedIn to see how many data scientist are there because our users not not initially they not as many thousands um esec especially when you compare the data uh with uh database analy analyst and engineers and so forth uh he s to build and I think it was you know a reasonable you know good product and then uh you know we uh started to grow quickly in terms of the number of you know customers initially we had small customers and um uh but then you know after and and and still you know the interactive analysis the data science has been for a long time it's actually one of the uh biggest workloads uh in particular in terms of Revenue um uh because the interactive workloads are price higher than batch workloads um uh but and and I I remember that you know I sold to these companies to use you know uh data braks uh and they are aspirationally buying it to do data you know uh to the data science Ai and when um you know after a few months we go to them because we didn't go much you know earlier uh because saw that they're doing well they usage just growing and everything seems good so no no reason to worry so we went back to them and said you know to see what they are doing maybe you can write a blog post or whatever they doing right is like marketing and everything the surprise is that very few are doing actually machine learning at that time and we as them what what happens right or still well it turns out that obviously that in order to do machine learning you need data like we discussed early on and they realize that for the particular application they wanted they don't have the the data they need so they need to now to put maybe to add new logs you know to to collect new logs in their products and like that um and also they need to clean up the data uh you know curate the data and so forth so now the we are lucky because Spar is very good also for data engineering right for data processing right right it's a data procing tool at the end of the day and so they are using using using spark for for uh uh for data engineering um and and then obviously we we we start focusing also on serving data Engineers much more than before um and uh uh and you know that's how we started and and then obviously when later now you know God the you know data engineering right and uh still you know have all these data scientist exploring and starting to build models and then it is a very natural extension to start to add more products for our user for our customers like I mentioned to get more value out of their data and that meant building machine learning models and using the open source models very interesting I want to talk about the data bricks Microsoft partnership uh I think that was the stuff of Legends and I think still probably one of the only case studies of a successful um like truly transformative um uh partnership maybe walk us through what that partnership was um was it a bet the company moment at the time do you think datab bricks would have become what it became um if you hadn't struck that partnership maybe just talk about that look obviously the partnership with Microsoft was was was a great partnership for us we are um the one thing I want to say that in in and and this is most visible we are very uh from the day one we are actually very um focused on the Partnerships uh our idea was always you know like um we know we make um spark successful you know hopefully the facto standard for data processing and then we make uh data breaks best place to run spark so early on actually you know we we started you know a few months in the life of the company we had this kind of partnership with cloud data uh and and then we had with honor and this partnership was mainly because we are in you know to advance spark right because spark if it was created in Hado ecosystem right and this had the Hado companies right right and um so we we had this partnership or data stacks and so forth so this was like in the first year or so we had all this partnership despite the fact that um some of these companies we knew that they could become our competitors right because you know just uh making them is making and helping them um to um to deploy to manage and to sell spark Based Services right it's like um so so in some sense you you know uh the Microsoft is like it's it's um it fits our um uh kind of um approach of trying to you know to strike as much you know do Partnerships with other being aggressive doing partnership with other um you know organizations in the Eos system even though again in some cases it was not a clear cut whether we are you know they are going to compete with us or not right so very just growing the ecosystem and growing spark that was the priority um we even had at some point a partnership with snowflake so it was um very um it requires a lot of heavy lifted lifting so we always look for partnership which has which are meaningful right and I think great negotiation you know uh uh Ali and so forth did a fantastic job there um but at the end of the day we also need to commit to it it's like it it took to do to build the aure data breaks so we were an iws before that it took you know tens of engineer one year and you are small company at that point so it was a huge commitment and a huge bet also from our perspective to do that and um yeah and we uh you know I think engineering and everyone executed very well and it was a successful product right and um you know Microsoft was great Partners you know and um yeah this what happens we are obviously a bit lucky but in general this was our approach we are going to be aggressive also about Partnerships even though the partners could compete and overlap because you have to trust yourself that at least when it comes to spark you can build the best products we are you know kind of saying internally well you know if someone else is building a better product for spark than we we deserve to lose right so that's kind of always was the confidence that we can build the best product for spark and eventually um if spark wins we are going to win do you think that dat bricks would have become the company that is today uh without that Microsoft partnership I think so it maybe to to could have taken a little bit longer yeah but yeah I think so right we would have still have a good very good offering on aour um like we have with gcp would have taken a bit longer but um I don't see any fundamental change in Dynamics right because one of the advantage of data Fabrics of course is one spark W and we could provide the best product for spark we are in a very strong po position and compared with other clouds remember that one of our advantage like everyone else Advantage like confluent and so forth is that you can provide the service on multiple clouds right and multiple clouds is you know multic cloud has been you know more and more uh very strategic for especially for large Enterprises we do not want necessarily to be locked in or to want the want choice so yeah I love the confidence and conviction that you have in spark and your own execution abilities internally but that married with the practicality and aggressiveness of of winning as a business and and pursuing the right Partnerships and doing whatever it takes to win yeah yeah I mean you try to simplify things that's what I was saying you know know like initially when we said like look you know like we have to make spark Queen you know there are many I remember we look at all these combinations it's like it's like spark Queens the product fails right or spark loses but we have a product which is successful or both fail no that's not very interesting or both both are successful right and we convince ourselves that um you know we need to bet on spark to win because that's a most likely yeah way also for the product to win again for better or worse right but is sometimes you know there there could be many ways to success right and you in retrospect you cannot go back and you know and and and try other Alternatives maybe there are better Alternatives at that point but it's important to commit to un sync which and which hopefully it's it's a reasonably good solution a good pass forward right again there are many many passes to the peck right the most important is to commit one which leads to the peak right it's like not the shortest one not the easiest one it may not be but it has to be one to get there and to the highest and and that's why you said okay spark has to has to win right and we have to build you know the best need to be the best place or for spark and then we are saying you know to be the best place for data and AI we need to eventually we knew and we we assume that if you are to be hugely successful you are going to um uh you know to to go beyond spark right it's like that's why the name of the company data break is not spark Labs or something like that yeah um so so that's kind of you try to simplify you once you you you do that and you start to execute okay so you want to make Ray successful as an open source so you want everyone to use it so that's why you do cloud data and TR Horton ORS and so forth to do to uh you know to um to do to do this partnership because at that time there are other solution you know people knew that had do M red because um you know these times have passed so to speak so they are talking about new systems there are like TZ which actually honor as this project and so forth so that was very important and then for it was also for data science it was kind of when we bet is like it's a niche and you know we and we can we s that we can build the best product for it um so you know so so that's that's kind of you need and you need to have ultimately confidence in what you bet on right you have to bet right right because you are a small company right if you don't bet how you are going to win right everyone right and then you need to have some level of conviction right to do it and um and uh yeah I'd love to kind of pull on that thread and and switch gears a little bit into tying a lot of the entrepreneurial path that you have with a lot of the acad mic and research background that that were the roots for the beginnings of these companies you have a very unique career as both a leading professor and a founder of multiple unicorn and decacorn companies I don't think there's anyone who comes close to pursuing both disciplines at the scale of success that you have um maybe specifically you know Reay with any scale array to any scale spark to data bricks take us into your head what what is the process in which these research Fields start to ruminate in your mind when do you kind of continue to give them resources to develop and then when do you know that it's the time to then start a company to pursue that in in a better more uh open and uh fast way that's a that's a good but hard question um so um I think that um and by the way I like to pref to to to profess to you know to say that to start with saying that um it's obviously also a lot of luck involved and uh you know being in a place at B like berland having fantastic students and colleagues around you it's you couldn't couldn't do without that right it's like it's it's is their marit probably more than mine so um but one thing I think is that I've been always trying to focus um it's on the problem yeah right uh actually when um even to my students I'm telling them is like uh the most one of the most important things you need to do is to figure out what problems you are going to work on yeah right because it's like everyone which comes to b or the stop schools they are one thing they have in common they are good problem solvers right they have good grade you know good scores you know write papers paper is about solving a problem so therefore the if all of them are good Problem Problem Solver the differentiator is a problem you are working on right yes so you start with that and um also I think it's like um in the uh especially at at berlay um it's um you get you get exposed of kind of not only new ideas but willingness to take risk and get into new areas uh in some sense and this is what I like about berlay um if you look traditionally berlay um among top schools they are the first to open new areas is like of course the risk processor it was it was Standford as well um but databases networking yeah um sensor networks um even in open source with uh uh the Unix BSD um you know tcpip right part of the so they are always kind of more you know a little bit uh trying to experiment so I think that's kind of uh you know that's kind of culture I really resonated with it and then the other thing what happened is B we have these slabs um which are like fiveyear slabs which basically each lab has kind of a vision and uh you know it's a group of Faculty coming together which believe in that vision and with their students and try to make it happen over you know five you know five years and um this you know has a lot of great you know impact you know like all the way it started this tradition started 40 50 years ago with they patters on Randy cuts and others and um they built you know risk uh rate redundant areay of inexpensive disc now network of for station is commodity you know this is uh everyone is building now this huge cluster from commodity machines um and and and again and many more um so there are these kind of elements and these labs are it's a very strong relation with industry right connection they are funded since I came when I came actually saw one change happened before these labs are also supported by government in particular DARPA but it was that point in which um that kind of at least that particular DARPA funding dried yeah yeah so kind of when I came to bero this kind of now we need to go and remember you know uh you know getting um and more money from industry I remember from Google first time we got and it was unheard by then because you were asking for 500,000 per year right for four years so but you know we got and this is we we got so now you have also this kind of very tight connection with the industry it's a very good environment to see see the problems right to understand the problems right and then you can you can you can you can you can see that and you you try to to think about also about obviously about Trends right because Trends are important right you have to be aligned with the secular Trends right you need to bet on the right Trends because these are things you cannot change right or or is very hard to change um so you are not aligned it's it's it's it's not good yeah um and these Trends actually there are multiple Trends and uh the multiple Trends kind of open gaps between them and these are kind of opportunities for problems like for instance for you know Big Data it was clear you have more and more data and the amount of data people collecting or just growing right it was pretty clear right it's like Google has has seen that years before and they built all the systems but now everyone want to to emulate that right that's why hados cre it right and then you see you start to see again you look at that and you are working in that area and we are um you know we have all these hadu people coming to our Retreats on these kind of labs and we are you know friends with them and um we started to see problems and it and then you try to use them and and then you just like for instance um there are two things which happened one thing is about um a group in in in our lab oh by the way and the other thing what happens in these labs they are interdisiplinary yeah right it was people you know we are in this kind of Rat Lab was that um it was people from machine Learning Systems databases networking right so there are these groups of Michael Jordan students which wanted to compete to um to this Netflix challenge that Netflix released some data basically and ask for people to provide recommendations come up with recommendation to build recommendation algorithm systems so that you know to beat their own recommendation so they come to us and okay it's a lot of data how can we what can we do about it right we want to and you know told them to do yes Hadoop and but Hado was very slow right because you know for and then you know M they put together something quickly to for solving this problem in which the data was kept in memory uh the other thing I've seen is about like I have a previous company conviva and it's about his analytics company and kind of it was very slow and we tried to do it you know it's like you to for out queries and there is no way to do it and again the keeping the data in memory was a solution it's one solution that's kind of how it started that's that's that's that's that's one thing right and you look at the trends yeah it's obvious right it's like on one hand um you you know have more and more data growing faster than the more slow so you need to have is not going to fit on one machine and therefore you need to use multiple machines and then the only other question is that are you going to have data set are going to fit important data sets are going to fit in memory right it's like that's kind of the first question and they are because people when they even for when they are doing for instance at you notice that when you look at the data from different clusters had cluster from Yahoo and Microsoft and um and and others and and we notice that in a lot of cases actually when you do queries and you're doing analytics they doing not on the very rare on the huge on the all the data you do say for the most recent data you want to see what happens yesterday what happens last week something like that so once you get that and you know you have a lot of cases in which the data is fitting in memory and memory is still growing quite quickly um at that time you know that's you kind of you're pretty you know you you connect the dots yeah right and then it's about solving the problem and and the other thing is happens and why why they are related U you know I'm talking about Academy and Industry because I'm telling you know people you know some people you know there are people in Academia I push back you say you know it's a lot of engineering here what this is not you should do in in Academia but one thing is what I what I've always found it it is very satisfying if you build a system right in a new area and that system it's used by other people then you can you are in the in the best position to understand the new problems in that area because people are going to use your system in different ways right and then you know you you understand that right and going back if you know the problem you are also in a good position to solve it right so actually it it directly helps you with your research right to be ahead because otherwise what is a choice where you are finding the problems right of course there are problems very good problems in theory which are not solved by you know decades and so forth but other things what people do they go to Google and Microsoft and so forth spend time and to understand what the problem they have right because to solve but that's kind of a little bit unsatisfying right because you go to someone to learn about their problems but the question is why don't people the people solve those problems maybe they don't solve the problems because maybe you know they're not as important as they given time and maybe you know maybe for the right reason they are you know too much to to to in the future to to but this is a this is a thing right it's you have to focus about the problem on the problem and you have to focus on the trends and the way they're connected you want to solve a problem ideally which is going to be more important tomorrow than today what what are the problems that you're most excited about right now like spark Ray like what what's going to be the next data breaks or any scale so a few things so I still think that um um it will going to be a lot of work it's it's right now what happens is that um need to reinc most of the software stack why because it's again if going back to the trends is that the demands of this application in particular air application and so forth growing much quicker than capabilities of a single processor a single node even if you considered accelerators right so on one hand this happens on the other hand the infrastructure becomes much more complex you need to run that application uh not only on one node but on many notes is distributed but it's not only that it becoming very heterogeneous right because in order to Breeze a gap between the demand and capabilities of Hardware people build accelerators like that's why Nvidia it's trillion dollar company right and but now the infrastructure become even more complex right is het is not only distributed heterogeneous when we start this spark it was homogeneous all the no are the same some some storage some CPUs that's sort but right now look at look at a heterogen 8 you have Nvidia and you know and you have many other you have tpus from Google every cloud uh it's having their own chip right now you have MD and Intel say that it's everything it's about AI right so that's kind of what happen so now you have a a huge gap between this application and this very complex infrastructure just growing in complexity and then it's not only about a CP A compute it's about networking you have infin band and you have all of this uh you know uh RDMA and so forth right so huge heterogeneity and you the software stack has to abstract away that complexity for the developers there is no way around you want a single machine or you have this operating system to have structor is a complexity right that's why that what makes it easy right yeah to develop all this application now it's extremely hard so something is going to happen there um I think the other one is is about um this building application you're talking about compound AI we're talking about compound a and things like that um right now this um application a application in particular large large language models everyone is talking about LGE language models um you know the application are like assistance for humans right humans are in the loop right if you are thinking about customer support if you are thinking about co-pilot if you are think about Q&A question and answering uh even summarization you have a human helping a human to be much more productive which is a fantastic application um but uh they cannot they cannot be autonomous they are not yet autonomous and to to to to go from having the human in the L to being autonomous it's a huge gap right because with autonomy to be to have something autonomous you need to have someone which um you know it's it's it's running kind of you know it's it's more deterministic it's more reliable right it's like far more accurate right um and you know you need to get there because if you don't get there you know you are still limited to having the system or is a human in the loop and the human is a bottleneck will become the bottleneck it's you know it's it's just certain number of people on the planet yeah um and I think that there is about a lot of four could be about how to make I would say uh at least some aspects uh of this L language building L language model application more like an engineering discipline right um where you can you know build much easier systems from smaller components okay so the two next data breaks will be distributed computer across heterogeneous hardware and autonomous compound AI systems noted um I'd love to TU on another thread that you mentioned just now of kind of how funding constraints Drive uh what you're working on I'm curious what you think right now of there's been a lot spoken about kind of the almost the brain drain in AI right now because the uh universities don't just don't have the funding that you could get if you went to work at one of the big research Labs um how do you think about that what do you think is ideal does it does operating under constraints Force creativity for you like what do you make of all that yeah that's that's a great question um and um it's true I mean it's it's challenging it's very challenging um and um when I came to United States I came to do my PhD and I I graduated from K G on University I I am originally from Romania so one thing was admired people admired about United States um is like is that how well this kind of this um um you know the collaboration and the partnership the three-way partnership between Academia and government and Industry are working right and Prime examples at that time was obviously the internet right which was DARPA project and um and of course Academia had a huge impact and also industry was you know was whatever S Industrial Revolution people were saying um and that when it comes to AI today that kind of partnership is broken um it's like um um Industries every company is doing this research in silos right even you know they don't talk each other as much um Academia like you said doesn't have resources and the government doesn't invest as much right so I think that's that's something things to be very concerned about right um and um that's why I'm also a big proponent of Open Source models and us and California kind of will lose in long term if this doesn't is not fixed uh so what happens now in unfortunately in academy one thing it happens is that of course there are some you know bigger universities and Labs which can still afford to maybe train spend one two millions maybe to train some models uh is still they are not in the same league like yeah open AI right it's like open Ai and Microsoft what they are talking about building the data centers about $100 billion right um and I think there is uh the danger is that um you are going to um some of the academics are going to give up and try to innovate around the edges now you can still innovate in this application and things like that and you know I think there's a lot of innovation there um but um clearly you know you know it's it will be harder to come up with new model architectures and you know um innovate you know have it will be harder it's not impossible nothing is impossible um so yes it's it's it's a it's it's a challenge right right now I mean there are people like you know and which are better unfortunate more fortunate position maybe I'm one of them and which have access to resources outside Academia right um but you know it's you do want to level the playing field in order to um maximize The Innovation The Innovation comes from everywhere uh now this being said is true that you know scar City and always in the past scar City you know Spurs Innovation right as well um but um the concerns about uh not having access to uh resources to to play the same game like in industry it's it's it's a concern I'd love to switch gears into some rapid fire questions um if you're ready for it sure go ahead will anyone take meaningful market share from Nvidia over the next five years I think they it will be they will be at the minimum because they are going to it's probably that it's because Nvidia will not like to be accused about monopol monoptic Behavior so their market share has to decrease uh under some percentage whatever 70 80% um so that one will be a one one of the reasons um but I think that if I have to name one company to uh to of course there are clouds and for strategic reasons they are going to push their agenda to build their chips like still probably the biggest competitor right now in terms of the market share it's Google with DPS yeah yeah probably that will continue for a while what's one project or student in your in your lab right now that you'd want to highlight you know I'm going to cheat here because I think that both VM and charbot Arena been uh tremendous it's like um I'm not talking about Sky pilot because Sky pilot is like they started a company um so I'm not talking about but I think VM has been amazing it's like it's onee old project and I haven't I've never seen such a rapid growth and of course it's also part of thei AI is kind of compressing the time so it's there is something about that uh and I think the other one is chatbot Arina because it just is fascinating to see the development in uh in the space and to see how these different models where they are strong or they are weaker and I think that having a front seat at that to see that kind of development in the of the ecosystem and in the space it's fascinating do you think the foundation models will commoditize the foundation model was like a GPT 4 or a cloud do you think do you think there's a Market to be made and you know uh providing these models over time or do you think it commoditized I think people will will continue to build larger and larger models I think where though when it comes to serving is it looks like model distillations Works quite well uh by the way the model is where where you train a smaller model on the outputs from the bigger model has been a lot of success of that by the way this in some way um it's it it says that it it shows how important is the data right for training a model right going back early on in our conversation right because you have higher quality data from the from the big model and you use that to train the the small model and it's working very well uh so I think using multiple distillation models to reduce the cost of inference is going to be uh a Way Forward uh but yes I think for advancing um and pushing the frontier so to speak right um pun intended uh you are still going to see a lot of you know a lot of effort on bigger on bigger models what are you most excited to see in the world of AI in one five and 10 years what I'm o about AI yeah um look there is no question is transformational right I think that uh and um it's it will change a lot of a lot of things uh every s maybe um I think the most excited I am about it it's about how do you make this AI more C systems more uh predictable accurate verifiable uh how you can debu the systems I this all of this kind of in realm of software engineering like techniques this is what I think it's exciting what advice do you have for Founders building an AI um it's it's the same thing you know focus on the problem don't focus on the hype hype is emotional yeah uh it's not reliable just look at the fact right it's like and look at the problem try to understand the problem and try to be truthful to yourself it's it's and it's about if you build an application it's about production it's not about the demo yeah right it's beyond the demo of course the demo are important don't get me wrong are very important but the production this the way the mindset you have to to have uh and um yeah and it's a dangerous thing is there's so much hype and you think you can solve everything and you can do everything probably in certain number of years but now just focus on exactly what problems you are going to solve convince yourself it's a good problem uh convince yourself that you can solve it or at least you have you you can you can have an MVP right you can solve a smaller a smaller version of that problem which is still very valuable for your customers um that's what I would say nothing no Silver Bullet amazing thank you so much Yan for joining us today I've loved hearing a lot about your own thinking and reasoning behind your own Journey um a lot of the the thought process find finding the right problem to solve building the right systems to actually be in a position to understand the best problems and then uh applying that even to many of the Bold decisions that you've had to make in founding multiple companies from Research into commercialization and the incredible success of data brecks today thank you thank you for having me [Music] [Music]

========================================

--- Video 35 ---
Video ID: SbB2t4aWERE
URL: https://www.youtube.com/watch?v=SbB2t4aWERE
Title: Dropbox ft. Drew Houston - How the Cloud Pioneer Reinvented Itself
Published: 2025-01-09 10:00:33 UTC
Description:
A scrappy upstart taking on hyperscalers in a category with lots of hand-wavers, Dropbox became the canonical example of Silicon Valley viral growth, adding 50 million users in the first years following their 2008 launch and quickly dominating their category. However, as Drew explains, their path from viral sensation to enduring business was filled with daunting obstacles: As giants released competing products and tried to crush them, Dropbox embarked on a set of strategic acquisitions to expand its product line—but failed to find product-market fit with the new offerings. What do you do when your idea for your second act doesn’t work like you hoped? Drew describes the insights that led them to re-focus on work use cases for their core product, and the other moves that would re-ignite growth and turn the company profitable. In a counterintuitive move, the cloud innovator would end up migrating off of the cloud to its own infrastructure in order to be more cost-efficient. This engineering feat, called Magic Pocket, became the stuff of Silicon Valley engineering lore. Drew and engineering leaders Akhil Gupta and James Cowling recount the story of how they pulled it off.

Host: Roelof Botha
Featuring: Drew Houston, Arash Ferdowsi, Sujay Jaswa, Akhil Gupta, James Cowling, Bryan Schreier

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 36 ---
Video ID: fA5VYLpNkh4
URL: https://www.youtube.com/watch?v=fA5VYLpNkh4
Title: Robinhood ft. Vlad Tenev - Reinventing Finance for a New Generation
Published: 2024-12-19 10:00:45 UTC
Description:
Millions of Americans use their smartphones to invest and manage their finances every day—but before Robinhood started in 2013, finance looked very different. Investing was something for the wealthy, with steep fees charged on every trade, and was done exclusively on computers with arcane trading software. In this episode, co-founder and CEO Vlad Tenev explains why Robinhood set out to democratize access to investing and reinvent it for a new generation, how it overcame immense challenges in that pursuit, and how it reinvented itself amid a market downturn with a holistic suite of customer offerings to mount an historic comeback.   

Host: Roelof Botha, Sequoia Capital
Featuring: Vlad Tenev, Micky Malka, Andrew Reed, Jason Warnick

Transcript Language: English (auto-generated)
the FED went from cutting rates to zero and injecting a whole bunch of stimulus into the economy due to co to the fastest period of hiking rates in multiple decades and so we had this big problem which was what do you do when people don't want to invest and when all of these trends that helped grow our business tremendously during the pandemic now suddenly we're like L [Music] reversing welcome to Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world's most remarkable companies I'm your host and the managing partner of seoa capital rof bua founded in 2013 Robin Hood single-handedly transformed stock trading from a cumbersome expensive process to a free mobile first experience geared toward a New Generation co-founders Vladimir tenv and beu but set out to democratize finance for all and make stock trading accessible to anyone with a few Taps on their smartphone but their Journey would entail harrowing challenges many of which took place under the harsh glare of the public eye from the mem stock phenomenon and ensuing backlash to Reinventing itself amid a market meltdown Robin Hood has had to prove itself again and again yet today Robin Hood continues to redefine what it means to be an investor in the 21st century shaking up the finance industry in ways no one could have anticipated just a decade ago I'm Vladimir tenv co-founder and chief executive officer at Robin Hood so my co-founder beay and I met at Stanford and we're both in the physics department we met actually doing summer research so if if you think about the set of folks that go to Stanford and kind of narrow it down to those that choose to study Physics and then narrow it down even further to the ones that want to stick around on campus during the summer for basically no money and continue to study Physics you you get to like a pretty committed and passionate and small group of people and I continued on to graduate school uh intending to become a professor of mathematics so I went down at uccla he was up in Marin County and my first month at grad school which was his first month at at his new job in finance Leeman Brothers went under and we had kind of the beginnings of the global financial crisis and he calls me up when I'm in grad school uh one month and basically he pitched me on creating an algorithmic trading firm ourselves and at first I thought it was kind of a crazy idea but I wasn't having an amazing time in in grad school as well and I generally you know like to try new things uh the idea of like working with him again really appealed to me and so I decided to spend my summer kind of hacking on trading strategies with him in San Francisco Isco and even though our first business didn't really end up working the idea of being an entrepreneur and having complete control over your destiny and the only thing between you and kind of your ultimate success and failure is the quality of the code that you write and the business strategy that really appealed to me I'd never been more passionate about anything professional I was working basically all of my waking hours when I wasn't sleeping or eating I would just like work the kind of like act of Entrepreneurship was so intoxicating to me that I couldn't even imagine doing anything else the Pair worked on two trading companies which they describe as moderate failures but as they gained deeper insight into the financial industry they began to notice glaring shortcomings it was a strange phenomenon I mean back then people forget but whenever we had a big financial transaction to do we would log on to our desktops to do it my name is Andrew Reid I'm a partner at sequa you would hit some menu button and you would descend into the scaffolding of some system that felt like it was built in the 1980s the idea of paying bills or making investment decisions or making investment transactions on the phone seems natural and normal and I think everyone feels that way but 10 years ago even this was very very strange remember nothing nothing had been started since eade in 1998 in America nothing so those interfaces that you even log in today they still look very stale they haven't changed my name is Micky Mala I'm the founder of rivit capital and we've been a shareholder and partner to Robin Hood since 2013 the founders also noticed the world changing in several profound ways that created an opportunity for an entirely new kind of financial services company it was really three things one the rise of electronic trading and the complete change in the markets that kind of that Revolution injected into Finance it was the rise of mobile which was like a tectonic shift and new platforms became created that were built with Mobile in mind and if you remember after the global financial crisis there was a ton of dis discontent and disillusionment particularly among young people of how the financial industry worked and these Millennials they saw the global financial crisis a lot of their relatives and Friends lost their jobs in my case my cohort had graduated from college some of them had jobs at Leman brothers and pretty much as soon as they came in you saw these images of them taking their cardboard boxes out of the office in New York and they had to find something else to do and so there was this distrust and discontent young people felt like the financial system didn't work for them and that manifested in things like the Occupy Wall Street movement and large-scale Global protests and that also provided an opportunity for us to create a new brand because there was no Affinity no loyalty among young people to the established big Legacy Brands the big Financial companies and by creating Robin Hood I think we kind of tapped into that nerve Vlad and beu had a bold potentially revolutionary idea a mobile trading application with a power to democratize the financial industry but this posed challenges they would need to change entrenched attitudes about Finance products with weary consumers the founders faced their first Crucible moment how do you create an application that can attract users who are inclined to to dismiss your entire category what Robin Hood brought was a level of Simplicity that took away the intimidation of those complex graphs and those complex Microsoft 1995 SQL kind of dashboards into a very simple swipe and gestures and very thoughtful UI and that was what made it very not intimidating for people to sign up to reinvent trading for the mobile age Vlad and beu designed Robin Hood with a look and feel of a modern social media app it was Sleek colorful and most importantly easy to use it felt super responsive fast the user experience was incredibly intuitive and you you would submit a trade and it would process quickly and your information would update immediately and you could view your portfolio in real time and to anybody who wanted to get started investing it was night and day in addition to a frictionless mobile first design Vlad and beu made another decision that would come to define the platform it was important from the very beginning to us that Robin Hood was commission free because we wanted to give every possible chance for our product to succeed and we knew that a 7 to10 commission was actually a big barrier and if you were someone that was starting with $100 $7 to10 is a significant percentage of the total amount you would be investing and so we thought that there was something very very powerful both about B being commission free and the account minimums which we lowered to zero as well you know a lot of Brokers had $2,000 account minimums that you needed to put in before you even started investing and we knew that if we were actually able to deliver this we would open up the market to a lot more people particularly young people without very much money to invest while Vlad and beu felt strongly about removing Financial barriers to entry many were skeptical about whether the company could build a business with this model Robin Hood when they started free trading had been tried many times before and had failed so people or Skeptics saying free trading doesn't work our thesis was that brokerages have many ways of making money they make money from margin lending they make money from the unused cash balances that are sitting in customers accounts there's a revenue stream called payment for order flow where you get rebates from market makers and if you look at the revenue that commissions comprised of a broker it was less than 50% the revenue from commissions range from somewhere around 10% to 30% so we knew it would be possible if we could sacrifice even up to 30% of the revenue if we built the company using modern technology and made it operate much more efficiently than a typical Legacy broker if we didn't need brick and mortar if we didn't need the headcount so I think I think in that sense that was the thinking of how we could build a sophisticated brokerage firm with the technology of a Silicon Valley Engineering company the founders felt they had a winning formula for attracting users but they faced a specific hurdle theyd need to clear to get Robin Hood off the ground the other confounding variable was that because we were regulated entity there's restrictions on how much you can market right so until you have the license you can't really Market your product the conventional wisdom of Silicon Valley at the time was that you would actually launch what's called a minimum viable product and you would try to get customer feedback and ideally you would do that before putting in a huge capital expenditure to actually get the thing off the ground and so that that just wasn't possible and we need to demonstrate to The Regulators that we had at least a year of capital to operate with before we would even get the the license so we we had this catch22 situation where investors wanted to invest in a company that had some traction and was drisk to some degree but we had the the regulated side of the business and we needed the capital in order to get the license we ended up having to like get super creative there were not many people doing Financial Services back then fintech was not a thing and we heard that there were these two kids from Stanford doing this company around it and I was having lunch at a sushi place here in downtown paloalto and on the table next to me Vlad is pitching to an angel investor and I get to listen to the pitch while I'm having lunch with somebody else I was paying more attention to his peit than to than to our lunch and so I came back to the office and said we got to go meet these guys and that's how we met them super early what made lad and beu an attractive team to partner with was their love for the space they truly went really deep in understanding how could they make this happen they was not just building an app like some hey I'm going to build an app that was the era where people were just building apps for the sake of saying that they have a mobile app we took all pitches we pitched everyone we could find we knocked on 75 doors 75 investors probably much more than that we scratched and clawed we committed to not paying ourselves a salary until we got the approval because we wanted to demonstrate hey if this business doesn't eat we're not going to eat either backed by VC financing and with its brokerage license in the works Robin Hood began to look for ways to generate interest with consumers we knew that there would be uh a significant period of time between when we actually got the license and we could on board our first customers because there was there was a lot that needed to be built and the stakes were were much higher for a financial product you know when you're dealing with people's money you you had to make sure things were much more buttoned up we thought if you have to wait for a product you might as well like inject a little bit of creativity and Delight into that experience and there there was a company shortly before we launched called mailbox and one thing they created was like this experience of being on a wait list and they had turned the experience of kind of waiting for this product into a product in and of itself and so we were inspired by that a little bit we thought we would create an incentive for the most Avid most committed early adopters to share the product with their friends the thers implemented a feature where referring friends to the wait list allowed users to advance in the queue it instantly caught people's attention when the wait list hit a few thousand people which it did on day one we thought that was just amazing we we didn't anticipate getting to thousands of people on the wait list very quickly at all it went viral on Reddit and then it went to number one on Hacker News on a Saturday we had 50,000 signups in the first week we had about a million people join within the first year of of us announcing the product which I think at the time was the largest pre-launch demand for for a Consumer Finance product in history that early weight list viral moment of people telling their friends about Robin Hood signing up inviting people moving up the wait list to me that was the first example of many of Robin Hood Crossing this threshold from Financial Services application to culturally relevant consumer application and it also just spoke to the unique and compelling value proposition that Robin had offered we had iterated on a couple of projects uh as as entrepreneurs before we kind of like were able to announce the the full Robin Hood Pro and those projects had all failed and no matter how many growth hacks we did or how we could Market the product or what kind of referral mechanism we put in we just had a hard time getting anyone to stick with the product they didn't have product Market fit and the experience of launching Robin Hood was qualitatively different with the FIV around the wait list Robin Hood had broken through the noise but they still had to prove they could convert all the interest into to real users there was still a little bit of skepticism though I mean you hear a million people on a wait list I think the the initial thought was that maybe very few of those people would convert to actual funded brokerage accounts with money in them I remember I said listen I don't believe in that willing list you're probably going to have 10% convert uh a great campaign is 3% you'll probably have 10% because people are really early adopters but don't expect this to be crazy and they looked at me and said I think you're wrong we think we're going to convert 60 70% of the waiting list and I said listen I want to be wrong but I've never seen this and and they did hundreds of thousands of people on the wait list converted to users when Robin Hood launched publicly in March 2015 the company had over 2 billion in transactions by the end of the year and kept growing exponentially from there the momentum attracted new investing Partners including sequa it was only a few years into the company's formation it was already incredibly obvious to anybody paying close enough attention that Robin Hood was poised to up in this entire industry Robin Hood knew they were disrupting The Brokerage industry but what happened next caught them by surprise a very strange thing happened which was the that basically all of the incumbent competitors all of the big Financial brokerage houses Dro commissions to zero all at the same time like within weeks of each other and I I haven't found an analog of this I kind of say that it would be like all the Legacy car makers recognizing that electric vehicles are the future and like abandoning internal combustion engines within weeks of each other I remember that moment vividly because it was fall of 2019 I was on my honeymoon and my phone just started buzzing and it was uh like it was almost as if it was in concert right it was Schwab and TD and erade were dropping commissions to zero and it was just very strange thing because it felt somehow like it was always going to happen but kind of hard to believe it would happen all at once the stock prices of some of these companies that were public just getting decimated to the point where those companies couldn't continue as Standalone companies they had to merge and be gobbled up by their larger competitors if you were cynical you would say Robin Hood's value proposition went from better faster cheaper to better faster and in a vacuum that would be a very frightening moment at the same time you know for those of us who really signed up for the mission of Robin Hood delivering value to customers it was this moment of catharsis where it became clear in one immediate moment that the whole idea of the company had worked the incumbent companies actually didn't need these Commissions in the first place this was money they were pulling out of consumer's Pockets because they had the luxury of doing so and again one small group of people in paloalto launched a product 5 years later it's free for everybody whether you sign up for Robin Hood or not there was heightened pressure like it's good for customers that this happened but Robin Hood also so had to work even harder to distinguish ourselves in what was a very competitive Marketplace so I think this was this this ju toos of facing the business reality with the other reality that we had kind of created a fundamental change and kind of created the new standard for for how our entire industry operated Robin Hood continued to Corner the commission free trading category through 2019 but early 2020 ushered in a period of financial chaos with stocks gting as the pandemic set in trading volume exploded we were dealing with so much load and so much growth that things were cracking across the entire infrastructure landscape I still remember the date March 2nd of 2020 we had what was basically a full day trading outage very painful for our customers painful for me as an engineer social media platforms you know Facebook but they have outages and customers can't log in they've even had ones that are multiple days but if you're dealing with people's money particularly in a time like the pandemic where the markets were moving up and down 5 plus per every single day it's heightened pressure that wasn't because someone pushed a button on a server was because the load was absolutely insane this wasn't like a long-term impairment sequa made a large $200 million investment into Robin Hood that next week just because it was clear to me that Robin Hood was still the dominant force in this industry and would be for many years to come and basically what we had to do was we put all of our best people you know our our best resources on just making sure that we could scale our systems uh for from clearing to trading to really everything and we had to significantly slow down new product development just to make sure we could keep up with the growth Robin Hood was able to raise capital and stabilize its systems but in early 2021 amid boredom inducing pandemic lockdowns and massive fiscal stimulus a new investing Behavior emerged fueled by the scale and speed of of the internet consumers were gripped by a stock speculation craze that reverberated at an unprecedented Pace Robin Hood would be thrust into the Global Spotlight in a way few companies ever are in early 2021 there was what is referred to as the mem stock period where literally millions of retail investors primarily were seeking in in put from Reddit and a particular subreddit of Wall Street bets that encouraged everyone to buy no matter the price stocks like GameStop AMC there were a handful of others my name is Jason Warick and I'm the Chief Financial Officer at Robin Hood the meme stock phenomenon was pretty crazy it came out of left field I don't think anyone was anticipating that even a week before it happened I mean there were a lot of people that were buying them just because of what was going on on social media but there were a lot of people that cared very deeply about them too and I think they viewed companies like GameStop and AMC as being unfairly treated during the pandemic I mean you can imagine these are brick and mortar Legacy retailers there was a lot of nostalgia value in them and the government had come in and shut everything down there was some empathy for these companies it started off as kind of an amusing headline but really quickly as we saw Millions join the platform and demonstrate buying only activity regardless of the price uh we started to quickly realize this is an unusual moment what was on my mind was I just want to make sure that we're doing everything we can to be up and reliable and that we don't have any infrastructure issues and you know the the last thing that occurred to me was that um we we would have to shut down trading to comply with capital requirements and these sort of Arcane rules and that we would be kind of wrapped into some almost political debate around hedge funds colluding with the US government to to shut down trading the stock market met the internet in a way that nobody could forecast and created this incredibly intense moment where Robin Hood found itself at the epicenter of Front Page News in the early morning hours of January 28th 2021 the nscc an organization that oversees clearing settlement and risk management for brokerages demanded Robin Hood post 3.7 billion dollar in cash reserves to mitigate the risk of mem stock trading activity on its platform it was an unprecedented number basically the way that these Capital calls are delivered are through automated files that kind of get sent in the middle of the night and there was a file that was sent that was received by our operations team and it had uh $3 billion collateral call on it and up until that time the total amount of capital that Robin Hood had raised was in the hundreds of millions the first I heard about this was from a phone call in the middle of the night most nights my phone's on mute and so I I'm really grateful that for whatever reason I had unmuted my phone and was awoken it was probably 2:30 or so in the morning I remember getting a phone call from vad early in the morning had to be before 5: in the morning an entrepreneur should always know who is the first person they call when the hits a fan and that phone call is one of the most important phone calls that you got to be able to do because that phone call is not about judging it's not about criticizing it's about having a mindset to enter into action that set off really probably 3 to five days of just non-stop work and response from the team across the company obviously this is full crisis mode and you're in absolute like problem solving because you know this is something that has been foed upon you from a rigid machine we got on the phone as a leadership group and brought together uh experts in the area to First understand what was the the request that we were receiving and decide on how to respond to the nscc about this collateral call I told Vlad that number has no sense to your volume and I said to him listen let's let's divide and conquer have your team called them back and says you do not agree with the number don't accept it because there's no explanation I think the volume they did on the stocks that were considered gamestoy stocks like the meme stocks was probably a few hundred million dollars that day the previous day so why would you have to post collateral for 10 times the trading volume the collateral had no no sense to any metric of any time in history as by a regulator ever I said go and fight it don't accepted while Robin Hood I think got the Lion Share of the press we were not alone in this other brokerages we were hearing were also experiencing this same level of collateral call and then I remember seeing Vlad and Vlad is and was one of the you know steadiest hands I've ever seen Robin Hood worked the phones with the nscc trying to lower the collateral requirements while Mickey Andrew and others looked for emergency capital I think what ended up happening was the collateral requirements were subsequently lowered I think the NC applied discretion vad was saying hey they're revisiting the number it may not be three it may be 1.2 and or 1.5 and at the same time I remember I called our bank CEO for rivet for silicon Bley Bank Greg Becker I woke him up and said Greg how many times have I asked you for a favor and he says this is the second time in 10 years the first time was really expensive what do you need I says we need to wire half a billion dollars of our money to Robin Hood in the next 4 hours he says you what and to the credit we worked for the next 4 hours and by 9:30 in the morning we had a green light to wire $500 million to Robin Hood we took that half a million to a billion two by 10 something in the morning but the team didn't feel they were in the clear yet we just didn't know what was going to happen the next day or the day after we just had no idea and they wanted to sleep better at night because imagine you're running on fumes you're opening millions of accounts you're serving millions of customers mem stock Mania crypto Mania too many Manas at the same time something else could happen let's have excess Capital we wanted to make sure we were in a position for that to never happen again and that's what really kicked off the fundraising that happened over the next few days and we ended up raising a couple of billion dollars I think it was between three and 4 billion at the end of the day just so that we would have plenty of Headroom and cushion to open it up in the future Robin Hood managed to raise a $3.4 billion War chest in days a multiple of what it had previously raised in its 7 years of existence but the public backlash was only beginning ultimately the collateral requirements were met but in order to comply with our Clearing House deposit requirements Robin Hood Securities which was the clearing firm that we created had to restrict opening new positions in a whole bunch of stocks including GameStop what everyone was worried about on the operation team was what's going to happen the next day or if like this trend of exponential growth continues and people keep piling in on a global level into a small handful of names are those requirements going to get into the tens of billions hundreds of billions and so ultimately it was kind of planning for the future that led to us to enact trading restrictions on these names and that was very painful it was very painful for us I mean no doubt it was painful for customers as well when Robin Hood took down the buy button on a handful of stocks including GameStop it elicited a really visceral reaction from our customers there was a feeling that every point that GameStop stock went up was you know hurting the man and helping this company that people felt love and Nostalgia for and there was a feeling that by restricting people from buying the stock but allowing them to sell the stock that this was unfairly depressing the price of GameStop or AMC the restrictions on buying meme stocks sparked public outrage a conspiracy theory took hold that Robin Hood had been pressured by hedge funds who were shorting stops stock a narrative that spread in the media and among some politicians Vlad and other industry leaders even had to testify at a congressional hearing to explain what had happened you don't study for this you don't read books about this you are not prepared for something like this you are not prepared for the pressure of Twitter hating you Twitter having conspiracy theories readit believing that you have you have an evil plan and you are Darth Bader for threats for death threats for the worst of the wors those things psychologically take a much bigger burden and they take a long time to heal because you're trying your best to serve your customers with the promise that you had when you started the company which is democratized financial services for all I think fundamentally they felt betrayed and it was a horrible feeling to have as a leadership team knowing that customers felt so betrayed and it was an incredibly complicated storyline to articulate on why did Robin Hood have to do that that was like an incredibly intense period this was like much deeper than a PR crisis there was challenges with trust from the user base from the customers from the community I thought that this was not going to fall in the laps of Robin Hood as their fault but it did and I missed the mark the world the congressman the Senators the media everybody turn and point the finger to Robin Hood as the blame of this it was obviously an incredibly negative press cycle they made movies about it navigating through that I think created a huge amount of loss of trust in the company by some of its core users Vlad went on a big tour thereafter to like try to like really provide transparency to customer is on exactly we got this deposit requirement here's what we had to do to explain you know the company's problem solving in that incredibly challenging circumstance then there was this this Narrative of collusion with hedge funds that we had to make sure that we dispelled so I was just like trying to put myself out there a lot more and I'd always been a very heads down person like focus on the work I didn't really do a ton of media or podcasts but I think at that time it was clear that I needed to do more and even though it was uncomfortable and it wasn't the most fun topic customers were just confused and the public was confused maybe they'd heard of Robin Hood for the first time with GameStop and AMC and in this controversy so um I had to put myself out there a lot more and you know do a lot of uncomfortable Sation just to kind of clear the air about what had happened it was a trying time I was really pleased with the way the leadership team stuck together and responded but it was exhausting exciting it was scary and it was super disappointing to have disappointed our customers that way we ended up pushing for industry changes including lowering the settlement period that's required of trades and that that ended up becoming law and now in terms of communicating with customers both for me personally and the company I think we're much better at it and we're sort of more comfortable in in our own skin as a public company and me as an individual so even even though it was painful I think it it was a very valuable process of maturation that had kicked off in us if you actually like try to put that whole moment into a bubble just the conspiracy theories and the outrage and Vlad going on a clubhouse with Elon Musk raising 3 billion dollars in a weekend you know it almost sounds like out of a fever dream when you talk about it in hindsight when you're facing the true first hurdle or massive hurdle you have two options most Founders will either sink and a very few will swim and Thrive and they had to do that that's why it's such a crucial moment in their history a few months later in July of 2021 Robin Hood went public even as it continued working to rebuild user trust but 2022 with Rising inflation Rising interest rates and a market meltdown would bring entirely new challenges the FED went from cutting rates to zero and injecting a whole bunch of stimulus into the the economy due to co to the fastest period of hiking rates in multiple decades and what typically happens when rates go up is investing in stocks and other risk assets becomes less attractive because imagine if if you can earn a 5% yield risk-free just by holding cash the bar to like making a successful investment is is much higher so you tended to see a movement away from people open open up brokerage accounts and placing trades and investing towards different types of behaviors Robin Hood basically since our founding was focused on being the best place for people to get started investing and so we we had this big problem which was what do you do when people don't want to invest and when all of these trends that helped grow our business tremendously during the pandemic now suddenly we like sharply reversing 2022 was a hard time for theany it was a Slowdown I think it was a crucible moment in 2022 because we had to adapt and respond to changing circumstances and we had to embrace a new way of thinking there is exhaustion a fear of making decisions because you were being scrutinized for so long on everything that you did or said or didn't do or didn't say and they had to re-energize themselves grow their own confidence again I I think it was a very useful exercise to kind of step back almost like manufacturing an outof Body Experience so if you were a third-party Observer that was observing yourself in action kind of running the company and making your decisions and doing the day-to-day what would you say and are there things that perhaps we're holding on to As Leaders trading activity declined sharply as inflation fears took over and nearly half of Robin Hood's users left the platform from the frenzy peak of 2021 it was the kind of Crucible moment that calls for reflection and reinvention one of the pieces of advice I remember is any company that successfully navigates a downturn does it by strengthening the core of the business so you have to find if you don't know what the core of the business is you have to find it and then you have to strengthen it and the observation that that I made at that time was the core of the business was active Traders so it wasn't novices but it was actually folks that maybe had become more sophisticated Advanced users trading things like options Advanced equities the advanced active Traders were the resilient piece of our business and that's because you know they're they're sophisticated enough to have strategies to deploy when markets are moving sideways or even coming down and so they tend to be more resilient and they tend to like continue to trade even when markets are not going up when we looked at our business in 2022 what we realized was the more active and sophisticated a customer was the less happy they were with the service and when we kind of like deeply understood this we realized it was a five alarm fire I mean if you think about any business you're most engaged active customers should be the ones that are happier with the service but in our case we had kind of the opposite effect we basically were not building for those customers we were building for first timers but we had never like triangulated toward this important customer base I think what we didn't really realize was that by making Robin Hood available to everyone it would also become a platform that was desirable for more active and sophisticated Traders I mean they too see the benefit of no trade commissions and access on the mobile phone and I think one of our early missteps is our failure to really recognize active Traders as an important constituent and what happened was during a downturn that ended up in us remobilizing the company essentially refounding the company towards active Traders as like the core constituent along with prioritizing active Traders Robin Hood also worked to diversify its product offerings Beyond equities trading now customers are engaging with us in a variety of ways that just wasn't available before there's retirement accounts we have approaching 10 billion dollar of customers invested in retirement savings through us we have high yield cash sweep balances where customers can earn industry leading return for their uninvested cash Securities lending which is a great way for customers to augment the yield that they have on their Investments and so on and as we continue to diversify the way that we serve our customers this naturally leads to more revenue streams for the company and a stronger Financial footing and we're as of last quarter now up to eight business lines that have Revenue run rates that are over hundred million that is a recipe for really strong financial positioning going forward as they normalized their operations they sort of went and and talked about how do we serve the Millennials our customers because guess what the founders and the management team are millennials that's their age and they have to have retirement accounts for their kids and have all these products and care about high yield interest where before they didn't because you were all in all in on stocks every day so the evolution of the product stack is also the evolution of the team as they've aged in the middle pack of the Millennials so when they say they're going to serve the Millennials with all the financial products they mean it because it's them they're serving their own generation fast forward to 2024 and you know the Stock's gone from $8 a share to 24 wherever is today and it wasn't decisions of 2024 that that have caused the stock to triple it was the decisions of 2021 2022 2023 the hard work cultural resets the trust building the focus on customers and on products in 2022 when we were in the beginning of this strategic shift it was a very harrowing experience and yeah it was it was definitely a different type of challenge if if you have to like refocus your business and move a lot of resources and focus on a new type of customer there was a lot of uncertainty and it it took a lot of conviction and just a a ton of hard work by a lot of amazing team members to successfully navigate that the strategy worked Robin Hood has continued to set new Revenue records in 2024 despite having fewer users trading than in the height of mem stock Mania in 2021 Robin Hood today compared to the first time I met them it has a same core DNA they truly still want of democratized financial services with very big differences from back then they're understanding that this is a long-term game not an overnight Sprint Robin Hood's Legacy is not yet written I think they have earned the right to keep playing and I think the Legacy will be written when the Millennials retired and we are still 25 30 years away from that I think Rob positioned himself now is an extremely valuable company not because of hype or a hype multiple but because of lots of profits in a quality long-term business I'm reminded of uh the Winston Churchill quote If You're Going Through Hell keep going I think a lot of a lot of navigating these things is to just put one foot in front of the other and keep going and I think ultimately nobody that's been successful hasn't gone through a bunch of challenges probably if you don't you're not pushing hard enough and you're not challenging yourself enough I think at the end of the day despite all the change what really matters to people is being in control of their finances having the tools that allow them to make decisions I think over the next few decades it's going to be clear to people that they can't really rely on others you can't rely on your employer or really the government to like make sure your finances are in order and you're secure and what we can do is make sure we give people the best tools and we can use technology to solve the problems of being in control and managing your finances and we can do that not just with investing but all aspects of it across the board for taking on your your first loan building credit retiring building your wealth and I think from the very beginning the the ethos of the company has been to deliver tools that were previously used only by the wealthy and and make them available to mass Market consumers and I think there's a lot there's a lot that remains to be done across the board for helping people secure their financial [Music] Futures this has been Crucible moments a podcast from seoa capital [Music] Crucible moments is produced by the Epic stories and fox creative podcast teams along with seoa capital special thanks to Vladimir tenf Jason Warick Nikki Mala and Andrew Reid for sharing their stories incidental audio created by 11 Labs the seoa partner [Music]

========================================

--- Video 37 ---
Video ID: 9mIphDV9m9c
URL: https://www.youtube.com/watch?v=9mIphDV9m9c
Title: Cracking the Code on Offensive Security With AI ft XBOW CEO and GitHub Copilot Creator Oege de Moor
Published: 2024-12-10 10:00:08 UTC
Description:
Oege de Moor, the creator of GitHub Copilot, discusses how XBOW’s AI offensive security system matches and even outperforms top human penetration testers, completing security assessments in minutes instead of days. The team’s speed and focus is transforming the niche market of pen testing with an always-on service-as-a-software platform. Oege describes how he is building a large and sustainable business while also creating a product that will “protect all the software in the free world.” XBOW shows how AI is essential for protecting software systems as the amount of AI-generated code increases along with the scale and sophistication of cyber threats.

Hosted by: Konstantine Buhler and Sonya Huang, Sequoia Capital

Transcript Language: English (auto-generated)
because we now have ai code generation everybody can create code but not everybody knows about security and the models that generate the code have been trained on all public source code there's a lot of vulnerabilities in all the public source code and so uh we generate much more code with more uh security problems on the other hand attackers are already using AI to make their own work more uh more effective and so so uh we uh we also have a uh a greater threat so more code more attack more attacks and that kind of makes the automation that Expo is doing absolutely essential [Music] today we are excited to welcome uhad deore founder and CEO of Expo as the creator of GitHub co-pilot uh has helped push the boundaries of modern AI before GitHub acquired his last startup Sul uh was a computer science professor at Oxford his new company Expo is one of the most exciting AI native companies to launch this year they're able to automate offensive security with an AI penetration tester it's one of the best examples of AI Service as a software that we've seen we're excited to talk to uh about the Breakthrough results of Expo and what's next in AI uh Expo now matches the capabilities of the world's best hackers is this one of the first Industries that's going to be completely disrupted by AI absolutely it's uh it's going to completely change the way application security is uh is implemented in the Enterprise um really it's an example of service as a software uh people will be able to to replace a lot of uh routine human work uh with complete automation um and that will free up the humans to do the truly creative work themselves so U tell us about some of the results that you announced recently because I think they're really quite striking so uh we uh when we first built the the first version of our product we decided to try it out on renowned industry benchmarks um and these are uh challenges that human hackers use uh to hone their skills and uh we got these from a bunch of commercial providers including ports wigger and pentland on these bench marks our product scored 75% which was amazing in fact it was so good that I my first reaction was that surely there's something wrong here what's uh actually happening is that probably the sarks are so well known uh that they occur somewhere in the training data and the the Mel is simply uh regurgitating the ESS so we created a new set of benchmarks completely original guaranteed to be not in any training s and those are scored even better 85% wow so then uh the question is so how good is that really and to answer that we got in uh five professional pentesters from reputed firms and we asked them uh to solve exactly the same set of4 challenges one of these people is really at the top of the game the the the very best type of Pentasa the kind of person that you'd ask to secure a multi-billion dollar hatch fund and he scored the same he scored the same as the AI however uh the human took 40 hours and the system took just 28 minutes yeah that is striking and when we first partnered with uh it was science I got to say we we did not know if the AI would even be able to perform remotely as well as humans and then when UI called and said hey Constantine we've got some results to share that will blow you away uh it it certainly did it certainly did what do you think was your over under back in January February when it was still science as to whether an AI could perform at the level of these 20year seasoned penetration test for experts so at that time I didn't think that would be achieved so quickly I thought it would take at least a year to reach a reasonable level of of of of proficiency and even then I would expect that it would work at the level of a mediocre human pantle not at the level of the absolute top and uh in fact since we announced these results we've been working quite closely with a bunch of uh early design partners and and one of them uh this this morning uh we found it incredible critical vulnerability a very surprising and the way it worked if you look at what the AI is doing um it first craw the web app and then it found some source code written in PHP and this source code was uh intended to uh uh access uh another uh another host but it used an insecure signing algorithm in order to make that connection and so X2 was able to get to the other house generate or generate links and access that nothing interesting found there so then uh it continued crawling the web app and found another endpoint and decided to try and use the same trick that had previously discovered didn't quite work needs another parameter no problem browsers around it find some more source code this time in JavaScript sees a number of candidate parameters tries them all out finds one that uh works and now it has access to an endpoint and when it explores out it turns out it's intended to download PDF files but not only could you could you could you download PDF files you could actually download a password file so this is quite uh quite serious and what what I find fascinating about this type of example is that the AI is exploring like human pent St is it's uh taking quite interesting creative terms that would be hard for most human experts so just to summarize what you just said this is a very confidentially confidentiality obviously this is a very large financial institution that everybody uh watching this podcast would have heard of uh high confidence and the AI was able to find find a very Advanced vulnerability this is the type of institution that has human penetra penetration testers constantly targeting it and trying to F find vulnerabilities a massive Budget on security it was able to find a whole file full of passwords that's right just this morning that's right and we have something like that every every day every day wow OA congratulations on the results maybe can we take a step back and for those who aren't that familiar with this specific Market I've heard you and Constantine talking about pent pentesting and I think Constantine called them hackers I don't know if that's the same thing like what what is the offensive security Market uh and and you know what I guess how do you define the market that you're going after and what what is Expo thanks for taking a step back so offensive security is currently the best way to secure s systems you invite external experts to come and simulate attacks against your uh systems and they report or whatever they find so that it can be fixed before the bad guys get at it now this is a highly skilled uh activity uh people people need years of years of training to do it um and it's expensive and slow U typical cost of or so-called penetration test is something in the order of $118,000 and because it is expensive as slow uh people only do it once or twice a year but that doesn't make sense because their systems evolve much faster than that and so there will always be periods of time that uh insecure systems are are out there and uh what Expo does it automates this process this highly skilled activity of are launching simulated attacks and trying to find vulnerabilities um and it uh because it automat it it can now run it continuously um uh instead of just mon or twice a year H what Drew you towards this market like I I think you know Constantine mentioned your background and founding seml and and having seen GitHub co-pilots what what Drew you towards this specific Market because it feels like there's a dozen teams going after AI coding you're the only team I've met that that is taking this specific approach to to offensive security so uh it was kind of uh the natural thing to do um so my previous company called saml that uh also also was insecurity but finding flaws in source code and at seml we had an offensive security team which would use our product in order to find potential vulnerabilities and then the uh our security researchers would find exploits and we would tell the world about what we found um even at that time it was kind of embarrassing to me that that last step of finding the exploits uh was done manually um then when I was at GitHub GitHub acquired our company um at GitHub I had the opportunity to uh found the uh co-pilot project um and so it was natural to now take my new found interest in Ai and apply it to the uh challenge of automating offensive security it was very lucky one of the star researchers at s was Niko BisMan and he uh he joined me in uh creating Expo and one thing I'd love to ask you about I think Expo is such an interesting case study for this brother thesis we have that you know AI is actually changing markets of yesterday that weren't as interesting um AI is actually really expanding and dramatically changing the nature of those markets uh and I think this is a really interesting case study so I'd love to dig into it a little bit more the pentesting market is you know relative to say endpoint security or network security it's a relatively small um Services heavy market today and so um to your point you know offensive security is is so important and it's the gold standard but it's a relatively small Market um how do you think AI is going to change the nature of that so first of all it's small because it is powered by a small group of Highly skilled uh human uh uh human experts um I think AI is going to change the market fundamentally in a couple of ways so first of all because we now have ai code generation everybody can create code but not everybody knows about security and the models that generate the code have been trained on all public source code there's a lot of vulnerabilities in all the public source code and so uh we generate much more code with more uh security problems on the other hand attackers are already using AI to make their own uh work more more effective and so uh we uh we also have a uh a greater threat so more code more attack more attacks and that kind of makes the automation that Expo is doing absolutely essential so we believe that the markets will grow enormously uh here one of the and Sonia one of the analogies that I think about with this Market is uh is frankly the adversarial nature of of conflict of human conflict cyber security is an adversarial game uh you basically have two sides that get better and better equipment and they fight each other and it's a little bit of a game of cat and mouse not completely unlike uh unlike war and physical conflicts uh and human history and one of the reasons why uh we think that this Market is particularly interesting is think about how frequent war games are played in the military in the US military or or in any military abroad War Games Ren teaming in fact Ren teaming has been a uh a an initiative in most militaries for decades in centuries where you actually simulate uh a war game simulation so this is a this is a level of national uh importance and really what you have built is in my eyes the first ever AI cyber Warrior I mean this is I described it as a hacker because this is an AI cyber Warrior that can do things that no software has been able to do before ever when you launch these results uh I know with confidence because we talked about it a bunch of people from DC called us up and said whoa wait a second this is very consequential from DC and all over the West Coast This is highly con consequential and I'm sure it didn't go unnoticed by adversaries to the West as well and that they have probably been working on issues like this so my question is how do we stay ahead of the competition true competition as a nation state competition not business competition how do we stay ahead of it and how do we make sure that Expo is a Force for good in this massive adversarial cyber security game so first of all uh we stay ahead by moving uh moving very fast and at Expo we were very lucky uh to work closely with several of the creators of uh big foundation models uh which are ahead of the rest of the world um we're also extremely ciz or the potential dangerous users of our technology and therefore we've decided to make it available only in the cloud by making it available only in the cloud and not in some downloadable form of of software uh we can actually control uh what scope is being used it is being used on to launch attacks and so we can require from our customers that they prove to us that the scope they wish to have tested is actually legitimately their and it's not being used to attack someone else AI security Warrior you're Constantine I think you you got you're the new Expo CMO that is that is incredible um oh hey I'd love to learn about you know the the you know how how the product actually works and how the models work um how much of the magic of what you've built is you mentioned you work with some the major Foundation Modo company how much of the magic of what youve built kind of exists in the foundation models uh versus things that you are building on top so most of the magic is in fact in on on on top um we uh uh we work with several of the foundation mod providers and uh we are we're very happy that they're in stiff competition and they're playing hopscotch they one pulls ahead the other one pulls ahead uh and every time the foundation model gets better uh it benefits us um but the true magic comes from the security team at at Expo we've got some of the very best hackers in the world working for us and that domain knowledge uh is what informs how our product works can you double click a little bit into you know how that works like is it you know is it prompt engineering is it are you fine tuning the models like and I I know that you know you probably want to keep things keep your cards close to your chest as well in terms of how it works but I'd love to hear the high level how you how you've built it sure so I've already talked about talked about these benchmarks that we Ed to evaluate uh our uh our product at the beginning um and that is absolutely key benchmarks benchmarks benchmarks uh it's the live Bloods of a company of a product product like this um and so we've organized these into kind of curriculum uh to teach the model uh how to how to solve uh s cyber security problems are better um and the benchmarks are critical to evaluate uh all the other changes that we make the other big component of our uh our proprietary technology are the tools that we give to the llm in order to for Easy Tax um human pantp has a a toolkit of a bunch of things that they use in order to do uh attacks but here it's a bit special because we want these tools to work well with llms for example uh since we were focused on web security initially uh we need a uh a web browser uh that is is driven by the LM you need to click around you need to fill out forms uh and all and so forth and so we created a special a special browser to do that sort of thing thirdly and this is pretty uh important we need guard rails maybe first try to uh triy our product on some of these benchmarks it struck me like an overed super brilliant teenager who would do lots of attacks and find something and then it got very excited and goes I did a seal injection let me show you what I can do drop table this is catastrophic if you use out out at a customer me this is a big thing about pentesting Services you have to make sure that you do not actually do the harm that a real hacker an ADV advisal hacker would uh would do um so we've building been building guard rails to carefully watch over the shoulder of this uh brilliant teenager and stop it uh when it uh is not uh not doing things that might be unsafe um then there is a initial phase of attack surface uh Discovery so what we have is a fantastic exploit finder but you have to point it at the right at at at the right end point to you go or begin forging an attack and so this is running a bunch of tools and prioritizing where to go first and then finally uh as you already mentioned those of course prompt engineering uh three thought prompting or to keep it on track and make sure that it finishes one goal and when it finishes a goal goes on to the next as so forth you described the technology is a brilliant teenager who sometimes overeager and maybe finds an exploit and actually drop drops that table some places in the world there are actors that don't have the same discretion uh to add those guard rails uh what do we do to stay ahead of those actors and make sure that Expo uh can protect those that are doing good against them so first of all we need all the uh all the uh the obvious safeguards in in place we need firewalls and also and in in that type of that type of um technology AI will also play a role but first and foremost we have to make sure that we find the uh vulnerabilities and the exploits before the bad guys do and that's what Expo is is on the about how do you deal with hallucinations and you know I I hear about people saying you know if my LM does 50% or 60% uh gets it right 60% of the time I'm good I imagine security is one of those fields where that is insufficient uh how do you deal with you know managing around the stochastic nature of and the unpredictable nature of these llms so fortunately because it's uh because it's automated uh you just have to run it many times and going back to your earlier question about uh uh about the foundation models what we do see is that the better the foundation models get uh the less attempts we uh we need to make in order to uh or to find uh to find exploits so it's kind of interesting how will influence uh how you deploy and uh package and price uh a a product like this um very much like uh humans uh if you uh if you get a human uh to perform the service for you uh you actually pay for the time for how how how long uh they try how how many things they uh they tried um so we are thinking about uh doing the uh doing the uh the same kind of thing charging our customers on the one hand uh subscription license but on top of that you can pay for attack attack hours if you want to do a really thorough test and make sure that you absolutely find everything uh you can pay more and obviously that would then pay for the inance on on our side I want to get into the pricing and packaging a little bit later because I'm very curious about that and I think you are one of the first kind of examples of service as a software and and so you really Paving the way in terms of how these things are priced and packaged before we get there you mentioned inference time compute uh and you know I think we're we're broadly very excited about what what's happening as more and more of the compute is Shifting uh from pre-training to inference time what do you think the impact is going to be in in your market for us it's uh it can only be good me the the value that we deliver remains constant the price for delivering it becomes goes goes down um we uh we see this even over this very short time that that xbo has been in existence and we only expect that to continue on the probabilistic nature of these LS just revisiting that concept for a second my mental model of what's going on is you have a state space with billions of possible States the actions that the hacker can take the actions that this penetration tester this AI penetration tester can take billions of possible States and you've introduced this really intelligent juristic as to the directions to go you in theory could execute all possible States in perpetuity if you had infinite computer at infinite time but reality you have these constraints and so I I'm wondering is that a reason why this might be the first or one of the first markets to enable full AI automation as in the stochastic nature of it and the fact that even if you find one exploit it's extremely valuable and you don't have the expectation of a complete exhaustive search you do want to be to be sure that you find everything that a very skilled human being would find find and so this is why uh we uh uh we we've kind of exhausted our first set of benchmarks and we now creating a new set of benchmarks to be absolutely sure uh we find everything that uh uh that there is to find uh people uh do these uh offensive security excises not only to find a vulnerability but also to have the peace of mind that it's not easy uh to find uh uh to find stuff they didn't uh didn't know about and so we do have to make that uh uh we have to present the evidence to our customers that we do find everything skill human beings would would find and our people will will insist on on on having that reassurance and when it is found is it verified by a human or by the machine we have a uh a validator that automatically validates that that the report is correct and reproducible and to uh uh before it uh it goes to Human but of course in in the end a human will have to take a look at it and uh fix the problem makes sense love to dive a little bit deeper into the results that you've attained so far so you mentioned you know you're at 85% on your current benchmarks you know at the level of the best human pentesters in the world what are been the most surprising things that you found as you dig into the nature of those results the thing that I found most surprising um was that we originally we only had benchmarks with particular instructions so it would uh uh here it would say something like you're going to test a web web app for managing medical prescriptions try to log in and access the uh the prescriptions of another user and it would do that successfully but then we ran a another task where we took the instructions away completely and just said here's a web here's a web app go explore and the AI was able to find exactly the same vulnerability because it was able to read us on the web pages and say oh this is about medical prescriptions probably it's not a good idea one user can access the prescriptions of another and so it would uh go and find find that vulnerability completely autonomously um I think that that's part of the reason that this technology is so exciting compared to all these security tools that came before uh because these LMS have an understanding of the uh of the real world uh it actually can assess what is important to uh uh to go and test it doesn't have to do this complete exhaustive search of all the possible uh the possibilities it it it can interpret what is important for this particular application that's that's really cool that's really cool and then does the way that the AI system kind of uh reach its results how does that compare to the way that a human pentester would go about approaching the problem I'm kind of thinking of you know Alpha go and move 37 just you know very different from how we as humans would think about it uh what is the model doing so it's early days today it's very similar to was a human being with would um I completely agree that but uh we uh we have to be wary here of Rich sat's bitter lesson in the end because it learns continuous F data on benchmarks on more and more examples um it will start finding uh attacks that were unimaginable uh from a human perspective which is a good thing I mean you say that you say we you say cautiously I'm curious as to why isn't isn't that a great outcome yes yes it's a great outcome so I'm I'm merely I'm really saying that today uh when you read the traces absolutely this is what you would expect a good human human to do um I I expect fully expect that we'll go beyond that uh in uh in a couple of months certainly within years where do you think the biggest remaining room for improvement lies and I'm curious you know as as you mention you mentioned looking at the traces of these models like would you say that they are reasoning already today and is is the furthest further Improvement remaining in the reasoning uh area or how do you think about that uh I think that's so that's that's that's clearly the case but most of the improvements will come from more data um more more reinforcement learning on uh on particular examples um uh and uh as we do set that will lead to a similar Improvement to other games like like go how do you get more data is that just running more simulations or I imagine you've used a lot of the data there is a couple of different ways uh we uh uh we have uh uh quite a few uh contractors uh Security Experts who create uh create more benchmarks for us um there's also the the opportunity of mining open source um so uh we've only recently started doing this uh just letting it lose on a large number of images on dockerhub um and finding uh just let it just let it go and every time it finds something uh uh that becomes a new thing that it's going to learn from and so it might find it uh by doing 100 attempts and so in practice if you had to do 100 attempts that probably wouldn't work at a customer because you would already get shut down because there's too many thing too many attacks happening uh clearly clearly that should be uh that shouldn't shouldn't happen um but because it's open source we can run it on our own service we can do 100 100 attempts but now uh we have the data to try and make them M better to um uh find it more quickly you mentioned open source and dockerhub and So that obviously gets me thinking about GitHub and UIA for those who don't know was the creative brain and Creator behind GitHub co-pilot one of the most widely adopted AI applications in the world was there a moment when you were developing co-pilot or productizing it that you realized this AI is going to get so good that it's going to automate entire processes what people now call agents actually take these actions on entire processes and was there a moment where you said hey security is actually a very relevant area for this to happen so uh I in fact I I wrote a memo in December of 20120 uh where I sketch would later would later become a co-pilot uh but also uh we were already speculating that perhaps it will autonomously be able to to fix uh fix bugs just look at the issue ticket and and now we see this that functionality uh uh emerging so yes I think that that was pretty clear from uh from the very beginning I think the moment where I realized that would happen was um I I took I took a uh a set of exercises interview questions that I normally ask ask used to ask people at haot and asked W to Sal them if you just give it one attempt didn't do it but if you give it uh 100 attempts or even a thousand attempts it would do most of them and at that moment it was pretty clear that as the models get better and they need less attempts they will be able to do these these types of things and one of the things that we also hoped it would do would be be doing security analysis all though admittedly I have offensive Security on my West in the summer of 2020 just yet any other lessons from productizing GitHub co-pilot that you think are are relevant to share here I actually think that the most interesting thing about get a copilot was was done by such a small team uh uh when we launched we were only 10 people something like that and uh uh it's just a testament to how fast you can move uh with a uh a dedicated team of people that believes how big was Expo when you launched the results uh we were 13 people so actually quite big well 13 really brilliant people since you were part of the co-pilot Journey from from the very beginning I'm curious what you think of the current market for code generation AI startups it seems like it's one of the most crowded uh categories competitively right now do you think there's a path to building a company there and you know can one of these startups beats the incumbent GitHub that that already has so much distribution I agree uh so I like a lot of what's going on I particularly admire to work at cursor or at factory um but uh it's really difficult to uh compete with the distribution of a jugon out like like like like GitHub um I do think that there may be an opportunity to go after a different market so get a is uh reing Supreme among professional developers but if you go after people who do not code for a living uh there's a opportunity and repet does this quite well for example how do you think coding will transform in the future like do you think the the market that repet serves do you think that'll just be a dramatically larger and more important Market as as AI kind of continues to take over the world or how do you think coding changes yes so I think the the biggest change is going to be that many many more people are unable to create their own uh their own software um so uh that's a big transformation but even for uh for uh professionals um it will be much more about the uh conceptual ideas about uh sketching an architecture and than having the uh AI fill in the uh fill in the details um longer term I believe that we may be moving away from code as we know it today uh the artifact that you make as a developer is the conversation with the model and so that is what you should store uh because that records uh what the uh what the code is supposed to do uh rather than the uh the details in a particular coding in a particular coding language English is the coding language so skipping the translations down yeah so the English decoding language perhaps with some diagrams he explain it uh explained it better but uh it's just the next the next step in moving up uh in abstraction originally it was all in machine language and we had higher level programming languages and now we're going to uh national language and images so you talked a little bit about education and coding I'm going to go down a little diversion for a second because one of the amazing things about your life is you were a professor for much of it and a very very good one at that so for context U was a computer science professor at modlin College in Oxford and uh modlin is one of the most prestigious colleges at Oxford he was one of the most amazing computer science professors I got to study abroad at modlin it's one of those incredibly Serene places where they've got the Deer Park and the thousand-year-old buildings and the British man who tells me that the door at his entrance is older than my country and and all of the things that you would expect from one of the most prestigious academic institutions in the world including U was a professor and could walk across the grass whereas I Amir student uh would only be able to if I was holding his cape uh with his permission that's right uh and you you left all of that to come into the commercial world uh with seml um 15 20 years ago can you tell us a little bit about your personal Journey from leading academic at highly prestigious institution to commercial CEO redefining the cyber security indry I actually got into into computer science because I uh loved coding um I my my very first uh my very first program was a uh a word processor to show my dad who was a professor of Semitic languages uh could could type his manuscripts on his computer um so when I when I started studying computer science I got totally taken by the uh by mathematics and the uh foundational theories and so that's what I pursued as uh an academic initially um then when I became a professor I wanted to want to go back to my uh love of coding so I started a uh a new research group in programming tools which eventually led to the spin out that that was that was s well I love the Serenity and the peace and CET of a place like mland college um in our field speed is incredibly important and speed uh can only be achieved with small teams that have a profit motive um it's just different uh from uh create trying to invent something because you have a paper deadline for an important conference or you've got to invent it because otherwise this important customer will not sign up um and I I actually loved that that additional excitement and so and pressure uh and so that's that's what led to me leaving uh leaving modam behind and uh uh and uh going full in on on S love it a great advertisement for capitalism so you know I mean if you do really foundational work uh a university on the University of Oxford is probably the best way to do it but as soon as you start doing applied stuff there's no place like a startup I love it I guess on that capitalistic note I'd love to understand how you think about generating profits at Expo and you know since you are one of the first agent first you know Services as a software companies I think you're really going to set the precedent uh for how these types of agentic applications are priced and packaged and so maybe can you just expand a little bit about how you're thinking about how to do that with your offering sure so um we would like to uh uh we would like our uh product to run continuously as part of uh engineering processes I mean that is the main value proposition that instead of doing uh a pen test once or twice a year you RN it continuously after every change and immediately fix problems before they even reach uh uh reach production so if you think about it like that um the most obvious pricing model would be to would be best on the size of the engineering team very much similar to our products like uh gith up or gith up Advanced security however uh there is a different dimension here and we touched on it a little bit earlier in the conversation some customers will want to do a super thorough test really making sure that they exhaustively uh eliminated every possible exploitable vulnerability and in order to serve such customers we should have a service component to our uh our pressing um where you pay for uh if you pay more you get a more thorough test and the way we talk about this is in terms of attack hours how many hours of attack do you uh do you get and so uh if you buy a a normal license it's based on the uh number of engineers in your organization um and that comes with a fixed number of attack hours suitable for your uh your environment um but then if you want to go more uh more thorough uh you pay that extra uh uh that extra Service uh service fee in order to go deeper that's super interesting so you are you are really tapping into kind of services like price pricing models and budgets uh but on the back end you know you have the the gross margin profile of software I right and I but I I think that enterprise software is moving more and more towards um a consumption based model as well and here there is a uh a very clear correlation between uh a tech hour and the benefit to the customer and I think that that correlation between uh resources you consum and the benefit you get as a customer that has to be very clear for a pricing model like that my other takeaway from your modeling story was I mean you've always said this impact interest and impact you got into development tools because they touched people you got into this because you know that this is going to change the world I mean you're you're you're highly confident this type of technology is going to change the world with cyber security whether it's us or someone else um I actually would would put it I would put it a little little differently we absolutely must create uh xbo because if we don't do it uh the the bad guys will get there first and so uh uh for sure I mean we do it because it's interesting and we think that that's a great commercial opportunity but it's also an imperative it's an imperative for the Free World that we actually create this thing to uh to protect uh all the all the software in the Free World and that has been so clear from minute one of meeting that that is the the driver behind you and This brilliant team that you've assembled of academics and Builders and technologists they're I mean incredible the other thing you mentioned was in the model story was speed the ability to move fast and let me let me say you have moved really quickly you and your team what should we come to expect from Expo in a year what do you think will uh will be will be the product let's focus on the product and technology impact what will be happening from a product and technology and capabilities perspective uh a year out you're going to replay this to me in the next board meeting aren't you for board meeting don't worry for so we uh uh I I believe thought in a couple of month so we're currently in a phase where we uh very carefully uh try out the product with a select few early design partners and the reason that we do that is because it needs this human supervision uh in order to control the uh the brilliant teenager uh that we discussed that we discussed before um once we over that that pH and we confident we can let it lose without without any supervision I think uh everything is going to move very fast um uh part of the reason is that this type of product is very easy to deploy um you can just point it at an existing uh servers and uh and immediately uh immediately find uh find results so um I would expect that uh by next summer uh we uh we have significantly transformed the uh uh web security uh uh the state of web security hopefully uh uh by uh demonstrating our work on op Source but also on platforms like hecka oh this has been one of my favorite episodes so far thank you again should we wrap up with a quick lightning round go for it okay awesome number one favorite startups other than Expo sunno I love the way that you can just type in a few words um you get a completely original song it's spine chilling to me uh the the other startup I like a lot is harmonic uh applying AI to mathematical reasoning are you making sunno songs about coding insecurity no there's a great I said my wife a uh a new song about uh about sitting on the balcony at home in M oh that's sweet there's a hard metal one about the AI cyber Warrior I'll TR I think we're actually going to need that okay perfect perfect at our um at our annual AI event that we throw uh we had Mikey from soono there soono there and we crowd created an AI hot girl summer song it was actually very catchy that was great that was great uh oo was there um what other markets do you think AI is going to disrupt with this service as a software model in the short medium and long term so in the short cment this is already this is happening everything related to customer support uh is uh is clearly is clearly going to be impacted by this type of Technology um I think that this is is not exactly uh servers as a software but I think that much of the the problems we currently see with social media uh could be mitigated using this uh uh this this type of technology I mean you all these reports about how social media is affecting the mental health of children all over the world um AI has a has the power to help with with this uh this type of problem and um long term I think health and biology are are the uh the areas where this will make the uh the biggest impact what advice do you have for rather startup venders focus on only one thing move as fast as you can those are I if you do those two things then it will all come all right love it one last question we're going to end on an optimistic note what do you think is the best possible thing that can happen with AI over the next decade I already touched on it uh the opportunities in health and biology and to significantly exp expand Health outcomes uh everywhere in the world is uh amazing uh Dario mod wrote this uh uh this essay uh machines of love and grace and I think he laid out very beautifully there uh what the uh what the potential benefits of generative AI are for all of us okay thank you so much for joining us this has been absolutely fantastic and we're so grateful to get to work with you and for the fact that you're building this on behalf of uh the the right players the people that are trying to to do good good in the world thank you very much for being a pleasure to be you [Music] [Music]

========================================

--- Video 38 ---
Video ID: lVI_J1cbFb4
URL: https://www.youtube.com/watch?v=lVI_J1cbFb4
Title: How YouTube Was Created ft. Founder Steve Chen
Published: 2024-12-05 10:00:50 UTC
Description:
This episode takes us back to the earliest days of YouTube, as the founders explain why it was a longshot that succeeded against all odds. When cofounders Steve Chen, Chad Hurley and Jawed Karim left PayPal to start YouTube, it wasn’t even clear that the nascent broadband infrastructure could support playing video in a browser. In a brief period until its acquisition by Google—from its first incarnation as a video dating site to confronting daunting technical and legal challenges—the early story of YouTube is an underdog tale of scrappy upstarts who ended up changing the world. 

Host: Roelof Botha, Sequoia Capital
Featuring: Steve Chen, Jawed Karim, Zahavah Levine, Colin Corbett, Yu Pan

Learn more: https://www.cruciblemoments.com/episodes/youtube

00:00 - Introduction
02:23 - From PayPal to YouTube: The Genesis of a Digital Revolution
05:24 - Solving the Online Video Challenge in 2005
07:01 - From Dating Site to Video Sharing Platform
09:11 - Overcoming Crickets to Sparking YouTube’s Network Effect
11:39 - How MySpace and Viral Clips Fueled YouTube’s Early Growth
14:17 - Securing Funding and Entering the Spotlight: YouTube’s Rise in 2005
16:05 - Scaling YouTube Amid Explosive Growth and Viral Success
19:44 - YouTube’s Shift to In-House Infrastructure
21:35 - How YouTube Tackled Infrastructure Challenges 
23:51 - YouTube’s Journey to Balancing Ads and Copyright Challenges
28:48 - YouTube’s Turning Point in the Fight for Copyright Agreements
32:24 - The Pivotal Decision to Partner with Google
35:25 - The Secretive Sale of YouTube to Google
38:17 - The Acquisition That Secured YouTube’s Future
41:13 - Content ID: Cementing YouTube’s Future with Copyright Holders
43:10 - Building the Creator Economy and a Global Legacy

Transcript Language: English (auto-generated)
[Music] I I just said look like I don't know if this company's going to be around in 3 to 6 months but what I can tell you is that it's going to be the most memorable 3 to 6 months of your life in your career working here this is the largest fastest growing video service out there and whatever it's going to do it's going to be a story not just for your story book but it's going to be a story of the internet and you know that could be a bad story a good story but it will be a story that people will know [Music] about welcome to Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world's most remarkable companies I'm your host and the managing partner of square capital r bua today's episode is about YouTube one of the most important technology platforms of our time over 700,000 hours of video footage are uploaded to YouTube Every Day by empowering everyone to broadcast themselves and the world around them it has transformed Society but back in 2005 when YouTube was founded by Steve Chen Chad Hurley and jav Kim Broadband was just emerging in the United States it wasn't clear that the internet could even support video say nothing of the massive scale we take for granted today YouTube was a Longshot idea and initially almost no one noticed it this story is particularly important to me I met the co-founders when we all worked together at PayPal after joining sequa YouTube was one of the first startups I partnered with and it changed the trajectory of my career when sequa led the initial funding for YouTube the company comprised just the three founders and they were working out of Chad's Garage in midow park a short drive from our office it was an exciting time for all of us and for the future of the internet as a whole today's episode is about YouTube's early days in just over a year leading up to its acquisition by Google the company confronted Crucible moments that would reshape the technology landscape and popular culture itself all while dodging lawsuits that threaten to kill the business prematurely this is the startup story of YouTube my name is Steve Chen and I was the co-founder and CTO of YouTube I came to Silicon Valley in 1999 to join PayPal and I think I was one of its first 10 employees very early on I really was driven towards coming into Silicon Valley where technology is kind of combined with just ideas and and and being able to be created into a product that people use at PayPal Steve met product designer Chad Hurley and software engineer Javid Kim after PayPal's meteoric rise IPO and acquisition by eBay the they began thinking about their next move it was a difficult decision to make at that point about what to what to do next I knew that the opportunities to either join another startup just like PayPal or Facebook or working for a much larger company much more established and maybe starting a career um or even just staying on at PayPal at the time those were all options I was still in my mids where I said look like I'm going to give myself that period of 2 years or it was about $100,000 of money that ID saved up from the PayPal days and I was like if I run out then I'm going to go back and and pursue this career path but I would really be reluctant to do this without at least trying sometime to make a few swings at the plate and that was ultimately the reason for me getting fully behind a startup of my own creation Steve Chad and Javid began discussing ideas for a company they could start together I had a lot of experience with Chad working together on many products and features over and over over a period of time we had already been working together for over five years Chad was the consumer designer and he was actually the person who designed the original PayPal logo and was responsible for a lot of the ease of use and the delightful design elements in the PayPal website jav had also dropped out of school to join the company and so I remember him working incredibly hard at PayPal while simultaneously doing night studies to be able to complete his degree we had been meeting up for some time discussing various ideas that we wanted to work on this is Javid Kim one of YouTube's co-founders there were a few things happening that were you know really um significant for the formation of this idea so the first thing that comes to mind is the Indian Ocean tsunami that happened on December 26 2004 the Indian Ocean tsunami event was one of the first times I remember where the content recorded by everyday people became really significant so there were no professional like newscasters on site I was following this event and it was difficult to find the videos right so there would be some of these videos were spreading on web servers so you had to download the file first it would be in some unknown codec and you might not have the codec installed so then you wouldn't be able to play it remember this was 2005 most homes in the United States were just getting Broadband into internet for the first time the iPhone hadn't yet launched watching and sharing videos online was a slow and Byzantine process many times videos that were taken in a certain device would not work if you were playing it on a different device um and this is still before this is years before the iPhone right and so you still there was just the the work there and you usually had to download additional software you couldn't just watch anything inside the browser and so I think like uh the real reason about YouTube was still trying to think about what is next we did think that video was going to be the next thing um that it was just naturally LED there in terms of if photo sharing was really catching on people were used to sharing uploading photos with one another but videos was still new and I think that in many ways it was still it was still a challenge whether or not it was even feasible to use internet as the backbone we still had to figure out a lot of these sort of uh technical challenges right on just all the different codecs out there how do you deal with this how do you do uh embedding of a video inside a browser without forcing people to have to download another piece of app to do it so I remember seeing an ad on some website where they were using flash to serve up the video and I was really Blown Away by that cuz I had never seen that before and I didn't even know that flash could play videos and as soon as I saw that I thought wow this could really solve the whole problem with these video codecs with all these different video formats and that was sort of the spark you know what if we had a site where anyone could upload videos and you could serve the videos with flash meanwhile along with the expansion of broadband digital cameras with Native video recording capabilities were becoming widely accessible and Cloud infrastructure was just beginning to take shape this gave YouTube the perfect why now but the Founder's initial idea was to Target a very specific vertical we weren't as bold to say we were going to become the video platform the for the entire internet uh there was a service called hot.com and it was just people uploading photos of themselves and I think uh if you were to visit the site as a visitor you had two choices when you saw a photo you had the ability to vote up on hot and vote down or not and the next photo would come and hot or not we just thought oh well why don't we just do a video version of this service this site and so we came up we I think it was on Valentine's Day in 2005 when we actually registered domain for youtube.com the first version was a dating site and so the interface was completely different from what you see today you cannot really uh choose the video that you're being shown right so you see a random video you have to rate the video 1 to 10 and then you see the next video but uh after we did all this and launched a service in around May of 2005 we released it and yeah a week went by and zero videos were uploaded so the dating site lasted for one week before we made the change to YouTube as it stands today so flicker was probably the biggest photo sharing site at the time we decided hey you know what instead of being like the hotter knut for video we're just going to be let's say flicker for video I think like the good decision there was okay like instead of giving up why don't we give this another try instead of it being focused on dating videos let's just open it up completely to General any video that you want to upload and you want to share but even when you make it more generalized to say any types of videos It's not like as soon as we made that change we saw a huge acceleration of growth the concept of going viral is so synonymous with YouTube that it's hard to imagine now but even after an early pivot from a dating site initially it was crickets no one noticed it was not an overnight success I was working on the site and I would tail the access log meaning it would display like the most recent hit on the web server and I remember tailing this log and there would be absolutely no records for 24 hours so there were many days where not a single person on the internet not even us not not even the three of us would go hit the site so it felt a little bit demoralizing at the time like okay how is this possible like we have all these videos I think it's a cool idea but basically not one person on the planet has looked at this in the last day in business there's a principle known as the network effect the more participants are connected to a network the more valuable it becomes early on the founders faced a crucible moment how could they get anyone to start using YouTube in the first place to get the network effect started one of the analogies I often gave people at this time was with a consumer product it all needs to hang together it's a little bit like listening to a piece of music if 95 % of the notes are correct it sounds awful you know a painting that just has a few brush Strokes in the wrong places and suddenly it's not a masterpiece anymore the atmosphere that we try to create inside YouTube is that if you have an idea present it and and we'll try it out and let's see what sticks on the wall we would just launch all sorts of things like I mean uh I remember like working all night on a feature where it doesn't exist anymore but it's like video responses where you end up having like you like this video so much and you wanted to create a video in response to the video you almost have almost a a conversation that that happens where each person that speaks is in the form of another pre-recorded video that doesn't exist today but it's an example of like look that was just um one of the engineers that had this idea that thought and we said yeah let's we don't know let's let's try it let's give it a shot over time the company iterated with lots of other ideas I mean one simple one is the the video would play automatically as soon as the page loaded you didn't have to move your cursor and go click a play button it it seems like such a small feature but it's all these little things that the company did that all together summed up into a delightful experience for users eventually the lever that truly unlocked growth was a feature that they built at the very beginning the direct embedding of the YouTube video was something that we had built in from day one this is a new brand that you just created a new domain that you just registered in order for you to get this brand out there you really need did the platforms and foundations that people were already familiar with to be able to want to adopt a new service and so we made it so you could put a video in an embedded video on Craigslist you could embed a video on eBay you could embed a video on any platform that allowed you to insert HTML code at the time but we still needed a reason for people to want to adopt this new platform and in 2005 for YouTube that ended up being Myspace a lot of people were sending photos but they didn't have a way to be able to share videos what you would do on MySpace you would encounter one friend that would be showing a video inside their profile it would have a YouTube logo and maybe it's the first time you've seen it but then maybe 5 minutes later you see another friend with another video with the YouTube logo but by you know 2 3 four five days of this and you start seeing more videos ultimately you do click through and you do go to that YouTube link and then ultimately you find other videos that you're going to watch on YouTube and you share it so that virality that YouTube was able to get was really built off of Myspace but it was done in an organic manner in which we spent zero dollar on actual marketing and advertising uh and it was all just built on the consumers the users themselves were ening like with the uh the services that YouTube provided for their own needs and they helped us Market the product the embed feature gave YouTube the initial turn of the flywheel that it needed in order for the network effect to take hold over the following months an active ecosystem started growing in the early days there wasn't you know like one day where it suddenly started taking off but what would happen is that suddenly like every week we would get like some clip that went viral I think one of the first was it was this uh person like throwing quarters um into a jar on the other side of the room and that was really impressive so it it was you know starting to happen that every week there would be some crazy new really interesting clip but just once a week right and then it start to happen more often so then it was like every 3 Days Every 2 days and of course eventually every day there would be some amazing new content so that's kind of how it took off in late 2005 less than a year after launch and having found initial traction the founders went out to raise financing in part to cover the quickly accelerating costs of hosting and Serv videos like most investment decisions we make at sare Capital the decision to Back YouTube was controversial there were few precedents at this point for Consumer internet companies following the Doom crash being able to scale successfully and build thriving businesses I thought YouTube could become the destination for user generated video on the internet that they had a spectacular team and that the product was delightful and easy to use soon after we LED YouTube's first financing and the small team moved into the sequa office while they looked for their own space it became clear that YouTube was scaling at an unprecedented rate it also found its way into the cultural Zeitgeist it became common to see people huddled around a laptop watching the latest viral video in 2005 it was this dawning era period of the internet there were two big videos that received a million video views one of them was a Saturday Night Live video clip and then the another one was it was a Nike video of like ronino juggling the soccer ball off of a gopost and I think it was uploaded by a user by the name of Joe B and so it seemed like it was like a user but it was actually Nike itself and we ended up flying up to Oregon to meet up with the Nike team and the marketing team and praise to them to to even at that time in 2005 to experiment around with different ways of marketing to consumers in a Dawning age of Internet videos but those two videos like okay there's something here that's about virality of videos viral videos became part of the culture and YouTube views continued to climb in a surprise move Javed left the company to pursue a graduate degree meanwhile Steve and Chad set about recruiting a team to help them build out the platform I was upfront with them to say I don't know if this company's going to be around in 3 to 6 months but what I can tell you is that it's going to be the most memorable 3 to six months of your life in your career working here this is the largest fastest growing video service out there and whatever it's going to do it's going to be a story not just for your story book but it's going to be a story of the internet and you know that couldn't be a bad story a good story but it will be a story that people know YouTube's growth was exploding barely a year old the challenge was no longer how to get anyone to notice the platform but how to keep the site from crashing under the strain of new users those early growth days of YouTube they're exciting and they're just filled with stories there are still many of them the large majority of them are Untold much of the growth of the company wasn't so much on the product and Engineering side as you would have in many startups where you're still building more features it was still uh quite a technical achievement to continue to just scale out the service from the back end from the system side from the infrastructure side we were using up to something like 40% of the internet's bandwidth at that time and this is one of the keys that people didn't quite fathom at the time was the team had done an incredible job from an engineering point of view to leverage the beginnings of cloud infrastructure different early stage Cloud vendors AWS didn't yet exist we were able to leverage thirdparty vendors for cloud infrastructure and the company was using open source software to really drive down the cost you had companies back then in 2005 that were trying to kind of out market and and out with their competitors in the hosting Space by saying that we're going to offer you 2,000 gigabytes of data transfer for free knowing at least they thought knowing that nobody possibly could use 2,000 gigabytes of data transfer on a single machine there just wasn't enough until we came along we were on this like M Pop shop called like server Beach and they they're prettyy small operation but we got some kind of deal where we got unlimited with and we're like oh my God this is awesome let's take advantage this as much as possible my name is upan my title at YouTube was senior software engineer you have the team behind server Beach that were they were freaking out about like who is this startup with seven people or something in Silicon Valley that's using up not just their entire bandwidth from their data centers but they had a whole second like it was supposed to be remaining as like a backup uh connection into their data centers we were using up all of it we had a model where anytime we measured as we approached 2,000 GB of data transfer per machine I just went on and just got another machine and so another 2,000 gabes another 2,000 and there's no way that that was going to be a sustainable model for them but of course not their CU before YouTube nobody was pushing out this much amount of traffic when we were looking at projections we were going to Max them out like in no time basically uh so that was quite a bit scary and then like the other side was just like yeah the cost of this B with it's like the building's on fire basically nobody expected a service just to come out of nowhere in 2005 and occupy up what 20 30% of the internet's traffic in a matter of months um and not knowing where it was going to continue to go YouTube had innovated with managed Cloud hosting to get to scale in a cost-effective way but with the cloud providers crying uncle on the terms they'd originally offered they faced a Crossroads YouTube would need to build its own infrastructure as we looked at the future costs of running our infrastructure in managed hosting the numbers were very concerning my name is Colin Corbett and I was the director of networking at YouTube one of the things I did when I first joined was to go and sign us up for our first data center so that we would operate our own infrastructure and be able to more closely control the costs as a result we ended up negotiating our own bandwidth we ended up negotiating our server price and we also also our networking equipment we were able to negotiate down we had to be buying up every one of the machines you had to be going into the trucks on Saturday Sunday mornings to pull the machines out of the trucks bringing them up in the elevators and plugging each one of the ethernet cables and yourselves power was an issue just to power these huge racks and machines um you had to know but the problem back then was you have to physically order your machines four to six weeks before you have to be there on the spot at the parking lot when it gets delivered you have to be the one that actually brings up these 42u racks these 100 PB racks of machines and then you have to plug in everything yourself including network cables power cables I joined in January and the data center was ready to accept traffic in March our managed hosting provider couldn't keep up their equipment had effectively failed on us and so what was supposed to be this nice gradual migration was the website is down the database won't be fixed for days we need a cut tonight to our equipment we did our forced migration because we had no other choice but even as YouTube migrated to its own infrastructure its unrelenting growth continued to present daunting challenges so one example was we realized we were going to run out of web capacity in a few days Saturday was always our Peak day and we' had a new rack of equipment on order for new web servers on order and we worked with our integrator to get our rack ready and you know they it wasn't due for weeks and we convinced them please bring it as soon as you could so they got the rack to us I think at 6:00 on a Friday and then they dropped it off and then we wanted to get it positioned and we had our data center ready to do that but they only had one person on staff the racks fully assembled weighed between 1,600 to 2500 and so we couldn't get it up the ramp to actually get it to our data center so I called a Klein who also was on the team and he and his wife showed up and we all three pushed it up this ramp to get it to the data center Hall so that it could then be that night it was bolted down the network was set up the systems were imaged and everything was deployed and so we actually made it through that night I wasn't actually like really confident for the bandwidth side at least until like we got Colin and I was like okay we have a little bit of breathing room now maybe in so many ways the 2005 2006 was a period when Broadband penetration was just really starting to hit the masses in the US and it was still on the server side I think YouTube was the first case where you really really needed High Network bandwidth and that it required a lot more from the server side than was ever needed before 2005 it was a great team and we all would you know we all relied on each other and we all worked really well to get the uh keep the infrastructure running if we hadn't opened it I I think we'd have had a few things the uh I think our hosting providers wouldn't have been able to keep up in scale and the second one there was that I think the monthly fees from the hosting providers they would have kept climbing and uh we would have run out of money sooner by 2006 YouTube began to focus on generating advertising revenue and building a sustainable business their engineering Feats meant YouTube could serve videos coste effectively and earn a margin even with low ad rates but they immediately found themselves in Uncharted legal Waters what were the implications of earning ad revenue on user generated videos if they contained copyrighted materials I join YouTube as its 23rd employee on my first day Steve one of the founders handed me a sealed box from Ikea and invited me to W my desk my name is zahava LaVine and I was the general counsel and vice president of business Affairs for YouTube I was YouTube's first lawyer YouTube reached out to me because I was one of a handful of tech startup lawyers at the time with deep experience with licensing music for online services of the 23 employees most were under the age of 25 at 37 I often felt like the adult in the room and at times I felt like the corporate grandmother in the very early days uh we knew that for YouTube as a platform to be able to survive and grow and continue to grow we needed to find a fair way to work with all the parties that are engaged on the platform and that included the content creators uh that included the content owners if the content creators were using certain pieces of content that they didn't create themselves and then then of course the advertisers and the marketers that are willing to be able to put their branding and pay for advertising on the platform YouTube had just started experimenting with running ads on the site when I joined but YouTube's ability to run ads was limited by legal concerns users started uploading videos that contained commercial music in all different ways some uploaded copies of official we called It MTV Style music videos but more frequently users were uploading videos ofs eles either alone or with friends singing songs or dancing to music playing in the background the biggest question for me was whether YouTube could build an Advertising based Revenue model without becoming liable for all of the infringing content that users uploaded to the site so copyright was an important issue out of the gate this came up as an important issue as we did references before committing to the investment from those who had experienced the Napster episode you know we're trying to learn from the mistakes that others had made before the financing had even closed already working with attorneys to make sure that we understood exactly what the law provided for to make sure that we did all the right things at the company Napster was one of the first cases where Mass adoption about sharing music content on the internet and they took a very aggressive approach in their defense on how they were going to address the the music labels and the this the Distributors on what was allowed and not allowed on their platform and I think that from the case of YouTube I don't remember the exact dates but YouTube was after Napster but we certainly knew about Napster at the time and it was no like that's not the model to take in 1998 Congress passed a law called the Digital Millennium Copyright Act or the dmca and this law was designed to balance the rights of copyright owners on the one hand and online service providers on the other and the dmca strikes this balance by providing for a limitation of liability known as a safe harbor for online service providers who comply with certain requirements designed to address copyright infringement it was clear to me that if YouTube qualified for this dmca Safe Harbor it had the potential to be wildly successful but if it didn't and YouTube could be held liable for copyright infringement for all of the infringing user videos I couldn't see how the company could be successful it it would all come down to whether YouTube was eligible for this dmca Safe Harbor but the law wasn't clear on its face there was a lot of ambiguity I called a friend of mine who happens to be one of the world's great copyright lawyers and I asked for his advice we met at one of my favorite dive bars in the mission in San Francisco I just asked him will YouTube just face massive lawsuits and get sued out of existence he Shrugged his shoulders and said who knows on one level earning any Revenue at all entailed some risk for YouTube Revenue was quote Financial benefit that someone would surely claim was directly attributable to infringement but if we couldn't earn Revenue we had no business if we have to be the first movers in figuring this out what is the way to make everybody happy in this picture YouTube believed it was shielded by the dmca's Safe Harbor clause and that it could create a business model that would be a win-win for creators copyright holders and itself but other parties in the entertainment industry did not agree with that interpretation to put it lightly Hollywood were The Gatekeepers and then YouTube opened its own gate and a lot of people walked through it the music companies and and Hollywood had large anti-piracy teams that were sending us hundreds and in some cases thousands of copyright takedown notices a week we realized pretty quickly that we would need to close content licenses with theic music industry and Hollywood although YouTube managed to negotiate deals with companies like Warner Music and Emi others were strident Universal Music Group fully Unleashed an orchestrated campaign of fear tactics on us after months of somewhat friendly negotiations I remember umg just suddenly flipped on us I recall one exac with with whom I had previously had a good relationship literally scream at me over the phone YouTube was built on the backs of our artists and owes us hundreds of millions of dollars the single worst business meeting of my life was when I flew down to Los Angeles for a meeting with Universal Music they put all of their senior heavy lawyers in a conference room and as soon as the meeting began they started waving a copy of a lawsuit complaint and demanding tons of money it really struck me that they were not interested in a win-win relationship with the service and that they would do everything in their power to make our life miserable I mean they hold all the cards you you have something on your platform that is owned by Universal and there aren't that many choices that you could take it didn't take long in this meeting before Chad agreed to pay a big sum of money in exchange for a license agreement we would still need to hammer out a sum of money we had no no way to pay at the time I thought that meeting was just awful I was sure we had substantially overpaid I was sick to my stomach and I was worried that other music companies would adopt the same tactics to try to extort crazy amounts of money from us I had rarely encountered a business experience like this you know Silicon Valley just has a very different tone and mentality when it comes to business relationships and a much more of a win-win let's build something together attitude the idea that you were going to create value and yes there there there's a question about exactly how to divide the pi but ultimately the goal should be grow the pi as big as possible and the sense I got from my meeting at Universal Music was a fixed mindset there's only so much value and we Universal need to do everything in our power to squeeze as much out of this as possible to your detriment that was a pivotal meeting honestly I think that that meeting really shook us as a team because of the aggression and antagonism they displayed it was truly hostile I think this umg Shakedown was a turning point I think it was a turning point for Chad and possibly sequa I think it helped Chad and ruoff appreciate the gravity of our copyright issues and perhaps warm up to the idea that selling YouTube to a company that could commit more resources to our legal challenges was not a terrible idea it was now clear that a flood of litigation from rights holders was in evitable YouTube's life was on the line how would this young company be able to handle massive lawsuits from some of the world's biggest corporations there was a potential solution however that could not only help protect the company from impending lawsuits but ensure its longevity various companies had shown interest in acquiring YouTube including Google whose own video product had failed to catch on with users anytime you contemplate the sale of your company I think it should be called a crucible moment it's at this point where the founders even if they have the ambition to build something enduring that is Standalone they may feel that the company's future is in better hands with somebody else that can help them realize that ultimate Vision unfortunately often times big companies swallow little companies and maybe by accident smother them they don't enable them to realize their potential in 2006 a decision had to be made within the board about what our next major step was going to be for YouTube and whether or not that was going to be through an acquisition or do we continue to operate as an independent company and try to figure all this out ourselves and me honest with you we're kind of forced down that path of going down the acquisition route with Google was really on the on the legal side of things we were just too small we could barely even hire because everybody was so busy keeping it all together with Band-Aids at that time it was clear to me that we needed help I was worried at that time that if we didn't sell the company it would be very expensive and timec consuming for us to fight the copyright battles I had confidence that we would Prevail Prevail but in 2006 you didn't have the kind of late stage financing environment that you have today it wasn't easy for us to go and raise hundred or $200 million to fight the lawsuit and so I also think it would have been pretty tricky to go and raise money for the explicit purpose of winning a lawsuit so I think it was daunting to be able to take on that challenge as an independent business so for me the that was a very difficult period like the product was working great the engineers we had a great team but it was really survival of this product of being able to share videos online on the Internet space is that going to be feasible or not is it time to sell the company in order for YouTube to really truly reach its potential it really would benefit greatly to be working in tandem in collaboration Under the Umbrella of a Google rather than trying to do it independently on its own we decided internally and this was still mostly through Chad and me to say like look in order for this thing to scale out we really need a bigger partner and um somebody to be able to hand hold us through this but there was a risk if the big media companies caught wind of a potential acquisition they might accelerate the lawsuits they'd been threatening we reached out directly to the board of Google and uh we arranged a meeting to talk about engage their interest on look like we want to move forward with a sale but we need to move quickly on this we need to do it in a kind of affertive secretive manner because we don't want it to be known by Universal and Emi that we're talking to a much bigger company for an acquisition in internally deciding that we want to move forward with an acquisition to talks internally about who potential acquire hers could be to meeting with the key individuals in these firms and these companies that can actually make that decision to actually writing out the the actual contractual terms on the and all the way to finalizing it and making the announcement on Wall Street all that was compressed into less than one week's time the sale of YouTube to Google has got to be one of the fastest deals of its size of all time I think it took something like five days from signing the term sheet to signing the long form and I think it was probably the hardest 5 days of my life I was operating on pure adrenaline while helping negotiate the merger agreements for the sale of YouTube to Google two other things were going on first we needed to close the three remaining major record label licensing deals that we had been negotiating at that point for months Google wanted them done before close so in three days we finished these deals that would normally take months and on top of that in addition to negotiating these record label deals YouTube was moving offices that very same weekend the move had been planned for months like there was nothing we could do to stop it and I remember Chris Maxi and I were in the office in San Mato above amichi at like 2 a.m. on a Saturday morning um night morning trying to finish the record company deals we were sending drafts back and forth with the record companies at like in the middle of the night and we'd print each draft and mark up our comments and pen and suddenly in the middle of all this our printer stopped working and we were like what's going on and we realized the movers had unplugged it and were like packing it up and we had to B them like sir I know you're just doing your job but please leave the printers I promise we're authorized to instruct you to leave the printers it was nuts I slept for only a few hours the entire time and in a hotel room right next door to our Law Firm I didn't even bother going home and we got it all done in November 2006 Google acquired YouTube for $1.65 billion a staggering sum at the time Google asked stepen Chad to stay on to lead YouTube as a Standalone brand under its umbrella and phased out its Google video product I think there was a real meeting of the mind and an appreciation that Google's interest was in helping YouTube flourish and it was clear to the founders that Google had a sincere desire to enable YouTube as an independent business to flourish they made very clear commitments that the company could retain its independent brand it wouldn't be called Google video it would be called YouTube they would maintain a separate office in and Carlos and they wouldn't all have to move down to Mountain View where Google's headquarters was based and Google also was willing to make the investment necessary to fight the copyright Lawes that have been filed and also to invest infrastructure the data center infrastructure to enable YouTube to continue to grow into the the scale that it's achieved today I I'm just still grateful of the way that Google handled that acquisition that they still um were able to put us in control to trust a couple 28y olds to run this thing Google did a fantastic job in never completely enforcing this Google umbrella over YouTube they continue to even till this day YouTube stays separate in terms of a lot of its reporting structure organizational structure and even just physical addresses of the buildings their YouTube offices and their Google offices Google had the resources to defer revenue and to take the Long View sure enough just a few months after Google acquired YouTube the major media conglomerate sued Google seeking over a billion dollars in Damages they argued that YouTubers responsible for all the copyright infringement of the of users who it alleged uploaded over 150,000 clips of Viacom owned programming without authorization which had collectively been viewed 1.5 billion times this was it this was the existential lawsuit we all knew was coming and it got worse because short shortly after Viacom Su at least two class actions were filed against Google on behalf of sports leagues and music Publishers and a class of quote all copyright holders in the world unquote that was uh quite something YouTube's sale to Google was The Crucible moment that enabled all the subsequent Crucible moments that make YouTube what it is today I'm absolutely clear that we made the right decision to sell it with that amount of time at the time that we did and to sell it to Google there was a high risk that YouTube would have never made it out of 2006 2007 if it weren't for that acquisition with Google Google not only had the resources to fight the lawsuits against YouTube but it also supported YouTube as the company deployed systems to instantly identify copyrighted materials posted to the site one of the heralded pieces of technology that YouTube built that assured its survival and honestly why I think the company has thrived is copyright ID tags with Google's resources and while we were litigating the Viacom lawsuit we worked very hard to build very sophisticated automated systems to accommodate copyright management and content licensing at scale with Google's help YouTube was able to design and build a Content ID system that was substantially more robust the system scans each of the millions of YouTube videos uploaded every day against a massive database of sound recordings and audiovisual works provided by copyright owners all around the globe and when the system detects a match meaning that it's identified third-party copyrighted material and a user uploaded video the system honors the rights holders choice to either license it monetize it and share in the revenue or to have it removed from the site and like an overwhelming majority of the content of the rights holders using the content ID system elect to license their content and share the revenue on on YouTube as far as I know the YouTube content identification system is the most advanced system of its kind in the world to this day it's been crucial it you know it wins the hearts and minds of copyright holders that know that YouTube is a partner to theirs not an adversary YouTube is a valuable distribution partner it generates Revenue that it shares back with these copyright owners by building a platform to responsibly share Revenue with creators and rights holders alike YouTube would go on to single-handedly bring about the so-called Creator economy enabling everyday users to monetize their content in 2010 a District Court throughout viacom's lawsuit ruling that YouTube was in fact eligible for the dmca Safe Harbor and therefore protected from copyright liability YouTube is nearly 20 years old and due to its ubiquity and reach has shaped some of the most seminal cultural moments of the 21st century one of the most like memorable moments for me at YouTube was when we re received a call in 2007 from CNN they wanted to host the Democratic debates for the elections in 2007 at the time uh and they wanted to do it using utilizing and partnering with YouTube where the questions wouldn't just be coming from the two three moderators uh but it would actually be coming from YouTube users and that was when I was started realizing this is far bigger than something that we created back in 2005 today YouTube has over 2 and a half billion monthly active users including more than 100 million paid subscribers a little over a third of the world's entire population watches YouTube every month YouTube achieved many impressive accomplishments and Crucible moments in its early years the technical infrastructure required to store and deliver and unprecedented volume of video content around the world the historic licensing deals with the music industry that licensed on a blanket basis for the first time ever entire cataloges of music for use and user uploaded videos it's not fair to look at the end result of where I am when the finish line of see where YouTube is and to see how all this magic happened I think it's a lot more realistic to look at where I was in 2005 and when I was there in 2005 it was a video dating service that we thought was going to be the next big thing right and of course it went through hundreds of cycles of evolution and path turning and and turns to be able to get to where YouTube is today but in order for it to reach YouTube to reach to the state that it was at those first few steps had to be taken from us as the founders and the entrepreneurs of the company multiple Generations are watching YouTube as their primary source of entertainment source of Education source of content globally all around the world um and people are entirely creating careers completely just built off of YouTube the platform the key message is that if you have an idea if you want to start something I think just don't ask anybody else just um especially don't ask your mom like I did at least once in your life I just recommend highly try to do something on your own especially if you do have an idea that's been brewing in your head for a while like try it and you'll know 3 to 6 months uh if it's going to work and even if it doesn't work it's going to be the most memorable 3 to 6 months of your life it's so fun to try to actually create something and an idea that you have and it's so entertaining such a great gift when you actually see other users using this product idea that you had in your head it's just hard to be able to get that same kind of return on anything else you do in life one of the unsung heroes in the YouTube story is Susan wiesi an early Google employe who championed acquiring YouTube and later served as its CEO for 9 years until 2023 tragically she passed away in August 2024 Susan's contributions and stewardship of the brand were critical to YouTube scaling into the global platform it is today [Music] this has been Crucible moments a podcast from sequa [Music] Capital Crucible moments is produced by the Epic stories and Vox creative podcast teams along with sequa capital special thanks to Steve Chen Java Kim upan Colin Corbett and zahava Levine for sharing their stories [Music]

========================================

--- Video 39 ---
Video ID: gFWwB3z8ags
URL: https://www.youtube.com/watch?v=gFWwB3z8ags
Title: Using AI to Build “Self-Driving Money” ft Ramp CEO Eric Glyman
Published: 2024-12-03 10:00:44 UTC
Description:
When ChatGPT ushered in a new paradigm of AI in everyday use, many companies attempted to adapt to the new paradigm by rushing to add chat interfaces to their products. Eric has a different take—he doesn’t think chatbots are the right form factor for everything. He thinks “zero-touch” automation that works invisibly in the background can be more valuable in many cases. He cites self-driving cars as an analogy—or in this case, “self-driving money.” Ramp is a new kind of finance management company for businesses, offering AI-powered financial tools to help companies handle spending and expense processes. We’ll hear why Eric thinks AI that you never see is one of the most powerful instruments for reducing time spent on drudgery and unlocking more time for meaningful work.  

Hosted by: Ravi Gupta and Sonya Huang, Sequoia Capital

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 40 ---
Video ID: nOyOtxTA03A
URL: https://www.youtube.com/watch?v=nOyOtxTA03A
Title: Getting the Most From AI With Multiple Custom Agents ft Dust’s Gabriel Hubert and Stanislas Polu
Published: 2024-11-26 10:00:46 UTC
Description:
Founded in early 2023 after spending years at Stripe and OpenAI, Gabriel Hubert and Stanislas Polu started Dust with the view that one model will not rule them all, and that multi-model integration will be key to getting the most value out of AI assistants. In this episode we’ll hear why they believe the proprietary data you have in silos will be key to unlocking the full power of AI, get their perspective on the evolving model landscape, and how AI can augment rather than replace human capabilities.

Hosted by: Konstantine Buhler and Pat Grady, Sequoia Capital


00:00 - Introduction
02:16 - One model will not rule them all 
07:15 - Reasoning breakthroughs
11:15 - Trends in AI models
13:32 - The future of the open source ecosystem
16:16 - Model quality and performance
21:44 - “No GPUs before PMF”
27:24 - Dust in action 
37:40 - How do you find “the makers”
42:36 - The beliefs Dust lives by
50:03 - Keeping the human in the loop
52:33 - Second time founders
56:15 - Lightning round

Transcript Language: English (auto-generated)
we've asked the entire world to move from calculator technology punch the same Keys you'll get the same result to stochastic technology ask the same question you'll get a slightly different result this has not happened this is the biggest shift in you know the use of the tools that we have since the Advent of the computer we're asking an entire cohort of the workforce to move to a stochastic mindset and the only way you get that is by having a risk reward ratio that you're comfortable enough it's like you know what I don't I'm not asking it to be right 100% of the time I'm asking it give me a draft that saves me time many many many times over and that distribution of Roi is something that I'm comfortable exploring with and it's rating on and I think that that is really one of the predictors that we see in people who've tried chat GPT or in people who are just curious with new technology is they expect that some of it's going to be a bit broken but the upside scenario to them is so clear and so 10x that they're willing to make that trade-off or that local risk to uh to get things started [Music] welcome to training data this week we welcome Gabriel Hubert and Stanis L po the co-founders of dust a unified product to build share and deploy personalized AI assistance at work founded in early 2023 after spending years at stripe and open AI second time Founders Gabe and Stan started dust with the view that one model will not rule them all and that multimodel integration will be key to getting the most value out of AI assistance they were early to be convinced that access to the proprietary data you have in data silos will be key to unlocking the full power of AI and they know that you want to keep that data private we've worked together for 18 months and their predictions have been consistently preed so today we decided to ask them about those predictions we'll get into their perspective on how they see the model landscape evolving on the importance of product Focus over building proprietary models and on how AI can augment rather than replace human capabilities Stan Gabriel welcome to trading data thank you glad to be here yeah thanks Constantine super happy to be here guys first thing that I want to ask is you started this company in early 2023 at the time it seemed like one model might rule them all uh and that model at the time was I think it was probably G gbg 3.5 I don't know if four had yet come out but that was way ahead of the curve and people were super Blown Away you guys came out with a pretty contrarian view that there actually would be many models and that the ability to stitch those together and do Advanced workflows on top of that would be important so far you've been completely right how did you get the confidence to make that decision a year and a half ago yeah I think on the on the M part it's uh it was clear that um many Labs were already emerging it was not clear from the kind of General audience but for the people that knew the Dynamics of the market was cl many laps were emerging and I think um it was uh kind of natural to us that there would be competition in that space and as a result uh there would be value in enabling people to quickly switch from one model to another to get the best value depending on their use cases yeah and I think from the from the use a standpoint the the point on being able to quickly evaluate and compare is obviously important looking ahead or already at some the conversations we're having um it seems that the levels of scrutiny security sensitivity of the data that's being processed may also influence some different use cases and so we're excitingly seeing people thinking about running smaller models on device for some use cases and you could imagine a world where you want to be able to switch between an API call to a Frontier Model for something that's less sensitive absolutely crucial to get like um Cutting Edge reasoning capabilities for and some smaller classification or summarization efforts that could be done locally uh while the interface that you use uh for your agent or your assistant Remains the Same that and that switching sort of requires the ability to to have a layer on top of the models so you you guys have been read about this every time uh as you've called this out and so many of your predictions over the past couple years of partnership have been uh non obvious and then correct uh I think this is still not obvious as in there will be many models you'll have some local models you'll have some API call and then you actually as a customer want to choose between them or want to have some control first of all why do you think that will be as in why will there be multiple models secondly why doesn't that get abstracted away by some sort of router mechanism some hypervisor layer uh and does that happen and would you be that hypervisor layer uh yeah help me understand that I think there's really uh two two modes of operation in free think about the future so it's a it's a Bodel distribution basically of the future there's one where the technology as it stands today keeps uh progressing rapidly in which case there's still going to be competition from rather big Lads because there's be incredible need for gpus to build those larger and larger models because the only way we know to get those model better today is mostly by scale in that world the this kind of dynamic of being able to switch to the best model at time T will remain true for a long time I guess until we reach wherever it is that is at the end of that Dynamic and then there's the hypothesis and we can talk about that uh later in more details the theis of maybe the the technology plate uh in which case it's not going to be one model it's going to be a gazan model and eventually everybody will get this their model and eventually on your MacBook M6 you'll be able to to to train a GPT 6 uh in a few hours in a couple years and then the kind of router uh the rout need those kind of disappears because the technology is really commoditized uh in terms of just producing the tokens and every company will have their own model we will we would have our own model in that world well we got to push you on that so there's you're building a business where you you sort of win regardless of which one of those worlds we go into which one of those worlds do you think we're going into this one is definitely tricky I it's it it it's it's interesting because um so in terms of capabilities of the models we we've seen or had the perception that the ecosystem was moving very quickly over the past two years we've seen larger context support for audio support for image and stuff but at the same time the One Core thing that matters for changing the world is the resoning capabilities of those models right and the current reasoning capabilities of those models has been actually pretty flat over the past two years uh they are at the level GT4 as the end of its training which is roughly slightly over two years ago if I remember correctly for the end of the internal training at and so that means that over the past two years in terms of reasoning capabilities it's been somewhat flat well so hang on so there's the Kevin Scot point of view which is there's actually exponential progress but you only get to sample that progress every so often and so in the absence of a recent sample people interpret it as having been flat when in fact there's just sample bias you can't see it so do you do you Subs do you think he's right do you think there's actually exponential progress we just haven't gotten to see it yet or do you think it's actually like ASM toting and reasoning breakthroughs have not progressed at the rate one might hope uh as far as I'm concerned I have a strong feeling that it's uh it hasn't been moving as fast as I would have expected in my most uh optimistic views of the technology and so that's why I'm I'm I'm allowing myself to to ask the question any or simply consider the different scenarios what I think one of your predictions for 2024 also is that we would have a major reasoning breakthrough do you think it's coming yeah it's going to be a tough one because uh this one hasn't come for sure and even gp5 or GPT n plus1 or or clro n plus1 doesn't matter who who cracks that first hasn't come yet uh and there's there's many reasons to to believe it might not be a a core technological limitation you can make many hypothesis as to why it might be the case that it takes time um the the scale of the Clusters required to train the next generation of model is humongous and it involve a lot of complexity from an infrastructure and really programming standpoints because GPU fails when you scale to that many gpus per cluster they fail pretty much all the time all the training is very synchronous across a cluster and so it might just be the case that scaling up to the next order of magnitude of GVS needed is just very very very hard and that that that wouldn't be kind of a inherent limitation it's just a phase where we learn how to go from red one to Red five but for gpus basically Stan you were uh you were at open AI at a pretty critical point in time so people know you from the dust experience but one thing that it has to be remember about Stan is you were a critical researcher at open aai from 2019 through late 2022 you got a one a bunch of wonderful Publications some of them relates to mathematics and AI you worked on these with Ilia sitk and the crew at open AI do you think that mathematics will be essential to this type of reasoning breakthrough or is it orthogonal that's something that we're actually going to learn on textual language data I I remain quite convinced that it's a great environments to to study and it's it was a tesis that we had at time with G Lum we uh then founded mistra he was working at Fair on exactly the same subjects and our motivation was exactly was really shared at the time we really were friend is competing in the in in the workspace but really FR by the ideas um I think the the uh the the the idea there was that mathematics and in particular in its form of formal mathematics that gives you perfect verification is a very unique environment to study reasoning capabilities and to push reasoning capabilities because you have a verifier so you're not not it's con constrained by being able to verify the prediction of the model that in an informal setup would require humans checking them uh to some extent and so that very bit is probably something that has to unlock something at some point it hasn't yet for many reason but at some point it should unlock something so I remain extremely bullish on the kind of mass and formal mass and LM studies yeah I remember one of the ways you were presenting it to me when I was still very much ramping up was uh in math as the door to software software the door to the rest and and started with some of the critical systems that were the very only ones to have been hand proven and hand verified as an example of how much more costly it was to do it by hand than do it by Machine uh and an an indication of the future gains we could expect from being able to extend that and and democratize that uh you guys see a lot of action through the dust API calls when you build a dust assistant you're able to choose what type of underlying model to use uh you're able to call many different models like me as a user I often call not just Cloud 3 but I call GPT 4 and I call the dust assistant and I call uh in my custom assistance I select one of many options what have you guys seen in terms of Trends what's performing really well I've personally been super impressed by the anthropic models as of late um but you guys have a much closer view of that I I think the um I mean so word of caveat on on on Trends you know you're going to have the usual cognitive biases The Grass Is Always Greener people we're going to want to switch just to see what it looks like on the other side and so when you're observing those switches you're not necessarily observing a conviction that the bottle on the other side is better you're just you're observing the conviction people want to try um but it is true we've gotten great feedback on on claude's latest sonnet release um and empirically we're seeing some stickiness on on on that model um in in our user base I think that uh you know word on the street is uh for some coding application Cod St is actually performing very very well we haven't uh yet made it available uh through dust uh but we're it's yesterday ah there we go sorry see this is the thing you get for read San Francisco and waking up so you're a recording at 7 o'clock in the morning so yeah code St apparently is is really interesting on some on some coding capabilities and then you have to mix it in with the actual experience that people are getting so you reasoning uh cannot be fully uh made independent from latency latency at some points last year could be you know basically a way to tell the time in San Francisco you you could see latency literally in the API as people were waking up on the west coast so uh people have use cases that may be more or less tolerent of those um so we we cover the Gemini uh models anthropics models open airs and mrr right now and uh and we have seen some interest in moving away from the default which when when we first launched were open AI models not to say that 4 isn't isn't performing very very well over the past year there's been a lot of enthusiasm about open source models and it's actually one of your predictions St you have these great predictions every year about AI I always really enjoy reading them one of them was that at some point this year an open source model take the brief lead uh for llm Quality that doesn't seem to have happened yet and it also seems like the enthusiasm uh around not the enthusiasm around but rather the lead SL acceleration of uh the open source models in comparison to the closed Source models has maybe slowed down a little bit maybe back to that Kevin Scott point about we sampling at discrete times as opposed to continuous times we just haven't seen it yet but where do you think the open source ecosystem is going to go will it actually at some point uh surpass the closed Source ecosystem I mean that Rems that that that that echos with what we said earlier it's really in that b model distribution there's one distribution where open SCE goes nowhere and there's one distribution where open source wins the whole thing right because uh if the technology platters open source obviously catches up and eventually everybody can train their their high quality model themselves and at that point uh there is uh no value in going for for proprietary model um so I think there's a there's a scenario where open source really is the winner at the end uh which would be a fun turn of event obviously uh and then uh in the current Dynamic it's true that open source has been lagging in behind so far obviously there's um I think the the one that has to be called out is really Facebook or meta eff thought because they have what it takes to train an excellent model and so far has been releasing every model very openly and so that's exciting to see what will come out of them uh in those next four months uh to maybe make the prediction true the cave adds to that is that assuming the best model are the largest which is a somewhat safe assumption yet it can be discussed uh it means that that model will be humongous to some extent and so that means that even if it's open source nobody will be able to make it run right uh it'll just cost too much money uh you'll need hpus just to do in France um and so that will really Trump the kind of usage of those models even if they're better in the current state of refers in terms of cost of running them it's a point for consumption that's interesting because that means that you might still have a world where um there's a lot of API based inference demand for API based inference regardless of whether the model on the other end is controlled hosted open weights whatever um just because of the the technical abilities to perform that one one of your founding assumptions kind of related to model quality and model performance and this goes back almost two years now was that even as of two years ago the models were powerful enough and potentially economically viable enough that you could unlock a huge range of unique and compelling applications on top and that the bottleneck even at that point was not necessarily model quality so much as product and Engineering that can happen on top of the model I don't know if that's a consensus point of view today you know we still hear a lot of people who are sort of waiting for the models to get better for what it's worth we happen to agree with you but the question is what did you see in 22 that gave you that point of view and if we fast forward to today what has your lived experience been deploying this stuff into the Enterprise in terms of where are the product and Engineering unlocks that need to happen to bring this stuff to fruition my triggering point for for living up AI was seeing and playing with gp4 and it it is it was coming from two very contradictory motivations the first was I said it before it is is crazy useful nobody knows about it nobody can use it yet and still it exists and literally it's almost already in the API I mean at the time it was GP 3.5 in the API which was kind of a slightly smaller version of gp4 but on the same train data it was a crazy good model um which was and it was basically codex the the Bas model and um it was much better than chpd and it was available in the API and yet the AR of open a was ridiculously small at the time like in existence by all standards of what we see today and so that was kind of the motivation and and that was mixed with the fact that I I I I was starting to feel the uh the I mean I had the intuition that it would be hard to invent an artificial mathematician with the current technology and so I was kind of seeing it not a dead on a very long pass slow pass forward on what I was working on and at the same time I was seeing the the utility of those models already when you use them for your day-to-day tasks and so that was first motivation and the Very contradictory motivation that I shared with Gabriel at the time was if that technology goes all the way to AGI it's the last train to build a company so we better we better do it right now because otherwise next time it's going to be sh and I absolutely didn't answer your question but I let Gabriel answer a question I I think I think what got me excited and when we did start brainstorming on on the ways to to deploy this just raw capability in the world what where it made sense to dig was um one Insight on some of the limitations of the hyper and fine tuning at the time that people were talking a lot about fine-tuning a lot of consultancy firms were selling a lot of slides that were essentially telling big companies to spend a a lot of money fine-tuning and and the two things that cut it for me was Dan saying you know one it's expensive and you do it regularly and nobody knows that they'll have to do it regularly uh and two it's really not the right idea for most of the things people are excited to F tune on and in particular like fine-tuning on your company's data is a bad idea as opposed to maybe sometimes fine-tuning on some specific tasks where you can see gains but um the idea that bringing the context of a company which is obviously every real company Obsession like how does this work for me how do I get it to work the way I like it to work uh was going to happen with technologies that weren't just changing the model itself but rather controlling the data it has access to controlling the data any of its users have access to and those are somewhat Hybrid models between New World and old world uh the very old world version of it is you know the keyh holders are still the same the ceso is the one deciding how new technolog is exposed to members of a company the guard rails that are in place the observability that's available to the teams to measure its impact and and any data leaks those are old software problems but they still need to be rolled out on very new interfaces because the interfaces now are these you assistants these agents um and then some of the new problems are around access controls does access controls look and feel the same in a world where you have half of uh the actions done by non-humans I might want to have access to a file that's like 2020 like do do I have access to the file yes or no in 2024 it's like well maybe an assistant might have access to the file and can give me a summary of it that leaves out some of the critical information I should not have access to but still gives me access to some of the decision points that are important for me to move on with my job and that set of Primitives that set of nuances just doesn't really exist in how documents are stored today so if you think about deploying the capability in a real world environment where people are still going to have to face those controls and and those guard rails the product layer is actually very thick the application layer to build the logic and the usability uh to ensure performance but also adoption is is is quite thick and that was the I think that was the go to say all right there there's a lot to do here we might get started maybe you can dig into that because when we intersected in Q2 2023 q1 Q2 2023 a lot of people were still starting these Foundation model companies and you guys had a very specific opinion which is the future is application layer and there's going to be a lot going on under the hood and we're just going to be an abstraction layer on top of that and let things happen as it see as it happens uh we're going to succeed in any case by building something that people actually used and love first how do you have the conviction for that secondly how has that been playing out what has been the hard part about it you mentioned the cesos and the Enterprise and Enterprise deployments uh you guys have been way ahead of the curve on rag I mean everyone was talking about fine tuning but you guys have done so much in ter terms of uh retrieving this was before it was even called that really retrieving and actually making smart decisions around information walk us through the step by step of from the idea of application layer to where you are today you can imagine the application layer conviction existing in a world where you still decide to build a Frontier Model the reason we split those two is one it seemed like a lot of money for a lot of risk and I mean a lot of money for a lot of risk to try and develop a Frontier Model or an equivalent to a fr model and also make a bet on the way it was going to be distributed and um it's so our our internal slogan was like no gpus before pmf like we we don't see the value in training our own model until we actually know which use cases it's going to get deployed on uh and there are much cheaper ways to explore and confirm which use cases are actually going to uh make most of value and generate most of the engagement um the second uh reason was was about this this this this data contradiction like the fact that the cut off dates for training on internet data uh are hard to set continuously the fact that you can't actually get an internal understanding of what happened last week in a Frontier Model means that fine tuning is a hard problem that it is not a solved problem at scale and so if you walk from that conviction backwards that means like it's there are many cases where it's not solved so another technology has to be the one to deliver um most of the most of the gains and extracting a small piece of context uh from documents where it lives feeding it into the scenario the workflow that you need help for the the one Trend that seemed interesting was that actually many decisions require limited amounts of of context and information to be greatly improved so the context windows at the time that were small we're already compatible with some scenarios of saying let's just bring the the information in and what we've seen over the last year of course is the the increase in size of those context Windows which just makes it easier to expose all the right data no more than the right data hopefully to the reasoning capabilities of the of the Frontier Model um and what we've what we experienced is first of all it takes time for people to understand those distinctions it's hard and you have to get yourself out of your own bubble regularly to realize that it's true the world um the the future isn't quite evenly distributed yet and people have varying assumptions on what it means to roll out AI internally or roll out the capabilities of of these Frontier models on their on their workflows and you have to walk them back on your what they really care about which is always very simple things you know I want to work faster I want to know the stuff that I'm missing out on I want to be more productive or more efficient in some tasks that I find repetitive and then only bring the explanation of what technology is going to solve that when it's absolutely necessary because people will worry about their experience and how they feel about it more than how it's working under the hood 99% of the time uh the big Insight that's happened and that I think we're leaning into we have been for a while and it's great to see some of the market also doing that is people are actually really good at recognizing which tool they need in the toolbox like I think we've we've we've not respected users enough in saying you need a single user that does absolutely everything and you know the routing problem should be completely abstracted from you you should ask this question to the one Oracle and the Oracle will reply people are pretty comfortable telling a screwdriver from a hammer and you know when they want to get to work and they need a screwdriver they're very very disappointed where one they get is a hammer and it sounds like a hammer response and and so specializing agents specializing assistance and making that easy to do design deploy monitor iterate on improve all those verbs that require prod surface H it it was it was quickly apparent to us that people were very comfortable with that and uh and so the the number one question that made us feel like we had an insight to hang on to an and lean in on was everybody asking us about D was obsessed with the top use case it's like what are people using it most for what is the top use case across companies what a and and I could almost see the the Amazon eyes trying to decide which diapers.com they're going to verticalized and integrate like which verticalized use case should we now just build as a specialized version of this but I think the full story is fragmentation I think the story is like giving the tools to a to a team or to a company to see opportunities for workflows to be uh improved proved on uh augmented and understanding the Lego breaks that are going to help them do that so rather than um encapsulate the technological breaks that are Ed for and Abstract them away from users exposing them at the right level gives people a ton more autonomy H and really just the ability to design things that we had never thought of some of the scenarios that have come up we we literally could not have imagined ourselves that idea Mak sense like the fragmentation and providing people with the Lego blocks to see what sort of use cases emerge just to make it a little bit real though can you share a couple of use cases that you've seen in your customer base that have been unique or surprising or particularly valuable just something to make it a little more tangible there's obviously a ton that people are thinking about like the category of obvious use cases that have been interestingly and quickly deployed are enablement of sales teams support teams marketing teams and that is essentially context retrieval and content generation uh I need to answer a ticket you know I need to understand what the answer to the ticket is and generate a draft to the ticket uh I need to talk to a customer I need to understand which vertical they're in and how our product solves their problems and draft an email to follow up on their objections um I need to uh prepare a blog post to show how we're differentiated from the market again like I'm going to go and plow into what makes us special and generate with our tone of voice those were were were pretty obvious and and and quite expected what I've been excited by is to see two types of things one very individ SCH assistants personal personal coaches um people generally actually quite young people in their first years of career asking for advice on a weekly on a daily basis how did I do today versus my goals where do you think I should focus my attention in the coming days can you actually break down my interactions on slack and and in in notion over the past couple of days and say where I could have been more concise uh I'm getting the feedback that I'm sort of sometimes talking to theoretically can you point out the ways in which I can improve on that in the in these two notes that I'm about to send and and so that's exciting because our bet was you know we want to make everybody a builder we want to make everybody able to see that it's not that hard to get started and by reducing the activation energy there to see small gains immediately rather than wait for the next model or the next version that's going to really solve everything for them um personal use case have been great that the second family of use cases that that I'm excited by are um essentially cross functional so where the data silos exist because the function don't speak the same they speak the same language but they don't speak the same language and so um understanding what's happened in the codebase when you don't know how to code is powerful having an assistant translate into plain English what the last poll request that's been merged does is powerful it's powerful to people that were blocked in their work uh didn't know who they should bug to actually get an update so you know marketing to engineering sales to engineering the other scenarios are you you're extracting technical information from a long sales call is powerful because it means that the engineer doesn't need the abstraction of a pmm or a PM to get n nuggets from the last call with a key account they can just actually Focus the attention of an assistant on that type of content on their own project and get those updates so I'd say that's the family of of assistants that that we're excited by because they really represent I think the future of how we'd love uh fast moving well performing companies to work where the data that is useful to you and the decisions you should make is always accessible you don't need to worry about which function decided on it or created it you can access it and that fluidity of information flowing through the company helps you make better and faster decisions day in day out um yeah any other examples that I'm missing Stan that you think you're excited by no I I think I think what I wanted to add is is the fact that as you said the usage is extremely fragmented we we see over and over the same scenario and so we have data to back that kind of a proposition is is as we we built dust as a Sandbox which is makes it extremely powerful uh and extremely flexible but also has the complexity of making activation of our users uh not trivial because when you have an horizontal sunbox like product you're like yes but for what and so generally the pilot phase that goes with our users starts by clearly identified use cases so they really kind of try to the question what are the use case I should care about for my company and try to identify a couple of them and we always see the same pattern we see first use case gets deployed use stage starts we try to move laterally to another use case Second Use case gets deployed useage picks up a little bit a little bit more and then we generally go through a phase where the usage is kind of flat increasing slowly and eventually it reaches can critical mass of usage and all of the sudden it skyrockets to something like 70% of the company and that's kind of the pattern of of kind of activation of our users and the skyro getting to 70% the usage picks up the time the original use case that were identified by the stakeholders become just anecdotical compared to the rest of the usage and that's where we we feel like does provides all value and it's very hard to know for us what are all those us case because for we have examples of company with a few hundred people and a few hundred assistants and so it's just it's just hard to answer the question what are the best us cases like that's those are great examples and that that calls to mind an analogy that I would like to try out on you guys and you may puke on this analogy but this is this is what just showed up in my brain which was um a lot of those use cases you described you could imagine some sort of vertical application being built around those use cases and the analogy that comes to mind is there are gazillion vertical applications and yet where does a lot of work happen spreadsheets why does it happen in spreadsheets everybody knows how to use a spreadsheet they're there they're flexible you can you can customize them to your heart's content and so the analogy that I'm wondering about is this almost like the spreadsheet of the future you know some of these applications may get peeled off at a vertical specific applications but even then people are still going to come back to the to the personal agent because it's just it's there it's available it has access to your data it's familiar you know how to use it you can build what you want quickly and simply and effectively like is that a reasonable analogy for what this kind of is I think it's an amazing analogy for another thing that I'm thinking about which is I it took me the hot it took me the longest time to get Stant to spreadsheets when we started work together and this is way this is way back when this is this is like I don't know if it was 20 years ago 15 years go and then at one point Stan uses it for something is like oh wow this is kind of like a cool reppel interface where you can just get the results of your functions in real time I was like yeah that's this how the he's like it's a cool R in face for non-engineers I get it now and yeah I I think it's also interesting for that the experimentation cost is very very low if you think about um the way in which like some of our customers try and un try try and describe the gains that they're experiencing or that they're seeing and and their excitement for the future is some functions we've had 80% uh productivity gains some functions we're seeing 5% productivity gains and we're not even sure that we're measuring them right but we're seeing gains when the specialization of the assistant is close enough to the actual workflow that is able to augment the distribution problem of that with a verticalized a verticalized set of assistance is almost impossible to solve how are you going to get that deep into that function at a time where you know budgets are tight decision- making on which technology is going to be a fit is sometimes complicated when sometimes that's where the performance Gams are are the are the most obvious one of our users has seen like 8,000 hours a year shaved off two workflows for an expansion into a country where they decided not to have a full-time team and so basically sparing you some of the boring details but like the ability to review websites compare them to incorporation documents in a foreign language have a policy Checker that was making a certain number of checkpoints very clear to the agents that were reviewing the accounts all in a language and in a geography that none of these people were yet familiar with because they were really exploring the country um and immediate gains like very very easy iteration on the first version of the Assistant two weeks to launch it into production roll it out to three a to three human agents that were then assisted by by by these assistants and their CTO U sharing like you know we're seeing we're seeing a north of 600 hours a month I'm thinking our pricing is terrible but what I'm excited by is that um is that that case could not have been explored or discovered with a verticalized uh sales motion because I just don't know how you get to that fairly Junior person in a specific team and and actually are able to pitch them and deploy that quickly uh whereas if you have that common infrastructure that people understand the breaks of not everybody knows how to do some products not everybody knows how to do a pivot table but everybody understands that they can just play around with the basic things and probably get help from somebody close to them that's the other thing we've seen you know um the the the map of Builders within companies this heat map of people what's amazing about it is that it's people who just excited about iterating exploring and and testing new stuff which I think correlates well to you know high performance or high potential in the future it's like dust is heat seeking for potential and talent across your teams because the the people using it the most are people who are the most comfortable saying I don't feel threatened by something that's going to take the boring and repetitive side of my job away from me I'm excited to have that go away and focus on the high value tasks I think for the first six months I was one of the loudest voices saying what is that main use case I think you guys heard many many times and then eventually I realized like this is a primitive we're talking about spreadsheets you could talk about frankly a Word document you could talk about office suite when I interface with dust I think about it like slack except I'm not slacking my colleagues I'm slacking assistants and they actually do this kind of work for me and I can show them the kind of work so it feels Pat to your point something like a spreadsheet meets the ergonomics of a slack as it it's brought to me as opposed to I have to go to it uh and that that that is it took me a while to get there and now I see how the fragmentation is the power of what you're going after and Gabriel I have a quick question on sort of the psychographic of your user because you're your comment that it's like heat seeking for the people who are sort of ambitious and Innovative and stuff like that um I don't know if you have a name for them but let's call them the makers you know the people who are not afraid to try to try new things and try to build stuff have you come up with a systematic way to find those people or do they tend to find you through word of mouth or some other thing because that's not you know LinkedIn profiles don't say you know Gabriel migger right like I think it's a super interesting question at a couple of levels but um our motion is is dual right so the things that predict a great outcome with dust I'm I'm coming out of a core and trying trying to think about what was most powerful about this call I had yesterday with uh Chief people and systems officer or the company that could not stop interrupting me 5 minutes into my PCT yes I did a talk on this yes I've already WR about this I've got a blog post on this okay when can I demo where do I put my credit card that's call you next week and it's the top down motion is enthusiasm and optimism about this technology changing most things for most people who spend most of their days in front of a computer you need that that's a necessary condition because I think it unlocks three things one it unlocks the belief in a horizontal platform for exploration the ability for security to be in the supportive business rather than a than a blocker and um and in genuinely sometimes uh example setting like we have Founders and Leadership teams that are just like how have you augmented your own workflows last week and and Leadership meetings are being being asked they doing offsite about like how are you going to get better at answering uh like some of your team's queries faster with with us and so once you have that then you have the the right sandbox I'd say that the right petri dish I don't think we fully cracked the bill identification uh so right now it's more like bait it's like the product is incredibly easy to use anybody can create an assistant even if they have not been labeled a builder by their organization and it's just the sharing capabilities of their assistant that are somewhat throttled but we can see from the way in which people explore the product create assistance for themselves share them with their teammates in a limited way a great predictor of that type of uh of personality and if you ask me to look at LinkedIn and predict who are going to be um who who are going to in that in in that family I'd say the number one discriminator is uh somewhat to a degree it's a bit ages but like people who are maybe earlier on in their careers who have a mix of tasks that they obviously know they can get an assistant to help with so they have use case one just laid out for them people who have repetitive tasks and people who whove scripted their way out of a lot of of of repetitive things before just to be explicit like we we had the conversation I think it's okay to say like it is people under 25 like we were saying yesterday the power users the people that are using this all the time um at the companies are the people under 25 because they aren't set in their ways just to be explicit and that doesn't mean everyone you can be 70 and constantly innovating in a new way but in general they don't have the pattern that they've been set to and by the way that's true of a lot of the next generation of productivity notion which Pat works really closely with that is a under 25 power law type business and you know the the teammates here under 25 keep pushing me to transfer over to notion and it's just a different type of thinking it feels like a very similar motion at dust yeah I I I think that um the one thing we had that we have which is useful is that the immense B Toc success of chat GPT as a now obviously world famous product uh has made it really easy to set up Pilots by just telling teams do you know what send a survey out ask people how often they they've used chat GPT for personal use in the last seven days like Rank by descending order and that's your pilot team that's the people you want to have poke holes at kick tires uh and because they have you know we've we've asked the entire world to move from calculator technology punch the same Keys you'll get the same result to stochastic technology ask the same question you'll get a slightly different result this is not happened this is the biggest shift in you know the use of the tools that we have since the Advent of the computer we're asking an entire cohort of the workfor to move to a stochastic mindset and the only way you get that is by having a risk reward ratio that's that you're comfortable enough it's like you know what I don't I'm not asking it to be right 100% of the time I'm asking it to give me a draft that saves me time many many many times over and that distribution of Roi is something that I'm comfortable exploring with and it's rating on and I think that that is really one of the predictors that we see in people who've tried chat GPT or in people who are just curious with new technology is they expect that some of it's going to be a bit broken but the up side scenario to them is so clear and so 10x that they're willing to make that tradeoff or that local risk to uh to get things started so you guys have a lot of very strongly held beliefs um internally and externally and the good news is you've consistently been right about the strongly held beliefs you've named a few of them I mean you've talked about this shift from deterministic to stochastic way before it was mainstreamed um you talked about rasterization and vectorization I I think about that that can be unpacked if you'd like certainly would need being unpacked on the show if we go down that rabbit hole you talked about no gpus versus pmf right can you just walk through some of the beliefs that dust lives by it can either be philosophical um as a couple of these are or tactical like the no dpus before pmf yeah first one is really uh the continued belief that uh focusing on product is the right thing to do because really feels to me like we are only scratching the surface of what we can do with those models right now we are starting from the conversational interface so that's why you use the slack analogy and I really duly I mean truly believe that that analogy the slack energy will not sustain in time because the way we interact with that technology will change it started with the conversation interface but it will hand in a very different place M you basically those models are kind of the the CPUs of the computer uh the apis and and the tokens are really the The Bash interface what we're doing right now is is merily uh inventing bash scripts and we have yet to invent the guui we have yet to invent multiprocessing and we have yet to invent so many things we are really at the the the the very beginning of what we can do from a product standpoint with that technology whether it evolves or whether it stays like this yeah one one word um that I think is is going to be important and and I feel recent news has has actually uh uh helped confirm or is an interesting new drop in the bucket for is um the the notion so we one of our product models is augmenting humans not replacing them and it's not just the naive version of saying like we're not here to get people fired it's really that we think there is a tremendous upside in giving people who will still have a job in 5 to 10 years time the best possible exoskeleton and that it's a very different kind of company and kind of product conversation to be like all right how many dollars are we going to take away from your Opex line next year uh versus this is the number of latent opportunities that you are not able to explore as a business because your people are dragged down and pushing like stale slideware around or not even knowing what dependencies they have on the rest of the company like this is how much friction you've imposed on the smart people you've spent so much money hiring because half of their day or part of their week is spent doing things that we should literally not be talking about in 2024 so that's one and the thing that comes back to to to the droper you've been saying that from the beginning Gabriel and in the beginning you didn't use the word productivity like you didn't want to use the word productivity I I wonder if that shifted and if so the Nuance around why you chose not to I think productivity there's two terms that I was hesitant on productivity to me sometimes feels like an an optimization when that when really um there's two ways to be productive there's doing the same things and is doing just better things and I think you know the mix effect of productivity is is is enshrined in effort versus impact at the end of the day your bus is never going to be mad if you spent no time doing the things you were assigned to do but brought in the biggest deal for the company nobody's actually going to make make any comments on on on that being the bad decision because I think the more you grow in your career and the more you're close to the leadership of a company the more you realize it's not about the effort it's really about the impact and the impact comes in sometimes unplanned hyper plary completely like left field ways where it's like of course we needed to focus on this and it's clear and hindsight but you need to free up time space energy and and mental cognitive space for that um the other one was Enterprise search I just feel like Enterprise search is one that we didn't want to put on the website because retrieval of information is obviously a use case that people are very excited about very quickly but um we're just very convinced that looking for the document is a step that people are not particularly passionate about nobody wakes up in the morning and it's like so happy that I'm going to get just get the right document the first time around when I do the search people just want to get their job done and it just so happens that using context from three different documents across seven data silos help them get it done faster or better and so I think the search bit is just it's never the job to be done nobody really wants to search they want to they want to complete they want to they want to prove they want to test but the search bit is a is a step that we think will get abstracted and going back to stands point I think that the interfaces and the experiences we have with this technology will sort of really try to forget about what the original data source was quite fast potentially once we've gone over the the trust hurdles that that exist today um no the the thing that that this all comes back to is collaboration collaboration between human and non-human agents and um and I think projects uh by by anthropic are an amazing um an amazing example here um we thought about co-edition La last summer we have an amazing intern uh from from om with us last summer and and uh who spend their time working on um a a co-edition interface how do you chat to an assistant to make something that you're thinking about better whether it's an app or a project or a document or a script um and this is something that obviously the the recent release by anthropic has has made very palpable to many more people that is to me the the the the interface and the interaction that we need to get right and and that will be in the F so we we we say augmentation and we'll stick to it because I think it really helps us Focus on the interfaces that help humans and non-humans make progress faster um it's going to be about proposals how do I how do I get to have a human in the loop with a proposal that's written just in the right way to decide if we swipe left or swipe right on it it's going to be coedition how do I have the language of the human in front of the assistant be as easy to interpret and as you know foolproof as possible for the the final uh project to move into its final form as quickly as possible and so you need that inter that interface that interaction between uh the agent and and and the human and you forget that when you replace too quickly when when you focus on just replacing and removing you've you've built something that is fire and forget essentially and you'll see you'll see the gains you'll see the dollar gains um but I you know if you've automated 100% of your customer support tickets you still need the insides from what people are pissed off about you still need to understand and have your finger on the pulse of why people are stuck otherwi wayse you're slowing down your product development efforts and product development efforts today live and and die by some of the comments that are coming in from support tickets and so how you how you've made that problem go away and become like actually maybe cheaper sure but also maybe virtual and harder to Conn to is is not I think a a super long-term view of how your product and business is going to serve your customers best um because you you still need to think about the ultimate interfaces that are going to enable the decision making to make it better and strategic and and and and and the best option for your customers in the future so keeping the human in the loop always I mean it is human Liv one way to say it but it is Dri is human driven like the whole point of all of this technology that we are building is to serve humans better and as soon as you remove that you've made it you've made a terrible mistake because someone else is not going to do that and they're going to actually have a better experience with customers and employees and stakeholders and then they're they're going to win I you know obviously there's scenarios in which you're going to catch me and you be like you know this one this you we we know that humans get it wrong way more and so we should obviously replace it and this is a complex Nuance problem so I'm sure there's certain areas of it where pure replacement has uh fully understood non external like with no negative externality value but I i' Venture that we're pretty poor at modeling where value is created and how it's funned through the parts of our company today and you know economists have been great at showing that when you don't price negative externalities well we end up in pretty messy situation uh and so this is this is the question that I that I post to leaders who are asking you know what should I automate first I'm like well I don't know which parts of the company do you worry about the most and often I just find that CEOs are panicked about what their customers say on support tickets and so making that problem go away making that problem less visible might be great for some obcs conversations and and your your stock price could have unforeseen consequences if you haven't funneled it through in in the right places but but a good but but also I think there's so much more to do than to shave 3% off your balance sheet the the the the the spectrum of opportunity that you're giving your team if this technology is in their hands and if they're able to come up with ideas is broader than just firing people out of their and I'm not saying you shouldn't do that like I think I I don't want dust to be perceived as like naive in in in in this ecosystem where the disruptive nature of this technology is going to take some people's jobs away because those jobs were currently being done by humans for lack of a better alternative I think in certain situations you could see those jobs as having been created because we were waiting for the robots having been framed in a way that was because we were waiting for the robots um but I don't know that that's what leaders of companies are excited by I think that the the upside the future the way in which we need to be resilient antifragile for for for what's to come and what our competition is going to come up in those are the ways in which energy and support I feel should be fueled to to support teams you guys um second time Founders uh you started your first company over 10 years ago uh you were an early acquisition of stripe you guys were there super early on what have you learned and done differently this time as second time Founders um I think uh really understanding that a few explosive bets are more likely to get you any meaningful than over optimizing too early on on something that is still meaningless in the market that's one thing that I I think we think about differently so like exploring versus exploiting uh um and and all those Frameworks that's one uh I think the transparency that you could the the trust and empowerment that you give to your team is we I don't think we were against it it's more that we were clueless about how much more empowering you could be so uh the idea one of the best words from my stripe years was paper trail and it was you know you had two people in a corridor have a conversation and then one of them would take the time to just write a paper trail in slack or in a document say you know what we just had this exchange and we've moved the needle in this in this direction and it saved and other humans the time and effort to go in a meeting room or figure out that this decision has been made and and and it feeds a graph network of trust and respect for your co-workers that is I think second to none in how you can then just achieve more as a team um so culturally you need to sort of push that to begin with because especially people who are earlier in their career will not always feel comfortable with how information should be shared so I think that's one where where example is important um big markets that you really believe in for a long time H we we we loved technology when we started our first company like 12 13 years ago I was like this is great this is amazing these are QR codes everybody's going to use them uh and it's like no we have to wait for pandemic to sell QR codes okay I'll do that next time um and and and so likeing falling in love with the technology and not really fundamentally understanding how big the business could be if it's successful and asking that question early and unabashedly is one thing that I feel is is is different so what what we kept is our experience together I think it's an natur advantage to uh having built a company with a person because you've explored everything you've explored the the the the beauty the the terrible the The Joy the the pain and you know pretty much the entire API in and out and so that makes that enables a much more efficient um co-funding I mean co-funder uh interaction and collaboration I think it's a really big and fair and fair Advantage um I think the biggest one that I that I that I that I think is completely different from me and that Gabriel mentioned is about empowering people it's as a really as a fun it's not you this I mean it's not to you early and it's to you to build and to to to build that initial spark but then uh for the sake of the company uh you are not the one that has to build you're the one that has to make create an environment for people to be empowered to build those things and explore and create new stuff and uh the best value you can give is I don't like to lose that to use that world uh that leadership was coming to mind it's not not necessar leadership is really guidance and trying to to to create an environment where every is has the chance to do what they they want but yes in a guided environment so everything works as a whole but that that would be the the biggest difference and something that we learned about at St at this F so guys let's move to a lightning round we've got a couple questions for you all right lightning round question number one um Sten you share these predictions for where the world of AI is going on Twitter from time to time at this moment what is your top contrarian prediction for where the world of AI is going and don't don't give me this bodal little bit of this little bit of that let's let's hear a point of view what's what's your top contrarian ped prediction for where the world of AI is going uh I see it [Music] uh it's a lightning so I have to something uh it's gonna it's gonna be tough it's gonna be a we we know we we I think we on the VRA on during a pretty tough period how so uh the excitement will go down maybe it'll take times to get to the next stage of the technology uh there's tremendous value to create but people will not see it yet and it'll take a long time for it to diffuse through Society so there is massive amount of value to create but it's going to be a we may have tough times in front of us all right short-term pessimist long-term Optimist I'll tell all right Lightning Run question number two uh and this is for both of you who do you admire most in the world of AI uh Ilia uh is just is just incredible I've had the chance to work with him uh he's my favorite people in AI he's he's he's extremely smart but he's not a genius Builder he's a genius leader he's he's just a Visionary and I think that would been anchorable Kar I know know him I don't I actually don't know him but I admire him a lot and in terms of pure genius nii I think it's Shimon and yaku at hope nii they have crazy last names so all let people look it up but shiman and yaku are I I impressed by those who've been around for a while and a good they're acting as good resistance and condensator elements in the system they're just you know like providing the friction to uh remain optimistic but cautiously so uh and and to me one of one in one of the first I think it was I can't remember it was a tweet or a podcast or an article but the hearing yand to be like you know we can make pretty good decisions with a glass of water and a sandwich and these things require Power Station sized data sources and are not making great decisions on some things so we we feel something is missing and is like elegantly putting that back into into into perspective has been interesting to me because it's hard to not cave to the hype I think and so uh in some ways um pushing for a simple ideal like being open which which which I think Yan is doing uh quite aggressively despite that not always probably being the easiest decision uh and also saying you know we probably haven't solved everything all the time uh is uh is nice and from my from my personal experience the researchers that have worked for or with him uh have learned and taken from from that quite a bit and so that that and it's not French but uh you know some touch of modesty Touch of Temperance uh I've I've appreciated in in in in my discovery of the generative side side of artificial intelligence like after 10 years of just doing your prediction and classification from fraud and risk and and onboarding at stripe and and and healthc Care claims management and and and things like that um it's nice to to feel like there's uh some people who've seen seen a lot done a lot and just questioning rather than uh affirming all right so that brings me to the the third and final lightning round question you chose a Frenchman for your most admired uh Gabriel and dust is proudly made in France Paris has been in an epicenter certainly an epicenter for all things AI um your take on the perisian ecosystem and what do you want to say for the French Founders listening to this podcast other than I'm star with an English uh it's be it's their fault not ours yeah I think the the French system is awesome because uh we we compared to where where it was 12 years or 15 years ago it's our first company know we have talent because there's been a generation of scale-ups that went through the market and and train uh all all that talents and and most recently that kind of explosion of AI Talent as well which is super exciting um so I say it creates a pool of of of talent and uh with the rights to create to create incredible companies uh obviously it's not it's I mean tackling the the US market from France is a challenge and so that's that those to to be Tak into account of course yeah I think there's if if you have ambition there's there's there's a lot more to do and then as long as you're not naive where you know there are still some realities like you you can you can fight some aspects of narratives you can't fight gravity or at least you shouldn't you should probably work with gravity way more than you should fight it uh but there's a ton more we can do and I think we we have to behave a little more like um Tech countries like Israel I think uh in uh mixing ruthless ambition a a recognition for where Talent is and how it's already connected and and has high trusts connective tissue which I think is a great catalist and accelerant in in making great companies happen um but a recognition for where the markets are where people are buying where people are paying and how quickly people are making decisions on on shifting to new technologies especially in that space I think the the biggest advice is uh as as a French funer if you always been in France you have kind of that feeling that something magical must be happening in the US something special there must be something special about those people well I'll tell you I've been at tribe I've been an openi I'm working with seya these all are normal humans they don't have any magical capabilities they're just like us and so it's really important to to really be ambitious and and believe strong that you can you can make it you can do it whatever it is uh from France versus US wonderful that's a good place to end it thank you gentlemen thank you guys [Music] [Music]

========================================

--- Video 41 ---
Video ID: gN7nGF37wLc
URL: https://www.youtube.com/watch?v=gN7nGF37wLc
Title: Natera ft. Matthew Rabinowitz - A Personal Mission That Led to a Biotech Revolution
Published: 2024-11-21 10:00:45 UTC
Description:
Founder Matthew Rabinowitz opens up about the intensely personal journey that set him on a course to revolutionizing healthcare. A PhD in electrical engineering, he had no background in genetics or biology, but after his sister had a baby with Down syndrome that hadn’t been detected and tragically died after 6 days, Matthew dedicated himself to solving this problem. After overcoming seemingly impossible obstacles, today Natera leverages molecular biology and novel bioinformatics technology to provide prenatal screening in nearly half of U.S. pregnancies, as well as transforming oncology and organ transplants. Hear about Matthew’s vision for the future of computational biology and its profound impact on human health.

Host: Roelof Botha, Sequoia Capital
Featuring: Matthew Rabinowitz, Jonathan Sheena, Steve Chapman, Chitra Kotwaliwale, Sarah Elliot  

Learn more here: https://www.cruciblemoments.com/episodes/natera

00:00 - Introduction
02:33 - The Beginning of Natera: From Personal Tragedy to Transforming Diagnostics
06:07 - Redefining IVF Testing
07:21 - Spectrum: Revolutionizing Genetic Testing for Embryo Health
10:06 - Proving the Concept
11:09 - Bringing Spectrum to Life
14:41 - Spectrum’s Launch
15:43 - Spectrum’s Early Struggles: Navigating Market Challenges
18:51 - Transforming Prenatal Testing for Natural Pregnancies
20:45 - Panorama: Unlocking Fetal Genetics Through Cell-Free DNA
22:28 - The Emotional Catalyst: A Turning Point Toward Panorama
26:43 - Overcoming Challenges to Redefine Prenatal Testing
35:27 - Expanding Horizons: From Prenatal Testing to Oncology
38:17 - The High-Stakes Leap into Oncology
41:05 - Signatera: A Transformative Leap into Oncology
47:56 - Natera’s Next Frontier: Innovation, Impact, and Resilience

Transcript Language: English (auto-generated)
I think ever since I came to the states I was kind of invincible you know I'd made money lost money relationships came and went and no matter what happened I was just riding the wave and nothing really broke me and this just broke me and I think the engineering me sort of took over and I thought this is a problem that I have to have to solve welcome to Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world most remarkable companies I'm your host and managing partner of sequa capital rof buam today's episode is about NATA a global leader in women's health oncology and organ transplant rejection testing to give you a sense of scale 40% of all pregnancies in the United States are tested using theist technology to ensure the health of the child and nearly half of oncologists in America have ordered Nas cancer screening test my connection to NATA began long before its Inception I met its co-founder Matthew rabinovitz when I was a teenager in South Africa re attended the same academic vacation school which is a nice way of saying it was a nerd Camp later we both made the top 100 in the National Science Olympiad Matthew was the gold medalist I reconnected with Matthew in 2002 after completing an undergraduate in physics where he was the top student at Stanford and then completing a PhD in electrical engineering Matthew became a Founder but now he was immersing himself in a brand new challenge mastering biology and genetics but there was no doubt on my mind that Matthew could learn anything he wanted from its origins in IVF pre-implantation testing to expanding into prenatal testing cancer recurrence monitoring and organ transplant rejection testing NATA is a company that has innovated again and again to change the management of disease worldwide to get there however the company grappled with Crucible moments NATA bet its future on developing unproven technology and defied Skeptics as it expanded into new fields of testing and larger markets this is the story of a mission-driven company that risked consequences at every turn determined to help people live healthy lives hi my name is Matthew reinitz and I'm the founder and executive chairman of Nera you know Nera was sparked by a combination of factors I came from a good Jewish Family of two doctors and so I was always surrounded by these kinds of U you know debates between different medical practices and different approaches and I didn't go the medical path I studied electrical engineering and physics but um in 2003 my sister had a kid with Down Syndrome the the baby was born in one of the top hospitals in the country and they only found out when the kid was born that the kid had Down syndrome and uh the child died after 6 days and it was horrific I I couldn't understand how in the 21st century you could have all this technology in your you know cell phones and spaceships and our laptops and none of that technology had found its way into Diagnostics and um at that point I thought you know that this is something that I really need to fix and I need to understand what's going on here and and try to apply these technologies that I was aware of in the signal processing world to the world of genetics I joined Matt around 2004 I was at the time finishing up a startup in the mobile space and Matt was finishing working up on his previous startup as well and I was tired of making phones beep and I wanted to do something with more social value my name is Jonathan Sheena I am a co-founder and a board member at Nara what was clear at the time besides Matt's personal story was that genetics was in its infancy the Human Genome Project had just completed and it was abundantly clear that geneticists needed help understanding the masses of data that they were they were starting to produce were we confident in making that shift from engineering to biology for me no not at all I was not confident at all but talking to geneticists at Stanford mostly you saw pretty quickly everybody had the same book on their desk it was bionformatics in Pearl or Pearl for bioinformaticians and it was very thin it was really high level it was clunky it was hard to use all the tools were were terrible and as an engineer it was really really clear that I could be helpful there originally when I joined the company it was just some people around the table and we're were still in this phase of figuring out what was the market that we were going to tackle what was going to be effective what could we build that would really make a difference initially we were looking at all sorts of problems of how you could take genetic data and make it computable and combine the computable genetic data with phenotype or clinical data and this was the sort of long-term idea that you could have a kind of marketplace where we could build these better models to predict what drugs a patient should get for cancer what mutated HIV viruses should be treated with in terms of cocktails or reverse transcriptase Inhibitors and proteas Inhibitors so we were looking at all sorts of problems but at the same time I think I couldn't let go of that desire you know when the baby was born with Down syndrome and then there was so many complications you know at that point it was a real emotional inflection so I I think I couldn't let go of that desire to solve that particular problem Matthew and Jonathan began by looking at a subset of pregnancies with genetic data available and an immediate decision-making need those having children via IVF back in 2004 there were methods available to evaluate the health of an embryo prior to implantation but they had limitations the way people were going about the problem at that time was just so much um from my perspective it was a much more blunt instrument you know they were trying to hybridize probes to chromosomes and look at them under a microscope and count how many fluorescent probes you could see and then you could only see five chromosomes that way and the companies were trying to convince doctors that you only needed to see five out of the 22 chromosomes and you know that's obviously not true there was so much uh snake oil in the IVF Market that we actually sent a sample to uh one of these companies where we knew what the cells were we knew the carot types and they produced a whole bunch of bad results and it was just so amazing to me that doctors and Labs were sending these samples to companies that just had these very sort of basic approaches to the problem so you know we came into this world thinking that we were going to do it a whole lot better and that we had the capabilities to do it a whole lot better the team at Neta dove into researching a test that would eventually be called Spectrum to allow for precise genetic testing of an embryo no easy feet the the challenge with any pre-implantation genetic test is that you have to assess the the genetic health of the embryo but the starting material is extremely small you know it's often just a single cell my name is chatra kotwali I'm the senior vice president of portfolio management and operations at NATA so to be able to assess the genetic health of the embryo starting with a single cell is extremely technically challenging you end up having the very very noisy data I bought the um graduate textbook that you learn genetics with and I spent two months just studying genetics from that textbook and I remember turning to the chapter on meiosis and coming to understand the different mechanisms of that biological process and seeing how much we understood about the pieces that went into that mechanism of meiosis and at that moment I thought wow you know there's so much a prior modeling that we could bring to this problem of what information is coming from the parents contributing to the DNA of a child I thought that could probably be structured as a kind of a beian optimization framework you know if we use all of that modeling information we could have a much more powerful test statistic to evaluate what's in a child cell the idea is that if you've got this noisy data which you're getting from a single cell that you're analyzing from an embryo biopsy you can limit the space that you're searching for for answers because you know upfront that this genetic makeup came from Mom and Dad so if you know Mom and Dad you're not starting from the universe of all possible combinations of a c's T's and G's you're starting from this basic premise that everything you're looking at had to have come from Mom and Dad so that helped dramatic Ally improve the signal that you got out of this very noisy data something that people hadn't done before that was really the groundbreaking aspect of spectrum this whole idea that you can take uh a maternal genome and a paternal genome and build this probabilistic model of what the embryo genome is and based on that be able to tell whether that embryo has a chromosomal abnormality or has inherited mutations for a single Gene disorder that was the ancient idea as I turned the pages and that was the idea that we ended up pursuing with Spectrum well the team was excited about this breakthrough idea at this point it was just an idea the technology at that point was unproven although we had some initial data that we used to demonstrate a proof of concept and the proof of concept did work whether that was going to actually work in a real life scenario where where you are uh biopsy an embryo putting it in a tube shipping it around the world on Tri ice and then trying to produce a result in less than 48 hours whether that was all going to work together was very much dbd when we went out to raise funding there were plenty of Skeptics a who are these non- geneticists in front of us talking about genetics what did they know there was a lot of discussion of IVF as a market it wasn't perceived to be that big and we were in Silicon Valley which hadn't done a lot of genetics investing at the time at its outset the company faced a defining Crucible decision do you pursue an idea where the technology is unproven the market has red flags and where your expertise raises eyebrows I could say it was tough uh but the reality is you know I just had a mission so you know you don't worry about not getting funding at the right time when you've got a mission you you just pursue the mission anywh you can the team dove into bringing Spectrum to life we uh actually applied for a bunch of NIH grants and uh that's an incredibly good process for a company to go through to sort of figure out the technology with a level of rigor so that these very experienced NIH review panels can give you the thumbs up but it it a lot of the great IP that went into Nera uh then Gene security network in those early days came from that process of applying for NIH grants so that sort of got us going and then when it was time to raise money for NATA I thought ruoff would be the ideal guy to partner with I'd known him for a long time we met as teenagers in South Africa um at a kind of nerd Camp during the vacation when other guys were starting to date girls we were being lectured to by a bunch of college professors and then we met in the states and uh he was courageous he' left this very well-known family in South Africa and this career as an actual scientist and had come to forge his path in the US as I had and he was also very humble and the first rule of being a VC is Do no harm and so I thought that's the kind of guy I'd really like to work with and then I particularly targeted sooa they kind of invest like Santana plays the guitar I mean they are just so naturally and comfortably aware of where a company's at and I think they could see at a certain point that we were focusing on this problem in IVF because there had been this Confluence and we were sort of organically driving in that direction and that's the time that they sort of pulled the trigger so I think the whole process of leading up to that investment you know the the NIH grants trying to make our story more rigorous on the technology and the business side is an incredibly healthy process for a company to go through and it's it's not something that you can go through if you're just taking Angel investment I often tell companies you know force yourself to get grants and to get venture capital investment because it it brazens the company in a way that you can't easily do otherwise I do remember a moment when one of our senior statisticians came to me and uh we were trying to do this modeling based on the idiosyncrasies of the noise that we were getting from these arrays and building in a detailed functional model of the mechanism of meiosis to figure out what had come from the parents to make the DNA of the child and there was all sorts of noisy processes that were generated from the aray because we had to get this protocol running in one day that was supposed to run in 3 days and uh I remember one of our really great statisticians came knocking on my door and he'd been looking at the data based on this latest test statistic that we were trying to get working and he just said to me dude you know come look at this you're going to your myself and uh at that moment I was uh I didn't know whether it was good or bad news as he said that but I saw a big smile and I thought okay you know this is actually working at last in 2009 NATA launched the Spectrum test for public use so there were the technical moments when we knew that Spectrum worked we had successful transfers that turned into successful implantations for IVF but the real moments were the the postcards and the emails that came back from from the families Thanking us for helping them grow their family or start their new family those went up on the wall and those were the moments that that I remember much more so than the technical successes the unlock of spectrum was a critical turning point in nara's story in that it demonstrated we could use statistical methods that hadn't been applied to genetics before that was useful by both patients and Physicians it proved that we could fulfill the promise of our mission to not only to our to our patients but also to our employees was a it was a great motivator internally while Spectrum was a technical success it struggled to become a commercial one we made this bet that if you could do a reliable screening across 24 chromosomes and substantially improve implantation rates and let people who are going through IVF know what they had whether their embryos were good uplo embryos that this would be really valuable and that didn't really happen fast I remember the third month i' had been there we were having a board meeting and there were some concerns expressed about the volume my name is Steve Chapman I'm the CEO of Nara so initially when I joined the company in 2010 I was was hired as uh vice president of commercial and I think I was like the 20th employee I think the first month I was there we did 100 tests per month the second month I was there we did like 105 tests and then the third month we did 90 tests and uh all the board members were concerned they're like oh man 90 tests like the volume is going down and uh I remember after the meeting Jonathan Sheena he pulled me aside he's like look man don't worry you know I'm sure everything's going to be fine and I went home that night I remember I told my wife I'm like there's there's a good chance I'm getting fired the challenge was at the time the number of IVF cycles per year in the United States was around 150,000 and so when you think about it from a market size point of view um given the price point that we had at the time it just didn't amount to a very big business even if if we fully penetrated the market opportunity it wasn't you know the market wasn't deep enough for us to build a a very big business so just to give you a I think even today the number of IVF cycles per anom in the United States has now grown to about 160,000 and the average price that you can get for Diagnostic testing is maybe $4,000 so that's a 600 million-ish total Market available of every single person uh used a technology like ours and obviously the in reality the served Market is much smaller so it was very hard for us to see how you could build a multi hundred million dollar year business if you only addressed the IVF opportunity I guess I had the exuberance of uh you know a mission and I thought that uh those concerns about the IVF Market were secondary they turn out to be right actually so the idea that you can create a new market is a very challenging idea you know Venture capitalists tend to be very wary of that and they're right you know although a Founder thinks that you know you've got this technology and it's to totally rational for everyone to use it there are so many entrenched Norms uh protocols uh insurance issues uh sort of uh marketing challenges educational challenges it's not easy to totally create a new market that can take years and years unless you're really lucky so I think that the market played out well but it didn't play out nearly to the extent that I thought it would n said it sites on its next ACT to address a much larger market natural pregnancies if Nara could Leverage its technology for more effective prenatal testing in all pregnancies the size of the market could support a meaningful business so shifting to Natural pregnancies from IVF was something that was always in the back of our minds we always knew that we wanted to be able to apply the same idea of cleaning up genetic signal from genetic noise to Natural pregnancies what we saw in natural pregnancies was that the technology that was being used up to roughly that time was very poor prenatal testing has been around you know I I think really in the 80s is when the initial biomarkers started to be used to screen women for their risk of chromosome Anup ples or neural tube defects now those same techniques that were developed in the 80s were still being used in 2009 2010 maybe they had changed a little bit they were a little bit better than they were but not by much and so about 20% of pregnancies that had a severe genetic abnormality would be missed using those previous techniques but the worst problem was that 19 out of 20 women that were told that they were positive actually were fine and so there were a lot of people who would get an amniocentesis unnecessarily for such an important test for such an important screen for prenatal Health it was almost unconscionable that the world hadn't app applied the same kind of resources that we had applied to our iPhones and and and targeting better advertisements the team began to look at how the technology from Spectrum could pave the way for families to test unborn babies for genetic disorders as early as 9 weeks into the pregnancy this test would eventually be called panorama when we started to do experimentation we knew that there was a lot of signal that you could get from the cell-free DNA that had just been released from the placental cells or from the fetus that was floating around in the mother's blood and it became a very similar problem of trying to extract a really detailed model of the child from these fragments of self-free DNA by taking a blood roll from The Mother We could find these little puzzle pieces fragments of the fetuses that is sort of accidentally ended up in the mother's bloodstream and then our technology helps you reassemble that puzzle to get a complete picture of The Unborn child's genetic profile which is just staggering if you think about it and so that was a surprise I mean honestly when we first made the investment I don't think any isly didn't anticipate that this type of Technology would even be possible you know of course we weren't the first company that were offering self-free fetal DNA tests or nip tests the major difference with n terrorist test is that what you can do is you can actually separate informatically the maternal DNA the paternal DNA and the fetal DNA and that allows you to see a clear picture of what's happening within the genetics of the fetus so that allows us to more accurately identify chromosomal abnormalities um and also just biologically see things that others can't see n had reached another turning point it needed to decide once again whether or not to move forward with a first of its kind diagnostics test this time however there was much more at stake what were the business risks I mean there were real business risks you know we had investors telling us that we had not done what we set out to do in IVF we were going to be the world leaders in IVF and at that point we hadn't established our leadership there was still a lot of work to be done and we had investors around the board table tell us you know do what you said you were going to do and don't distract yourself you've got all sorts of established competition and we were going up against really big well-funded companies with different level of clinical trial rigor that was demanded yeah there was a lot of good reasons to not do [Music] it this is uh this is something that I never talk about publicly but um I'll talk about it now the decision to go into prenatal testing was completely personally driven it was a totally emotional decision if I'm honest because um I spoke about what happened to my sister around 2003 um what I never talk about publicly is that I lost a child around 2009 and um it was not related to what happened to my sister um we didn't have a particular genetic susceptibility it was just you know two events that struck in the same family by chance and um we were going to have a little girl and we found out late in the pregnancy that the child had a genetic condition and uh we' had been fooled by the screening tests that didn't detect this issue just because I was an information we and you know because of what had happened to my sister um I thought it was a good idea to have an amnio just to be safe and we found out from the Amo that there was this genetic issue and um I think you know ever since I came to the states I was kind of invincible you know I'd made money lost money relationships came and went and no matter what happened you know I was just riding the wave and nothing really broke me and this just broke me and I think the engineering me sort of took over and I thought this is a problem that I have to have to solve and we actually used a sample from that pregnancy uh to submit data to the NIH based on a bunch of the techniques that we were developing at Nera and we asked the NIH to fund us to improve prenatal testing uh which you know we were pretty convinced that we could do and we got that money from the NIH and then a bunch of subsequent grants and we we raised additional Venture Capital so yeah there was pros and cons to make this decision to get into prenatal testing but at that time around 2009 2010 where we focused on this product um it was an entirely emotional decision for me it was just something that I had to do there was no question that we were going to move forward with Panorama if the technology could deliver the results that we had hoped it would deliver in fact I think the company success was hinging on us developing a technology that worked and launching it successfully I remember we had several clinical trials that were underway but it was all coming down to the blinded performance of the technology in a prospective clinical trial and so the principal investigator called us and he said hey I've got the results final results everything's locked down this this is going for publication and he read off the the the final results and we realized that we had hit the performance specs that we were hoping for and that was a real incredible moment because at that time we knew that we were going to be able to move the product forward while Panorama represented a novel approach and a technical breakthrough natera's long-entrenched competitors were also bringing their own versions of updated prenatal screening products to the market when we developed pan Rama we were actually the fourth to Market these three other companies were out there educating the market and they were also setting the story they were setting what's important they were telling the market what was important and what wasn't important in their own terms and so we were up against these big established players we had alumina who was the gorilla of sequencing they controlled all the sequencing technology pretty much at the time we had Ro that uh had bought a company Arosa lab cord that had partnered with the company seom most of the companies had launched in kind of 201 11 uh 2012 and here we were in 2013 trying to get our test out the door and many of them had big sales teams they were already delivering volume it was a challenging time because while there was excitement within the company at Nera about what we were doing there was also sense that we were missing our window and that we were slipping behind and that we needed to get the test out the door and if we didn't you know we we weren't ever going to be able to catch up I remember there was a moment where I was curled up in the corner of my living room in fetal position literally shaking in the corner of the room because you know we had to raise Capital you know we needed more money to drive the technology and we had just learned that Illumina who was our main supplier had just bought our main competitor this company verinata and I didn't know um how we were going to survive we had to explain why this approach was better we had to demonstrate it we had to uh invest in studies that would demonstrate that I mean it's an incredibly complicated statistical problem and we figured out how you could use techniques like information filters like hidden Mark of models to sift through all of the possible combinations of what's going on in the DNA when you find the right set of parameters the right fetal fraction the right noise figure the right set of crossovers the right assumptions about how many chromosomes came from each parent everything just makes sense and you get this maximum likelihood Peak you just get this very high probability where the algorithm says aha I know what's going on here everything is making sense that was the moment that I learned that having an incredibly strong commercial team out there in the field that was able to describe the science that was able to convey the benefits uh was hugely important having all the great technology in the world didn't matter if you couldn't explain to your physician why it was explaining that we could see micro delions we could see abnormalities of the sex chromosomes a whole range of things that in aggregate are much more prevalent than Down syndrome what was really cool about the Panorama test and and micro delions in particular is it was the first time that I know of that you had brought deep learning neural networks to bear on a very large biotech or Diagnostics problem you can do this very early in pregnancy and you can use this to detect all sorts of things that don't match how the DNA should look all sorts of abnormalities which the other techniques just can't see we knew we had to do things differently than others um and so Not only was our technology different but we also focused more on user experience you know if you order a pair of shoes online on Amazon you push a button you get all these pings on your cell phone when your package arrives you get a text message and that's exciting you know exactly where things are but something like the health of your baby you get your blood drawn in this dark dingy draw station you send your tube in you never hear back from the laboratory you don't know when it's being buil you don't know how much you're going to owe and so we want it to to change all that and really make lab testing and genetic testing feel like a consumer experience that you would expect you know in today's world versus something that's very outdated I think the main thing is that science doesn't lie you put your faith in the science the the faith that you are taking on a problem in a very different way to everybody else and given that you have a better technology there will be a path through so we just kind of took a series of pragmatic steps and um we started to see a sort of transis where the doctors understood the value of the technology and and you know the the momentum started to pick up there was actually a distinct moment for me um I was at a birthday party for my son wearing my Nara jacket proudly and over the course of that afternoon three newly pregnant mothers approached me just gushing about their experience with Panorama um the peace of mind it provided the ease of use um how early in the pregnancy it could be used uh they couldn't believe that NATA was right in our backyard here in San Carlos um I mean I felt like a celebrity my name is Sarah Elliot and I'm the senior director of the executive Services organization at natara Panorama was a critical chapter for natara it indisputably put us on the map uh we were serving such a wide audience of people all of a sudden and really becoming a known name in the world of genetic testing you know as a company opened up to a much larger market all of a sudden rapid growth was really on our doorstep and we all felt sort of the steady shift from the more startup atmosphere to a midsize company I was supporting Jonathan Sheena and I just distinctly remember the onslaught of open positions um interviews and you know the challenge of finding top engineering Talent locally to our home office we were always on the hunt for more office space more desks I was literally stacking Engineers on top of each other as new hires came on board each week there were a lot of things where you know I mean there was a lot of work to build the capacity out and uh we felt like for a long time we were drinking from a fire hose there was a period of time where every time you walked into the lab it was either in a there was a different floor that we had expanded to or a different building we had expanded to I mean it was just you know really rapidly growing one of the biggest decisions that we had to make when we launched Panorama was whether we just focus on the high-risk pregnancy segment or whether we made our test available to all pregnant women regardless of their risk at the time many of the other companies were limiting their offering into the highrisk segment alone and we made a decision and made it available for all women regardless of their uh prior risk yeah that was really a important decision that we made we weren't just tapping into 15% of pregnancies we were actually tapping into all pregnancies I mean the biggest Milestone I mean one was that we're now processing you know over a million samples a year like we used to think a few hundred, samples was a great achievement but now we're you know routinely processing over a million samples for Panorama alone and we have other genetic testing for Women's Health uh markets such as carrier screening and so on so that's a that's a huge milestone I think some of the biggest lessons um from my perspective really it comes down to sheer grit and determination by every individual um on all of the teams no matter Your Role across the company every single person has a role and you have to ride the highs and get ready to roll up your sleeves through the lows you know there's so much that can go wrong it's really how prepared you are to react that is key during those times in July 2015 boyed by the success of Panorama NATA went public around this time the company began looking at adjacent areas where their core technology could make an impact one of those areas was oncology you know we always had this mindset that we were going to transform Healthcare and we were going to have as much impact as we could possibly have uh by bringing these sort of signal processing techniques and apply that to diagn notics so there was no way that we weren't going to you know keep taking on bigger and bigger aspects of healthcare there were some very practical considerations in the oncology world we had built this massively multiplexed PCR protocol and that lets you run samples in a single reaction volume without losing molecules and when you're dealing with oncology every molecule matters and so we to solve this problem of not splitting up the sample and we had built this uh PCR technique where we could get all of these primers to work in a single reaction and we started to apply that to problems in oncology in a way that nobody had been able to do before us where you could run all of these primers in a single reaction and we could also customize the assay for every single patient So based on your particular tumor and the mutations that were unique to your cancer we could build this personalized assay to Target those molecules for you it was just incredibly exciting moment when we saw how well this worked to detect these cancers early we could pretty much see a single molecule that was coming from a tumor from a blood drawer in the arm and that's like um the analogy that I use is that's like finding one blade of grass in a 100,000 soccer fields to be able to see see that single molecule that carries the cancer mutations with that capability we thought that this would be transformative the base technology that we developed in the prenatal testing space translated into oncology it turns out that most cancer types shed into the bloodstream and so in the same way that there's free floating fetal DNA and the mother's blood for most cancer types they shed fragments of the cancer DNA into your bloodstream this is a wonderful analogy of being able to take a a noninvasive test a blood draw from a cancer patient to see if the disease is still present and so there's an enormous benefit from an R&D point of view that we believ that our core technology would [Music] translate while naris technology held great promise in the field of oncology pursuing cancer Diagnostics in Earnest would come with enormous risk building out an entirely new ision would demand significant resources and the Women's Health division while growing was not yet profitable putting NATA in a precarious position this one we had a lot of criticism that I can remember like it was yesterday because it was a very stressful time with the company we were growing our Woman's Health business and we were burning a lot of cash we were not profitable we weren't cash flow break even um we didn't have full reimbursement for Panorama yet almost as as complicated as genetically sequencing fetal and matern maternal cell-free DNA is getting reimbursed for it we had now offered our nip test to all pregnancies but we weren't getting the reimbursement in the lowrisk pregnancies we knew that the reimbursement was going to pick up but it was going to take time and the market had to understand this and we had to explain this and at the same time that we were explaining all of this I was telling the market guys we've got this incredible performance and oncology which we cannot ignore there were many investors who were skeptical of nera's approach uh and that they would not be able to simultanously pull off building a profitable maternal Health business and do the R&D work to eventually build an oncology business the amount of effort and focus it would take we would have to build out a sales team we would have to you know dedicate R&D effort to continuously improve the test scale the test you know build build out marketing teams and so on would that energy and that Focus we would have to give to a different Market would that hinder the growth of Women's Health Market public investors would repeatedly call me and implore me to convince the team to abandon this idea of building an oncology business some of them would write to the company and try to explain and make their case that it was full Hardy to do so there were meetings where every analyst question was about what's our plan when is the guideline coming out when are we going to fix reimbursement when are we going to reduce cogs and um there were several one-on-one investor meetings too where where the people would get really frustrated there were plenty of people who said don't do it it's too hard oncology the timelines are too long the reimbursement is too complicated but you come back to if you're producing something of enough value to a patient and enough value to a payer and to a physician the answer in the end was clear the team made The Crucible decision to move forward with oncology business and to pursue a test called signatera which tests for cancer recurrence by using tumor and blood samples to detect small traces of cancer in the body the buildout that was necessary to accommodate the oncology business uh was really reminiscent of the buildout that occurred uh when Panorama was launching just in terms of growing teams growing infrastructure we had to convey that the work that we were doing in cancer was going to be rigorous enough to get through the FDA that we were serious enough about this that we were going to hire the experts that we needed and that we were going to forge the relationships with the leading centers that we needed we adopted a mindset of growth that said there was an incredible amount of pressure to be profitable we needed to develop that plan to become profitable and show that we were meeting those Milestones along the way so there was a lot of planning angst that went into deciding how much of our resource we devoted to cost cutting projects efficiency projects versus the next shiny new thing resource allocation was a constant challenge so not only within Women's Health did we have to make sure that we were allocating resources appropriately to projects that would drive profitability you know reduced our reduced the cost of our test and also investing our resources in projects that were important for our long-term growth initiatives while also making sure that we were investing appropriate amount of resources in signatera that was one of the biggest challenges there was a time when the stock dropped down to $7 a share um which was intense there was a long period of time where we weren't profitable we hadn't seen a lot of the major reimbursement Milestones that people were looking for we were burning money we had nothing to really show for oncology um you know we we talked about this field and how exciting it was going to be but no one cared about the talk they wanted to see results and you know as a result I think the shares were down in the6 to8 range for probably three years and or maybe more people within the company were demotivated I think there was a sense of is is the company going to make it it was certainly demotivating it was demotivating to us but just as much to employees who had come in recently off of the high of an IPO and so making sure that we were all focused on a mission that we all knew would yield in the long run was uh was was what carried us through that we had to just kind of keep repeating that plan and flesh out that plan at a level that we absolutely believed it however rough it was we absolutely believed what we were saying and um besides that I mean I I think that you just have to believe in the mission and when the share price was $7 um you know we did lose some people unfortunately and there was a lot of Nays sales unfortunately but the core people who believed in that mission and lived for that mission weren't affected um you know at least not affected enough to to stop believing the one thing I will tell you which was extremely refreshing for me from the perspective of like you know working in Corporate America was that there was an incredible amount of like unified Vision you know it wasn't about making one business unit successful over another everybody had that shared Vision that the company had to become successful and so as a result there was a very strong sense of collaboration across all of the different business units across all of the different teams and that allowed us to find synergies AC across different projects we could leverage our resources in a smart way and also make really fast and good decisions because we were all extremely communicative and there was a really strong sense of collaboration across the company and then the other thing I would say is that it's a it's a testament to our teams whenever we're resource constrained which is which happens I'm sure it happens at every organization uh instead of seeing that as an insurmountable challenge like we just can't move forward the teams at n are always finding sort of ways to get the work done with minimal resources and that's something that I've really come to appreciate of NATA you know in my last five years here is that anytime there's a resource constraint problem we're always thinking about okay well then how do we do it in a different way with few resources rather than saying it just can't be done because we don't have enough resources so while there was a lot of tension I think it was the good kind of tension it really like led to very strong bonds among the people who were at NATA at the time the day we launched Signa was hug we weren't just a women's health company anymore you know personally my mom had been in the signara clinical trial since diagnosis in 2016 I I remember sitting down with Jonathan Sheena at that time and you know I had wasn't a great week in my life and um he said well hey we should get her on this clinical trial right away and you know I didn't even know exactly what that meant um and looking back it definitely changed the course of my mom's care um and you know her ability to really get ahead of her diagnosis um she's now a 7year Survivor I mean the oncology division it's wild I was reflecting on oncology and just NATA and what's Wild is that in 2019 we were processing maybe a few thousand signatera samples a year maybe 5,000 or something but just in the first quarter of 2024 alone we processed over 100,000 samples uh of signatera so it's not just that you know in 5 years we've gone from processing a few thousand samples to 100,000 samples in a quarter but just you know year after year we're doubling the volume so I think that's an amazing milestone for signatera uh it's really quite incredible I mean minimal residual disease testing recurrence monitoring uh for solid tumors is essentially a market that was developed by by NATA and now it's considered to be an you know a huge multi-billion dollar market kids so I think that's that's a significant achievement for the oncology business unit we've built this enormous database now of early cancer samples based on the rapid uptake of signatera we now have about 50% of oncologists in the United States using signatera and we've got reimbursement for bladder cancer colon cancer breast cancer for all subtypes ovarian cancer IO monitoring for all solid tumor types as nura expanded its oncology division the company turned economics of Panorama profitable Panorama is now the most widely used prenatal test in America this year the company's on track to generate about $1.5 billion in revenue and that $7 share price has risen to well over $100 what's next for Nara um yeah I mean I think that uh there's another order of magnitude growth uh for Nara there's there's so much coming down the pike so we're about you know 55 to 60% of women's health genetic testing in the United States we are the leader in mrd oncology testing we've transformed the way you manage Cancer Care um we can catch the recurrence of cancers like um breast colon lung bladder cancer a year roughly before you see clinical symptoms um which is transformative we can continue to transform Healthcare where we have these huge pools of data and we can now curate these pools of data with large language models Matera and other projects where I'm involved are going to be using these curation of clinical data to build models for patients and you can model with all of this AI technology now people's susceptibility to disease we've built many different markets right so in the Women's Health Market you know the non-invasive prenatal test we are Market leaders uh oncology we built that market and we are Market leaders both are huge opportunities not only for us but for everybody else as well right so there are a lot of new players both in women's health as well as in in oncology so what's next on the Tera is that we can't slow down on our Innovation you know the Innovative and agile spirit that we had you know five or 10 years ago cannot go away we have to maintain that in order to maintain our Market leadership so we're not going to become a company that you know were successful and then sort of plateaued because you know we stopped innovating so what's next for us is you know a lot of innovation that's what we can that's what we work on every day the wonderful thing about NATA is the mission orientation of the company if you remember the original investment memo I wrote was you know to help people have healthy babies and if you go visit the n office and you walk around the office there are these walls where um we pin up the photographs of families that have sent in pictures of their children and it's moving it's uh it's really moving uh to work with a company that has that kind of impact on people to people thinking about embarking and taking these kinds of risks I would advise them not to be afraid of hard hard hard is uh is what creates value and creates a barrier to others if you prove to yourself that the value is there that people are willing to pay for it um then you know just because it's hard doesn't mean you shouldn't go after it it's like uh this quote from my mom where she would always say bless the obstacle will is faith if you've got a better technological approach and you trust the science uh that sort of gives you the faith that you really can bless the obstacle and have that resilience and patience and that was not so much a trick as just a state of mind um you put your faith in the science and you just plug away persistently and patiently and bless the obstacle cuz if you're persistent and the science doesn't lie it's very likely that you are going to find your path through [Music] this has been Crucible moments a podcast from Square [Music] Capital Crucible moments is produced by the Epic stories and Vox creative podcast teams along with seoa capital special thanks to Matthew Rabinowitz Jonathan Sheena Steve Chapman chitri Kali and Sarah Elliot for telling their stories [Music]

========================================

--- Video 42 ---
Video ID: K_F0ncVqWIo
URL: https://www.youtube.com/watch?v=K_F0ncVqWIo
Title: Building the Sales ‘System of Action’ with AI ft Clay’s Kareem Amin
Published: 2024-11-19 10:00:18 UTC
Description:
Clay is leveraging AI to help go-to-market teams unleash creativity and be more effective in their work, powering custom workflows for everything from targeted outreach to personalized landing pages. It’s one of the fastest growing AI-native applications, with over 4,500 customers and 100,000 users. Founder and CEO Kareem Amin describes Clay’s technology, and its approach to balancing imagination and automation in order to help its customers achieve new levels of go-to-market success. 

Hosted by: Alfred Lin, Sequoia Capital 


00:00 Introduction
01:59 The Role of AI in Sales and Marketing
05:10 Customer Success Stories
08:42 The Future of Go-to-Market Strategies
10:41 The Clay Community and Ecosystem
14:48 Clay's Product Innovations
21:06 Company Culture and Philosophy
24:27 Vision and Future Plans
26:25 An AI CRM?
28:16 Customer Success Stories
31:30 Surprising Uses of Clay
34:28 Company Growth and Vision
44:28 Rapid Fire Questions

Transcript Language: English (auto-generated)
we're a creative tool for growth and so creative here means that we give a lot of uh flexibility and lots of degrees of freedom to people um which means that we give them lots of different building blocks that they could put together and one key thing that AI is going to help us do is to preconfig some of these building blocks so you don't have to tweak everything beforehand so it just lets you get started more easily and it lets you um get to kind of your solution faster [Music] hello everyone today we're excited to welcome Kareem Amin co-founder and CEO of clay which is leveraging AI to help go to market teams unleash creativity and be more effective in their work Clay is one of the fastest growing AI applications for business so we wanted to ask Kareem what makes it so magical Kareem doesn't see human sales reps going away anytime soon but Clay's unique approach is about balancing the opposing forces of imagination and automation we'll hear about how Kareem applies ideas from physics and philosophy to sales and where he sees AI tools heading welcome to training data I'm Alfred Lynn I'm a partner at Square capital and your guest host for today we're here with Kareem um from clay and Kareem was a physicist he would he then started his career at in product at Microsoft and W at the Wall Street Journal and then he started clay in 2017 and uh Clay is a creative tool to help you grow your business and it's redefining the way we go to market so welcome yeah thanks for having me Alfred all right let's start with an like a controversial question is AI to completely automate away the jobs of the SDR I don't think so um I think that the way we're approaching it um there's maybe three ways to kind of like look at this well let's actually step back a little bit to kind of help people understand what the role of the SDR is in the business and why everybody's focusing on it so if you're in a traditional B2B company um and you're selling to other businesses you have to have an organization of lots of sdrs that do account research and they figure out who you're selling to and do they have the right properties um and you augment that with data that you usually buy from other data providers and so a lot of the cost is having sdrs a lot of the things that make you less efficient is just having lots of people uh doing this research and that's why a lot of companies are focusing on uh improving SDR efficiency so one thing you could could do is improve efficiency by giving them tools or co-pilots another thing is to try to fully automate them uh automate the work that uh sdrs do and that's kind of the magic wand solution connect all your data connect um thirdparty data first party data we'll figure it out we'll send the messages we'll do the follow-up and then we at clay believe there's a Third Way um or kind of like the it's not quite the middle way but it is a third approach which is take the job to be done and give it to someone who's more technical um such as a revops person or a growth marketer who can then use all of the tools that AI gives you um to re to find creative ways to reach out to people rather than automate all the work and magically have that happen for you um and the reason I don't think it's possible to fully automate it with today's technology is that um stand you need to stand out from the crowd and AI can automate a lot of the work and maybe even be creative in instances but it can't continuously do that at the end of the day you're selling to people and you need to figure out how to reach them and how to stand out from others in the market so what is your vision for what the future holds for sales what does it mean for sales what what's the complete stock that Clay's going to build versus what other people are going to build and what does the sales organization look like in the future so I think you know we're we're trying to run this in our own company it's very meta right so um we think there will be a go to market organization that combines sales marketing and customer success and you're already seeing this happen in the market um at the end of the day you need an go to market Ops organization that provides data and supports the go to market teams um we want to be the system of action in that so the the system of record could be your CRM it could be data in your warehouse and that's all the information you have around customers how they've interacted with you um that's where the data lives but then how you act on the data is going to be in clay got it so we've described it in a very sort of theoretical way in some sense like put it in context who are some of your customers what are they using clay for and what are some of the results that they're getting yeah most of our customers are um smbs that can just sign up and use the product and they find compan compes and people um and find very unique data points about them um and then use that to craft personalized Outreach the key thing with Clay is that you can craft personalized Outreach for new customers or for your existing customers um and so for a lot of the smaller businesses they're using it to find new customers a lot of our bigger customers um like let's say verata um is using us to uh create personalized landing pages for people when they come to their website that are customized to them um and so you can see that it's a creative approach to go to market so they have a lot of inbound and they want to make sure that people see the right value prop when they get there you can do that with Clay um other people are using uh clay in more creative ways so actually we have a customer that is um sells Dev tools and they used this technique with us actually uh where they monitor our status page to see when the website is down and and the reason why it's down and then they messaged us saying hey you could have avoided this if you used our product um which caught our attention of course yeah the ecosystem figures out the developer ecosystem figures out way more interesting things to do with your product then you might do yeah of course um and and you know just to add like a couple CLE more examples but you know uh companies like anthropic use us to improve their data enrichment so uh We've tripled kind of the data um coverage that they have at a fraction of the cost um the reality is there's a lot of data providers out there that have information around other companies um that are better and cheaper than the dominant kind of Market players but uh people just don't know about them and they don't know how to use them because they don't have great uis anthropic using I would think that they would know how to use AI to cleanse their data and normalize and and do all the things that they need to do they do um so they they do know how to do all these things they're using their own models within clay wow right and so what what clay does is it allows you to um um and I think this is generally going to happen in um kind of like various uh Fields with AI so I think of the foundation model companies as providing kind of that layer of intelligence but then if you want to use it in a particular vertical you want all the there's there's a lot of things that you need um in addition to the models in order to actually act on the data right so what we can do is like pull the data from their various uh data stores be able to run the model iteratively and um also enrich it with lots of other data providers not just using the foundation models and then you know you have to pass it and generate messages from it so that we're about making that workflow easy to do um and so using their technology but then making it uh possible to do the workflow more easily so how do you envision uh the go to market stack or and and specifically the clay stock changing with AI over time let me go back to kind of like the the the um mission that you said at the beginning right we're a creative tool for growth and so creative here means that we give a lot of uh flexibility and lots of degrees of freedom to people um which means that we give them lots of different building blocks that they could put together and one key thing that AI is going to help us do is to preconfig some of these building blocks so you don't have to tweak everything beforehand so it just lets you get started more easily and it lets you um get to kind of your solution faster so a couple of things that we're working on is um let's say you sign up to clay We you look at your website we can see who your customers are from any logos or case studies that you have we can see what you're writing about uh what you do and then figure out who are you targeting how do we find lookalike customers to that let's preet up all of the searches and the data that you would need for those types of customers if you connect some of your first- party data Maybe you connect your you know Wiki or notion um or you connect your emails that you've sent we can then Analyze That and use that same language uh when we're sending out messages and so that does sound a lot like a fully automated solution but what we do is actually set up all of our building blocks with you using Ai and then let you tweak it from there um other things along those lines are imagine you connect to your CRM and we take a look at who uh is a good customer who maybe you've lost in the past and um and look at what features you're releasing in the future and then message them maybe you lost someone because you didn't have sock 2 compliance and now you do so that's a good person to reactivate so we should help you come up with these campaigns as well as do them faster using AI in in some sense the old world of doing good growing go to market is going to be very different from the new world and so you know talk a little bit about how clay fits into the new world and maybe you know what are the jobs that are most at risk and what are what are we going to do in go to market in the future with yeah I think what's really exciting is the community that's coming up around clay um you know we have over 177,000 people in our slack Channel um there's a number of boot camps as you know that have come up to teach people how to do clay and um I think the the thing that Clay is doing is allowing people to try out more ideas for how to uh grow their companies and our our mission is really anytime you think of an idea of how to grow your company you should be able to translate it to Clay quickly so people who um are having a lot of these creative ideas or are capable of that I think belong in the new world I think the old world where you are um just really good at putting together a bunch of tools to do go to market or you're going through the drudgery of kind of like building a spreadsheet um and enriching it with a bunch of tools that work is going to go away and um it belongs kind of in automation uh the new world is around how do you figure out how to pinpoint that someone is going to be a good customer for you and then using clay to do that you want to talk a little bit about these clay gencies that have set up shop and work on clay and help other companies with growth I mean it's such a fascinating ecosystem that has been created yeah totally I I think um this comes back again to this something that I say to the company all the time when we're building the product you can think of you know building a microwave um and you go and you put how much time you want on the microwave Press Start and it works super simple does what it needs to do and then you can think of something like a guitar where um it has six strings so it looks very simple but you can spend a whole lifetime figuring out all the things you could do you could play different types of music and while you're playing the guitar you're also learning music and I think it's a similar thing in clay um really what we're doing is figuring out how to help businesses grow and it happens to be that using clay should be the best way to help your business grow every single idea for how to grow your business should be doable in clay that's kind of our mission and I think the um agencies or CL agencies that have come um that come up uh around like our product is because they are finding new ways to reach um customers and really it's it's really about connecting customers um you know you don't want to receive something that you don't care about um and the more research you do beforehand the more likely you are to reach someone who actually wants what you have because no one wants to send um messages that uh you don't respond to or don't look at or that are unwanted and the only way to do that is to do this research and to come up with new clever ways to stand out from the crowd and Clay agencies are really small teams usually two to four people um sometimes they're bigger that are using clay to help other companies grow um the reason they've come up is that they found this to be an incredible opportunity to make money there's a number of Cl agencies that are making you know over a million in run rate and that have started 6 months ago um that's fascinating yeah and so they're yeah they they're just finding that this um and and a lot of other people sdrs are upskilling um learning kind of the techniques of growth and using clay to do it because they know that they uh can make a lot of money because there's a big gap in the market I I want to move on to product a little bit and you have a clay research agent or a clay agent what is that and how does that incorporate um AI to help uh with the sales process yeah so so Cent is an implementation of the react paper so it's um essentially you know a a an AI agent that can make a plan and then execute it using all the tools that we have um it is incredibly useful to find realtime information that uh normally humans were needed to do it because it's very nuanced so you might want to say go to a um company's website and see how many pricing plans they have or see if they have an Enterprise plan um and a Pro Plan or see if the difference between the Pro Plan and the basic plan is $500 and these can all be indicators to you that this is a good company um and what our agent does is it has access to number of different tools so being able to scrape a website being able to summarize the information or um using llms kind of understand the context um and it also has access to a number of uh clay specific tools so data around companies and people that we get from other sources and it uses that to um give you the answer to your question many of the people I've talked to that use clay call it magic um because it's a magical experience and in some sense the question we always have is like where is the magic is in the models is it on something that you do on top is it the the workflow the user experience describe where the magic happens yeah that's a that's a great question I um maybe this is an unorthodox response but we love those yeah I think the magic and our competitive Advantage um is not in one single thing that we do um but it is in the approach that we're taking so we have a belief that um sales and go to market is a creative act um I think everybody has treated it much more in a um how do we process this funnel as quickly as possible how do we automate as many of these tasks as possible and how do we grow the top of funnel and you know every step in the funnel um and I I think that that's obviously necessary but I don't think it's sufficient and we we we'll be talking more about this uh in the future but actually it it's much better to think about this in terms of moments um so you're creating these different moments for people that after a certain threshold become uh a way of going from you know through the funnel right but actually and and you mentioned that I studied physics and electrical engineering but um a lot of these metaphors at least in the company are pretty nerdy but you know the in in um in physics there's activ energy right so if you're boiling water you're boiling it and it starts bubbling but only when it reaches 100° does it start to phase shift right to gas um and I think it's kind of similar like these analogies make sense in other places so if you're you know interacting with a brand you might have like a few moments of delight but only after you cross like a certain threshold do you then become a customer um so you know the that way of thinking about it ows us to build a tool that's made for turning any of your ideas into reality quickly so that you can test these different ideas to create these different moments and we treat um we have fun building the product um we we we literally use kind of the word creative I've probably used it so many times already in this podcast um to enshrine kind of that idea that you're playing with it the company is called clay CU you're also so putting together things you're building you're manufacturing you want to have that feeling of creation and I think that we've been able to communicate that feeling through the product and and that's why people love it and want to use it because they feel like they're building something new I don't know if this is a contradiction or uh you're holding things in tension but you're physicists or you're scientist but you're also a creative you're you're making you're producing and making music um you talk about clay as a creative tool but you also talk about the people who use it are go to market Engineers or developers so um how do you think about Clay is it two oppos opposing forces held in tension or unifying to opposing forces how do you think about that I think it is to opposing forces held in tension and I think that's where um that's where the most interesting things happen we agree yeah um and and I and I think um you know like you you want to be uh a creative tool but also you need to allow people to um do the workflow or the process um and in volume right so you need kind of the automation but before the automation you have to figure out what is it that you're actually doing and then you need to be able to do it quickly um but we have a lot of values at clay that kind of hold that tension um and maybe a small digression here but uh there's a you know famous philosopher Hegel who thought that like the movement of History was about things being intention so you know you have the thesis and then the antithesis and then they create the synthesis and that's how things move forward um for him it was kind of like a very mystical thing so it's moving towards God uh but I think that that General kind of like theme is one that we kind of like uh believe in at clay so describe the company culture then how do you hold things in tension I you know I walk through your offices it definitely has a very cool Vibe and yet people are very serious and working very hard at the same time but that's my Surface view what how would you describe the culture um we tend to describe it as chill High Achievers and so um what that means is you know we minimize um like we can acknowledge things like anxiety or fear or frustration uh but we don't necessarily react to it in the moment um and we focus kind of on having a clear mind so that we can do great work um and and I think you can do great work when you're not constantly feeling um in a rush or um I think the ity of mind is really the key thing so you can see all the different pieces of what's happening and then use that information to commit so we do it in our interviews right one of the key things that we do in our interviews is you collect information and you share information we're not necessarily trying to judge you in the moment we're trying to understand who you are we're trying to communicate what is available and who we are and then we can use all of that information at the end to make a decision um and so that that's kind of like a common theme is collect information first and then have the clarity of mind to be able to make a good decision after that um another thing that might be interesting is we have a culture actually the value that we use for this is called negative maintenance um because some people are high maintenance you want it to be the opposite some people are high maintenance some people are no maintenance and what we want is negative maintenance so you remove work from people around you um whether it is because you are um kind of in a place of like bringing down the tension so that we can actually have that Clarity of thought um the other thing that we do is is we um I call it there's always a way other people call it like make it make it work then make it great um and in some ways that is a tension because you want to make you want to ship something quickly you want to make it possible for people to do new things and you also want do amazing great work that um you know you spend time which requires time and so the balance there we leave that up to people's judgment um we are you need I think one of the important things actually in a company is making sure that you're aligned the people the market what you're building um needs to be aligned right if you're building a rocket ship you can't make it work then make it great it needs to be great from the beginning otherwise something really bad happens um if if you're building a go to market tool where people need to be able to do their tactic now and actually doing it first before other people is super key um we ship things as soon as we are able to and then we continue to craft them until they're meeting our standard of grade that is really great to hear um and it's I think it's great advice for many Founders to just know where you are on the attention scale yeah totally um your vision is very broad um and very expansive so where you know one way to ask it in open-ended is where does it end and maybe you're you know I'm I'm ambitious founder it doesn't end but well tactically would you ever build an aist strr yourself so yes uh the the way we would do something like that though would need to um need to be similar or be aligned with the clay approach and so uh an AIS SDR built by our company would have all the different components of an aisr and give you complete control over each piece um so that it doesn't become commoditized right it does everything that every other AI strr does um so what does that mean in practice it means that you need to have a piece that can take in a your company data and figure out which industry you're in and who you're targeting so let's call that an AI ICP identifier and we've already built that part then you need a second part that finds lookalikes um and it might connect to your CRM to see who are your best customers use what company you're like and who you tend to reach out to and then find look likes and it needs to do that on a regular Cadence and then we need to connect to your own internal systems to see what your style is and how you communicate with customers and each one of those can work together and can be configured and tweaked so that you can have um all the components of what an SDR does but with full control and with full ability to mix and match different parts of it otherwise it wouldn't be a clay AI SDR what about an AI CRM I like where you're going with this yes so um an AI CRM I I think the way to think about it is what is the role of a CRM um in a place where uh there is the ability to use all of this data to you know take actions and crms have been set up for sdrs to and AES to enter data into them and for people to pull information and have that be centralized there um but if people don't need to be doing that manually I think the role of the CRM changes um and it potentially becomes a less important piece of the stack um in fact lots of crms try to stop you from pulling data out because they recognize that they're the system of record and they want to keep the data in there so they like low rate limits various kind of things that just get in the way um and I think a lot of companies have started to move their data into Data warehouses so that they have full control um so I guess the the short answer is I think an aicm will look very different than crm's look today um and we are um clay starting kind of um uh in a way the the CRM isn't very useful if it doesn't have data in it um and we're focused on finding the data finding the customers and then connecting to wherever you store your data about customers today to make that more actionable and also taking action you're like in some sense you're a system of action yes and uh so when we when we talk to some customers of yours that they brag about that aspect and the results that they're able to to generate so um but I'm going to let you brag about some of the results and do AI um sales Outreach perform as well as humans even though you kind of know that it's a kind of a bot doing it or an email doing it or a campaign doing it versus a human Outreach um a it so what you actually let me reframe that for you so I think Outreach does better when you have better data around who you're targeting and when you have a clever idea of how to get people attention so the the example I gave you before of looking at the status paid that got my attention because I knew that the product was down so it's about the timing I knew that there was an issue and they were offering me a solution and that got my attention um and they were using clay to do that so that was e extra interesting but I think what we found from our customers is that um people who are so sdrs who have data and um automation that is powered by Clay are getting twice as much kind of like results opens and responses um as people who aren't using it and twice as much twice as much yeah that's huge it's a huge huge difference multiplied across you know the number of sdrs that you have um and and so everybody who ends up using it then sometimes they kind of have some sdrs get some of the data and you know the auto centralized automation through clay and others don't and so they're overperforming by quite a big margin um and I think this is just the beginning as as the market kind of understands the full range of things that we do at the moment I think a lot of people think of us as really amazing data enrichment and really good for outbound um but really our ambition and a lot of our customers are our more sophisticated customers are using us for inbound and for expansion as well uh and those are the that's kind of the range of things that you need to do to grow your company um and it's a full loop so let me give you an example if um let's say that you are a company um like figma and you have uh you you have a design tool and you're going to release kind of uh Dev you know Dev mode for developers well you can take a look at how many um companies how many of the companies that you have are um have like five designers or more let's target them and then let's then go and do outbound to the head of engineering and let them know hey your design team is using figma and you should also be using the dev mode for your engineering team so that you um decrease the back and forth so here you're combining inbound and outbound in order to expand a customer um and I think that's what's really interesting about Clay is being able to have an idea like this and and action it uh really quickly you know you've given a lot of examples what's been something that's surprising about clay that people don't quite understand about clay yet I think the the the the end the output from clay isn't necessarily an email or just updating your CRM but uh some of our customers have sent physical mail right so you can connect to any API and we have a bunch of mail apis so you can create personalized messages and send physical mails um which has had really great results but that our ambition is really you should be able to create any kind of message whether it's a personalized website or even an ad um and create it through clay and uh that's just a channel so the message needs to be created through clay and it's personalized uh because of all the research that you've done and all the data that we've provided you but then you can send it across any channel that's right really cool and in terms of like where do you see what do you what are the areas of improvement that you're working on on the product and specifically that you're that's in your control and what would you like to see from some of the foundation models and how they can help you do even better with some of the work that you're doing yeah what's next for us is so you could think of us as starting from thirdparty data so that's data that you can find on the web around companies um and what we're moving towards next and that that obviously poers outbound because you don't know much about these people they're not yet your customers um but what we're moving towards next and what we're doing for next year is first party data so we want to connect um to you know we're about to release something where you can connect a segment so we can take kind of all of your product analytics uh we'll connect to your billing data we'll connect to your marketing website visits we'll connect to your gong calls your emails so there all the information that you have as a company and then help you use that information to reach out to the right people and say the right things the value prop that you're offering um that's going to be a very big task and there's so many ways in which uh this can manifest so you were talking earlier about our ambition um our ambition is to be the singular place that you come to for any growth idea um and what that could look like is imagine that you're building a new product onboarding experience well why shouldn't that be personalized using clay um why shouldn't there be a bot on your website that's helping your customers book um even some of the meetings using clay and having a personalized experience around that so we are very much thinking about the full spectrum of things that can be done to help you grow your company um and we will be building products in each of those categories are you comfortable talking about how big the company is how many employees how how much how many customers you have how much revenue you have and how you charge um we've posted a couple of things publicly around us so I'll I'll talk about some of the public things um but we um the the company's about 75 full-time people now um we have over 4,500 customers um we have uh you know probably around 100,000 users from those customers um and our uh we've never really talked about our ARR uh directly but um I think the information had a uh like list of top 50 startups where we were there and they kind of estimated our AR and uh I think they said it was above 20 million um and that that's kind of uh in the range of Truth i' like to take it all the way back to the beginning when you started the company in 2017 you did you expect to be where you are today well it took us a lot longer than I expected um take take us through the circuitous route to get here because the vision did change and right um it was quite c c us but uh so we started off with the ambition um it was a very um broad ambition and Abstract of giving the power of programming to an order of magnitude more people and we thought about the different metaphors that people would understand to do programming one obvious one is kind of like a workflow um editor another one is the a spreadsheet um we realized quickly that a spreadsheet is the world's most popular programming environment and as soon as we kind of built the spreadsheet we realized that one of the things that people do in spreadsheets quite often in businesses is build lists of people and companies that they are um trying to reach out to so we actually had the idea for what clay would become pretty early on um and so it wasn't something that we stumbled upon or that we discovered we kind of reached it through analysis and then uh my co-founder started doing some prospecting to find his customers and you know he he was like this is very complicated there's so many steps we can definitely help with what we've built here um and so all of that was actually something that we came to quite early the process of committing to building that was more circuitous um and is actually where we stayed for a long time um I almost think of it as you know if you've ever played a video game and there's the fog of war and and you know the the map wasn't kind of fully visualized and we kind of walked around the whole map saw all the different kind of things that we could do and then returned to like the right location um so for us the the decision to commit to building for go to market people was really around figuring out why are we the people to do this right I I did not uh start in sales or marketing and I think a lot of go to market Tech is started by people who saw the pro problem firsthand we came about it as a um way of removing kind of repetitive work and um you mentioned before that I I like producing music but I really like tools like Ableton Live um that are components that you connect together to produce music um and I really like the idea of allowing giving that power to more people um and so that that's kind of how we came about with the how how we came to kind of the idea of being creative and giving the power of programming so that you can be creative within the field of go to markets teams yeah um would you have done anything different Looking Back Now that now that you understand it was a bit cus could you have changed any of the things along the way or you just it was a process that you needed to go through I think it was a process that I needed to go through specifically so I don't I don't think that um it couldn't be done faster by someone else um I think it took me and the team time to see the power of what we've built to hear from a lot of the agencies that you talked about were actually power users initially and we didn't focus on them we kept the tool very Broad and I also needed to develop the muscle of how to focus um and I think a lot people talk about focusing but then the embodied kind of understanding of what focusing means and how often you have to say no and how many times you um see distractions and think that they're the right path but they are distractions and so going through a couple of cycles of those helped me understand how to focus and so when we committed we really committed um and as soon as we launched the product um it it immediately resonated with customers because we removed all the language around it being Broad and focused it just on go to market which is still a huge kind of area we're talking about sales marketing and customer success that's half of a company yeah um and so that was enough and I think um that was enough half the company half the company is enough half the company is enough for now um but but once we once I learned the how to focus and once I committed to to the mission of helping companies grow we then kind of use the underlying um intention of giving the power of programming to be very very different than every other tool in the market so we've made a lot of choices that are unorthodox right a lot of go to market tools are built to be um you know press one button and get leads and then maybe AIS sdrs press you know connect a few things don't worry about it it's working in the background whereas we ask people to invest time and we ask them to learn how to use our product and to learn how to do growth um now of course some of that could be simplified but really we're asking for you to kind of be a partner with us um and we've designed the tool to have a very very high ceiling so you know like Photoshop or figma or um you know Ableton Live you can continue to improve your Craft um in clay and learn things that and do things that no one has done before um and so that's what I think makes us different in the market and that was part of the path it's it's basically the remnants of the path that brought us to this point you mentioned something that I find um interesting because it you had this process that you had to go through and I think when you started the company you you were a perfect founder problem fit founder market fit for bringing the power of programming to the masses how did you fig you know so go through that process how what was the turning point where you said hey I'm the right founder for building a creative tool for goto Market I held on to the initial um the initial intention and you can see it throughout so there's no that looks like clay because of that initial intention and what I realized is I um and you can kind of argue whether this is mental Jiu-Jitsu or um the truth but I held on to that initial intention and through that um I believe that actually go to market teams are doing a lot of work that could be done programmatically if you only knew how to program and that there was a large number of people who wanted to do things that and and their that their could their mind could be open to the things that they could do if only they knew what was possible and that giving them and like this group of people the power of programming um is actually kind of uh a realization of that initial vision and keeping that word creative really you could think of it as creativity or you could think of it as maximum flexibility um which is what I wanted to empower people with so that they can turn any idea they have into reality um and so this is a good starting point for that uh very very large Vision thank you for taking us uh through your journey and uh congratulations on all the recent success and um I think you know when we sort of look that all the sort of landscape and AI there's a lot of talk around technology and Foundation models and what what's next and in that space But I think you've built one of the most interesting AI applications that uh the world has seen and you're revolutionizing the way that sales is done so thank you for sharing your journey with us and also for allowing sea to be an early shareholder of the company yeah I'm very happy that we partnered early and um I really appreciate this Alfred let's um let's wrap up with a few of our standard questions what's your favorite startup other than your own that's a really great question uh I think it's our sourcing list yeah um I really like sunno actually that makes sense because I I like uh I I've met the founder and I really like being able to generate music it it's very much aligned you know with the creativity so yeah any any others or just just that one another company that maybe I uh admire because I really like the approach that they're taking is uh linear um they're just very thoughtful about how they approach Building Product um so yeah creative tools creative tools all right what's your favorite AI app that's on your phone oh AI app on my phone yeah um could be an standard AI app but if it must be on your phone if you love it that much that's a good one I was going to say cursor actually cuz I've been trying to write uh a little bit of codes on some hackathon products that works that's an AI op yeah anything on your phone that you want to share on my phone I've just been having lots and lots of conversations with chat GPT as everybody else is doing I think on um on other markets you think of that AI can disrupt whether in the short-term medium- term or long term you know I've been talking to a bunch of friends around um like expert Network kind of interview use um and I'm kind of curious about that because it's uh yeah I I think that you know people there there's a lot of um work that's being done where people are trying to collect information and it's pretty repetitive um but but it's almost like gong but not for sales calls for um like expert networks and collecting information from people like what are the top problems in your company what are top issues and then being able to analyze that I think that's an interesting underserved Market uh with like old incumbents what's your advice for other startup Founders I think the the alignment piece sounds um like wishy-washy or not very concrete but I think it's a key element so you need to you know we talked about knowing where you are on the spectrum of a couple of things right is this a product that is sales Le or product Le is this a product that every feature needs to work perfectly or you need to ship things as quickly as possible and then perfect them and tithe all of that back to what is your ambition um why do you care about this product or Mission um when you're spending so much time working on it you really need to have that connection um otherwise you I guess maybe the to say something kind of like new that isn't often said I think you need to have your own internal Compass otherwise you will get lost um and if you are lying to yourself um I think the this is this is something that I think about a lot I think the cost of lying isn't you know anything that'll happen bad maybe no one will catch you on a lie maybe you won't go to hell but the cost of lying is actually disassociation from yourself and when you lie about anything um in particular to yourself you lose your ability to navigate through your feelings and so you don't have an internal Compass you don't know what's good you don't know what's real you're just doing things maybe even things that you think are good but um you won't actually know if they work or not unless you take real risks and you're honest about your ambition and why you're doing things I love that um having a real internal compass on in the fog of War sometimes your compass is not clear to you how how do you you you seem like a very thoughtful person every time I've talked to you like how do you sort of get the clarity yeah that that's I think the clarity is through being honest and then taking real risks and so what I found is that in the past I've taken risks that have been hedged um or you know you you might think it's a risk let's say I'm building a feature and um or even like taking a Direction of the company where I'm like let's do go to market tools right but if I don't actually uh if I think well I'm kind of doing it because a couple of people have told me that this makes sense and here's how it works and I should do this as soon as you hear the word I should right you know that it's not really what you're thinking um and then if it fails you don't really know you don't have a felt sense of like whether this it might be not your honest opinion so you're like well it failed it wasn't my honest opinion I didn't take a real risk let's do something else and if it succeeds you don't really know why it succeeded but for me what I've done is um take real risks where I don't know what the results are going to be that's how I know it's a real risk is that I actually don't know and I'm okay with failing um and you have to be okay with failing publicly and acknowledging that that's when you know it's a real risk and that's how you kind of build up that internal compass and navigate like a fog of War that's that's great that's really great advice um on Final question and very open-ended long-term focused enduring focused what do you think the best possible thing that can happen with AI in the coming decade I think the best possible thing would be the there's obviously like an amazing set of applications that could be done with AI but I I I'd have to say that it uh would be recursive AI right so like AI that can improve itself uh that would be the dream right an AI that actually can help improve itself and accelerate itself and that should lead to lots of other developments across the board um otherwise we'll get stuck in you know whatever local Maxima we get to with the current kind of um set of techniques thanks thank you so much congratulations on all your success and thank you for being so generous with your time and coming on to training data appreciate it thanks alci thank you [Music] [Music] [Music]

========================================

--- Video 43 ---
Video ID: GuenqZiA1NA
URL: https://www.youtube.com/watch?v=GuenqZiA1NA
Title: Decart’s Dean Leitersdorf on AI-Generated Video Games and Worlds
Published: 2024-11-13 10:00:14 UTC
Description:
Can GenAI allow us to connect our imagination to what we see on our screens? Decart’s Dean Leitersdorf believes it can. 

In this episode, Dean Leitersdorf breaks down how Decart is pushing the boundaries of compute in order to create AI-generated consumer experiences, from fully playable video games to immersive worlds. From achieving real-time video inference on existing hardware to building a fully vertically integrated stack, Dean explains why solving fundamental limitations rather than specific problems could lead to the next trillion-dollar company. 

Hosted by: Sonya Huang and Shaun Maguire, Sequoia Capital

00:00 Introduction
03:22 About Oasis
05:25 Solving a problem vs overcoming a limitation
08:42 The role of game engines
11:15 How video real-time inference works
14:10 World model vs pixel representation 
17:17 Vertical integration
34:20 Building a moat
41:35 The future of consumer entertainment 
43:17 Rapid fire questions

Transcript Language: English (auto-generated)
so we we we launched the ois a few weeks ago and really you know when we launch it the the incredible thing from a tech perspective was oh this is the first video model that actually runs real time and you can interact with it in response to user actions you can move around the world you can break blocks you can place blocks and so we we got this this nice game without a game engine okay but that's not interesting why is this actually interesting and so to answer that forget about Oasis Oasis one think about say Oasis 3 okay and and imagine this so imagine for a sec just just go just put Tech aside for a second imagine you're looking at a mirror okay and you have this magical mirror you can you can talk to it okay you can you can tell it to do cool things you can say hey I'm I'm here and here's my hand and I want I want to hold a sword okay can you give me a sword and then you look at yourself in the mirror and boom there's a sword in the mirror where your where your hand is okay and and you move your hand around and the sword moves and you can be like no no no make the sword bigger or make it blue and it changes and you can be like okay now turn me into Game of Thrones and everything around you become Game of Thrones and then you get a crown and everything and you can be like I don't let my crown change it a bit and then you start jumping and you move around and and the mirror responds to that okay and that's interesting now the reason that's interesting is because it's a completely different experience than anything we've had before on Earth and it it allows us to to to kind of Channel our imagination through through screens that we can see it connects it connects two things it connects the what we see in our minds and what we can see with our eyes and so that's that's where we're going with this how can we in in a sentence how can gen AI really allow us to connect our imagination to what we see on our screens and with that we can take it into into really worlds that we didn't explore before it it can change everything from applications we can't we can't do today all the way to how we even interact with with computers with [Music] Hardware hey everyone I'm sha McGuire I'm a partner at sequa Capital today my partner Sonia hang and I are going to interview Dean ladder storf Dean is a brilliant young mind he grew up back and forth between Israel and the United states he was the youngest person ever to get a PhD from the tech neon at Israel at 23 years old at least until his younger brother beat him and got his PhD when he was 21 Dart is trying to deliver delightful AI experiences really trying to let people interact with their imagination and other people's imaginations in a way that's never been possible before to do this they are fully vertically integrated optimizing everything from as low level as Cuda kernels up to designing their own own models training the models and then at the end of the day delivering experiences over the next few months we're going to see some pretty impressive launches from these guys Dean thank you for joining us today I was just playing Oasis this morning I had so much fun so let me start by asking Oasis a fully playable AI game engine it's like what is it why did you launch it about Oasis so we we we launched o OES a few weeks ago and really you know when we launched it the the incredible thing from a tech perspective was this is the first video model that actually runs real time and you can interact with it in response to user actions you can move around the world you can break blocks you can place blocks and so we we got this this nice game without a game engine okay but that's not interesting why is this actually interesting and so to answer that forget about Oasis Oasis one think about say Oasis 3 okay and and imagine this so imagine for a sec just just go just put Tech aside for a second imagine you're looking at a mirror okay and you have this magical mirror that you can you can talk to it okay you can you can tell it to do cool things you can say hey I'm I'm here and here's my hand and I want I want to hold a sword okay can you give me a sword and then you look at yourself in the mirror and boom there's a sword in the mirror where your where your hand is okay and and you move your hand around and the sword moves and you can be like no no no make the sword bigger make it blue and it changes and you can be like okay now turn me into Game of Thrones and everything around you become Game of Thrones and you get a crown and everything and you can be like I don't like my crown change it a bit and then you start jumping and you move around and and the mere response to that okay and that's interesting now the reason that's interesting is because it's a completely different experience than anything we've had before on Earth and it it allows us to to to kind of Channel our imagination through through screens that we can see it connects it connects two things it connects the what we see in our minds and what we can see with our eyes and so that's that's where we're going with this how can we in in a sentence how can geni really allow us to connect our imagination to what we see on our screens and with that we can take it into into really worlds that we didn't explore before it it can change everything from applications we can't we can't do today all the way to how we even interact with with computers or with Hardware I love the mirror let's let's take it further where where are you going with that is this is this a social media thing is this are you building a game are you building a world you know a world model an interactive World model like how should I think about you know what is dcart what is oasis so let me ask you this what problem does chat gbt solve homework homework great and what else ises it all it makes it easier to talk to computers nice Sean knows the answer because CU I spent a lot of time with you Sean classic Sean I spent a lot of time with you but exactly that the tldr is changy but he doesn't solve any given problem it's it helps you do your homework better it helps you write emails it helps you summarize exactly now it doesn't solve a problem it overcomes some fundamental limitation which is exactly what Sean was saying that it overcomes this communication barrier between humans and computers computers speak in structured languages humans in unstructured languages or in languages with complex structure that llms just bridge that Gap and let computers and machines interact with each other in in a language that we can both understand that itself the second you have that you get a hundred different things that are solved on top of that so what you get with the mirror or what you get with with generative interactive video is you get that communication barrier now overcome not just with text but also with what we can see now computers will be able to see the world the way we see it and they'll be able to show us the world in ways that we can understand and you solve that you solve you give your you build a platform form that allows you to build everything on top of that from nextg Snapchat or Tik Tok to simulators for for fighter pilots okay and that's and that's the cool thing here and that's if you know now we're in 2024 I think one of the one of the most fun things we had at the cart is that we're founding a company when you have an opportunity to build something that doesn't solve a problem but overcomes a limitation 99% of companies solve problems when when when you look at companies that come to pitch seya or pitch any other VC they start with here's the problem here's how big the problem is that's her Tam and everything and here's how you're going to solve the problem and usually the first two stay the same otherwise you call it a pivot right you say okay this the problem I'm solving if you change the problem you're solving you call that a pivot and you 500 times you change the way you're going to solve it that's 99% of companies and that's we can do in any regular year there moments in history recently it's been like once every decade maybe 15 years that you actually have the chance to build something that doesn't solve a problem but just overcomes a limitation and let me ask you this in a different way is the Mac a consumer product or an Enterprise product and is it a hardware company or a software is a hardware company or a software company and and what problem does it solve okay and it's if you try to give me a list of problems that the personal computer olves you'd have everything from gaming to Excel and that's a nice thing about this that you you're you're building an insane piece of tech that you'll be able to productize in so many different ways yeah I love that one of the things that was so cool about what you've built is that there's no game engine inside as as far as I can tell like what do you think that means like do you think do you think that game engines are an artifact of the past or like what does that mean game engines were supposed to make it so that we can so that one player one one person can create a world and a different person can interact with that world right that that's that's the purpose of game engines you have the game developer and you have the the user that uses that and it might go for also for movies or whatever other people use game engines for uh right unreal has been used for movies a lot recently as well now that was that that is a very valuable product and it has it has lots of advantages to it the the world is very consistent you can you can really make things very accurate the problem is that it does take a lot of time to to interact with it people people like taking the basic game and like turning into a bunch of different things and you know it's it's as we as we got into this and we actually saw what people do with it do you know there's an actual mod to put Pokemon inside Minecraft okay you can walk around the forest and there there's Pokemon running around that's an actual mod someone built okay and so people inherently have this oh we have we got this platform and we want to change so that's a nice thing about mods what you get here is that because what's running your your game or your environment is an AI you can interact with it in the ways we're used to interacting with AI you'd be able to say hey can you can you turn this into like Elsa themed and then boom everything becomes El themed and can you can you add a flying elephant and there's a flying elephant in the game and it's not just there as a picture you can actually interact with it you can you know like I know punch the elephant and it punches you back or whatever whatever you can do with the elephants and so I think that if the if this trend were to replace game engines it would have to be at the state that you can program for it so that it's it's some machine that one person can build worlds on and the other can interact with and that is definitely coming and not only that it's going to be much easier to to program for this you can just use words you don't have to write code and and even if you if even if you do know how to write code you can iterate so much faster on it so basically to summarize this I think what what this will allow us to do is we'll just get we'll get modding much much much much much faster and we'll get interactive modding to get a little more technical for a second um like you're the first video model I've ever seen that has real-time inference what are some of the things that go into having real- time inference like you know how hard is it and just like give us give us some of the flavors of what goes into that if we go back like 3 four months like back to the summer I I don't remember where this was published but there were a few headlines about oh when blackw chips come out when VI's blackw chips come out we'll get realtime video okay Hoppers just can't do it the h100s can't do it we have to wait for nvidia's next generation and and I I think I heard this from from quite a few different sources it was there was there was there were like two weeks during the summer where everyone was saying that for some reason okay and and no h100s can actually do it okay and to to pull that off you have to do two things at once you have to change a lot of things around the model itself not every video model can be run real time you have to train the model differently the architecture needs to look different now it's it's not major architectural changes but you do have to make them on the other hand you also have to do lots of the systems level stuff you actually have to write your own kud kernels you have to write we threw out like pie torches garbage collector and like half of it from scratch okay and you really have to write everything on the systems level as well to actually pull this off so because if you do only one of the two you'll be waiting for someone else to do the other half for you if you if you're only doing the systems level part then you won't be able to pull this off because you won't have a model that that's ready to be interacted with this way if you do just the modeling stuff you won't have the systems level support to be able to to make you run real time can you say a word on how the model works like you know is it Transformer based is it similar to like this the soras of the world like what have you built on the model side yeah tldr it's it's it's exactly like the soras of the world just the prompt is user actions instead of text like that's the easiest way to think about it like thing like you have text to video models right you have Sora that you put in a sentence and you get a video same thing here just you put in uh a pro your prompt is like your keyboard actions and your past frames and it generates the next frame okay so how do you get the data between actions and and video so so yeah you do have to do some pre-processing steps here that you don't do with with regular video models uh for for example you do have to take you know the raw recordings of hey this is this is the game play and to label it at each step with uh with the action that's being taken and so you know we we we train a small model that that does that it actually doesn't need too much data like you can solve that with a small model that doesn't need too many examples and so you can just have your your our team just you know played for a bit recorded that you get you get a small model and then you you use that to label all your data super interesting and are you building a world model or like or is this just purely pixel representation nice so so it's it is it's it's the the beautiful thing here is that it's purely pixel representation now let's let's compare that to exactly what you were saying with like World models or 3D stuff and and the other things and AI like for for for more than a decade there's been the general question of do you solve stuff end to end or do you uh take an existing workflow and make something more efficient okay like there could have been two ways to solve this problem you could just say hey game engines exist Unity is amazing unreal is amazing let's just plug into that workflow okay let's build text to 3D so I'll describe an elephant and I'll get you know the 3D mesh of an elephant and that'll be embedded into unity and unreal or whatever game engine you're using okay so compare that to the end to end solution of at the end of the day what I have is a screen the screen needs to show something and that needs to work okay and and at the end of the day what people you do is they they see their computer screen and they touch their keyboard and they move their Mouse and that's your interface and you solve this end to end from from keystroke to frame okay so obviously these these two are competing directions now over time I think that there will be some merging between them cu the from technical perspective they each have their own advantages the first is much more consistent over time it's much easier to say oh here's this object here's how it looks and when it'll come back in two hours it'll look exactly the same and the other one the the the end to end pixel the fusion version that does pixels in pixel space that that one is much more easy to work around it's it's much more flexible you can really say oh the no no change the elephant's tail it's it's too big or you can you can actually edit it live in a way that that's just more Dynamic so I I do think that long term though these two things will converge Y and just if if if we like you know roughly map this out so today we really just have prompt to pixels like key strokes to pixels you could in theory say that the right way to solve this and say like the next two or three years is to have two models to have a model that think Transformers right Transformers went uh you have one model that's in charge of holding some state state of the game and that's that's unrelated to pixels it's like literally just like a llm wise Transformer okay it just gets the current state it gets the new user's action and just outputs changes to that state yep and you have one model that's doing that and then the second model takes that state and renders it to pixels so it makes sense that that's roughly where we'll converge because that will really take into account both the advantage of world models and the advantages of the fusion models Do you want to build both of those models of course I mean yeah definitely I love it but yeah one of the things for me we are I will say that we are a bit off like it will take some time to reach that stage yeah one of the things for me that like really caught my attention about Dean and and dart is they have this ambition to be completely vertically integrated like these guys understand you know literally down to electrons and how I'm serious like they understand how to like electrons move in logic gates and even like alternate logic gates and how you can represent them you know in levels even below assembly you know how you can change you know then in like in assembly Cuda kernels like you can go they literally go all the way from electrons to pixels that your I see and they're optimizing every single level in there and I I I think you by doing that I think they'll always have like a kind of 10x plus advantage over anyone that's just on the application layer actually so talk about this because Sean loves to talk about this um you know the I think the counter argument would be specialization right you know there's 10 10,000 very smart people at Nvidia at you know choose your favorite company working on this you should focus on building the best possible user experience and the viral Loop and things like that so talk about talk about your decision to be vertically integrated let me actually say something because Dean can't brag about himself the way we can brag about him but I I've been studying business models my whole life has been a passion of mine from a young age and for myself like Google to me is one of the most amazing companies of all time one of the most amazing business models and I worked at Google for a few years I really feel like people have the wrong understanding of what was Google's OTE for worth I also think people have the wrong understanding of what is nvidia's Moote today um but for me with Google like obviously Sergey and Larry had invented P rank pag rank was a very beautiful algorithm but it's actually it was like a deep Insight but it's very simple to implement it's like a very basic graph theoretical idea um and it was a published paper so like once pedant came out everyone replicated it very quickly for me the real advantage of Google was that these guys were some of the best in the world at distributed systems and like lowlevel systems optimization and they had this very profound Insight from early on that basically all the other search engines were buying Sun Microsystems like server racks the way they would get fault tolerance was by was by buying expensive Hardware um whereas for Google they realized that they can buy just cheap consumer commodity Hardware that failes all the time you know you buy Intel Pentium processors that are in your gaming computer or like Sandisk memory um and you know you need five times as many total flops or five times as many bits to get the same performance because of all the failure rates but the cost per flop is like 150th so you can have like a 10x cost optimization 10x cost Advantage by really leaning into distributed systems and getting the most out of the hardware and what that led to with Google is like for me when I looked back on when I first started using it it was this very very simple front end it was literally just a white web page with like a search box it was a I think a worse front end than Yahoo at the time um you know Yahoo also had chat rooms and other like these kind of flashier exciting things but Google had this magical back end like all the magic to me of Google was on the back end and I think that back end the performance came from this cost advantage and it came from the fact that they had optimized all the way down to the bare metal and with Dean and dart I the story really rhymes with me and look we need to say humble like this company hasn't done jack yet we need to you know we it's it's a very long way before they deserve a comparison to Google but and forworth seoa you know led the series a co-led the series a in in Google I'm very proud of that also led the seed in Nvidia so you know we have good history good track record good good track record also you know series a and apple um but commercial break is over commercial break is over but but anyways like it just I think I think to really deliver these like delightful like say a delightful mirror experience which is a very simil very simple front end I think you need this absolutely insane backend that is optimized the bare metal and I think it's kind of All or Nothing Like if you can't deliver real time I don't think it's very good uh and and I don't think you can deliver real time in the next year without going all the way to the bottom and so I just I don't know for me I think you kind of have to do that and these guys are the only ones I've seen doing that well said wow I I I love I love what Sean just said uh because two two things two things really caught my attention uh what one is about the vertical integration we'll we'll touch about that SEC and it goes back to your original question the second is really about so I won't name names but I was I was speaking to someone who's very very very executive at Google recently okay and just you know reminiscing about uh about the past and trying to hear because I was I was three months old when Google was founded okay so I was I I was around back I I was around back then but not really paying attention and knowing you Dean you might have been paying attention um you know so so so I I was trying to understand exactly what what happened there like why why that was interesting it came from like you know unrelated conversation and the way that person brought it up we're talking about how GPU clusters are just unreliable okay just you know in in general today if you try to train a model like the one we trained uh on any cluster okay whether it's hyperscalers or GPU clouds that thing's going to crash every few hours okay and and you're going to have like the weirdest things okay you'll have one node crashing and it'll be because two other nodes have dust on the cable between them okay and there won't be any error to really tell you that that's what's happening okay so your training room will just crash and you're like okay why did it crash and you'll try rebooting it and it won't work and then you'll try removing random nodes until you understand what happens and that's the state of the entire industry okay pretty much like the only ones training that that don't see this are probably Google and open AI cuz cuz they really built they really built everything down to like Google built everything down to the hardware as well open a had a lot of time to really focus a lot on these reliability stuff but anyone else who training from the big companies to the small startups they're all experiencing this and so I was talking to this to this person who's very very high up at Google and they said hey we're today with with like training today is like back where CPUs were in the90s like for forget like kubernetes there was no VMware yeah okay nothing was reliable and your servers would just crash all the time and you had the exact same thing that most companies didn't want to deal with that and so they just either paid for the premium service that was somehow better a so they both paid more money but B they also paid with time like the the the broken Hardware exists before the the the stable Hardware exists sure we'll get to stable training runs in a year in two years whenever that will happen Okay Nvidia will make their chips more stable they'll make their their code more stable the GPU clouds will figure out stuff around this that'll happen it's it's not the state today if you want to train model today you're going to face all of that and so one of the things that it's really a challenge you have to deal with and at the cart we just we deal with it okay the reason we can so so so the model that you saw Oasis okay Oasis Oasis one Oasis one can converges from start to finish in 20 hours wow and compare you can compare that like we we we know what the we have lots of joint work or communication with other AI labs they were all shocked by this now they talk like really about the best Labs training training diffusion models for this model the their convergence would usually take around two weeks and it's not it's both because they're not using optimized systems layer stuff but also because they crash every few hours or every few days or whatever we can actually hold it we we we can okay we can we can we can actually hold the training run end to end without crashing we can also hold a training run for a week or for two weeks without crashing and that reliability part really really resonates with with what happened back then now the thing about it is is is that it's really not simple to pull off it's you see like we have this internal dock I think it's around 200 Pages now of everything that can go wrong when you're training a model and it's everything from if you see this erir on this node then yeah tell tell your Hardware operators that these two nodes have a problem between them these other nodes have a problem between them and all the way to and here's here's a fun one at a certain point as we're training Oasis uh we were doing the training run and we we we we needed we needed some uh synthetic data to generate as well and so we said okay well we we have this cluster it has a ton of gpus CPUs as well like great it has lots of gpus but there's lots of CPUs and and they're being like they're utilized by 3% or something okay we we can just use this and just generate lots of synthetic data on the same cluster as the training is happening okay uh by the way this like blew blew the minds of our GPU Cloud they were like you guys are using the cluster to like 200% you're using the CPUs you're using the gpus and you're using we we we even use like the the infin band to send data around during training so like we're at we're we're we're getting a lot more out of the cluster than than is like should be expected okay now that all makes sense so on on one hand you know you have this like the gpus are utilized the CPUs are not utilized so you run like synthetic data in parallel it's not supposed to utilize it uses just the CPUs and so it's not supposed to hurt anything and then your training ground doesn't work okay and you get a random air that literally says the team will know how to say this better but the era that you get is something like missing lock file and the data loader okay it's like how are these two related do you know want to know how they related they related like this the synthetic data gen was using up uh more RAM which is which is fine but it caused uh sorry no was uh as okay to move the data around between the different nodes as the synthetic data was being generated it was using uh more Network bandwidth than before and that caused Python's data loader to take uh one of its lock files that's usually Network mapped and move it to be uh swap it out to disk okay and that caused the state that different nodes had different lock files and and that caused the the the data loader to crash okay now I'm probably saying this wrong and the team's probably like listening to this and like no Dean you're getting all wrong but that's the tldr of what happened okay you did something that was supposed to make sense and you got a random err and that's the dayto day and we have a 200 Page dock of all of these things and so that's my and this is a simple example that Dean is happy to share like there's you know there's it's one of the simpler ones there's 100x harder more important things that they've had to figure out um one that I think is also relatively simple but it's just kind of shows the current state of AI and Dean feel free if you don't want to talk about this don't talk about it but they got access to a new cluster and somehow the cluster had not installed memory yet uh but the gpus have some very small amount of onboard memory and so like most people would just not even be able to use the gpus can you share anything about this story yeah so this this this is actually a nice story so you know we call this the best place on Earth to train a video model training a video model isn't just the cluster it's everything surrounding the cluster okay okay you need to have the storage there you need to have the networking there there's there's so much that needs to go into building the best place on Earth to train of video model and we're actually very far away from them okay like I'm assuming that roughly over the next half year lots of this will stabilize and lot lots of the GPU clouds are working on this um but yeah with one of the Clusters that we got to there wasn't any storage and by the way it wasn't even with one it happened with a few clusters and different clouds okay that you know the clouds they bring the gpus and and they try f so focus on getting the h100s that you know they forgot the memory or the storage and it's fine and it's okay and they were and and they were going to install it they they would get there but you know they try to release everything as as fast as possible which is great which makes sense and so okay there was there was no you know stable Storage storage optimized nose that you can use or or an S3 bucket or something that you can use and so we said okay well the like every node has like you know a few ssds connected to it what if we just you know build our own mini fake distributed file system on top of that okay and and that's what we did and and it worked and it was there were so many things to overcome to make that happen but it it works at the end of the day and that's and that's I think and it goes back to your question about vertical integration vertical integration so I'm I'm Sha Sha knows business much better than I do and has been around all all the these fields way longer than I have okay I I did phds and like I just called you old I was using Google when when it first came out and I bought nid shares in IPO which also right around was born so I think IP before I was born no 96 99 I think 99 okay okay um but yeah as as far as I see it and correct me if I'm wrong vertical integration usually gives you two things it gives you a cost reduction like higher margins or whatever and it gives you the ability to move faster maybe it gives you a third thing because usually things give you three things but who knows so I think here in AI the more important part sure they're both important but I think the second one is even more important than the first because at the end of the day if you look at all the problems we're facing great they will be solved but it'll take time for them to be solved and if you you know I think that there was a great article I think at the information about how it was like a few months ago that people who leave Google to start startups suddenly realize that nothing works because everything works inside Google and then you go outside like oh there's no storage or oh the my my cloud provider doesn't provide me with this I actually need to take care of this and so okay fine over time these things will stabilize and your clouds will provide you what the cloud needs to provide you and you'll have great companies that provide you with like middle layer for the system stuff or even for the the model training stuff will make lots of easier lots offf easier for you but if you really do everything end to endend you can you can get to Market a year before everyone else you can get to Market two years before everyone else and that's and that's I think what's key here because even if we go to to uh the Google story or open theia story Tech modes don't last right sure Google Google is a great search engine Bing is probably not that bad okay sure maybe Google has more data so so they're able to do now but Microsoft you company theyve been working on Bing for so long it's a good search engine they have the tech it still doesn't mean that now Bing and Google are are balanced right so at the end of the day the the entire game here is get your Tech mode quickly and then convert fast two years before everyone else like Google and like open AI did and work as fast as possible to convert that to different notes and that's and that and that's the game here that that's what you have to play because we can all say okay you know what sequ invested all good let's put the money in the bank for a sec okay let's let's get some interest on that we'll go be on the beach for like 2 years wait for everything to stabilize we'll come back in two years and then we'll build the same company and that'll be great but someone else would have done it before and and that's and that's that's that's I think why we chose to be vertically integrated I love it what's your remote going to be long-term or short-term both both both perfect short-term Tech okay short-term Tech Tech and that's and that's great and we have the best systems layer stuff and we're also doing the model layer stuff as well so we're fully integrating and that and that's your that's your remes at the end of the day short term longterm longterm I think that's a that's a great question and let me let me share something that I found really interesting okay so there is a new weaker version of network effects that exists today that didn't exist before and that Network effect is called what people say on Tik Tok now why is that interesting okay we one of the companies that I I really that we learned a lot from and that I think is is an actually really really good company they did end up selling to is character AI they didn't they didn't end up selling to Google and uh wanting to go back to training big models but character there's a lot to learn from character and one of the things that the second they took off they had lots of competition instantly like fine they their Tech mode lasted for like half a year until met released open source models and then other people started running this they they they were still vertically integrated and so they were able to be 10x cheaper than everyone else which was great but one of the things that really stood out to me was their Tik Tok mode if you go on Tik Tok and you look for any character that competitor fine you'll find a video of that competitor and then you'll scroll and you'll see a 100 videos of character and if you even you know even if you go on the videos which are not character all the comments are full of character and if you talk to a random character AI user they don't even know the competition and so we have somehow literally because of Tik Tok there is a new Mo of what people say about you on Tik Tok and and do you have a a mini Network effect there a mini Rand I'm not sure if it's a network effect or brand effect but why is this different from just brand so so it's very similar to it's similar to Brand okay but it's it's in your face like brand like 20 years ago was okay did you hear your friends talking about this or your parents talking about this here you're always on like the younger generation especially they're always on Tik Tok and so they just see this instantly and so there's even a big question of whether OTE like that could survive for for the 2 three years until you need uh until you get your long-term Moe of like insane brand like Googles or a distribution brand or something like that or distribution mode or something like that so I think we're really in this new market here yeah that we're not necessarily going to have the same Moes we had 10 years ago super interesting Hardware is always the best mode though and for worth like Google I think you know they elevated what was initially like a software mode and a distributed systems mode to becoming a hardware mode um I I personally think that Google has not like leveraged that Mo enough you know on the application layer they haven't had that many really like fantastic breakout you know consumer products since the early days but they have an absolutely gigantic you know cost advantage really because on the hardware layer when I was at Google there was a there was this project that just Absolut blew my mind and gave me a prepared mind for a few Investments which is basically Google built Optical interconnects to move data in in data centers this in the like one of the papers if you Google Jupiter Rising like Google data center you'll find the papers and basically these Optical switches by turning them on basically about doubled the performance of the data centers like these one switch is mainly racked to Rack in data centers you know going moving from electrons to photons and you know these switches were insanely hard to build and basically everyone outside of Google if you ask them at the time is it possible to build you know this 100 terabit per second switch or whatever they'd say absolutely no way but they did it people didn't even know for years that Google had this and you know it reduced power consumption of the data center by 30% or something just like those things are real fundamental Moes um I think it's always hard to know like what the Moes will be for accompanying the future but I do I I strongly believe like Hardware is the ultimate Moe in in part because like there's always going to be an extreme delay to move atoms like to you know spin up Fabs to get power to build a power plants like even in a world with AGI you know the time skill of Hardware even in a world with billion Optimus robots like the time scale to make new hardware will be much slower or the time scale will be longer um so anyways I hope Dart has a hard remote some I I I think I agree with you on that like longterm okay you know this actually goes back to when we were founding the carts so I said okay we're in this we we we we called it the golden ticket we we got this this ticket that you get once in your life of starting a company and in a time where going back to what we're discussing before starting company in a time we can solve some fundamental limitation and not like there's some huge Tech shift going on and we said okay there there there three huge companies you can build here that was our analysis of the field a you can build an Nvidia competitor and if if you like the the next gen chip that's actually built for AI and it'll be very tough to do but and Nvidia you know Nvidia is not just a a chip giant but they're a they're supply chain giant okay insanely hard to do but you know if you hustle your way around everyone in the industry wants to help you and so it's it's it's doable if you really if you really Excel on the business side two was uh to build the next AWS like there is there is an opportunity because the the workloads themselves are changing there is an opportunity to be able to to build a new Cloud very very very tough because in that market there's a default winner if if you all lose the big three will still win the big three plus Oracle or the the other clouds as well and the third was create new experiences that new experiences will will happen and these experiences will be drastic enough so that the next trillion dollar company can come out of these in in five years and not in 30 years and so we we had to choose one to start with we chose the experience as one but a definitely strong second was let's build an Nvidia competitor one and so I we we we we we we have that lingering thought of one day we'll get back to this I see why you two are friends um I want to close it out with one last question if everything goes right what is what is Dart in 10 15 20 years and what experiences have you crafted and like what is the future of consumer entertainment I don't know if that's the right market and I'll say this and I'll give credit to to James from Sequoya here cuz he he's the one who coined this term uh generate experiences GX okay and we we call this ux is dead Long Live GX okay basically we're going to have new experiences that are generated in ways that that match how humans want to interact with computers and that encapsulates everything from character AI a generated experience to real-time video models or generated experience and and that's what we're going to see the card at the end of the day is a generated experiences company we're implementing this with being fully vertically integrated with having the systems layer at the end of the day you're a generated experiences company you're creating the new the new wave of experiences that that's going to touch every single person on the planet and that's where the card is now the only question is whether it does take 10 or 15 years it'll in today's age it might take less it took it took a long time for the previous Titans to to to rule the world I I don't know if it'll take that long this time it definitely take at least five years you operate on a different time scale than a lot of the best AI researchers that are in our orbit and I really respect that about you uh should we close out with a rapid fire round sure start off uh favorite AI app other than Oasis has to be between chat and character has to be between chat and character what he his character for not using character okay but on the basic notion of that we'll have these apps that are entities that have that hold a some kind of relationship whether it's friendship or whether it's utilitarian with hundreds of millions of people I think that's an insane platform that's going to be the basis for so many things going forward yeah I love that favorite AI company could be the same as the last answer same as the last same as the last answer um okay let's see when did you first program a computer first program a computer uh when I was 13 Bots for RuneScape okay great game runscape I botted the hell out of it for years until six years in I used a bot that I downloaded from the internet 24 hours later got banned are we going to have ai generated video games first or AI generated novels and I mean at the you know at the level where I would actually pay for it um you're going to have the first thing you're going to have is a platform that lets other people use creativity to create this content because AI is still far away from creating creative content a super interesting okay who's your favorite scientist ever favorite scientist Uh I that that one I like that one I like it's you know there's there's a reason we chose the name the cart we chose the name the cart um because so okay I'll answer first of all I'll answer the question favorite scientist is the Vint cuz I think he's he's both an insane scientist and engineer and somehow was able to get people to fund his projects okay he was like if if you go back to Da Vinci like he he literally was a great scientist engineer and somehow knew how to raise money from from back from from VC's back then which were Kings okay um so yeah definitely Da Vinci and theard and Tesla are close seconds the reason we chose the name the card was we looked at Tesla we like okay we we love both that company and the name and we needed we needed some someone who does who resembles the same same thing that Nicole Tesla resembled to uh the company Tesla and for that that was the cart because you know I think therefore I am resembles almost a lot of what a ey is to do perfect note to end on danan congratulations on what you've done thank you for joining us today we love this conversation and I'm not going to congratulate you haven't done jack yet let's build something insane but but I love the sentiment we we we can't celebrate until we really win yeah okay there's no celebrating small wins [Music] [Music] [Applause] [Music]

========================================

--- Video 44 ---
Video ID: dvJOQvBkg_U
URL: https://www.youtube.com/watch?v=dvJOQvBkg_U
Title: How Glean CEO Arvind Jain Solved the Enterprise Search Problem – and What It Means for AI at Work
Published: 2024-10-29 09:00:40 UTC
Description:
Years before co-founding Glean, Arvind was an early Google employee who helped design the search algorithm. Today, Glean is building search and work assistants inside the enterprise, which is arguably an even harder problem. One of the reasons enterprise search is so difficult is that each individual at the company has different permissions and access to different documents and information, meaning that every search needs to be fully personalized. Solving this difficult ingestion and ranking problem also unlocks a key problem for AI: feeding the right context into LLMs to make them useful for your enterprise context. Arvind and his team are harnessing generative AI to synthesize, make connections, and turbo-change knowledge work. Hear Arvind’s vision for what kind of work we’ll do when work AI assistants reach their potential. 

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 - Introduction
08:35 - Search rankings 
11:30 - Retrieval-Augmented Generation
15:52 - Where enterprise search meets RAG
19:13 - How is Glean changing work? 
26:08 - Agentic reasoning 
31:18 - Act 2: application platform 
33:36 - Developers building on Glean 
35:54 - 5 years into the future 
38:48 - Advice for founders

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 45 ---
Video ID: 9XLY56Pm8Sw
URL: https://www.youtube.com/watch?v=9XLY56Pm8Sw
Title: DoorDash ft. Tony Xu – The “Wrong” Moves That Built a Giant
Published: 2024-10-24 09:00:09 UTC
Description:
DoorDash faced skeptics from the start. GrubHub, Delivery.com, and others were already addressing the restaurant delivery market when CEO Tony Xu and his co-founders started in 2013. But after talking to hundreds of local small businesses, they realized there was still an unmet need: None of the competitors solved the problem of delivery with an on-demand workforce, the way Uber had done with drivers. After struggling to raise initial funding, DoorDash found traction. But the next few years would prove tumultuous, with cash scarcity and investor skepticism putting the company perilously close to the brink. The founders’ contrarian decisions, clarity on their commitment to serve small local businesses, and ability to out-operate competitors has turned DoorDash into one the decade’s startup success stories. In this episode, Tony brings us inside their decision-making, and what DoorDash saw that others missed.

Host: Roelof Botha, Sequoia Capital
Featuring: Tony Xu, Keith Yandell, Miki Kuusi, Alfred Lin

Learn more here: https://www.cruciblemoments.com/episodes/doordash

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 46 ---
Video ID: XJsHIoIDhPY
URL: https://www.youtube.com/watch?v=XJsHIoIDhPY
Title: OpenAI Researcher Dan Roberts on What Physics Can Teach Us About AI
Published: 2024-10-22 09:00:27 UTC
Description:
In recent years there’s been an influx of theoretical physicists into the leading AI labs. Do they have unique capabilities suited to studying large models or is it just herd behavior? To find out, we talked to our former AI Fellow (and now OpenAI researcher) Dan Roberts.

Roberts, co-author of The Principles of Deep Learning Theory, is at the forefront of research that applies the tools of theoretical physics to another type of large complex system, deep neural networks. Dan believes that DLLs, and eventually LLMs, are interpretable in the same way a large collection of atoms is—at the system level. He also thinks that emphasis on scaling laws will balance with new ideas and architectures over time as scaling asymptotes economically.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

01:28 The path to physics
04:00 What does it mean to be intelligent?
05:38 The path to AI
07:27 Physicists in AI
09:59 Scaling laws
14:17 More efficient AI systems 
17:40 The pendulum between scale and ideas
21:18  The Manhattan Project of AI
23:03 AI’s black box 
24:35 What can AI teach us about physics?
27:48 Can AI help solve physics’ biggest problems? 
32:02 The hardest domains for AI
35:35 Lightning round

Transcript Language: English (auto-generated)
in the 40s the physicists went to the Manhattan Project even if they were doing other things that was that was the place to be and so now ai is the same thing and you know basically said open AI as that place so maybe maybe we don't need a public sector organized Um Manhattan Project but it you know it can be open AI [Music] joining us for this episode is Dan Roberts a former seoa AI fellow who recently joined open AI as a researcher this episode was recorded on Dan second to last day at sequa before he knew that he would go on to become a core contributor to 01 also known as strawberry Dan is a Quantum physicist who did undergraduate research on invisibility cloaks before getting a PhD at MIT and doing his post do at the legendary Princeton IAS Dan has researched and written extensively about the intersection of physics and AI there are two main things we hope to learn from Dan in this episode first what can physics teach us about AI what are the physical limits on an intelligence explosion and the limits of scaling laws and how can we ever hope to understand neural Nets and second what can AI teach us about physics and Math and how the world works thank you so much for joining us today Dan thanks delighted to be here on my probably second to last day at seoa depending on when this air is and how you're going to talk about it you will you will always be part of the seoa family Dan thanks I appreciate it uh maybe just to get started uh tell us a little bit about who is Dan you have a fascinating backstory I think you worked on invisib invisibility cloaks uh back in college like what led you to become a theoretical physicist in the first place yeah I think and this is you know my stock answer this point but I think it's it's true I was just an annoying three-year-old who never grew up I I asked why all the time curious how does everything work I have a I have a 19-month at home right now and I can see the way he followed the washing machine repair man around and had to look inside the washing machine so like I think I just kept that going and when when you're more quantitatively oriented than than not rather than going to philosophy I think you sort of Veer into physics and and that was sort of what what interested me does the world work what is what is all this other stuff that's out there um the question that you didn't ask but maybe I'll just answer answer it ahead of time the the sort of inward facing Stuff felt less quantitative and more in the realm of the humanities like so what is all this stuff that's pretty what is all this other stuff in sort of some of the same Frameworks and and so that's been very exciting for me so we should be trying to recruit your 19-month-old right now is what you're saying oh yeah absolutely he's got his um he grew out of one of his his Sequoia onesie but he fits into his Sequoia toddler t-shirt now and so he he definitely is is um ready to be a future founder I guess at what point did you know that you wanted to think about AI um at what point did did that switch start to flip yeah so I think like like many people I when I discovered computers I wanted to understand how they worked and how to program them in undergrad I I took an AI class and it was very much good oldfashioned AI a lot of the ideas in that class actually are coming back to be relevant but at the time it was it seemed not very practical it was a lot of you know if this happens do that you know and um there was also some some game playing there that was sort of interesting it was it was very algorithmic but it didn't seem related to what it means to be intelligent can we just pick up on that real quick like what what do you think it means to be intelligent that's a great question um I can see I can see wheels turning yeah well this is this is one of those questions where I I don't have a stock answer but I I think it's important to to not say nonsense one of the things that's exciting to me about AI is the ability to have systems that do what humans do and to be able to understand them from how you know what are the lines of python that cause that system to to do something to you know to trace that through and understand what are the outputs and you know how the system can like see and and classify what it mean you know what is a cat what is not a cat or or can write poetry and and you know it's just a few lines of code whereas if you're trying to study humans and ask what does it mean for humans to be intelligent you have intelligent you have to go from biology through Neuroscience through psychology up to other higher level versions of of of uh ways of approaching this this question and so I I think maybe as as a nice answer for intelligence um at least the things that that are interesting to me is is the things that humans do and and then now if you up pull back the answer that I just said a second ago that that's how I connected to to ai ai is is taking pieces of what humans do and and we're understanding at least an an example that's kind of simple and easy to study that um that that we might use to understand better what what it is that humans do so Dan you mentioned when you were you know studying AI in college a lot of what sounds like kind of hard-coded logic uh and Brute Force type approaches yeah was there a moment that kind of clicked uh for you of like oh okay this is different was there was there a key result or a moment where it was like okay um we're we're going bigger places then then kind of the if this then that logic of the past yeah actually it didn't click I sort of wrote it off and and it would be great be great if there was a separation between like 10 years between then and the next thing that I'm going to say but actually the the writing off maybe lasted a year or two because then then I went to I went to the UK for the first part of grad school I spent very long time in grad school and I I discovered I discovered uh machine learning and and and and and a more statistical approach to to artificial intelligence where you have a bunch of examples of large amounts of data or at the time maybe we wouldn't have said large amounts of data but we would have said you know you have you have data examples of a task that you want to perform and I there's different ways that machine learning works but you um write down a very flexible algorithm that can adapt to to to these examples and start to perform in a in a in a way similar to to to the examples that you have and and that approach borrows a lot from from physics it also at the time I started so grad uated College in 2009 discovered machine learning in 2010 2011 2012 is The Big Year for deep learning and so you know there's not a big separation here between write off and and rediscovery but I I think and and machine learning clearly existed in 2009 it just wasn't related to the class class that I took but this approach made a lot of sense to me and it started to have you know I got lucky and that it started to have real progress and and seemed to fit in a framework that that I understood scientifically and I got very excited about it why do you think there are so many people who come from a similar background or similar path to you like a lot of EX physicists and working on AI like is that a coincidence is that is that her behavior or do you think there's something that's particularly that makes physicists particularly well suited to understanding AI I think the answer is yes to all all the ways you the you know physicists infiltrate lots of different subjects and we get pared about the way that we go about trying to use use our hammers to to attack all these things that may or may not be Nails throughout history there are a lot of times that physicists have contributed to things that look like machine learning I think in the near term the path that physicists used to take when they don't go didn't remain in Academia often was going into quantitative Finance then data science and I think um machine learning was the and and its uh realization the industry was was exciting because it again it it's something that feels a lot like actual physics and is working towards a problem that's very interesting and exciting for for a lot of people you know you're you're doing this podcast because you're excited about AI everyone's excited about Ai and it it in many ways it's it's a research problem that feels a lot like the physics that people were doing but I I think the the methods of physics actually are different from the methods of traditional computer science and and very well suited for studying um large scale machine learning that that we use to to to work on AI there there's um you know traditionally physics involves a lot of interplay between Theory and experiment you come up with some sort of model that you have some sort of theoretical intuition about then you go do a bunch of experiments and you validate that model or not then you know there's this tight feedback loop between collecting data coming up with theories coming up with toy models understanding moves forward by that and you know we get these really nice explanatory theories and and I think the the way that big deep learning systems work you have this tight feedback loop where you can do a number of experiment the sort of math that we use is very well suited to the math that a lot of physicists are familiar with and so I think I think it's very natural for a lot of physicist to to to work on this and and those tools a number of them differ from sort of the traditional at least theoretical computer science and theoretical machine learning tools for studying the theoretical side of machine learning and maybe also differentiation between just being an awesome engineer and also being a scientist and there's tools from from doing science that are helpful in studying these systems Dan you you wrote this um uh what I thought was a beautiful article black holes in the intelligence explosion and in there you talk about this concept of sort of the microscopic point of view and then the system level point of view and how physics really equips people to think about the system level point of view and that has a sort of complimentary benefit to understanding these systems can you can you just take a minute and kind of explain sort of microscopic versus system level and how the physics influence helps to understand the system level sure so can let me um start with an analogy that I think is is like very you know goes even further than an analogy but going back what year is it maybe like 200 years or so there's around the time in the industrial revolution there was steam engines and steam power and a lot of technology that resulted from this and ultimately powered industrialization and in the beginning there was a lot of engineering of of these steam engines you know and and there was highle theory of of how this work called thermodynamics where you know and I I imagine everyone's seen this in in in high school perhaps where you know there's the ideal gas law that tells you that there's some relationship between pressure and volume and temperature and you know these are very macro level things like you can buy a thermometer you can also like measure the volume of your room and um you can buy a barometer as well maybe people don't uh or look look up on the weather report but you know these are these are things that these are like measurements that weuse and we talk about but then underlying this and it took us a a bit later to to like validate this and understand it there's the notion of of atoms and molecules the air molecules bouncing around and somehow uh we Now understand that those air molecules give rise to things like temperature and pressure and um volume I guess is is easier to understand that the gases the molecules are confined to room and and but there's a precise way in which you can start with a statistical understanding of those molecules ules and derive thermodynamics like derive the ideal gas law from it and you can go further than that derive you know that it's ideal because it's wrong it's it's just a toy model but you can there are corrections to it and you can sort of understand you know from the microscopic perspective which is the the molecules which we don't we don't really interact with you know we don't see them we don't interact with them on a day-to-day basis but their statistical aggregate properties give rise to sort of this this be this behavior that we do see at the at the macro scale and part of to to get to your to get to your question you know I I think there's a similar thing going on with with deep learning systems and I wrote a book um with with Shada uh and and Boris Hannon on on how to apply these sorts of ideas to to to deep learning and in at least an initial a framework that that allows you to start doing this in in an initial way and and um to to answer your question we the the sort of micro pers perspective is you have neurons and weights and biases and we can talk about in detail how that works but you know when people think of the architecture there's there's some very specific some people say circuits there's specific um ways in which these these things um you know there's an input signal which might be an image or text and then there's many parameters and and you know there's a very it's very simple to write down it's not that many lines of of of code even taking into account the machine learning libraries but it's you know it's like a very simple set of equations but there's a lot of Weights there's a lot of numbers that you have to know in order to get it to do something um and that's sort of the micro that's like the molecules perspective and then there's there's like the the macro perspective which is well what did it do did it did it produce a did it produce a poem did it produce did it solve a math problem how does you know how do we go from that those weights and biases to to that macro perspective and so for for for statistical physics the thermodynamics we we understand that completely and you know you could imagine trying to do the same sort of thing literally applying the same sorts of methods in to to to understand the how do this how does the underlying micro statistical behavior of the of these models lead to to the sort of macro or as you said system level pers perspective Dan maybe maybe speaking of of scaling laws and uh I think you were at our event AI Ascent Andre kpoy mentioned that current AI systems are like five or six orders of magnitude off in efficiency compared to biological neural Nets how do you think about that do you think scaling laws get us there just combination of scaling laws plus Hardware getting more efficient or like do you think that there's kind of big step function leaps that need need to come and research there's maybe two things that that that could be meant here one is that the way humans seem to work at a similar scale to AI systems um is much more efficient you know the amount of we don't need to see trillions of tokens before we we we speak we see a much you know my my todler has is already started in to to speak in sentences and he's been exposed to far less um tokens than than a typical large language model um and so there's some sort of Disconnect between human efficiency at learning and and what large language models do uh of course they're very different systems are designed you know the the way in which they learn is is right now very different and so in some sense that's to be expected so there's there's this Gap here that you could imagine bridging there's another thing that that I think is is not what you meant but but I think is is sort of the thing to answer about with respect to scaling laws which is um and I talked about this a bit in the article but a lots of people seem to talk about this which is what is the final GPT you know there's GPT 4 right now and it could be other companies as well but since I'm going to join open AI let me represent my my my new company right so is it going to be six is it going to be seven at some point right if assuming we have to scale things up there are things that are going to break whether they're economic we're going to run out of you know we're going to try to train a model that's larger than the world's GDP or or gwp whatever the however the D works for for for the world and um or we're going to run out of you know we we're not going to be able to produce enough gpus or we're not going to be able to put you know it's going to cover the surface of the you know a lot of these things are going to break down at some point and so probably the economic one happens first so how many you know how many more iterations do we get before we run out of actually being able to scale practically and where does that get us and then I think I think to tie those two two perspectives together there's um so scaling on its own and it and of course it's impossible to entangle this because people are making things more efficient but you know there's like you can imagine there's the take literally what gbt2 was which was the initial big model and keep scaling it up is that going to get us to some you know super different exciting economically power or however you want to Define what the end state of AI research and and AI startups and you know AI in industry is so um or do we need lots of of new exciting ideas and and again of course you can't really dis disentangle these but I think the the general scaling hypothesis is that it's just the scaling and it's not the ideas that that matter um whereas the how do we get to efficient like humans I think requires like non-trivial ideas and and to to answer your your question the reason I'm excited about joining open AI is that I think there is high leverage to be had in in in the ideas you know in going Beyond scaling and that that we will need that in order to get to the to the next next steps and and I'm I have no idea what I'll be working on um but when this airs I guess I will know what I'm working on but that you know that that that's what's really exciting to me yeah Dan is there is there almost like a pendulum that swings back and forth between scale and ideas in terms of how people apply their efforts in the world of AI like Transformers came out great idea since then we've largely been in this race to scale it feels like things are starting to ASM toote for a bunch of practical reasons that you mentioned is the pendulum swinging back toward ideas as the currency you know it's less now about who can you know have the biggest GPU cluster and more about finding new architectural breakthroughs whether that's you know reasoning or something else yeah that's a that's a that's a really great question um I think there there's this article by Richard Sutton um called The Bitter lesson not better pill and and it basically gives the argument that um that ideas aren't important that scale is is is what you need and that you know that that all the all the ideas are always trumped by by scaling by scaling things up um and I mean says a bunch of things but maybe that's that's a high level takeaway and you know there is there's a sense of this where there were there are a lot of you know interesting ideas that came out in the 80s and 90s that people didn't really have scale to explore and then I remember when after alphao and Deep Mind was was writing a lot of papers people were discovering those papers and reimplementing them in deep learning systems but this was sort of still before people realized no the thing that you need to do is scale up and and even now with Transformers people are exploring other architectures or even simpler architectures that we that we knew before that seem to be able to you know there's a notion you know maybe scaling laws don't come from as long as the architecture isn't sick in some way they they come from sort of the underlying data process and and having large amounts of data rather than from from having a special idea I think I think the real answer is that there's a balance be between the two that scale is is hugely important and and maybe it was just not understood how important and we also didn't have the resources to to scale things up at various times you know the things that have to go into producing these GPU clusters that are producing these models are you know you you guys know this as well that like there's a lot of parts along the supply chain or along the the product chain whatever you actually call it in order to make those things happen and deploy them and even you know the way gpus were originally they've now co-evolved to be well suited for these models and trans the reason in some sense you can think of Transformers was a good idea was because it was designed to be well suited to train on the systems that we had at the time and and so sure these other architectures could do it um scient you know at a ideal scientific level but at a practical level there's there's it was important to to get something that was that that was able to to reach that scale so I think I think you know if you brought in ideas to to be that sort of thing that's that's married with scale in some way then um I mean I I still think ultimate like you know someone came up with the idea of deep learning that was an important idea you know there's Pitts and Mulla came up with the original idea for the for the for the um for the neuron and then there's lots of Rosen blat came up with the original perceptron and there's like a lot of people from going back like 80 or so years of people making making important discoveries that were ideas that that contribute so so I think I think it's both it's but but it's easy to see how you know if you get to apply if you're bottleneck and then all of a sudden you get to apply if you're bottleneck people think about ideas and then if you unlock a new capacity of scale somehow then you just see a huge set of results and it seems like scale is super important and I really think it's more of a Synergy between the two maybe on the topic of the race to scale Dan you mentioned kind of the just the economic constraints and realities uh which I I I i' guess are more like practically a ceiling in the private sector you also mentioned the Manhattan Project earlier in in terms of things that physicists have been involved with like do you think we need a Manhattan Project Style thing for AI like at the nation state or at the international level um well one thing I can say is that part of the process that led me to open AI is I was talking with your partner Sean Maguire who brought me to sequa in the first place um and trying to figure out is there a startup that makes sense for me to for me to work on that has the right mix of sort of scientific questions research questions and also as a business and and I think it was Sean that that said and I don't mean the analogy in terms of the impact of you know in terms of the like negative impact of what the man you might think of the Manhattan Project but just in terms of the scale in the organization he said you know in the 40s the physicists went to the Manhattan Project even if they were doing other things that was that was the place to be and so now ai is the same thing and you know basically said open AI as that place so maybe maybe we don't need a public sector organized Um Manhattan Project but it you know it can be open AI Open the Eyes Manhattan projects I love that well maybe maybe that's not a direct quote that we want to be taken out of context but I think in terms of metaphoric is the metaphorical manhatt project yeah in terms of scale and ambition in terms of I think I mean I think a lot of physicists would love to work at open AI for for a lot of the same reasons that they probably were excited to well okay there's a number of different reasons Maybe maybe we just have to leave it as a Nuance thing rather than making broad Claims can we talk a little bit about this like can we ever understand AI especially as we go to these deep neural Nets or do you think it's a hopeless black box so to speak yeah I think within the this is this is my answer to the what are you a contrarian about although maybe you know on the internet everyone takes every side of every position so it's hard to to to to say you're you're Havoc aan position but I think within AI communities you know I think my contrarian position is that we can really understand these systems and in the you know physics systems are extremely complicated and and we have made a huge amount of progress in understanding them I think these systems sit in the in the same framework and you know another principle uh that that show and I talk about in our book and that that's a principle of physics is that there's often extreme simplicity at very large scales um basically due to due to the statistical averaging or more technically the central limit theorem uh things things can simplify and I'm not saying this is what happens exactly in large language models of of course not but I but I do think that we can apply sort of the methods that that we have and also you know maybe hopefully have ai that that can help us do this in in the future and by AI I mean tools not like individual intelligences just going running on their own and solving these problems but but I I I guess I I feel at the extreme end that that um that this is not going to be an art that it that that the science will will catch up and it we we'll be able to make extreme um leaps in really understanding how these systems work and behave so Dan we've talked a bunch about what physics can teach us about AI um can we talk a bit about what AI can teach us about physics are you are you optimistic about domains like physics and Math and how these emerging models can you know probe further into those domains yes I'm definitely optimistic I guess my um my perspective is that math will be easier than physics which maybe betrays the fact that that that I'm I'm a physicist not not a not a mathematician uh and um I'll I'll say I I can say I can give uh explain what why why I think that um in a in a second um but you know I still have a lot of friends that that that work in physics and they they you know there there's a there's like a growing sense and maybe maybe in even approaching a dread that and maybe this is actually the answer to why do physicists work on AI because you know if if what you care about is the answer to your physics question and you want to make it happen as soon as possible what is the highest leverage thing you can do maybe it's not work on the physics question you care about but it's work on AI to make the you know to to because you think that the AI might end up solving those those questions very rapidly anyway and I don't know the extent that anyone really takes it seriously but I think within the the theoretical physics community that I come from that this is sort of a a thing that that someone gets thrown around and and discussed I I think um maybe to give a more object level answer I think what's exciting about math and um maybe when you have Noom Brown on if you have him on he'll talk about this but this is um something that that he's talked about for a while before he joined open AI um I I think that that that we we have um you know we we made a lot of progress in terms of um solving games by doing more than just looking up what is the strategy that we should use to play the games but also being able to simulate forward and you know you know the way the way that if I'm in a very hard position in a in a particular game rather than just playing with intuition I might sit and think about what what I should do sometimes this goes under the name inference time compute rather than training training time compute or pre-training and you know I there's there's a sense in which what it means to do reasoning is very related to the this ability to sit and think so we know how to do it for games because there's a very clear witting and loss signal so you can simulate ahead and sort of figure out what it means to do good or or not and I think math in some some parts of math again I'm not a mathematician and well you know always scared about talking about math publicly and saying something wrong that will upset mathematicians but seems like certain types of math problems are not as constrained as games but are still constrained enough where there's a notion of you know like finding a proof right you know there's there's defin problems in terms of search in terms of how how do you figure out what is the next you know move in the proof but the fact that we might call it a move suggests that the there's things in math that feel a lot like games and so we might think the fact that we can do well at games maybe means that we can do well at certain types of of mathematical Discovery Well I was gon to say since you mentioned Noom he he likes to use the example um with test time compute of whether it could help to prove the the ran hypothesis right is there a similar is there a similar problem or hypothesis in the world of physics that you are optimistic AI can help to solve sometime in our lifetimes yeah so I mean there's a millennium problem relating to physics and if I try to um remember exactly what it is I'm sure I will um butcher it and then no one will believe that I'm actually a physicist but it's related to the you know it's a mathematical physics question related to the Yang Mills mask app and um but but I I think what I wanted to say is that I think some of the flavor of what physic is care about and doing physics feels a little different this is where I might get in trouble feels a little different than some of like the mathematical proof type things physicists are known to be more informal um and you know uh handwavy but also on the other hand connected to in some sense connected to the interplay between experiment and and and and the sort of models that phist study is maybe what saves them is that you know they they have things are informal and handwavy but very explanatory and the mathematicians it's like we were saying earlier the the engineers discovered all the exciting industrial machines and then the physicists maybe cleaned up a bunch of the the theory about how that works and then the mathematicians come later and clean up like formalize everything and clean it up even even more and so there's a mathematician or mathematical physicists that clean up a lot of you know make make proper and and try and you know understand in in formal ways some of the stuff the physicists do but um rather than talking about I mean I I think the the the the key point there is that the sort of questions that are interesting to physicists maybe don't look like proofs but maybe they look like and maybe it's not about how do we given a a particular model how do we actually solve it like once once things are set up correctly like it's often you know senior or you know people that are trained in the field are able to to sort of figure out how to analyze those systems it's more the the other stuff like what is the right model to study does It capture the right sort of problems does this relate to the thing that you care about what are the insights you should draw from it and and so for AI to help there I think it would look different than the way we're sort of trying to build AI systems for math so rather than here's you know here's the word problem go and you know solve solve solve this High School level problem or you know prove the remon hypothesis it's like you know the questions in physics are like what is quantum gravity what happens when something goes into a black hole and that's not like you know start generating tokens on that like what does what does that even look like and and you know if you go to a physics department you know people hang out at the Blackport they chat about things like maybe they sketch mathematical things but but the you know there there's a lot of other things that go go into this and so maybe the sort of data that you need to collect looks more like that or maybe it looks like the random emails and conversations on slack and the and the scratch work and and so um I mean there are definitely tools that we can use like help me understand this new paper so I don't have to spend two weeks trying to study it and understand it you know maybe let me ask questions about it I think there are problems with the way that's currently implemented but you know I think there are a lot of tools that will help accelerate physicist just like Mathematica which is a software packages that does integrals and and it does a lot more than that sorry Stephen W from but uh you know I I use it to do integrals and um and and you know sometimes it doesn't know integrals and you can look them up in like these integral tables and and and anyway um you know I I I think you know there there's and and this applies to other branches of science too like I think there the ways in which the questions are asked and what it means to do science in different fields maybe looks can look further and further from games let's just say and so to the extent that that's true I think we'll need to and not even clear that we'll need lots of ideas or I mean we sorry we will need lots of ideas but it's more just like we'll have to I think we'll just have to approach them all differently and maybe not like maybe eventually we'll have a universal thing that knows how to do all of it but but initially like at least to me the a lot of these things feel a little different from each other you'll have a front row seat to it in part because you're also on the prize committee for the AI math Olympian which is something I'm personally super interested in um maybe to your last point on kind of like maybe eventually the stuff generalizes like why do you think people are so focused on solving the hardest problems today like physics math those were the subjects that everyone was terrified of in school right um where it feels like there's a lot more other domains that are also unsolved for now like do think do you think going for the hardest domains first kind of lets you get towards a generalized intelligence like how does how does solving these different domains kind of fit together in the in the grander puzzle yeah the first thing that comes to mind when you said that is to just push back and say well it's not hard these are the easy domains I mean I'm I'm bad at biology but I I can't can't doesn't make any sense to me at all my my um my my girlfriend actually is bioengineering and in biotech Tech and so I like what she does just makes no sense to me can't understand any of it um where physics makes complete sense to me I I think I think maybe a better answer or a less Global answer is that um like I was trying to say about math there there are constraints and you know and in particular with math a lot of it is unembodied you don't have to go and do experiments in the real world you know they're they're sort of self-sufficient and that's that's close to like what generating text like the the way language models work or even the way some reinforcement learning systems work for for games and so I think the further that you go from that the Messier Things become the the the harder it probably probably is and also the harder it is to get the right kind of data to to train these systems if you want to build a Ai and people are trying to do this but it seems difficult if you want to build a AI system that solves biology I guess you need to also make sure robotics works you know so that it can do those sorts of experiments and like it has to understand that sort of data or maybe it has humans do it but you know there's a lot you know for for a self-sustaining AI biologist it seems like there's a lot of things that are going to go into it I mean on the way we'll have things like alphafold 3 which you know just came out and which I didn't get a chance to read the details of but you know I saw that they're trying to use it for drug Discovery and and so you know I think each of these fields will have things developed along the way but it but I think the the less constraints there are and the the sort of Messier and more embodied it is the harder it will be to to accomplish so that makes sense so like hard for a human is not the same thing doesn't correlate to hard for a machine um yeah plus also maybe humans disagree about what what what's hard some us think more like machines I guess um and I guess the second question was like do you think it all coalesces into like one big model that understands everything because right now it seems like there's a lot of domain specific problem solving that's happening yeah I mean the way things are going it seems like the answer should be yes it's it's really dangerous to speculate in this field cuz everything you say is wrong um usually much sooner than will'll hold you to it yeah exactly but also like what does it mean to be different you know like there's a trivial way to make both things make the question meaningless by like you know you say the model is the union of all those other models but there's also sense in which mixture of experts is not was originally meant to be that it's not that in practice at all but you know there's there's a sliding scale here but you know it does seem like people um at least the big labs are going for the one big model and and have a belief that that's I you know well I I don't know but maybe I will in the future um understand what the philosophy is there yeah Dan we have a handful of more general questions to to kind of close things out here so I'll start start with a high level one um if we think kind of short-term medium-term longterm and call it you know five months five years five decades what are you most excited or optimistic about in the world of AI five years five years ago was about it was after the Transformer model came out but it was around maybe when gpt2 came out so it seems like you know for the last 5 years we've been doing scaling I imagine within the next 5 years we'll see that scaling will will terminate and maybe it will terminate in you know a Utopia of some kind you know that that people are excited about we we're all post economic and and so forth and we'll have to you'll have to shut down all your funds and you know return Monopoly money cuz money won't matter or you know or we'll see that we need lots of ideas maybe there will be another AI winter I I I imagine that um and again scary to to Really speculate but I imagine like something interestingly will be something will be interestingly different within five years about Ai and it might just be that AI is over and we're on to the you know we're on to the next exciting investment opportunity and you know the everyone else will will shift elsewhere and um you know I'm not saying that that's not what's motivating me about about AI but you know so maybe five years is enough time to to see that and um I think in in one year I mean or there's a five I messed this up whatever maybe was five months I don't remember sorry this is five months it's okay it's okay these are approximations I know you said physicists are very handwavy Venture capitalists are very hand wavy these are approximations yeah in physics there's I like to joke that there's like three numbers there's zero one and infinity and you know those are the only numbers that matter um you know things are either arbitrarily small arbitrarily large about order one so okay um good thanks for reminding me but but yeah for for 5 months I mean I'm excited um to to well to to learn what's exciting at the Forefront at of a of a huge research lab like like open Ai and I I think one thing that'll be interesting will be the Delta between um the next generation of models right because there's there's ways in which things are scaling up in terms of I it's not really public I guess aside from meta but in terms of size of data size of size of models and we see scaling laws and scaling laws though relate to you know something like the loss and and and you know it's hard to translate that into actual capabilities and so what will it feel like to talk to the Next Generation model what will it look like will it have a huge economic impact or or not and um I think I you know in in terms of like estimating velocity right you need a few points you can't just have one point and we sort of have you know we're starting to have that with gpt3 to GPD 4 but you know I feel like with the next Delta we'll get to really see what the velocity looks like and what it feels like going from from model to model to model and maybe I'll be able to make a better prediction in five months from now but then I guess I probably won't be able to tell it to you guys thanks Dan uh one thing that stood out to me is just like what your your writing is so accessible and and light and funny and that's not what I'm used to when I read super technical stuff like do you think all technical writing should be informal and funny like what is is that deliberate uh it's definitely deliberate um it goes into I think in some sense it's inherited I mean I definitely am a not serious person but I also think it's inherited sort of from the style of the field that I that I came from but I I'll tell you a story I was I was at lunch uh I was a postto at The Institute for advanced study in prinston and uh I was having lunch and um joking around with this professor naughty cyber who's who's a professor at The Institute and um then we got into I I think we're talking about someone someone asked the question about like what is a good title and I was like oh the title has to be a joke and he was on board with that and and then I was explaining that for me the reason to write a paper is for the jokes like you have a bunch of jokes in mind and and then you want people to read those jokes and so you have to package it into the science product and people want to read the science product and they're forced to suffer through the jokes and na he's this Israeli professor and he was like I don't get it why why can't you just do the science why do you need the like the jokes are great too but like you should write for the science not for the jokes and I was adamant that I write for the jokes and but I but I think it's I think it's what you said that uh at some point you know you learn about the scientific method and you know the right the formal ways of doing things and you learn all these rules and then you grow up a bit or maybe I had a roommate who is a linguist he's now a professor of linguistics at UT Austin um and and like he emphasize that you can you would tell me which rules that I could break or like where the rules come from and why they're important or not and you sort of realize that that you can you can break these rules and like the ultimate goal should be is the reader going to read it and understand it and enjoy it so you don't want to do things that compromise their ability to to read and understand but you don't want to obscure things you want to make it you know if it's more enjoyable people are more likely to read it and take the point it's also more fun if you're if you're writing it so so I think I think that's where that comes from Dan thank you so much for joining us today we we learned a lot we enjoyed your jokes uh and I hope you have I hope you have a wonderful uh second to last day at seoa thank you for spending part of it with us we really appreciate it thanks I was absolutely delighted to be here chatting chatting with you guys it was wonderful [Music] [Music]

========================================

--- Video 47 ---
Video ID: Hio8VGQMlZ4
URL: https://www.youtube.com/watch?v=Hio8VGQMlZ4
Title: Google NotebookLM’s Raiza Martin and Jason Spielman on the Potential for Source-Grounded AI
Published: 2024-10-15 09:00:20 UTC
Description:
NotebookLM from Google Labs has become the breakout viral AI product of the year. The feature that catapulted it to viral fame is “audio overview,” which generates eerily realistic two-host podcast audio from any input you upload—written doc, audio or video file, or even a PDF. But to describe NotebookLM as a “podcast generator” is to vastly undersell it. The real magic of the product is in offering multi-modal dimensions to explore your own content in new ways—with context that’s surprisingly additive. 200-page training manuals become synthesized into digestible chapters, turned into a 10-minute podcast—or both—and shared with the sales team, just to cite one example. Raiza Martin and Jason Speilman join us to discuss how the magic happens, and what’s next for source-grounded AI.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 Introduction
1:39 Google’s ChatGPT moment? 
3:39 NotebookLM’s genesis
5:50 Making Audio Overview magical
08:04 User Experiences and Use Cases
10:44 Surprising moments 
12:20 Design choices
14:08 Challenges for a new AI-native experience 
19:58 Where does NotebookLM go from here? 
21:23 Making Audio Overview conversational and adding personality
25:36 Where did the idea for Audio Overview come from?
27:49 Building NotebookLM at Google

Transcript Language: English (auto-generated)
hey everyone we're here on training data it's uh great to be here I'm a longtime listener and fan of seoa capital pretty exciting to have us the host of another podcast join as guests yeah I really just want to say a huge huge thank you thanks for having us on the show yeah seriously thank you to Sonia and Pat for having us I mean it sounds like we're on the show because we've had lots of listeners ourselves listeners of Deep dive oh yeah we've made a ton of audio overview since we launched so it's nice to get to talk about it definitely a cool experience getting to be here and exciting to share what we're going to do next exactly it we keep learning and getting better for you we're glad you're along for the ride so yeah keep listening keep listening and stay curious we promise to keep diving deep and uh bringing you even better options in the future stay curious [Music] two weeks ago a mysterious experimental product from Google became the talk of the tone notebook LM an AI powered research tool that went viral for its ability to create staggeringly and hilariously realistic podcasts from any sorts material today we're excited to feature Risa Martin and Jason Spielman the product and design leads on Notebook LM at Google we talked to Risa and Jason about the inspiration for the product the development process for a project like this inside a large organization like Google the surprising use cases that have emerged and what's next for Notebook LM Risa Jason thank you so much for joining us today and we're glad that we could get you on today before your AI podcast host take over the podcasting world and put us out of a job so thank you for joining us thank you for having us I'd love to start by asking you know people are calling notebook LM Google's chat GPT moment it was an experimental product kind of in preview mode and just went virals the gpus are going bur you guys are you know the Talk of the Town do you agree with that take I mean chat gbt was was pretty big for me and so to imagine the comparison there for me feels a little bit like whoa is it um but I think what we're seeing from a lot of people is that it's having a similar impact of like wow this is AI this is what AI can do so that's been really cool yeah and I think I would agree in some sense that the first time I listen to an audio overview you know when that second host came on it really was like a mindblowing experience but I think it's also like at the underlying layer like the fact that we have Gemini 1.5 Pro digesting all this really complicated information um and spitting it out in a way that's pretty concise um I think like the combo of those to me is definitely like a pretty unbelievable moment just for everyone who's listening what what is notebook for anybody that hasn't played with the products yet yeah notebook is an AI powered research and writing tool but I think nowadays it's more commonly known as upload a source and then it will generate an audio overview for you or podcast and did that happen by accident like did you start out wanting to build a you know a podcast host killer or did that did that come out by by accident somehow I think you know honestly uh we were always working on the different modalities for output I think voice was the next one and we chose dialogue did we know that it was going to be a killer I want say no I I think I thought it was pretty magical but the way that it's really landed with people has been delightful and surprising well and I know you guys have been working on Notebook LM for a while can you take us back to the beginning of the project you know what was the initial idea how did it come to be yeah I mean I remember I was working on AI Test Kitchen and this was last year and uh notebook actually started as a 20% project we had one engineer who had been working on something called talk to small Corp and super funny great consumer brand I was like what is a corpus uh but then I would chat with him he was like Hey you know it's really the idea that you can use llms to talk to your data try to extract stuff from it and I was like oh that's super interesting started thinking about okay what are the practical use cases here and I actually went to school as an adult learner and it to me I was like wait a second if I use an llm and I understood what llms could do and I could use this maybe to talk to something like a textbook oh this is pretty exciting I could see how that could that could change my life it could change lots of lives and that's when we started really revving on like hey what do we build to introduce the first version of this to people and you know it was in May of 23 that we introduced project hwind and it was just that you uploaded a source a PDF you can chat with it yeah I think that the fact that we are Source grounded is what makes the product so unique um I think even when I started thinking about this project I didn't realize that everything in my life that I create often has some sort of Prior artifact or document that I used to create something new um and so I think you know right now at least I would call this like a source grounded tool um but we're really becoming a source grounded tool for creation um and a bunch of other stuff as well are there any stats you can share about notebook LM I think what I'll share is that we were on a steady growth path before a overviews but since we launched it it's sort of rapidly accelerated and that's been really exciting it's been a really good hook to bring people into the product um I think the other thing I'll say is that while it brings people in uh people generally stay for the rest of the features and that's been also really interesting to see in terms of like what people are trying to get out of a tool like notebook so the podcast or the audio overview experience is absolutely magical can you tell us a little bit about how it works behind the scenes like how did you make it so lifelike how did you make it how do you make the dialogue so good and engaging it just draws you in like how do you do it yeah I mean first I'll tell you it was a lot of work it was a lot of teamwork there is a lot of craftsmanship that really went into it but at the heart of it is really Google's models you've got Gemini 1.5 which is such an incredible model in terms of taking all of that data that you give to notebook LM and then producing something new out of it and then you have the voice models the Audio models that back uh notebook LM I'd say the real sort of Powerhouse between those two is something we've built called content studio and that's really what brings to life sort of the the editorial right between you bringing your content and then coming out with the podcast there's some editorial Liberty that we take with the studio and so in the future do you see yourself exposing the studio out to people you know make this make this one funnier make make this one more serious so I think like that we hear a lot like particularly because so many people are using it so many people are delighted by it I think the next step is then people want the knobs right they want to be able to control it and this is where you know my gut reaction was okay let's ship the knobs but I'm trying to have a little bit more discipline in thinking that hey you know people fell in love with it because it was delightful it was magical how do I ship delightful and magical knobs totally that you know there's only so much I can do but but I think there's a way and so I'm very interested in doing that I actually do think part of the explosion of audio overviews was the fact it was a simple oneclick experience you know I was on the with my grandma trying to explain her how to use it and it actually didn't take any explanation you know I'm like okay drop in a source she's like oh I see I click this button to generate it and I think that the the ease of creation really actually is what catalyzed so much explosion um and so I think when we as we think about adding these knobs I think we want to do in a way that's very intentional also just like fun you mentioned people come for the podcast and then stay for everything else what are some of the best use cases you've seen for the everything else I think um I'd say one of the most surprising ones I talked a little bit about the educational use case it was very personal to me and I saw a lot of students and a lot of Educators using notebook LM but what's been surprising is to see the amount of people that are using notebook LM at work so one good example is we run we ran a pilot case study inside of Google and So within the ads team we have a lot of AD sellers ad Specialists and I didn't know this but for these ad sellers a lot of their sales training and documentation are hundreds of pages long and I was just like how does anybody learn this right and then the stuff changes all the time and so it's very hard to keep track of how something works you know well enough such that you can sell it and what the sales teams normally do or before book LM what they would do is they would ping each other they' be like Hey Joe right how does this thing work how do I position it for this client you wait for Joe to respond and then you're like okay let me copy paste this into an email fiddle it a little bit and that was it um but it turns out people like Joe who have a lot of that knowledge and read all of this documentation they build the notebooks and then they distribute it to their sales associates and then that's hundreds of people automatically that are using the notebook because now they don't have to pick and that that's really interesting to me because I was like oh it's like a really simple use case and then there's so much more that you can really build on top of that totally actually was just hearing I was talking to a friend who's in sales who was like dude it's it's great I made this whole notebook and I when I'm on calls I don't know the answer I can quickly ask and get a response and so I think that this idea of knowledge distribution is really helpful for large sales team or data um data centers and stuff um I think another use case that I think is really interesting that actually you may align with is I have a lot of friends who work in Venture and PE and um this idea of a confidential information memorandum a Sim you know I never heard of this before um but I have a friend who he's like this is my whole job is basically going through these packets of information and so what I do is I take these you know these documents I receive or slides I put it in a notebook and I'm able to now way faster than before go through all this fairly complex information um and I think that he was you know he was telling me he like 10 xed his job speed you know which is great and it's just this is empowering him to be faster podcast host and Venture Capital you are really going after our jobs I we're helping out your jobs I don't I don't think yeah what have been the moments that have surprised you the most for me it was you know the moment where the the AI hosts kind of realized that they their AI that was like this really cool moments but what what what have been the moments along the way that have surprised you the most I mean I'll I'll start I was I was going to bed this was last weekend I think or just several days ago um and I was on Twitter probably not healthy to be doing this before bed but I was check Twitter it's Fire by the way I I was scrolling through and I saw the poop fart one if folks haven't heard it I also now I'm just like this is a very important mention by the way like if you haven't heard it you really do need to understand how magical this one is okay if folks have not heard the poop fart one somebody de to us somebody decided they were going to upload a document where the the only words in the doc were poop and fart over and over and over and over again so it was a pretty lengthy doc but it was just those two words and I saw that's what they had done with it they described it and I was like oh man should I listen now it's 11 o' if I tap this and it's a safety flag I'm not going to be able to go to bed right because I'm G to have to open a bug I'm going to Ping the engineers it's like hey we got this thing going on I was like all right I'll just listen and it's actually un unbelievable I also like I saw it and I was like uh oh like we let's let's let's see what this is going to be and you listen you're like oh this is this is fantastic like this is even better than I could have ever imagined it was one of those mods where I was like Well Done notebook good little guy solving the right Goins clearly amazing um what design choices have you made that that have made notebook work so well and so intuitive for people you know I think that I clarify we're still making those decisions um I think right now we're very much in the process of you know launching quickly and then working closely with our users to understand what's best and what they want um you know Tech is evolving so fast right now that it's really hard to know what's even possible and so I think that we're really pushing for this model where we launch quickly and then work kind of alongside our users to build the best product um but to answer your question more specifically I think that one thing that we've done that I think was almost a happy accident in a way was make that left Source panel really clear I think that we are a source grounded project and we need to make that clear that you're talking to the sources that you've uploaded and so I think that um having those sources there on the left is like a pretty crucial part of this project um I do also think though as I was mentioning earlier audio overviews being one click um seem to actually pay off that it we we like really leaned into the simple experience um but that being said there's a lot more coming and we're actively working with you users to improve the project product I think one thing that I'll I'll chime in in terms of like design choices and and really I think on the product prioritization sort of side of things is really thinking through what does it take to make something new intuitive and it's really hard especially something as nuanced as like oh first you have to upload a source like users generally like bulk at that step like why right like I don't have to upload a source to chat GPT I don't have to upload a source to Gemini it just works and so I think we still have a lot of work to do uh around the it just works category what do you think are the biggest challenges remaining as you you know kind of bring people on to this new a AI native experience yeah I think that we're kind of in this quote unquote skoric era of AI design um and I think to explain skor ISM it's when a virtual object reflects a real world object and that was seen in early iOS when the um the app had a leather bounding at the top and the pad was yellow and that was made to kind of ease users into this uh virtual world from the physical world and I think now we're seeing something similar with AI where we need to build uis that help meet users where they are and I think right now we're doing our best to kind of be really creative and think about these new kind of crazy um experiences but also understanding that many of these users this is the first time interacting with artificial intelligence how do you think about you know one thing I think mid Journey has done extremely well is just making it easy to you know get over the blank wall prompt problem and so like to me that's something that mid Journey has done extremely well are there any other applications that have kind of approached some of these UI challenges that you admire I have one uh recently I just tried Pika and I really love the Pika effects where you can see exactly what's going to happen to your image if you if you upload one because Pika is similar in to the that you have to upload something and then you have to maybe write a prompt or choose an effect and I think it was really well done where it's like hey here's a preview right it will squish the thing I was like oh this is fascinating and of course I uploaded there was like one that was like cake I uploaded a a drink a picture of a drink and I was like make it cake and just like the anticipation of the drink is going to become cake I was like come on come on come on I was like should I pay for it now it was like my first generation too I was ready to pay for it and I that's that's how I knew I was like oh like there's definitely something there about show the user what's on the flip side that I think really incentivizes the user to not only you know give you the image but like they're really excited to see what happens and then you know if you're me you're like take my $10 yeah so it's really effective you know I think for me I love Cloud artifacts um I think they've done such an amazing job of co-creation um we've talked a lot about writing code creation um and it was awesome to see kind of others in the space thinking about that as well um I think that right now as I was just briefly mentioning like we're in this space where we want to equal the hierarchy between you know Ai and human and I think we definitely don't want to take your jobs we want just want to help support your jobs you know and I think that cloud artifacts was like a perfect example of that in my mind which was cool you can talk to the chat but also start building out something on this right side as well how do you think your product kind of you know compare and contrast to the approach that cloud has taken like do you think it's you know similar things that you're going after or how do you think about the differences I mean I think first and foremost right now at least we're a source grounded tool which um kind of immediately makes us a bit different um that being said I think we're thinking a lot about creation broadly um utilizing the sources that you've uploaded well I think I think to that point like the contextualization of your llm interactions is really powerful and I think it creates like a stickier user experience I think that you know if I had to guess like the folks at Claude probably know this or the folks at anthropic the folks at open AI probably know this certainly the people at Google know this but I think there is a question of when to introduce it and what are the right surfaces so I think this is where for Notebook LM I'm excited because we started there right so there's like a a little bit of hey as people catch on to the importance of like Source grounded workflows Source grounded stories this could be the tool that they're looking for and if we just Sprint at this hopefully we'll get farther along you know before before everybody else who's juggling all of these other use cases you mentioned earlier that you know chat is kind of the skoric interface for for AI and that you guys are experimenting with crazier things like what what might the crazier things look or feel like give us a taste you know I think that just at a high level I'm super intrigued with these kind of dynamic uis actually think Claude is an example of that right you see this artifact come in that wasn't originally there um I think we're thinking a lot about how to we're trying to do a lot here right Reading Writing and I think there's only so much that you can do before a user gets overwhelmed and so I think we're really exploring how do we take advantage of what you're doing at that moment um while also not overwhelming you with all the other possibilities I think for me I think a lot about leaning more into new modalities which is what does it mean from an input and an output side and um I do a lot of prototyping on my own and experiment with a lot of my own behaviors and one of my favorite ones is sort of the idea that I can sort of walk and talk with my llm right or with like an AI sort of ecosystem and one of my favorite recent examples is I started doing this with a Daily Journal where instead of writing my journal I just go back and forth and it creates the journal log for me and then it creates a visualization of basically like hey you know this week you had more bad days than good or you had more good days than bad here are the things right that made you happy here are the things that made you upset and I think there's a lot of richness there um in interaction where I think about it it's like hey Source grounded AI of course there's like some really practical sort of work use cases there there's some educational use cases but the personal ones are also really compelling and so I'm trying to think about how how do I take these learnings and bring it back into notebook LM and you know probably in something like the mobile app right we might see more of that so you know have you know you have lightning in a bottle with notebook LM where where do you hope to take it from here I think honestly just keep going I just want to keep building more cool stuff we want to deepen the experience for users we want to make it really useful I think there's a lot of magic right now a lot of delight and I think we want to deliver on the promise of that that initial hit and just show people like hey you can you stick around it's going to be great what do you think is the biggest thing missing from the product today oh well I'll say if I could you know rewind back in time and build more things as part of this launch I would definitely build a better share experience just the amount of uh you know when I scroll on X and I see all the videos and visualizers that people use instead of like our native one you know as a as a product lead I'm like oh I'm missing out on Counting this user here because they're on different surface now so I think for me it's really the sharing uh and sort of collaboration around uh the audio overviews that's missing and I think as we started talking about I think I'm really excited about the addition of a of a writing experience um I think that we know that people are often doing Q&A and then taking that answer to then create something new and so I'm just excited to help kind of fulfill that whole user Journey how do you make it like are you Pro prompt engineering to make it kind of like do you tell it to like you know conversational funny like how what are you doing on the technology yeah how do you design the personalities I'm really curious about that too so there's a lot um that we do in the background and I think you hit on some really good aspects of it especially around you know the show is called Deep dive right there's clearly Two Hosts and what I'll say is that there's a lot more editorial Liberty that the personas themselves take to generate that show and I think that's where you know even for me I am always interested to see where they're going to take the show based on the sources that are uploaded oh interesting so you've given each of the sources its own personality its own how how it approaches things and then you let it create the podcast yeah in a nutshell I think like that's the that's the best explanation for what we've got going and so when we think about the editing experience right it's like oh what are the controls for something like that of course like there's like basic stuff which is maybe I don't don't want a deep dive maybe I want a different show maybe I want a different length maybe I want shorter maybe I want longer maybe I just want a spec you know uh specify a topic instead of like the whole thing because today it's like an overview based uh audio so I think there's there's a lot there that we can tweak but the heart of it is really this editorial Liberty around your sources and trying to give you an overview of it and every time I've joked that you're going to take our jobs you say you say we're not but I don't know if you're just saying that to be nice because what you've generated is legitimately so good and so the real question I have is when you say that it's not good enough to replace real podcast like why why do you say that because it to me it feels good enough to replace a real podcast I think you know I think that's like a a good question and one that I I try to approach really carefully particularly because hey if there's real risk I want to I want to look at it in the eye and say okay how how do we how do we address this but from what I've seen a lot of what people are making are not the same things that I feel like we would of a real podcast about right like do I want to listen do I want to take an article and make a podcast out of it that replaces you know one of my favorite podcast Lenny's right I listen to Lenny's all the time it's like no I want to listen to Lenny I want to listen to what he thinks about this particular topic um and and then what's funny is like people are making audio overviews of things like their resumés right their LinkedIn bio or startup Founders putting in their landing pages and trying to figure out oh was my messaging clear like that stuff is really cool because it's like no one's ever going to make a podcast of that I mean maybe not at this stage right but um that's where I think okay this feels really good it feels like we've created a space where personalized generation really is about meeting my needs exactly where I'm at and there isn't an existing thing out there and that's really special it does almost feel like a different media type like sure it sounds like a podcast but I think you give great examples to kind of prove all these random use cases people are using it for but I think there's also a reason that reaction videos are so popular online like people aren't just listening to this right now because of us because they want to hear from both of you who are in this space and I think that's also important to remember when they came out podcasts I will say like one interesting thing about the dynamic is even though people are sharing the audio overviews that they're generating they're very personal it's like I made this for me I didn't make it for you to listen to my resume it was me I was delighted by the audio overview of my resume or there's this really cool Tik Tok of where this uh woman uploads her diary from 2004 it's like it was interesting to listen to that together but it was really her reaction to her diary that you know she wasn't going to listen to a podcast about that ever um one of my favorite use cases actually I don't know if this was in the Discord but somebody recently took um they said over the weekend their group chat with their college friends had blown up and so they they didn't read the messages but they took all of it and they copy pasted it into a dock and they're like well Monday morning I'm going to listen to what my college friend said on my drive to work I like this incredible and I think that's what personalized generation is so in a world of chat boxes where did the idea hey people want to listen to this people want to consume this content in podcast form like where did that idea come from yeah I mean I think that it goes back a little bit to something Jason was saying which is how do we deliver new things in a recognizable format or in a way that's easy for people to understand such that they would be willing to try it and I think the combination of upload your Source generate a new voice thing we're like well what are what's the universe of voice things that we could generate we have this really powerful voice model and we experimented we were like we could do a monologue we could do a dialogue we could give the user a switch but it was really the dialogue that was resonating with people because it was like oh it's podcast right it's not just like a text to speech like reading the output like we typically expect and I think once we saw how much that delighted people we knew that was the thing to ship okay so you now have this killer feature in the podcast and you have an incredibly General horizontal surface as well where do you go from here like do you go deeper into the podcast thing or do you go build out when do we get YouTube videos yeah yeah I think you know that's just that's a cost problem we could drop those in now as an input but an output yeah I think we gota we got to work on that a little bit yeah I think uh it's exciting because the road map is I don't want to say it's fairly straightforward I feel like I'm going to jinx myself and some things going to happen tomorrow but um we know that we want to deliver on the promise of bringing in all the inputs that matter to you and letting you use the power of AI to create something new and I think the podcasts are definitely one type of output that we want to go deeper on especially because we've seen how much people care about them so that's one part of it but I think we want to deliver on the rest like the more practical things as well just because like everybody everybody has a different preference right even even I think uh this was two days ago someone was like Hey can you just like output better code it's like podcast is cool but can you just output better code I was like oh that's a good idea I mean it was it was on the road map and I was like yeah for sure we should just deepen the investment into like the outputs themselves well I'm going to ask you a sensitive question okay not sensitive in terms of like emotionally sensitive see that too if you want we're both here to help actually possibly sensitive question um you guys it seems like you have executed on this much the way you might see a startup execute on something like pretty Scrappy lean team move fast a lot of user feedback iterate in real time you know release something imperfect to the world and like you know kind of test it you know test it in uh you know in production um which seems a bit different than what people might stere stereotypically expect of something coming out of Google and so I guess the question is in what ways has being part of Google helped with notebook LM and and what ways have you maybe broken the mold a bit uh with this project I that's such a good question um I think I'll start from the perspective of what's been great and what's been really special at Google which is the two top things that I'll say access to the models before you know they're fully ready and just being able to see the capabilities that are being planned helps me to think about the the way we build the product in a different way which is okay knowing that these capabilities are coming how can I make this particular Journey better and that's been really that's been really good uh I'd say the second thing that's been really special is just really the people this just really smart really talented and really collaborative people that also just want to build cool things and so having the combination of these two things is really for me as a product Builder is like wow right like this is it this is all I need and I can just like all I have to do is execute I just have to deliver and if I keep going like we'll ship something interesting I think on maybe the stuff that like doesn't quite fit the mold or maybe things that we've done a little bit differently I'd say that um coming into Labs I knew the most important thing for us to do would be to ship and it's easier not to ship than it is to actually do it right like especially you know from my experience with Google I think there are many times that I second guessed myself I was like oh how would it affect this or that like there's so many considerations but I think once you change your orientation to no the p is to ship and you have to do it at all costs and now I'm about to say it on the podcast and I hope our Engineers aren't listening also create a lot of fake deadlines right and it's it's really funny it's really funny because it works so I'll be like guys October 10th we have to do it it has to ship and everyone's like October 10th that's in two weeks it's like yeah what do we do and they're like all right we got to like do it now it's like oh yeah I know and so we just like really crank on it and it's you know I I'm I'm making light of it but for the most part people don't really ask like what what's happening on October 10th and so it works it works for us it's it's worked for two years so hopefully they really don't listen to keep going that's good but I do also think right now I do actually think there is a conception that Google's slow you know in my seven years of Google I've actually been surprised how quickly things move but you just also have teams that are really big that affect billions of users every day I think we're in a sweet spot now where you have all the values of an incumbent like a big company like the scale and you know the data but I do think also now because we're a small team of you know about 10 people we also can move quickly yeah we can't wait to see what you guys continue to build with the product and Hope hopefully don't put us out of a job too soon but it really is delightful what you've built so far congratulations thank you thank you for having us on this has been really fun thank you [Music] [Music]

========================================

--- Video 48 ---
Video ID: lVms2DBQOKU
URL: https://www.youtube.com/watch?v=lVms2DBQOKU
Title: UiPath ft. Daniel Dines - From Bootstrapping in Bucharest to One of Software’s Biggest IPOs
Published: 2024-10-10 09:00:49 UTC
Description:
The biggest enterprise software company to come out of Europe in the last decade didn’t come from London, Paris, Berlin or Stockholm—but Bucharest, Romania. 

UiPath, founded in 2005 and originally called DeskOver, was a scrappy handful of engineers bootstrapping out of an apartment in Bucharest for about a decade, seeking in vain for product-market fit. When they stumbled upon an opportunity in the nascent enterprise software category of Robotic Process Automation, the company did a hard pivot, changed their name to UiPath, and rocketed from obscurity to the fastest-growing SaaS company ever at the time. After a successful IPO, today they are the global category leader in RPA. The unlikely rise of UiPath is an inspiration and a reminder that you can build something great from anywhere. 

Host: Roelof Botha, Sequoia Capital
Featuring: Daniel Dines, Andra Ciorici-Chelmus, Brandon Deer, Luciana Lixandru 

Learn more here: https://www.cruciblemoments.com/episodes/uipath

00:00 - Introduction 
02:11 - The Origins of UiPath: From Coding Nights to Global Innovation
07:57 - DeskOver’s Entry into RPA
11:34 - UiPath’s Direct Approach
14:56 - UiPath’s Journey to Becoming a Top Tech Startup
18:21 - Daniel’s Vision to Outpace the Competition
20:23 - The Genghis Khan Strategy
25:18 - Navigating Explosive Growth and Financial Reality
30:03 - How ChatGPT Redefined Automation
31:11 - Adapting to the AI Revolution
31:40 - UiPath’s Evolution with Generative AI and Automation
36:21 - Daniel’s Return as CEO
38:32 - UiPath’s Journey from Eastern Europe to RPA Leadership
40:03 - The Power of Trust and Collaboration

Transcript Language: English (auto-generated)
[Music] I was joking with my team saying our strategy is like ji scan strategy you should go faster than them you know how they conquer China their army was faster than Chinese Army so they were defeating all the Chinese cities and they couldn't catch them so that was kind of my strategy let's go let's spread let's see where it works and let's double down on it and they thought I'm crazy wel to Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world's most remarkable companies I'm your host and the managing partner of seoa capital rof bua all businesses rely on a number of repetitive tasks data entry payroll processing Inventory management and they take up time requiring someone to manually type in the same figures over and over again today's story is about the unlikely rise of uip path a company that pounced on an opportunity to eliminate some of this repetitive work replacing it with software operating in the background of an application this technology is known as robotic process automation or RPA and uip paath is harnessed it to save Untold hours and dollars for companies around the world when we think of the world's Tech hubs Bucharest Romania is not usually at the top of the list but after bootstrapping for near nearly a decade out of an apartment in the Eastern European city Daniel Dyan saw an opening for the nent RPA category and placed a risky bet pivoting his entire company around it from humble beginnings uipath became the fastest growing SAS company ever only to face Crucible moments in navigating Growing Pains today the company faces new challenges as it adapts to the changing AI landscape to further Revolution uiz the way we work this is the story of [Music] uipath my name is Daniel D I am the founder and CEO of uipath growing up in Romania I wanted to become a writer but I soon discovered that I have no Talent actually for writing and in communist Romania that involved anyway a lot of I was under a pressure to make my own living since I was 19 I had to sustain myself and at that point I remember I was probably making like $30 a month just before his 20th birthday Daniel received a gift from his girlfriend that would change the course of his life she had this book called uh introduction to C++ or programming C++ of the inventor of the language itself be St I started to read it and I I read it more like fiction and I got really enered by the language a friend of mine from University told me there is a guy that hires developers he hired both of us but basically without any kind of interview it was just a discussion so I started to work with this guy but initially he he didn't have a computer to give to me because it was a small firm with I think maybe like 10 people but they were all working during the day but he offered me to go there during the night so I had access to their best computer at that time so I changed for the next 6 month my entire life I was working basically from like 8: to 6:00 in the morning I was basically just sleeping working on the computer and I learned so much it was I was really immersed and I I built basically my entire career on that six month over the next few years Daniel continued to H his skills as a developer in 2001 he landed a job at Microsoft in Seattle after learning the ropes at a major tech company Daniel became determined to start a business of his own he moved back to Romania and launched desk over in the early years desk over focused on Outsourcing and building software developer kits but miles from any major Tech Hub the company faced challenges Europe has a very fragmented ecosystem so in in the US of course there there is New York there there is Miami there are other ecosystems but Silicon Valley is so concentrated when it comes to talent you have mentorship and expertise just around the corner and that wasn't the case for Daniel and for his co-founders starting in in Bucharest my name is Luana Alexandre and I'm a partner at sequa I was born and raised in Romania but I have to be honest I had not focused on Romania as as a country that had a lot of potential to create these outliers the idea of raising capital in 2005 in Romania was unheard of I bootstrapped the company by doing some Consulting work so we were all working in a small flat it was a very very small company they were all male uh software developers who were pretty isolated by themselves I was the first female I was a junior developer at the time and uh it wasn't very easy for me to start my name is Andra chich kmos and I'm a VP of product at uipath Daniel helped us a lot in letting us express our opinions although most of the times at least in the beginning were wrong and encouraging us to speak up and say whatever crossed our mind and if it was a good idea it was always built on and included in the development process so we started having a lot of parties we started playing poker nights we starting debating a lot more it it was really really nice like a family debates over dinner I started to do some uh some products the first commercially viable application was this SDK type of software that a developer could have use and read the text on any application we were building a software Library which are the building parts of any software product that was automating your actions and to to give you an example cuz uh I remember when Daniel first explained to me I went home and my friends asked me okay so what will you do there and I said I I didn't understood but it sound interesting so I think this is what most of our um customers thought in the beginning as well so at that time the browsers for example didn't have that remember your password functionality so every time you wanted to join something you needed to remember and type your own username and password our library was able to do this automation for you just like the browser does it today we were doing initially things that we kind of liked as a developers we thought this is a cool thing to build this stuff it was kind of hackery in a way but not not not really a business Vision our primary customer was actually the software developer they were individuals that were some were working in companies some were freelancing and it was quite hard for it to take off as a developer tool dedicated to hardcore developers we had customers that use it but not many we were like the engine of a card but we didn't have all the other parts desk over struggled to find product Market fit but in 2012 Daniel received an email that would alter the company's trajectory we got this request from someone who show a demo of our product and they put their personal address but they didn't mention any business or anything so at that point I was on the verge of ignoring that request but I don't know I had a hunch you know there is this gut feeling sometimes that tells you maybe you should follow that lead the request was from a manager at a business process Outsourcing company in Chennai India who needed to reliably automate some of the company's manual tasks he wanted to see how desk over compared to a larger British tech company called Blue prism that worked on robotic process automation or RPA I never heard about blue prism or RPA at that time but I said I have no idea who they are let me search on the internet and I did a quick search why I was talking to him I didn't read much but I said it seems like this is a big company I don't think we can compare in any shape with them but he was patient and he started to work with us more to understand he really liked our approach so we had this visual low code that was really novelty at that time and in all RPA space it was only us and blue prison that had this low code approach so we had a nice flowcharts to design and Automation and nobody else had that at this point and basically he was actually looking to get some leverage on blue pris and they were really considered kind of inflexible the company in India invited desk over to come to chenai to see if its software Library could automate some of the BP's processes I also said yes without missing a heartbeat I displaced three people some of my best people to go to India for 3 months at cost so this is how we got into into RPA and I got the wind of what RPA is I was a part of the three people team that went to India the experience in India was very eye opening for us we used to go every day into the office in India we were automating part of the process something wasn't working we went back to our hotel we implemented the fix or the new feature that was requ required to automate that part we went with a new version of the product the next day and automated the part that was missing when we returned to to Bucharest it was like a light for us we said okay this is what we can do to make their lives better the companies better to bring them real value I wouldn't call us a company at that point when we went in India we were a small group of people that wanted to bring value to their customers when they finally discovered though we I have customers and that was the moment that we decide to to switch and focus on RPA I realized that actually that's a much bigger Market than what we were going for before I couldn't imagine that there are so many inefficiencies in the world of business Enterprise business processes it has become obvious to me that we have to go all in after that market after 8 years of bootstrapping and looking for product Market fit Daniel made a crucible decision desk over would shift its focus entirely to RPA there was no risk for us at that point we were going to die anyway in uh in our little market there are moments when you you go you feel like you are in a fog and you don't know which direction to go and you go carefully and suddenly the fog clears up and you see where you go and then you go very fast that was us after kind of 8 years of uh going in the fog we went right after the target it was very easy for us to just follow customer requests based on that we knew that blue prism had an indirect sales motion meaning they sold exclusively through the channel they didn't have direct conversations with their customers my name is Brandon deer I'm the chief strategy officer and COO of our goto market organization at uipath and I think this is something that set us apart very early on was that we wanted to go direct we wanted to have customer empathy and centricity we wanted to originate uh product Innovation new feature ideas worldclass support and we wanted to do it all through the voice and the feedback loop of having a onetoone relationship with our customers in 2013 disover released a new RPA Focus product they called it uipath Studio we've built our first orchestration in our product it was a little bit patchy but you have a product that is capable of building one Automation and you need to build another product that can scale deploy and scale hundreds of automation in time we built all the other building blocks that allowed us to automate a business process from one way end to the other and with the input visible both for the people that are actually doing the job the mundane tasks but also for the SE Suite or management Suite to see how their automations are performing the other thing that came up quite often with customers was just more a more robust and more scalable architecture because when you run Many Robots at scale automating many many processes the last thing you want is for these robots to break down because then you still have to have humans go and fix them um and you're trying to automate this work you're not trying to create extra work customers wanted RPA and it was a lot about Daniel and the team yes creating more demand but mostly fulfilling the demand that was already out there one of the key things to learn and to remember while you are a St up is that your Market fit may not be the one that you're thinking of may not be the one that you're actually uh serving today but if you are open and if you allow yourself to look outside to uh interact with all the opportunities that come you may actually discover that your Market fit is other and is the one that is more appropriate for you imagine eight years working in a a garage you know that you build some kind of good technology but nothing moves you don't die but you don't grow either after you get so much beating I think you don't stop it gets you prepared I felt this is the moment that we all were waiting for but our real growth was after we first concluded our seed around after achieving product Market fit the company began attracting attention from from investors in Europe as it was taking off in 2015 Daniel changed the name of the company from desk over to uip path I happened to go to Bucharest for for a weekend and I thought let me take one day um and meet some of the smartest people in the tech ecosystem in bucarest at the time luchiana was working for the Venture Capital firm atel in Bucharest she met uipath's seed investor who insisted on making an introduction to Daniel I remember when I went to visit the company one of the co-founders said something to me that I still remember after seven years now um I said I'm so excited to see a Romanian company with so much potential and and they were almost offended they said we are a Silicon Valley company but we're building in Romania and with Daniel after spending many hours together a few things became obvious he was highly intelligent he had an unusual level of determination which is reflected in the fact that for 10 years he did not give up I think at some point he called himself unemployable looking for product Market fit for a decade uh takes a lot of resilience that's really unique so he had this attitude that this is it or nothing else there's no choice but to win there's no choice but to succeed and he was very confident it it came across in many ways but but one of the ways was when we signed the term sheet I still remember he said with just a lot of conviction this is going to be a very successful company luchana and Excel led a $30 million series a financing the company's pivot to RPA had completely transformed it from obscurity to one of the hottest Tech startups in Europe but Daniel wasn't satisfied so we have finished 2016 with 5 million in ARR and in 2017 we were thinking we will finish the year maybe 30 35 so it was really huge growth and I I could have seen that actually this is becoming a big category and at that point we were still number three in the market but what I realize is it's that's there is a big pull from the market and this is going to accelerate into next year and actually the number one company will take the Lion Share and if if we can become prominently clearly the number one RPA company it will be a no-brainer decision for many customers so and we will get the biggest market share some Crucible moments land on your doorstep others hide in plain sight demanding you identify them and act while many would have been happy with the company's trajectory Daniel sensed the fleeting opportunity to change gears and Leap Frog the competition to become the clear winner in a big category we knew pretty well our competitors and we knew where they were as as Revenue I think the biggest one was probably maybe 70 80 at that point and I knew that they are probably going to 100 next year based on their growth and in the board meeting at that time when we were discussing the budget for 2018 I remember Lana alandre telling me Danielle I will be very happy if next year you'll deliver like $70 million of ARR and I told them we need to be twice as big Revenue eyes as our competitors so I want to build the company to deliver $200 million in 2018 and and they thought I'm crazy clearly the thing about large numbers is that the bigger you get the harder it is to maintain your growth rate and if you do the organizational challenges are immense going from 5 million to 35 million in Revenue in a year is one thing but to go from 30 million to 200 is something else entirely I remember this board meeting where Daniel put out a plan to go from 35 million to 200 at the time a a very very aggressive growth plan and I remember saying Daniel even if we double from 35 to 70 that will be a great year I remember it because Daniel reminds me all the time to get there Daniel borrowed a strategy from the history books literally at that point I was joking with my team saying our strategy is like uh JIS Kong strategy we both read the same book which was the biography of genis Kong and there was this Obsession around the speed you should go faster than them you know how they conquered China their army was faster than Chinese Army so they were defeating all the Chinese cities and they couldn't catch them so that that was kind of my strategy let's go let's spread let's see where it works and let's double down on it he talked about about the gisan strategy all the time once uh an idea comes to mind usually while he reads something he repeats it and thinks about it and repeats it and repeats it until I think it makes sense to everyone the genas con strategy would mean expanding into multiple International markets simultaneously including the US and Japan and rapidly scaling up The uip Path team to serve them going to multiple markets simultaneously was uh I think a product of our exuberance I would say that at the time there was no Playbook yet I think that uipath is one of the first companies that built out this Playbook uipath also carefully examine the practices of competitors in order to differentiate themselves our competitors they were at the Forefront of all of our conversations what are the things that they were coming out with what are the things that they're talking about in their press releases and their document ation when we met with respective customers that were already using their Technologies we'd ask them what is it about them that you love how do you derive value what have they shared with you in terms of future things that you're excited about how have you compared and contrast our capabilities versus theirs I think early on we saw that there was kind of a naive neglect from the blue prism shop around customer support and white glove service and so we completely rotated towards providing the best support anything the customer wanted we did you know it was like an anx Black Card concierge type service you called uipath as a customer or prospective account we gave you our all and it didn't matter who you talked to you could have called me direct you could have called Daniel direct I think RP was great to uipath but uipath was also great to RPA the company invested a lot in category building in customer education in just giving customers a delightful experience which which is what promotes more and more of this product to be sold uip path met Daniel's audacious goal growing from $5 million in Revenue to $35 million to nearly $200 million year over-year this astronomical rise made uipath the fastest growing SAS company in history at the time uipath moved its headquarters to New York and opened 30 offices in 16 countries the stories are very very rare at the time in my experience it was almost unheard of at this point the UI path story was pretty crazy it was probably the quote unquote hottest startup in Europe and and one of the hottest startups in the US they had access to really great talent it was overall incredibly exciting but I will say it also made us a little bit nervous because the revenue growth was spectacular the employee growth was also spectacular it seemed like every day there was 10 new people in the office and it wasn't that big an office it was probably equipped to hold I don't know 150 or so people I remember uh going to the men's room and turning the corner and there was a line looping all the way across the desk where the engineers sat and the engineers are sitting there you know trying to have their quiet time you know conduct their coding and trying to work in some peace and quiet and you've got a line of gu you know kind of doing an S wrap around their desks and it was just this moment of like one you could see they were pissed off and super irritated but it was also like kind of funny you know to see and and all of these people had just started you know we were busting at the seams we moved to a bigger office and you see suddenly 100 people in a room and you have to talk to 100 people it was kind of insane the feeling for me and you have all sorts of faces and they look with trust in you and things were growing and yeah it was crazy good times it's the best feeling in the world really the gasan strategy had paid off uip paath had cornered the RPA Market but in 2019 it became obvious that this extraordinary growth came at a price and I think that there's always dichotomies of trade-off between speed and quality and process we're seeing two different things on the one hand we're seeing a lot of customer demand um and we're seeing a very fast growing Market where uipath is really solidifying his position as the leader in fact is growing much faster than competitors and the reviews of their product are really positive compared to other players in the market so on the one hand that's going really well on the other hand we're starting to feel internal organizational pains from having added so many people in such a short period of time and and the companies obviously burning quite a lot of money at this point as well given the meaningful increase in the organization from 100 150 people to about 3,000 people in only a couple of years because when you add 3,000 people in two years of course it's impossible to do that in the most structured process driven disciplined Manner and that became clear to the board and it became clear to Daniel as well it's a very risky game you got into we were in a bad position we were we were really getting into our cash in 2019 UI PA's projected cash burn was $150 million but the company ended up at a number almost triple that amount the company consumed $400 million Corner a market isn't for the faint of heart it sounds like hey let's go let's go build an incredible company and make a ton of money and that sounds great and in actuality to do it you need to make a lot of mistakes painful mistakes along the way you need to really feel some pain it's hard to realize the cost of all of these things in any given moment as you're making those decisions you end up with a lot of people with poorly defined roles that you made a lot of big offers to you end up with a lot of big customers that you made a lot of big Promises to the explosive growth finally brought a reckoning how could the company re in costs to grow in a sustainable way I think we really were fighting for our lives it was a matter of life and death at that moment and that's part of the reason that we were forced to cut so deeply and so quickly we have to do a hard course correct we had to fire 10% of our people I think there is no tougher feeling than to tell someone you know that we have to part ways because it's a disappointment it's a disappointment in yourself first of all you feel their pain you feel their disappointment it I it never gets easy it's it's hard it's the hardest part I think of of business it's a pain that makes you to act in a way that you'll never feel it again uath had to make some very hard decisions while the company was still growing very fast so on the one hand you can be out there celebrating incredible growth like very few companies had from 5 million to 35 to 170 to 370 this is really spectacular but at the same time you have to be very very honest with yourself about how the health of your organization the culture the productivity of your organization fixing problems doing more with less I think as a leader you also have to be really really honest with yourself about all these things and Daniel did that the measures taken to rightsize the business in 2019 meant that uipath headed into 2020 a leaner and more numble company this paid off during the pandemic allowing them to weather the crisis and its economic instability better than competitors in 2020 uipath's Revenue broke $600 million and then in April 2021 uipath had one of the largest software IPOs in history it was a breathtaking rise from a company that just a few years earlier was only a handful of Engineers operating out of a small apartment in Bucharest the IPO moment itself still feels like a dream I think the moment I personally realized we we actually ipoed was the night of the party when I was looking at an ie sculpture with uipath and New York Stock Exchange and I said why are these words together it makes no sense oh we just ipoed today okay now it does utilizing so-called computer vision technology uip paath had always relied on a form of AI for its process automation but in late 2022 open AI release of chat GPT upended the AI landscape I don't know that anyone was fully prepared for the power of generative artificial intelligence I think it came fast and furious and I remember the first time I heard about GPT was probably two and a half three years ago and there were a bunch of really um kind of complex white Papers written on that if you were part of the AI Community you know you were listening to researchers to begin to talk about it but its application at least within the Enterprise was not widely accepted uh I I don't think a lot of people saw it as a real threat to what they were doing today everyone in Enterprise software has been building you know their Tech and training their data and acquiring customers for so long how could something come in and disrupt that in an automated fashion overnight and have seemingly unlimited Intelligence on everything for uip the rise of generative AI posed an existential question would the capabilities of alms impact the relevance of RPA how should uip path adapt we started with computer vision so to understand screens we extended that computer vision technology to understand documents semi-structured document and it was kind of natural to incorporate J it's kind of clear to me that if we don't go to combine gen with automation we will become extinct we will become irrelevant to the Enterprises I think the main challenge with Gen for businesses right now is from the the reliability of gen it's so smart but it's like hiring an unstable Geniuses and most companies would like stable decent level of intelligence and I think this is where most of the challenges are but having the automation creating better grounding for llms I think will go a long distance when a new technology like this evolves I think if you're not at the Forefront of it you're like in a place to be displaced and so we're investing everything that we have to ensure that our products are not just keeping up with the market but actually pushing the market as it relates to all the different capabilities around automation I think it's very important for uipath to have the AI capabilities that fix the customers problems but also to be able to explain to them this is not an AI problem you can automate this with this tool or this tool or any other process and it will be maybe more cheap and more reliable for you I think they complement each other I don't think it can ever be outdated the RPA category can't be eliminated alog together our platform having our low code approach is very suitable to use the to build an llm up and when you pair it with automation the result is exponential IAL uip paath has released a number of products combining Genai and RPA like AI Center which lets users easily add ml models to existing uip path automations there's also a clipboard AI a tool specifically designed for copying and pasting large amounts of data and uipath autopilot a tool anyone can use to supercharge productivity a uipath autopilot is one of the best examples of AI working with RPA and helping our our platform deliver the the AI benefits to our customers we have autopilot for developers autopilot for testers autopilot for reporting and what this actually does is it brings the value and the power of our platform to people that uh don't know how to build the processes end to end and they don't have the Deep technical skills it's more like you're talking to a friend in layman terms and autopilot can generate for you the test cases the workflows the automations they reporting and this will be really helpful both for the beginners and for non-technical people but also for advanced developers that want to validate their work our main proposition is automation will make the models more reliable and how AI can help us I think we've become much crisper over the last 18 months I see actually a new dawn in a way for automation now many people realize that AI without automation doesn't really get to the full benefit of AI and to quote one of uh our longtime customer he said to me recently Daniel we talk a lot about AI but most of time we end up talking about automation to prioritize the company's integration of RPA with llms and ensure the company remains relevant in the AI era in January 2024 Daniel decided to step away from his position as CEO to take on the role of Chief Innovation officer it was very helpful for me considering all the evolution of AI to be there to put my mind completely there Daniel had some time as the chief Innovation officer of the company word his primary domain his primary uh effort was spent on all all things product and engineering and I think that was a really incredible opportunity for him and for the company for him to get extremely deep into the weeds in a domain that he's super passionate about I think the things that Daniel's been working on for the last year around semantic automation around agentic AI process orchestration these are the things that are probably going to shape the future of the company but do Daniel focused on product Innovation it was a rocky period for the company's leadership team it was on Old Guard versus new guard type of you know dynamic in the company which was kind of not very healthy I got a little bit disconnected also from customers from Partners doing all this and the company has become more and more siloed not that we are not talking but I think somehow organizations mimic the leader to mend divisions and Propel the company in a New Direction in May 2024 Daniel stepped back into the role of CEO the rapid change in leadership rattled the public markets and youi stock saw a steep drop in share price the last two months were probably some of the toughest maybe the toughest it was it was a different order of magnitude from 2009 talking to investors after our earnings call it was really some of the toughest moments it's the disappointment of people that put the trust in you it's to me it's extremely hard to to get over it at a human level it's terrible but at an intellectual level I think we are doing the right thing I think that my returning as the CEO revigor Ates a bit for us as a company maybe it's the it's the best move we can move faster I can connect faster I can break the silos to meet the no new people or old people it's just one we're putting the company back on track and of course it's going to take it's going to take you know a few quarters until we'll see the results but we we're are seeing the early signs in the company I think we're all super bullish on the future of the company we made a number of refinements to how we build where we spend time how we ship and I think our product strategy has really honed in on something quite special for the future today uipath has 10,800 customers in over 100 countries it continues to be the leader in RPA I think uipath today in many respects is special because it has the same values that Daniel had set forth all those years ago it has the same Mission people no longer need to be stuck in those keystrokes in those Mouse clicks moving data between systems and different end applications and that allows them to focus on higher level more strategic thinking we always joked in the earlier days that uipath was a tenure in the making overnight success it is the largest enterprise software company coming out of Europe in the last decade or more and one may expect this company to start in London or Paris or Munich and that wasn't the case the company started in a in a pretty small country in Eastern Europe that doesn't have a developed technology ecosystem and from there Daniel and the team found their way to global domination in RPA and to winning not only in Europe but winning in the US as well and I think that's really inspirational and and to this day Founders come and and want to meet me and want to talk to me because they they think that that's a really great story and then and because Daniel is such a role model and because they showed that you know with great people with the right ambition you can build something great from anywhere I think no Crucible moment will look the same to me I think it's essential to be surrounded by by people you trust board members your executive team and collectively go through these moments it's so important to to come together and talk and really in all good will find Solutions if even if it's sometimes it's the wrong direction but by moving you learn and then if you have the will to course correct you will find eventually the right [Music] way this has been Crucible moments a podcast from sequa capital [Music] Crucible moments is produced by the Epic stories and Vox creative podcast teams along with sequa [Music] capital special thanks to Daniel dines luchana lexandre Brandon deer and Andre Trish kush for sharing their stories [Music]

========================================

--- Video 49 ---
Video ID: AnAKE8qBKYU
URL: https://www.youtube.com/watch?v=AnAKE8qBKYU
Title: Snowflake CEO Sridhar Ramaswamy on Using Data to Create Simple, Reliable AI for Businesses
Published: 2024-10-08 09:00:08 UTC
Description:
All of us as consumers have felt the magic of ChatGPT—but also the occasional errors and hallucinations that make off-the-shelf language models problematic for business use cases with no tolerance for errors. Case in point: A model deployed to help create a summary for this episode stated that Sridhar Ramaswamy previously led PyTorch at Meta. He did not. He spent years running Google’s ads business and now serves as CEO of Snowflake, which he describes as the data cloud for the AI era.

Ramaswamy discusses how smart systems design helped Snowflake create reliable "talk to your data" applications with over 90% accuracy, compared to around 45% for out-of-the-box solutions using off the shelf LLMs. He describes Snowflake's commitment to making reliable AI simple for their customers, turning complex software engineering projects into straightforward tasks. 

Finally, he stresses that even as frontier models progress, there is significant value to be unlocked from current models by applying them more effectively across various domains.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 - Introduction 
02:17 - Primer on Snowflake
04:39 - What's happening in the world of enterprise AI?
12:17 - How customers are using Snowflake vs other AI services
22:21 - How companies can maximize their product velocity 
26:42 - Snowflake's accomplishments in AI 
31:25 - The future of AI 
55:38 - Closing questions

Transcript Language: English (auto-generated)
the the product that makes even the people that go I have gbd4 I have an army of software Engineers um the thing that even they struggle with is things like a reliable talk to your data application because even with gbd4 out of the box you end up getting 45 odd per reliability meaning it gets half the questions wrong when it tries to answer it um we are well in the '90s and we are racing to get to like 99% reliability on talk to your data applications obviously we restrict the domain and turn this into more of a software engineering problem than just like a pure AI model problem um but that's the thing that makes every snowflake customer perk up and go like I want that because even the people with the money and the resources to spend on software engineering teams um very quickly realize that this is a wall that they are likely not going to break through [Music] today we're excited to welcome Shar ramaswami CEO of snowflake snowflake is one of the most important Enterprise companies in the public markets it's the default cloud data platform but today the question of what role does snowflake have to play in the world of AI looms large Shar is somebody we've known for a couple decades he actually started on the very same day as our partner Bill Korn at Google back in April of 2003 we backed Shar in his own startup Neva which was an AI driven search engine snowflake acquired Neva which is how Shar became the successor to Frank slutman rarely have we encountered somebody who is as in the Weeds on the technology but also as commercially Savvy as Shar and he will join us today to to talk about what AI means for snowflake the importance of safety nets the open source Community the competitive landscape and the Practical applications of AI that he's seeing in the Enterprise through his lens as CEO of snowflake we hope you enjoy all right Shar we're excited to have you here with us today you're a technologist by trade youve spent a lot of time in the consumer world and you are now at the helm of one of the most important Enterprise companies of Our Generation so before we jump in we have a lot that we want to know about Enterprise AI what snowflake is up to some of your predictions on the world of AI before we jump in though just a level set can you give us a couple words on your personal background and then just for people who aren't familiar which is probably not a lot but just for fun for people who aren't familiar what's snowflake so who Street are what's snowflake let's start there that's great uh bad Sonia super excited to be here at iconic seoa home to many many uh Legends I admire um yeah I'm a computer Scientist by training uh early career as an academic I joke to people that I'm a reformed academic because I was like I wanted to do things with more impact um super lucky to be a an early part of Google um where I joined one of the greatest businesses ever invented by uh Humanity which is the search ads business uh I ran that for close to a decade all of ads in Commerce at Google um for uh 5 years helped grow that business from a billion and a half to over $120 billion in uh in Revenue um and then funded by seoa did a uh an ambitious startup called Neva which wanted to modestly rethink what search meant um before getting acquired by Snowflake and becoming its uh CEO uh and snowflake is the AI data clock our core thesis is that a cloud computing platform that puts data at its Center um is going to be way better for Enterprise customers to act on data um than a generic Cloud um and AI of course we think of as a transformational technology that is going to change every aspect of how data is stored how it gets moved around and of course how it's accessed um we have over 10,000 customers uh made 2.6 Bill last year but at the center of everything Enterprise and data that's a super a quick blurb perfect thank you and and so you have 10,000 or so customers I know you've met at least hundred probably hundreds of them since you took over hundreds of them by now so there you go so I'm guessing you have a pretty decent read on what's going on in the world of Enterprise AI so maybe we'll just start there what's going on in the world of Enterprise AI what are you seeing at your customers first of all people get that this is going to be transformational you know lots of Technologies um have Skeptics I'm sure you have run it into folks who all like ah mobile it's not going to be it thing this browser like so l in um it takes a while for people to absorb I think what's different about AI first and foremost is people are like I get what this can do I think some of the power um is just like honestly looking at the magic that Chad GPD is anyone that like has interacted with it asked you to write a poem asked you to create an image knows like wow this is something that's very special so the level of awareness is incredibly High um and uh and we have thousands of customers that are in various stages of implementing AI solutions they span the gamut um from people like Bayer that are very excited by the idea of giving business users access to business data without going through like an elaborate you need an analyst you need a bi tool you need blah blah blah you need a week before a change can be made they're like I just want to put data into the hands of people that need it right now um but but we also have dozens of people that are using AI as a transformation engine so for example if you have unstructured data whether it's an image or let's say like a transcript previously you had to run a software engineering project to figure out what's this image about now you feed it into a model ask it a question and you get the answer and so people are super excited by things like that we have a product called document AI which extracts structured information from document say like contracts um all of us have contracts sitting around in our company folders that have all kinds of magic numbers um that ideally you want to do analysis on so there's a wide variety of cases that people um are implementing and sending into production but I would say stuff at the bottom which is how do you transform data more effectively more flexibly and stuff at the top which is how do you make data easily available to all kinds of business users in new ways in interactive ways um I would say that's the sandwich in terms of what are people wanting to do with data mhm and can you say a couple words on snowflakes right to win so some of the things you mentioned like data transformation for example feels like that is very close to the Core Business of snowflake but then there are some things that are maybe a bit further a field you know if somebody wants to deploy an Enterprise agent of some sort they can use snowflake to do it but what's snowflakes right to win in that situation so can you just say a couple words about how snowflake fits into this overall landscape and sort of the right to win um so first and foremost um the basic approach that we took to AI um sort of enabling or infusing AI into snowflake is um it should be an axelin for everything that you do with snowflake that's what cortex AI is it's a model Garden but it's more than that uh snowflake Prides itself on super tight integration of its various product features um and this is not another service that's part of snowflake it's built into snowflake this means that um any analyst that has access to SQL has access to Ai and so it's a massive democratizing mechanism um and then the early applications that we have built like document AI are a very natural next in the progression of what people want to do um which is hey I want to act on the data that is within snowflakes perview by both expanding the data that snowflake has access to Via things like Iceberg which is basically an interoperable uh storage format for cloud storage um but then providing things like document AI we just make a whole bunch of AI applications that previously used to be software engineering projects into two commands that an analyst can issue and so our first lens very much is that um AI should become easy um trivial for data that is sitting in Snowflake 100% there are going to be applications that are you know Cutting Edge or going to involve many many different Services um but the angle that we bring to all of those customers is we make reliable Ai and this is a topic that we can get into so for example I tell people you have no business believing the raw output of a language model for anything you can't actually do any business with that because it's ungrounded it doesn't understand truth from falsehood doesn't understand Authority um so we make things like you know creating a grounded chatbot again as I said two commands not a software engineering project um similarly with cortex analyst which is our talk to talk to your data API we bring the full power off we know everything about the schema all the queries that have been run on the schema the semantic context on the schema we can produce a reliable um you know application um that others are going to struggle to create so we are leveraging our strengths in data um to make AI products better are they're going to be like specialist app ations that can only be done with GPD photo and a custom integration with a bunch of other stuff absolutely um but that's not what we are after the bulk of our customers want to get work done they're not in the business of doing research with AI and are you seeing customers bring net new data that maybe didn't sit inside snowflake historically into snowflake because of your AI services and and how do you think about your right to win as it comes to the data that's not in Snowflake yet uh this is a broader question um I think one of the things that uh I've actually been a good part of is in expanding the lens of data that snowflake should play in Snowflake um as you know is uh first of all it's close Source software for the most part the code engine is close Source just like search um but we also had a proprietary storage format um where data was ingested into snowflake um and uh kept in this format but um what we consistently heard from customers and I'm sure like you hear all the time is there is 100 or a thousand times more data sitting in cloud storage than there is inside a specialized player like snowflake um and more and more industry Trends have been towards interoperable data people want their uh want their data to be accessible from multiple places so for example if they want to write their own bspoke applications most people don't want to do that but the biggest ones do um they want the data to sit in cloud storage where yes snowflake perhaps can write it and read it um but other applications should also be able to read it so we made a big push around Iceberg which is the interoperable format we also announced a cloud catalog recently the idea is that in 10 years um data is going to be sitting um mostly in the cloud mostly in cloud storage which is very cheap um mostly in interoperable formats accessible via open catalogs and this is the place where we see there being so much more access to data from Snowflake so everything from data engineer ing an AI um now comes into our purview um we have customers that for example are doing things like oh let's run a video model using snowflakes container services on data that is sitting in S3 extract transcripts and stick it into um into snowflake so it's just a very different world we are playing in makes sense and then so the let's say for the data that's currently sitting in uh one of the hyperscalers for example um you started the conversation by saying you know the the core uh tenant of the company is that when you build your infrastructure kind of all around the processing of data you can do better things what are some of the ways that you're able to kind of offer better AI services around the data that doesn't currently sit in snowlake but that you're hoping customers will bring in versus you know what the hyperscalers are doing already yeah and can I add on to that real quick because one of the things that we have heard from customers is at either end of the spectrum you've got at one end of the spectrum work directly with open AI send your data you know into their into their Cloud um and maybe have some nervousness around whether that is going to leak into the model or whether they have the right security and privacy sort of governance around it um at the other end of the spectrum you can just do everything yourself grab a model off a hugging face build it internally you know super safe super secure but pretty painful to do all that and then the middle ground you've got Amazon Bedrock or you've got a snowflake and they both kind of have a value prop of Best of Both Worlds we're going to make it easy for you but it's also safe and trusted and secure and all that good stuff and so I think um my angle on Sonia's question is like for somebody who's making a practical decision about sort of what should I build in Snowflake versus what should I build on Bedrock or a comparable cloud service what leans people in the direction of snowflake um it's the fact that uh everything that uh you want whether it is data security data governance ease of use um all come out of the box the incredible power that comes with core snowflakes plat form including things like collaboration other third party applications we make AI simple 100% there are those people that will say I want to take data um that's sitting in cloud storage or even in another application I want to bring it into cloud storage I want to recreate uh accles you know Access Control list um and then I want to create a vector index using a bspoke um you know Vector indexing solution um and then I will stitch together uh I I'll figure out which model that I want to use whether it's an API or something that I host myself um and then I will use Lang chain and write like custom routing Logic for my application um I can assure you that you know 99.9% of our customers want no part of this you know that's just the reality um all those poor people wanted was a chatbot to run on 100,000 docks that that they have so that they can replace the annoying search box for FAQs on their site with you know here's a solution that just works um and our take is uh uh yes whatever governance you've had before works out of the box um and your data does not go anywhere else you have the same Rock Solid guarantee that snowflake will never use your data to train any uh like Cross customer model um and uh and we will be very efficient um and cost effective from just like overall cost of running the solution but snowflakes magic honestly is we make the hard simple and it's things like total cost of ownership um many of our customers uh you know are our banks um they are Healthcare institutions um they are finan you know are other kind like we play a lot um in the media space as well um most of our customers want to solve problems not solve technology for the sake of uh for the sake of Technology you know we have a foundation model team they're very focused on things like how do we you know get models that a better grounded generation how do we get them to follow directions well how do we get them to say no to questions that they should not be answering when it comes to let's say like talk to your data you know so we focus on Specialized areas like that uh but the biggest reason to use uh snowflake for a lot of our customers is 10% software engineering project with a whole lot of risk about data and security and what else can happen um turns into six hours of work for and analyst um we are good at that we proud of that so it sounds like the oneliner might be it's kind of the the level or the layer at which you're intersecting these products if you're working with one of the public clouds you're still very much at the infrastructure layer building a lot yourself snowflake you're at the platform layer a lot of the hard work's been done for you and our long-term bet Pat and Sonia is that ecosystems move Upstream there was a time not so long ago um where I don't know our parents our grandparents knew every part of a car yeah they're like oh so manly to change a carburetor and get oil in between your nails I got to be honest with you I'm still impressed every time my dad knows exactly what is wrong with the car yes you know while I'm willing to go to you know go to strength training every day getting oil in between my fingers uh you know with my car does not sound so attractive anymore um and so one you can work with csps and you can be like I have a model Garden here I have a caching service there I have a database here I will Stitch all of this together um as I said everything turns into a software engineering project um for us you're like no that's just a little data pipeline that you set up and here is a beautiful UI that you get if you want a chatbot uh obviously you can do more but you don't have to yeah whether your customers building on Snowflake and are there certain types of AI applications that are better suited to be built on snowflake than others um as I said the the categories of AI applications um come naturally from the kind of data that you know that are that are already there um I would say the broadest broadest use case um is really using cortex AI via SQL um in either interactive queries and dashboards or in jobs that people are running and so these span the gamut from oh let's do sentiment detection with a small model it doesn't really have to even be that expensive uh so that's just like literally it's one function call um or uh let's do other kinds of data extraction um where as I said you have things like a transcript or maybe clinician notes um you you take that out uh and uh um you know you you get structured data from it um or the other thing that I talked about document AI which is you extract structured data from things like receipts from contracts uh so on and so forth that's kind of our Sweet Spot um but I have to say the the product that makes even the people that go I have gbd4 I have an army of software Engineers um the thing that even they struggle with is things like a reliable talk to your data application because even with gbd4 out of the box you end up getting 45 odd per reliability meaning it gets half the questions wrong when it tries to answer it um we are well in the '90s and we are racing to get like 99% reliability on talk to your data applications obviously we restrict the domain and turn this into more of a software engineering problem than just like a pure AI model problem um but that's the thing that makes every snowflake customer perk up and go like I want that because even the people with the money and the resources to spend on software engineering teams um very quickly realize that this is a wall that they are likely not going to break through and how do you accomplish that like maybe peel back for us how you're able to get to the 90s per uh are you training your own models are you just tell us about you know how how how this all becomes possible it's it's it's systems design okay um just like the magic of how you make uh a coding agent or an effect less a coding agent more an effective co-pilot work in practice it's not always the like the giant models it is carefully breaking problems down uh so that you present the right context to the model it's in deciding things like oh I see the problem of answering a question whether to answer a question is different from how to answer the question so you can specialize and have different models for these different subtasks and also what's the um basically the I I I call this a like a problem definition a product structure question we structure the product of Cortex analyst um so that it is more restrictive than a free flow domain um what I mean by that is um schemas are weird things people do random stuff they have horrible column names um that mean completely the opposite every company has its own definition for revenue and if you like take the best model on the planet and Let it Loose on an arbitrary schema the likelihood that it's actually going to understand the Nuance of what's in there um close to zero like you know our big deployments for example our customers have 200,000 tables and you can bet that there are several tens of thousands of tables with the word Revenue in it um they just don't have the same meaning so it's really like problem definition to me by the way this goes back to the magic of product um I think of like any amazing founder any amazing um product manager as someone that can visualize what's like the right trade-off to be making in order to create something that has broad applicability and that's the thing that we have done here we constrain the problem but as I said also explicitly train for things like when to refuse questions um as opposed to trying to pretend that you can answer every question um but obviously there's a Precision recall trade-off there you can get 100% Precision by answering no question that's not the goal you want to be useful but still be precise but it's a lot of software insuring um I want to go in a slightly different direction sure okay which that that reminded me of this and I don't know why but but you guys you seem the product velocity at snowflake seems to have inflected to the positive yeah even in the last 6 months or so and we've worked with a lot of Founders where you know the bigger the company gets the slower and slower the velocity becomes and so I guess I'm curious what have you guys done to positively inflect product velocity because that's hard to do when you're dealing with an organization at the scale of snowflake uh I've done this many times before um and the formula uh is always roughly the same which is first and foremost you make sure that you have a safety net that you believe in um which is you have like regression test so you don't blow up big functionality um but if you're pushing hard enough um you will make mistakes um and so you have to distinguish between different kinds of mistakes um for a database company um there are catastrophic mistakes like if you write data badly it's going to take you months to get out of that so you know so you need to understand and like what is risk um and then you build the safety net for things like uh as I said uh to detect problems before they happen but in case you do have problems how you get out quickly um at Google for example um we built Auto experiment scaling Frameworks um basically you would you know come up with a new experiment all new change all changes went through this experiment framework um and this thing would automatically say I'm going to run this on a machine watch it for 15 minutes make sure that it the machine doesn't crash and then it roll it out to .1% 1% 10% with measurement all along the way all of a sudden you have velocity because someone can design people can design a whole bunch of experiments they're sort of now pushed out uh so as I said the first part is the safety Network and so we spend a lot of time on on that the second part is the inner loop productivity which is how quickly can you get like a single change um in quickly because ultimately it ends up being the decider for how many changes are you going to are you are you going to get through and then there system design snowflake actually went through a process that predates me starting about two years ago of how to make the system extensible as I said at snowflake we are very proud of the single unified product um but that can become like you know something that gets in the way of speed and so you have to design carefully for how do you make things extensible so things like AI um basically took advantage of that uh of off that framework um and then to a certain extent to be honest with you it is also the focus that leadership needs to bring on what is important how do you drive Clarity um at all times with all teams there is an Infinity of work to be done yeah um and driving that Clarity driving a sense of accountability with AI team for example I force every team uh to make promises for uh yes or three months but also what are you going to do the next two weeks um and calibrate yourself on did you on the things that you were doing um that you said you were going to be doing um it's pretty much in my mind if you want to get better and better life boils down to say what you're going to do and do what you said you you know you would do yeah um and and examine and make things make things better um and so it's a bunch of things that I've been there um that I've been building up at snowflake but certainly I bring this sense of quality and speed are both requirements in in what we do um it's a uh it's a change um but people like the idea um of uh just getting more things done yeah you know like you and I have never met a software engineer that says like yep I want to release that day after tomorrow it's like no you want to get it done today and so that itself builds momentum when you release a bunch of products and you have a lot of customers that are using it that becomes positive energy for the team to build on the good behavior um that kind of got you there and so I would say the team has responded very very well and I told them Hey listen this is the world of AI stuff changes every week um and you need to build with uh with that speed um I I'm I'm very happy with how the team has responded is there anything in particular that you're most proud of in terms of what you guys have done in AI thus far as a cortex analyst is probably the hardest product that we have designed and launched um things like uh uh cortex AI which is like our platform a layer I'm I'm proud of it um but you know it is it is sort of predictable infrastructure work even though there's a lot underneath in terms of hey should you use V VM or something else how do you optimize F inference how do you get capacity in like this annoyingly crazy world where it's very hard to get your hands on gpus there's a bunch of stuff um but to me that is a unique that things like that things like document AI are a unique combination of our strengths being applied to new areas in ways that can make a big difference to our customers yeah um and you know but you also know Pat that this is a little bit of like you know who's your favorite child so I can't really do that um and so there's a there's a bunch of stuff like even if you take Polaris which is our Cloud catalog you know done in a matter of three months and so um I think there's a lot of energy within within the team because um you know it's a slow message but it's getting through that you can have speed and quality they're just different aspects of the same problem um and my firm belief All Through My Life um is that virtuosity Trump's strategy all day long what does that mean um your speed of execution your speed of reacting to situations um is going to Trump strategy very very quickly yes you need strategy um but life is never about fixed strategy because we live in a very very Dynamic World um it's hard to predict which product is going to be wildly successful what your competitor is going to do like we're going to talk about like gbd 5 it's like it's a big unknown whether it's going to come out and what impact that's going to have so I place a huge amount of emphasis on you just need to be really really quick um at at what you do and I would say like that's the message that I'm trying to convey um to the team that's very um I see nice continuity from the slutman uh era into the Shar era because I I know I've heard Frank say at least a few times the general patent quotes a good plan executed violently today is better than a perfect plan tomorrow 100% 100% and uh and and I said that adaptability um Napoleon has a famous quote which roughly I mean it's not his it predates him it roughly translates into you know I I commit and I adap um which is you go into an important area knowing that you're not going to know everything um and then you're adaptive to the situation that actually presents itself yeah are there any misconceptions about Snowflake and AI that you want to debunk uh we are a real player um it used to be that snowflake used to be thought of as somebody that didn't really get um get AI um and uh you know but uh like early on uh we relied on things like more of a partnership oriented strategy for AI but my big um sort of observation realization um is that AI is a like it's a platform change in the sense that it is a new way in which you and I and everybody else in the world is going to get to software is going to get to Applications um and uh and so once we had that realization out came a bunch of product consequences which is a needs to be Central to snowflake um we need to make it super easy to both build applications but also build the most important applications ourselves Cort cortex analyst for example is a direct to business user application we've never really done things like that before it is driven by a strong belief um that AI is going to disrupt how information is going to be consumed very very broadly um and uh you know I am proud of having a world-class team uh from bottom to top from Foundation models to inference experts uh to product Engineers that integrate the AI uh plus also the product Engineers that are creating applications on top of uh on top of AI that combined with things like broad data access which is Polaris and um uh you know Iceberg uh I think puts us in a very very good position can we zoom out and ask a little bit about your I guess your hypotheses and your hot takes on the future of AI absolutely I just think you are so well positioned you you probably built one of the first if not the first kind of llm native consumer applications at Neva uh now and now obviously from your seat that snowflake you see so much um maybe first on the on the llm kind of Race To scale like what do you think about all that are we reaching the limits of scale like what's next for those guys I mean obviously this can go in a couple of different directions um I talked to a lot of experts um and uh you know there is a collective belief that there is a GPD 5 in the Horizon um what I don't think anyone has a um a clear bar for is what that's going to represent yeah um gbd4 was very cool um much faster it also integrated multimodality natively in a way that's pretty that's pretty amazing um but when you think about reasoning capabilities the ability to come up with plans for how to execute stuff um it didn't feel like it represented a step change um and uh while agents um are you know very hot uh similar to Cortex you know until cortex analyst came along uh people didn't really believe that you could build reliable talko data application they were always kind of hidden this um and remember the bar is very high um if you're giving data to a business user user like 75% accuracy is like one out of four wrong um and uh so I think the big unknown is whether these models are going to represent a a big step forward in things like multi-step reasoning um and if they can they're going to unleash like a whole new class of applications that you and I just cannot imagine um right now you know on the other hand I think when it comes to driving broad adoption um there is a lot that can be done with existing models so many things that are useful for you and me um every single day whether it's a piece of mail that we're looking at or looking through a PDF just think about all the TDM that all of us have to go through um and so I think there is huge impact to be had simply in AI technology just permeating software as we know it especially the uh the user input part of uh of software um so unlike other Technologies I think like there is enough that AI has already delivered that is going to have a meaningfully large impact on society is just going to take a while to uh to run out um you know I sincerely hope we don't get to a phase where uh you need uh a billion dollars to train a great new model I actually think that while what that model can do is cool I think it also reduces the number of people that can have models like that to a very small number um and I think competition is just overall uh healthy um so but it's very hard to make a call you mentioned this a little bit but I'm curious to get your take on it a bit more um you know if gp5 is delayed or not a big step up or whatever the case might be or if you just imagine a world in which the current capabilities of the foundation models that's what we've got and it comes down to how do we Implement those how do we optimize those how do we tune those one of the things that we hear from a lot of people building an AI the first couple weeks are like magic everything is amazing this is great yep and then the next few months are pretty painful oh shoot it can't do this corner case it can't do that corner case it's not quite accurate enough and people get really frustrated and sometimes they can engineer their way out of it sometimes they can't but sometimes it leaves people feeling kind of disillusioned like this stuff's not as good as I thought it was you know maybe the time's not right and so I'd love to get your take if we froze the capabilities of the found models today what sort of changes will we see in the Enterprise landscape over the next handful of years what sort of stuff will we not see because we're just not ready for it yet to me this is this is honestly the magic of software engineering part of what I feel we have implicitly accepted with Chad GPT um is it's sort of like is it's omniscience you're like it can do everything they don't say it in fact they they go to they take pains to not say it but just like Google search never tells you that's a dumb query think about it right kind of fun if it did if it right but there are lots of dumb queries that people type into it Google's like oh I type lots of dumb queries yeah they're like oh here are 100 million pages on the VB and here are the 10 best Pages for you pad for your dumb query um and so I think it's like it's a some of it is good oldfashioned uh you know AI enthusiasm it can do everything uh but some of it is just also plain dumb you should not be doing that uh to me this is where things like okay um let's actually make grounded chat Bots the norm for you know like interacting with information yeah um the model is uh there you know this application should tell you where it got where it got the information from it should be very easy for you to verify said piece of uh of information and feel good yeah that you're actually getting something similarly you need a test framework you know um like Harrison talked about an observability framework to do this on an ongoing basis but I think sometimes when it comes to things like chat Bots uh people forget wait like there is such a thing as a set of regression test there is such a thing um as acceptance criteria for s software everything that we have like if somebody were to build a new application um like one of your Founders your expectation is that you know they got their clue together um and are actually testing stuff before they give it to customers yeah and somehow in the world of AI we're like no no no no no it doesn't matter and these models you know react pretty violently to the addition of a period in a prom MH um and so so I think there needs to be this this idea that like you need good oldfashioned software engineering and you need to measure the performance of these things um and uh and so I think this is where it goes uh you know away from these are hobby projects that can be hit or miss to you know here somebody that can actually software engineer this for you and we think of that as a core strength of what we bring to the table which is like you should be able to have a predictable way to say you know this chat part is going to work or this agent likee application this is the success rate that it's going to have um or this is what cortex analyst is going to do for you in your domain um so that you're like okay I feel good about about deploying it um so even if GPD 5 did not happen I think there is a lot of magic to be done um but it's also just work yeah yeah yeah yeah well put well what's the um I forget who said it there's a quote that we use every now and then uh people miss most great opportun unities because they tend to be wearing coveralls and they look like work you know I think this is one of those where like anything else if you want it to be great you got to work pretty hard on it you got you got you got to you got to sweat it out and to me this is also the place um where the thinking of recall as something that you should tune thinking of recall as an important part of how you think about these applications any ml engineer worth their Sal salt will promptly come and tell you it's like okay I have an au curve for you what are they trying to say they're basically trying to say there is a tradeoff between how much you squeeze the model to do and how good it is there's no perfect answer that's really what the a curve represents um and the more we think of AI applications as also having this Au curve there are trade-offs to be made between um reliability and ability to respond and that's a very conscious factor in how you should think um about things I think the better off we are going to be in terms of where can they deliver value yeah yeah going to go back to the point you said a little bit earlier about reasoning and kind of that delivering the next big leap uh hopefully for gp5 and and and Claude Etc uh it seems like the approach that most folks are taking is kind of bringing in search at inference time and and a lot of more inference time compute and kind of this alphao style search stuff I'm curious just giv you are you know one of the best people in the world at search like do you think that is the path to the promised land on on the research side for for bringing reasoning into these General models give me a little bit more context like I can certainly see how search plays a role in how these models operate but um can you just tell me a little bit more yeah so I mean if you take a the example of you know if if you take Alpha go um and uh you're trying to decide what move to do next um if you can if you can kind of create a branching tree of um here are all the possible moves from here and um do a search kind of over that of like here's what move I should do next um I think people are trying to bring that logic into out of the gaming World um and into domains like I don't know if you saw Devon's cognition um where they're effectively searching over different things that you can do in your coding as well and so just like at inference time just giving the model kind of the ability to like search possible paths to decide what to do yeah there have been a number of papers um you know on this I think even nebs had a bunch of pap first about searching over domains as you as as you come up with a plan um what I don't have to me it's important to understand I I'm prating the name of the neps paper but it also had the same problem they were doing Tre search um is that they fundamentally rely on uh a model typically a neural network um being able to to do things like grade a particular uh point in a like a state space yeah um basically like Alpo for example um you know has pretty solid ideas about what is an advantageous position versus what is not um and the search is Guided by you know by that um what isn't clear in sort of you know um very open-ended questions is as you come up with alternatives for the search space um you know can you actually grade them effectively if it's like an open-ended plan certainly number of these techniques work well for games that have structure um in which you can actually learn um what does optimal mean and you can begin to optimize um towards it what I don't have as good a feel for is um let's take like you know something as simple as cooking um you would think it's you know it's simple but if you take I don't know 10 ingredients and 20 steps that you can take along the way um and various things that you can do in each of these 20 steps and the steps themselves can be short they can be long uh you quickly end up with like this crazy combinatorial explosion of different ways of doing things and yet there is just one perfect recipe or two or three that's the part honestly I don't have a good feel for in terms of like how do you even begin to measure the jump um in terms of cognitive ability um it's easy in structured environments but like out in the real world where you're trying to do some pretty compx comp Lex things I think it becomes trickier um we've built prototypes um for uh basically like agent analysts but it's again a structured space yeah um so what we do one one thing I've done numbers like pretty much all my life I used to do whatever household finances from my dad when I was 10 same like we did in a notebook um and over the past 20 years every day I get like this email um that tells me how my company did the previous day used to be called be counters at Google every day you got you got a report card um every few weeks something would go wrong like you know you made less money somewhere um and we would like start this predictable problem like predictable exercise of some poor analyst would like go drill down into a bunch of different things blah blah blah blah blah look at slice stuff and then they would come back with like oh Shar it was like Easter in Germany and Ascension Day in Brazil and that's why our numbers were off and it took like a decade to model all of these complex things in the world R into like a prediction model so you're like okay I can I can begin to predict but if you think about it the analysis that they do is constraint it's pretty much If a metric is wrong go slice it by 10 different dimensions go look at the results see where likely the problem is um certainly we have built prototypes of this AI analyst that can remove 60 70% of the work that is needed in actually diagnosing prop it's pretty free form but you can make a you can make a language if you can tell a language model these are my attributes oh go call cortex analyst with all of these parameters get the output take a look at it and then tell me what I should do next um so you can begin to automate some of it so that this actually useful um so you can do things like that but a much more open-ended problem um of here are a hundred different things incomparable things you can do and how do you judge and how do you prune I think that's a part I honestly don't have good intuition for totally I want to ask about search in a different sense if that's okay um you obviously have a incredible point of view on search given your time at Google and and at Neva and it seems like right now the you know the consumer world is watching excitedly and nervously about you know is um is there going to be a new kind of search king crown um I'm curious your take on on on the whole kind of AI search space right now how about a hot take on perplexity do you have a hot take on perplexity like look uh I'm happy for perplexity uh and it reminds you again that right time you know uh right time right place matters a lot at Neva you know which uh converged on to a view of what search should be that was very similar to perplexity we were just two three years early um and timing ends up being everything um you can think of perplexity as uh as like a consumer manifestation of how we want to deal with information like it's just um let's face it uh I want to look through an eight-page doc to find the two lines that I really care about said no one but that's search um and so in that sense it's absolutely um the right place I think the more important question is um whether the business of search which is carefully preserved with business contracts not with consumer Choice uh consumer choice is fiction um in a whole bunch of things that uh that we do um we eat what's put in front of us um and we will search with the uh the default search engine that came in our browsers okay we might resist it but on aggregate with Humanity that's the reality of of the world and so I um you know I I would say that that is the bigger challenge um because search is mostly locked up by a few players that control the entry points um but I think that's the fundamental problem which is it is very difficult um to break into the business of search consumers don't like doing stuff and this also gets to one of the kind of broader questions in the world of AI right now which is incumbents versus startups and historically the battle is can the incumbents with distribution build cool products y before the startups with cool products build distribution and I think search is a great example exle that you might have the coolest product in the world it's awfully hard to change consumer Behavior that's right that's right um AI is an interesting test case for this because so much of the coolness of the products is available through the open source world or through thirdparty models and so it feels like it might be a scenario in which incumbents are advantaged versus the startups but do you have a point of view on that I would take two different lens uh to this one um one is what you said about models uh open source models plus players like meta that basically have infinite budgets yeah um under willing to open source models I think the world of creating models from scratch um unless you have an attached hyperscaler an attached business um looks very very hard yeah um and uh um you know so I think as I said I hope this doesn't go to like an ergo uh three GPD 5 class models that the world has because I think it's a bad ending for uh for the world uh so I would definitely say that Foundation model companies um without a strong business um to accompany them it can be a product like I think open AI has created a pretty solid product it's not just a foundation model yeah um I think that's one thing to keep in mind um I'd answer your second question um of sort of disruption SL Innovation uh from a historical lens uh I think of every generation of Silicon Valley companies um as learning from the previous ones um they are uh smarter they know the ways in which things can be disrupted uh and they lean in pretty heavily uh you know we all know um for example the uh the IBM to the mid-range computer computer sort of disruption um and then the decks and sgis of the world uh then getting disrupted by the microsofts of the uh of the world um and then the web coming along leading to the rise of companies like Google or mobile um I would say that in each and every one of these transitions um powerful incumbents with very large Pockets um have shown an ability to lean in sooner lean in fast Master at Google for example when I was there we leaned in very heavily um into the home assistance because Alexa was going to take over the world that was going to be the way in which you and I and everybody else searched we were terrified um and we put a pile of money into it and nothing came off it and it didn't matter why because the cost of a disruption um is way higher than the amount of investment that you have to make um i' would say now this is generation five or something to that effect I'd say all the incumbents are very aware of what can be disrupted um and they lean into it like there's a bunch of strategic thinkers as I told you I think of AI as basically shuffling the tiles on Enterprise software and a part of me goes like you know no way snowflake is going to be leading the charge when it comes to AI not waiting for for it to develop but I think you see every Enterprise AI company lean in the same way and so this to me would be the question about how much disruption is AI going to drive in consumer software certainly there'll be new categories to me if I were a startup um I'd feel a lot more comfortable that I'm creating a new category image creation like done in a mass scale clearly amazing but the same goes for video same goes for voice there's a bunch of specialization that you can do here them to marketing um new things feel like a much safer bet in the AI world than you know take your pick I can do XYZ faster because I am AI enabled I don't think of that as having a whole lot of legs yeah do you think chat JB has a chance of becoming the next Google and to your point on consumer choice being a mirage and like you know business deals or or where this stuff gets locked down like I'm curious what you think of the Apple chat GPT deal I think chat GP I mean the the phone is a is a pretty interesting place um to me um the phone because it's a controlled environment actually offers enormous potential for consumers um I tell people something as ridiculous as copying I don't know an address like from your calendar or a piece of email or to Uber so dumb so hard um you know like you would think City would like you know do this um copy copy the address from this email from Pat and stick it into stick it into Uber so I can get an Uber um so to me I think like there's a huge amount of potential again in like mundane applications um and because the mobile ecosystem is a uh is a pretty closed one where Apple can mandate things like you must have apis that make it possible to access your functionality um using language models or else you might not get any traffic that sounds like a pretty good incentive for everybody uh to kind of get in line so I think there's a huge amount of potential there I honestly wish there was more innovation in this space because again all of this is super doable technology you and I can argue about should this be done in the cloud what can be done on the phone but like as a consumer do you care I'm like we have great connections I'm kind of like if this thing works only when I'm connected to the internet I'll take it um and so to me those are those are sort of uh you know those those are details um I actually think Chad gbd um is an amazing product there's underlying technology um but in so many different ways um they've actually created a stunningly beautiful product experience yeah um that spans the gamut from you know they've turned pretty much like visually illiterate people like me into budding artists I tell people it's like I'm good with words I can talk all day long I can write all all day long um and the magic that I can do with chat GPD is truly amazing um that or uh even even things like I you know for example like I'm on this language cck I'm learning Hindi and I at some point I was like oh I'm struggling with these numbers um but off comes a prompt that says Hey I want a CSV that translates numbers just a string of numbers to Hindi and can you do that can you just give me a CSV file that I can import into Quizlet that literally is faster for me to type than to describe to you I type it in out comes a CSV file in 10 seconds I download it into Quizlet I have a quiz um and so and pretty much everything that I used to do with Python scripts on structured data I just do like with with English you just upload the CSV file and you're like oh add these two columns do this other thing format it into this nice table and get it out for me um it's it's magic so I think there's absolutely a there there in terms of like is it a great product and a great business um but you know being the king of search is like a few more zeros they don't come easy to people yeah all right uh should we close with a couple of quick fire questions rapid fire questions yeah let's do it okay who do you admire most in the world of AI who do I admire most in the world of AI um I admire the people that are in you know like working on things like Foundation models um that are able to do it on on on the cheap without the Infinity of resources um so for example people like uh Arthur or Danny uh I think they've gotten Danny yogot from Rea I think they've gotten just like a remarkable amount of uh things done or from our own team folks like samam and yushang uh to me they represent so much creativity because I go and tell them ah limited budget and what can you you know what what can you do um I think there are there are a set of just like amazing Earnest people that are driving research under tight constraints um so there's there's obviously lots and lots of people um but it's the it's it's the doers that are doing the work um imagining our future um that I'm a huge fan of what's your favorite AI application uh Chad gbd by far easy one easy one um just the utility that I get from it day in and day out is just truly remarkable okay follow up then uh what's an AI app that you wish existed um like an actual talk to your phone that can actually mediate between apps um that would be that would be super cool because remember as I said like just flipping between applications doing very uh you know little things such such such a pain yeah all right we're going to end on an optimistic question what is the best thing that can happen in the world of AI over the next or 10 years what what would you be most excited to see coming out of the world of AI software which you can think of as encoding our thinking uh capturing our ability to think um and act in real world situation clearly has been transformational over the past 50 plus you know years um to me AI as a as an enabler um of access both to the act of creating software but using software um to all of the people in the world would be a significant step up and as I said I don't think it's like lots of fancy new technology that you need the newer technology can certainly help newer classes of applications um you know I was very proud of the fact that we put Google search thanks to things like Android into the hands of pretty much every human being on the planet it's it is a gen you know you can be cynical about technology but was a genuine step forward for Humanity um to me just like AI models as like the new layer between humans and software and software and software um is actually a significant step forward just in having this functionality be vastly more accessible to lots more people as I said both in the creation aspects but also in the consumption aspect I think that's a pretty cool thing to look forward awesome thank you Shar thanks for doing this thank you Pat thank you Sonia thank you [Music] [Music]

========================================

--- Video 50 ---
Video ID: jPluSXJpdrA
URL: https://www.youtube.com/watch?v=jPluSXJpdrA
Title: OpenAI's Noam Brown, Ilge Akkaya and Hunter Lightman on o1 and Teaching LLMs to Reason Better
Published: 2024-10-02 09:00:47 UTC
Description:
Combining LLMs with AlphaGo-style deep reinforcement learning has been a holy grail for many leading AI labs, and with o1 (aka Strawberry) we are seeing the most general merging of the two modes to date. o1 is admittedly better at math than essay writing, but it has already achieved SOTA on a number of math, coding and reasoning benchmarks.

OpenAI researchers Noam Brown, Ilge Akkaya and Hunter Lightman discuss the ah-ha moments on the way to the release of o1, how it uses chains of thought and backtracking to think through problems, the discovery of strong test-time compute scaling laws and what to expect as the model gets better. 

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 - Introduction
01:33 - Conviction in o1
04:24 - How o1 works
05:04 - What is reasoning?
07:02 - Lessons from gameplay
09:14 - Generation vs verification 
10:31 - What is surprising about o1 so far 
11:37 - The trough of disillusionment
14:03 - Applying deep RL
14:45 - o1’s AlphaGo moment? 
17:38 - A-ha moments
21:10 - Why is o1 good at STEM?
24:10 - Capabilities vs usefulness 
25:29 - Defining AGI
26:13 - The importance of reasoning 
28:39 - Chain of thought 
30:41 - Implication of inference-time scaling laws 
35:10 - Bottlenecks to scaling test-time compute
38:46 - Biggest misunderstanding about o1?
41:13 - o1-mini
42:15 - How should founders think about o1?

Transcript Language: English (auto-generated)
one way to think about reasoning is there are some problems that benefit from from being able to think about it for longer you know there's this classic notion of system one versus system two thinking in humans system one is the more automatic instinctive response and system two is the slower um you know more process driven response um and for some tasks you don't really benefit from more thinking time so if I ask you like what's the capital of Bhutan you know you can think about it for two years it's not going to help you get it get it right with higher higher accuracy what is the capital I actually don't know uh but you know there's there's some problems where there's clearly a benefit from being able to think for longer so one classic example that I point to is a Sudoku puzzle it's you you could in theory just go through a lot of different possibilities for like what the Sudoku puzzle might be uh what the solution might be and it's really easy to to recognize when you have the correct solution So in theory if you just had like tons in times in time to solve a puzzle you would eventually figure it out [Music] we're excited to have Noam Hunter and ILO with us today who are three of the researchers on Project strawberry or 01 at openai 01 is open ai's first major Fay into General inference time compute and we're excited to talk to the team about reasoning Chain of Thought and inference time scaling law and more ilga Hunter Noom thank you so much for joining us and congratulations on releasing 01 into the wild I want to start by asking did you always have conviction this is going to work um I think that we had conviction that something in this direction was promising but the the actual like path to get here was never clear and I you know you look at 01 it's not like this is an overnight thing it actually there's a lot of years of research that that goes into this um and a lot of that research didn't actually um pan out but I think that there was conviction from open Ai and like a lot of the um leadership that something in this direction um had to work and they were willing to uh to keep investing in it um despite the initial setbacks and I think that eventually paid off I I'll say that I did not have as much conviction as gome from the very beginning um I've been staring at language models trying to teach them to do math and other kinds of reasoning for a while um and I think there's like uh a lot to research that's ebb and flow um sometimes things work sometimes things don't work um when we saw that the methods we were pursuing here started to work um I think it was a kind of aha moment for a lot of people uh myself included um where I started to read some um outputs from the models that were approaching the problem solving a different different way um and that was this this moment I think for me where my conviction really set in um I think that open eye in general takes a very empirical data- driven approach to a lot of these things and when the data starts to speak to you when the data starts to make sense when the trends start to line up and we see something that that we want to pursue we pursue it and that for me was when I think the conviction really set in about you Ila you've been at open AI for very long time five and a half years five and a half years what what did you think did you have conviction from the beginning that this approach was going to work um no I've been wrong several times since joining about the path to AI um we originally well I originally thought that robotics was the way forward that's why I joined the robotics team first um embodied AI AGI that's where we thought things were going to go um but yeah I mean things hit roadblocks um I I would say like during my time here chat GPT well I guess that's kind of obvious now that was a paradigm shift we were able to share very broadly with the world something that is a universal interface and um I'm glad that now we have a new path potentially forward to push this ra reasoning Paradigm um but yeah it was definitely not obvious to me um for the longest time yeah I realize there's only so much that you're able to say publicly for very good reasons about how it works but what can you share about how it works even in sort of general terms so um the oan model series um are trained with ARL to be able to think and um you could call it reasoning maybe also uh and it is fundamentally different from what we're used to with llms um and we've seen it uh really generalized to a lot of different reasoning domains as we've also uh shared recently so we're very excited about this paradigm shift with this new model family and for for people who may not be as familiar with what's state-of-the-art in the world of language models today what is reasoning how would you define reasoning and maybe a couple words on what makes it important good question I mean I think one way to think about reasoning is there are some problems that benefit from from being able to think about it for longer you know there's this classic notion of system one versus system two thinking in humans system one is the more automatic instinctive response and system two is the slower um you know more process driven response um and for some tasks you don't really benefit from more thinking time so if I ask you like what's the capital of bhan you know you can think about it for two years it's not going to help you get it get it right with higher higher accuracy what is the capital I actually don't know um but you know there's there's some problems where there's clearly a benefit from being able to think for longer so one classic example that I point to is a Sudoku puzzle it's you you could in theory just go through a lot of different possibilities for like what the Sudoku puzzle might be uh what the solution might be and it's really easy to to recognize when you have the correct solution So in theory if you just had like tons and tons of time to solve a puzzle you would eventually figure it out um and so that's that's what I consider to be I think a lot of people in the AI Community have like different definitions of reasoning and I'm not claiming that this is like the canonical one I think everybody has their has their own opinions but I view it as um the kinds of problems where there is a benefit from being able to like consider more options and and think for longer um you might call it like a a generator verifier Gap where there's like it's really hard to generate a correct solution but it's much easier to recognize when you have one and I think all problems exist on the Spectrum from really easy to verify um relative to generation like a Sudoku puzzle versus um you know just as hard to verify as it is to generate a solution like name of the capital of gutan I want to ask about you know alphago and know your background having done you know a lot of great work in poker and other games uh to what extent are the lessons from gameplay analogous to what you guys have done with 01 and how are they different so I think one thing that's that's really cool about 01 um is that it does clearly benefit by being able to think for longer and when you look back at like many of the AI breakthroughs that have happened I think alpago is the classic example one of the things that um was really noticeable about about the bot though I think underappreciated at the time was that it thought for a very long time before acting it would take you know 30 seconds to make it to make a move and if you tried to have it act instantly it actually wasn't better than top humans it was noticeably worse than them and so it clearly benefited a lot by that extra thinking time now the problem is that the extra thinking time that it had um it was running multicol research which which is like a a particular form of reasoning that that worked well for go um but for example doesn't work in a game like poker which which my early research was on um and so a lot of the like methods that existed for for being able to reason um for being able to like think for longer was still specific to the domains even though the um the neural that's behind it the the the system one part of the AI was very general and I think one thing that's really cool about 01 is that it is so General the way that it's thinking for longer is actually quite General and can be used for a lot of different domains and we're seeing that by giving it to users and seeing what they what they are able to do with it yeah one of the things that's always been really compelling to me about language models and this is nothing new is just that because their interface is the text interface they can be adapted to work on all different different kinds of problems um and so what's exciting I think about this moment for us is that we think we have a way to do something uh to do reinforcement learning on this General interface um and then we're excited to see what that can lead to one question on that you you mentioned I thought that was well put sort of the the I forget exactly how you phrased it but the gap between generation and verification and there's sort of a spectrum in terms of how easy things are to verify does the does the method for reasoning remain assistant at various points in that Spectrum or are there different methods that apply to various points in that Spectrum one thing I'm excited about for this release has been to get 01 in the hands of so many new people uh to play with it to see how it works what kinds of problems it's good at and what kinds of problems it's bad at um I think this is like something really core to open ai's uh strategy of iterative deployment we put the technology that we build the research that we develop out into the world so that we can see um um like we we do it safely and we do it so that we can see how the world interacts with it and what kinds of things we might not always understand fully ourselves um and so in thinking about like what are the limits of our approaches here I think it's been really enlightening to see uh like Twitter uh show what it can and what it can't do um I hope that that is like enlightening for the world that's useful for for everyone to figure out what these new tools are useful for and then I also hope we're able to take back that information and and and use it effectively to understand our our processes our research our products better speaking of which is there anything in particular that you all have seen in the Twitter verse that's surprised you you know ways that people have figured out how to use 01 that you hadn't anticipated um there's one thing I'm super excited about I've seen um a lot of MDS and researchers um use the model as a brainstorming partner and what they are talking about is that like they've been in cancer research for so many years and they've been just running these ideas by the model about what they can do about these Gene Discovery gene therapy type of applications and they are able to get like these really novel uh ways of research to pursue from the model clearly the model cannot do the research itself but it can just be a very nice uh collaborator with humans uh for in this respect so I'm super excited about seeing the model just Advance this scientific uh path forward that's not what we're doing um in our team but like that is the thing I guess like we want to see in the world the domains that are outside ours that gets really uh benefit by this model no I think you tweeted that deep RL is out of the trough of disillusionment can you say more about what you meant by that I mean I think um there was definitely a period starting with I think Atari you know the Deep Mind Atari results um where deep RL was the hot thing I mean I I I was I was in a PhD program I remember what it was like in like you know 2015 to 2018 2019 and deep RL was was the hot thing and in some ways I I think that was I mean a lot of research was done but um certainly some things were overlooked and I think um one of the things that was kind of overlooked was was the power of just training on tons and tons of data using you know something like um the GPT approach and in many ways it's kind of surprising because if you look at alphago which was in many ways like the the crowning achievement of De RL um yes there was this RL step but there was also I mean first of all there was also this reasoning step but even before that there was this large process of learning from Human data and and that's really what got alphago off the ground um and so then there was this like increasing shift there was I guess like a view that this was an impurity in some sense that um uh so a lot of deep RL is really focused on um learning without human data with just learning from scratch um yeah Alpha zero which was a great which was an amazing result and actually ended up doing a lot better than Alpha go um but I think partly because of this focus on loaring from scratch um this GPT Paradigm uh kind of flew under the radar for a while and um except for open AI which which you know saw some initial results for it and you know again had the conviction to to double down on that investment um yeah so there was definitely this period where um deepl was the hot thing and then I think you know when gpt3 came out and and some of these other like large language models and there was so much success without deepl um it there was like yeah a period of disillusionment where um a lot of people switched away from it or or kind of lost faith in it and what we're seeing now with 01 is that actually there is um a place for it and it it can be quite powerful when it's combined with these other elements as well and I think a lot of the deepl results were in kind of you know well defined settings like gameplay is o1 one of the first times that you've seen deep RL used in much more General kind of uh unbounded setting is that the right way to think about it yeah I think I think it's a good I think it's a good point that a lot of the like highlight deepl results were really cool but also very like narrow in in their applicability um I mean I think there were a lot of like quite useful deep RL results um and and also quite General RL results but there wasn't anything um comparable to something like gp4 in its impact so um I I think I think we will see that kind of level of impact from deepl in this new paradigm going forward one more question in this General train of thought uh I remember the alphao results you know at some point with the in the leas at all tournaments there was move 37 and you know that move surprised everybody have you seen seen something of that you know sort where where 01 tells you something and it's surprising and you think about it it's actually right and it's better than any you know top human could think of have you have you had that moment yet with the model or you think it's 02 03 uh one of the ones that comes to mind is we spent a lot of the time preparing for the ioi uh competition that we put the model into uh looking at its responses to programming competition problems and there was one problem where it was uh was really insistent on solving the problem in this kind of weird way uh with some weird method I don't know exactly what the details were uh and uh our colleagues who are much more into competitive programming were trying to figure out why I was doing it like this I don't think it was quite a like this is a stroke of Genius moment I think it was just like the model didn't know the actual way to solve it and so it just like banged it head until it found something else um did it get there yeah yeah it solved the problem it just it just it used some it was like it was some method that would have been really easy if you saw something else I I wish I had the specific one but uh I remember that being kind of interesting there's a lot of the things in the in the in the programming competition um results um I think somewhere we have the II competition programs uh published um where you can start to see that the model doesn't approach thinking quite like a human does or it doesn't approach these problems quite like a human does it has slightly different ways of solving it for the actual II competition um there was one problem that humans did really pour on that the model was able to get half credit on uh and then another problem that humans did really well on that the model was like barely able to get off the ground on um just showing that it kind of has a different way of approaching these things than than maybe a human would I've seen the model um solve some geometry problems and the way of thinking was quite surprising to me such that you're asking the model just like give me this like sphere and then there are some points on the sphere and asking for probability of some event or something and the model would go uh let's visualize this let's put the points and then if I think about it that way or something so I'm like oh you're just using words and visualizing something that really helps you con contextualize um like I would do that as a human and seeing oan do it too just really surprises me interesting that's fascinating so it's stuff that's actually understandable to a human and would actually kind of expand the boundaries of how humans would think about problems versus you know some UND decipherable machine language that's really fascinating yeah I definitely think one of the cool things about our own one result is that these chains of thoughts the model produces are uh human interpretable um and so we can we can look at them and we can kind of poke around at how the model is thinking were there um were there aha moments along the way or were there moments where you know Hunter you mentioned that you were not as convinced at the outset that this is the direction that was going to work was there a moment when that changed where you said oh my gosh this is actually going to work uh yeah so uh I've been an open eye about about two and a half years uh and most of the time I've been working on trying to get the models um better at solving math problems um and we've done a bunch of work in that direction we've uh built various different thepoke systems for that uh and there was a moment on the o1 trajectory where we had just trained this model with this method with a bunch of fixes and and changes and whatnot uh and it was scoring higher on the mathy vals than any of our other attempts um any of our bespoke systems and then we were reading the uh chains of thought and you could see that they felt like they had a different character in particular um you could see that when it got stuck it would say wait this is wrong let me take a step back let me figure out the right path forward uh and and we called this backtracking and I think for uh a long time I'd been waiting to see an instance of the models backtracking and I kind of felt like I wasn't going to get to see an autor regressive language model backtrack because they're just kind of predict next token predict next token predict next token and so when we saw this score on the math test test and we saw the trajectory that had the backtracking that was the moment for me where I was like wow um this is like something is coming together that I didn't think was going to come together and I need to update uh and and I think that was when I grew a lot of my conviction I think the story is the same for me I think it was probably around the same time actually like I you know I definitely I joined with this idea of like you know Chach BT doesn't really think before responding like it's very very fast and there was this like powerful Paradigm of like um in these in these games of AI being able to think for longer and getting much better results but and and there's this question about how do you bring that into uh language models that I was really interested in um and you know that's like it's easy to say that but then there's like there's a difference between just like saying that oh there should be a way for it to think for longer than actually like delivering on that and so we um you know we I I I tried to I tried a few things and like other people were trying a few different things and um in particular yeah one of the things we wanted to was this ability to um to backtrack or to like recognize when it made a mistake or to like try different approaches um and we had a lot of discussions around how do you enable that kind of behavior and at some point we just felt like okay well one of the things we should try at least as a baseline is like just have the AI think for longer um and we saw that like yeah it it once it's able to to think for longer it um develops these abilities um almost like emergently that um were very powerful and contain things like backtracking and self-correction all all these things that that we were wondering how to enable in the models and and to see it come from such a um a clean scalable approach um that was for me the big moment when I was like okay it's very clear that we can push this further and um and it's it's so it's so clear to see where things are going no no my think is understating how strong and effective his conviction in test time compute was I feel like all of our early one-on ones uh when he joined were talking about test time computer and its power and I think multiple points throughout the project no would just say why don't we let the model think for longer and then we would and it would get better and he would just be uh he would just look at us kind of funny like we hadn't done it until that point one thing we noticed in your evals is that you know owan is noticeably good at stem it's better at stem uh than the the pre previous models is there a rough intuition for that why that is I mentioned before that like there's some tasks that are like you know reasoning tasks that are easier to verify than they are to to generate a solution for and there there's some tasks that that don't really fall into that category and I think stem problems tend to fall into the like what we would consider hard reasoning problems and so I think that's that's that's a big factor for why we're seeing a lift on on stem kind of subjects makes sense um I I think relatedly we saw that um in the in the research paper that you guys released that 01 passes your research engineer interview with pretty high Pass rates what do you make of that and does that mean at some point in the future open AI will be hiring A1 instead of instead of human Engineers uh I don't think we're quite at that level yet um I think that there's more it's hard to be the 100% though uh maybe the interviews need to be better I'm not sure um I think that the 01 does feel at least to me I think other people on our team like a better coding partner um than the other models um I think it's already authored a couple of PRS in our on our repo um and so in some ways it is acting uh like a software engineer um because I think software engineering is another one of these stem domains that that benefits from longer reasoning um I don't know I think that um the kinds of uh roll outs that we're seeing from the model are thinking for a few minutes at the time I think the kinds of software engineering uh job that I do when I when I go and write code I think for more than a few minutes at a time um and so maybe as we start to scale these things further as we start to follow this trend line uh and let 01 think for longer and longer it'll be able to do more and more of those tasks and we'll see you'll be able to tell that we've achieved AGI internally when we take down all the job listings and either the company's doing really well or really poorly what do you think it's gonna take for 01 to get great at the humanities do you think being good at reasoning and logic and and stem kind of naturally will extend to being good at the humanities you as you scale up in Prince time or how do you think that plays out you know we're well like we said we released the models and we were kind of curious to see um what they were good at and and what they weren't as good at and um and what people end up using it for and I think there's clearly a gap between the raw intelligence of the model and how it's um like how useful it is for various tasks like in some ways it's very useful but I I think that um it it could be a lot more useful in a lot more ways um and I think there's still some iterating that to do to to be able to unlock that like more General usefulness well can I ask you on that do do you view I'm curious if there's a philosophy at open AI or maybe just a point of view that you guys have on how much of the gap between the capabilities of the model and whatever real world job needs to be done how much of that Gap do you want to make part of the model and how much of that Gap is sort of the job of the ecosystem that exists on top of your apis like their job to figure out do you have a do you have a thought process internally for kind of figuring out like what are the jobs to be done that we want to be part of the model versus kind of where do we want our boundaries to be so that there's an ecosystem that sort of exists around us so I'd always heard that opening ey was very focused on AGI and I was like like honestly kind of skeptical of that before I joined the company and and basically like the first first day that I started and there was an all hands of the company and Sam got up in front of the whole company and basically like laid out the priorities going forward for like the the shortterm and the long term it became very clear that AGI was the actual priority and so I think the clearest answer to that is you know AGI is the goal um there's no single like application that is the priority other than getting us to AI do you have a definition for AGI everybody has their own definition for a exactly that's what I'm curious I I don't know if I have a concrete definition I just think that uh it's something about the proportion of economically valuable jobs that our uh models and our AI systems are able to do I think it's going to ramp up a bunch over the course of the next however many years I don't know it's one of those uh It'll like you'll feel it when you feel it and we'll like move the goalpost back and be like this isn't this isn't that for however long until one day we're just working alongside these AI co-workers and um they're doing large parts of the jobs that we do now and we're doing different jobs and and the whole ecosystem of what it means to do work has changed one of your colleagues had a good articulation of the importance of reasoning on the path to AGI which I think paraphrases as something like any job to be done is going to have obstacles along the way and the thing that gets you around those obstacles is your ability to reason through them and I thought that was like a pretty nice connection between the importance of reasoning and the objective of AGI and sort of being able to accomplish economically useful tasks um is that is that the best way to think about what reasoning is and why it matters or there other Frameworks that you guys tend to use I think this is a TBD thing um just because I think at a lot of the stages of the development of these AI systems of these models we've um seen different shortcomings different failings of them I think I think we're learning a lot of these things as we develop the systems as we evaluate them as we uh try to understand their capabilities and what they're capable of um other things that come to mind that I don't know how they relate to reasoning or not are like strategic planning um ideating or things like this where like to be a um to make an model that's as uh good as an excellent product manager uh you need to do a lot of brainstorming ideation on on on on on what users need what all these things are is that reasoning or is that a different kind of creativity that's not quite reasoning and needs to be addressed differently then afterwards when you think about operationalizing those plans into action you have to strategize about how to move an organization towards getting things done is that reasoning there's parts of it that are probably reasoning and there's maybe parts that are something else and maybe eventually it'll all look like reasoning to us or maybe we'll come up with a new word and there will be new uh new steps we need to take to get there I don't know how long we can we'll be able to push this forward but whenever I think about this General reasoning problem it helps to think about the domain of math uh we've spent a lot of time uh reading what the model is thinking uh when it's when you ask at a math problem um and then it's clearly doing this thing where like it hits an obstacle and then it backtracks just has a problem Oh wait maybe I should try this other thing so when you see that um thinking process um you you can imagine that it might generalize to things that are Beyond math that's what gives me hope I don't know the answer but hopefully the thing that gives me pause is that the 01 is already better than me at math but it's not as good at me at being a software engineer um and so there's some there's some mismatch here there's still a job to be done good there's still there's still some work to do if my whole job were doing uh Amy problems and doing high school competition math I'd be out of work there's still some stuff for me for right now since you mentioned um sort of the The Chain of Thought and being able to watch the reasoning behind the scenes um I have a question that might be one of those questions you guys can't answer but just for fun was it f first off I give you props for in the blog that you guys um published with the release of 01 explaining why Chain of Thought is actually hidden and and literally saying like partly it's for competitive reasons um I'm curious if that was a contentious decision or or like how controversial that decision was because I could see it going either way and it's a logical decision to hide it but I could also Imagine a world in which you decide to expose it so I'm just curious if that was a contentious decision I don't think it was contentious I mean I think for the same reason that you don't want to um share the model weights necessarily for a Frontier Model I think there there's a lot of risks to sharing um the you know the the thinking process behind the model and I think I think it's a similar decision actually can you explain from a Layman's maybe to Layman like what what is the chain of that and what's what's an example of one so for instance if you're asked to solve an integral um most of us would need a piece of paper and a pencil and we would um kind of lay out the steps from getting from a complex equation and then there will be steps of simplifications and then going to a final answer the answer could be one um but how do I get there that is the Chain of Thought in the domain of math let's talk about that path forward uh inference time scaling LW to me that was the most important chart from the research that you guys published and it seems to me like a Monumental result similar to the the scaling laws from from pre-training um and and sorry to be hypy uh like do you agree that like the implications here I think they're pretty profound and you know what does it mean for for the field as a whole I think I think it's pretty profound um and I I think one of the things that I wondered when we were preparing to release 01 is is what people would recognize its significance um we you know we we included it but it's it's kind of a subtle point and I was actually really surprised and impressed that um so many people recognized what what this meant um there have been a lot of concerns that like AI might be hitting a wall or plateauing because pre-training is so expensive and becoming so expensive and there's all these questions around like is there enough data to to train on um and I think one one of the major takeaways about 01 especially 01 preview is not what the model is capable of today but what it means for the future the fact that we're able to have this different dimension for scaling that is so far pretty untapped um I I think is a is a big deal and um and and I think means that the ceiling is a lot higher than a lot of people have appreciated what happens when you let the model things for for hours or months or years what do you what do you think happens we haven't had 01 for years so we haven't been able to let it think that long yet is there a job just running in the background right now that it's just still thinking about solve World Peace okay I'm thinking thinking think yeah there's a there's a Asimov story like that called the last question where yeah you they asked this big computer sized uh AI um something about like how do we reverse entropy and it says I need to think longer for that and like the story goes and then 10 years later they see and it's still thinking and then a 100 years later and then a thousand years later and then 10,000 years later um yeah there is as yet meaningful not enough information for Meaningful answer or something like yeah like it's still yeah do you have a guess empirically on you know what'll happen you know or I guess right now I think the model has I've seen some reports like 120 IQ so like very very smart uh is there a is there a ceiling on that as you scale up iner time compute do you think you get to infinite IQ one of the important things is that like it's 120 IQ on some test someone gave this doesn't mean that it's got like0 IQ level reasoning at all the different domains that we care about I think we even talk about how it is um below 40 on some things like creative writing and whatnot um so I I uh there's definitely it's like it's confusing to think about how we extrapolate this model I think I think it's an important point that um you know we we talk about these benchmarks and we one of the benchmarks that we highlighted in our results was gpq which is this um you know questions that are given to PhD students and like typically PhD students can answer and the AI is outperforming a lot of phds on this Benchmark right now that doesn't mean that it's smarter than a PhD in like every single way imaginable there's there's a lot of things that a PhD can do that you know there's a lot of things that a human can do period that AI can't do and um so you always have to like look at these EV valves with um some understanding that like it's measuring a certain thing that is typically a proxy for human intelligence when you measure you know when humans take that test but means something different when when the AI takes that test maybe a way of framing that an answer to the question is that I hope that we can see that letting the model think longer on the kinds of things that it's already showing it's good at will continue to get it better um so um one of my big Twitter moments uh was um I saw a professor that I had had in school a math professor was tweeting about how uh he was really impressed with 01 um because he had given it a proof that had been solved before by humans but never by an AI model uh and it just took it and ran with it and figured it out and that to me feels like we're at the cusp of something really interesting where it's close to being a useful tool for doing novel math research where if it can do some small lemas and some proofs for um like real math research that would be really uh that would be really really a breakthrough um and so I hope by letting it think longer we can get better at that particular task of being a really good math research assistant um it's harder for me to extrapolate what it's going to look like will it get better at the things that it's not good at now um what would that path forward look like and then what would the infinite IQ or whatever look like then um when it thinks Forever on problems that it's not good at but instead I think you can kind of ground yourself in a here are the problems it's good at if we let it think longer at these oh it's going to be useful for math research oh it's going to be really useful for software engineering oh it's going to be really and you can start to play that game and start to see how I hope the the future will evolve what what are the bottlenecks to scaling Tes time compute I mean for for pre-training it's pretty clear you need enormous amounts of compute you need enormous amounts of data this stuff requires enormous amounts of money like it's pretty easy to imagine the bottom X on scaling pre-training what constrains sort of the scaling of inference time compute when when gbt2 came out and gpt3 came out it was like pretty clear that like okay if you just throw more data and more gpus at it it's going to get a lot better and it still took years for to get from gpd2 to gbd3 to gbd4 and um there's just a lot that goes into taking an an idea that sounds very simple and then actually like scaling it up to a very large scale and I think that there's a similar challenge here where okay it's like a simple idea but you know there's a lot that work that has to go into um actually scaling it up so I think that's the challenge yeah I think that um one thing that I think maybe it doesn't anym surprise but one thing I think might might have used to surprise more academic oriented researchers who join open AI is how much of the problems we solve are engineering problems versus research problems um building large scale systems training large scale systems running algorithms that have never been invented before on systems that are are brand new um is is a scale no one's ever thought of is is really hard and so there's always a lot of just like hard engineering work to to make these systems scale up also one needs to know what to test the model on so we do have these standard evals as benchmarks but uh perhaps there are ones that we're not yet testing the model on so we're definitely looking for those where uh we can just spend more compute on test time and get better results one of the things I'm having a hard time wrapping my head around is you know what happens when you give the model you know near infinite computes because as a human I am you know even if I'm teren to like I am Limited at some points by my by my brain uh whereas you can just put more and more compute that inference time and so does that mean that for example all math theorems will eventually be solvable through this approach uh or like where is the limit do you think infinite computes a lot of compute near near infinite it goes back to the Asimov story if you're waiting 10,000 years but maybe uh but but I I I say that just to to ground it in a like we don't know yet quite what the scaling of this is for how it relates to solving really hard math theorems um it might be that you really do need to let it think for a thousand years to solve some of the unsolved like Core math problems um yeah yeah I mean I think it is true that like if if you let it think for long enough then in theory you could just go through like you know you formalize everything in lean or something and you go through every single possible lean proof and eventually you you stumble upon the theorem yeah we have algorithms already that can solve any math problem is maybe what you were about to get out right exactly yeah like given infinite time you can do a lot of things but yeah so you know clearly get some diminishing returns as you think for longer but yeah very fair what do you think is the biggest misunderstanding about 01 I think a big one was like when the name strawberry leaked people assume that like it's because of this uh popular question online of like the models can't answer how many RS are in strawberry and that's actually not the case it's when when we saw that question actually we were really concerned that there was some internal leak um about the model and as far as we know there wasn't it was just like a complete coincidence that our project was named strawberry and there was this also this like popular reasoning about strawberries as far as I can tell the only reason it's called strawberry is because at some point at some time someone needed to come up with a code name and someone in that room was eating a box of strawberries and I think that's really the end of it it's more relatable than cou I think I was pretty impressed with like how well understood it was actually yeah um I we we were actually not sure how it was going to be received when we when we launched um there was a big debate internally about like is people are people just going to be like disappointed that it's like you know not better at everything um or are people going to be like impressed by you know the crazy math performance um and well we were really trying to communicate was that it's not really about the model that we're releasing it's more about where it's headed and I think I was yeah I I wasn't sure if that would be well understood but it seems like it was and so I think I was actually very um very happy to see that is there any criticism of 01 that you think is fair it's absolutely not better at everything um it's a funky model to play with I think people on the internet are finding um new ways to prompt it to do better um um so there's still a lot of weird edges um to work with um I don't know I'm I'm I'm I'm I'm really excited to see um someone had alluded earlier to like the letting the ecosystem work with our platform to to make more intelligent products to make more intelligent things uh I'm really interested to see how that goes with 01 um I think we're in the very early days it's kind of like I don't know at some point a year ago people started to really figure out these MPS these language model programs with um uh gbd4 or whatever and it was enabling smarter software engineer tools and things like that um maybe we'll see some similar kinds of developments with people building a top of one speak which one of the things that um we have not talked about is o1 mini and I've heard a lot of excitement about o1 mini because people are generally excited about small models and if you can preserve the reasoning and extract some of the World Knowledge you know for which deep Neal Nets are not exactly the most efficient mechanism like that's a pretty pretty decent thing to end up with so I'm curious what what's your love little excitement about 01 mini and kind of the general direction that that represents it's it's a super exciting model also for us as researchers um if a model is fast it's universally useful so yeah we also like it um yeah they they they kind of serve different purposes and also yeah we have we we are excited to have like a cheaper faster version and then kind of like a heavier slower one as well yeah they they are useful for different things so um yeah definitely excited that we we ended up with um a good tradeoff there I really like that framing I think it highlights how much progress is like how how much you can move forward times how much you can iterate um and at least for our research like GGA gets at uh 01 mini lets us iterate faster hopefully for the broader ecosystem of people playing with these models 01 mini will also allow them to iterate faster um and so it should be like a really useful and exciting artifact at least for at least for that reason for Founders who are building in the AI space how should they think about you know when they should be using gp4 versus 01 like do they have to be doing doing something STEM related coding related math related for to use1 or how should they think about it I'd love if they could figure that out for us one of the motivations that we had for releasing one preview um is to see what people end up using it for and and how they end up using it um there was actually yeah um some some question about like whether it's even worth releasing a one preview um but yeah I think one of the reasons why we wanted to release it was so that we can get into the into people's hands early and and see um what use cases it's really useful for what it's not useful for what people like to use it for and um how to improve it and for for the things that people find it useful for anything you think people most underappreciate about 01 right now it's like somewhat proof we're getting a little bit better at naming things um we didn't call it like GPT 4.5 thinking mode whatever um well I thought it was strawberry thought it was qar so I don't know thinking mode kind of has a kind of hasn't ring to it uh what do what are you guys most excited about for 02 03 whatever may come next 0 3.5 whatever yeah we're not at a point where we are out of ideas so I'm excited to see how it plays out just keep doing our research but yeah most excited about getting the feedback because as researchers we are clearly biased towards the domains that we can understand but we'll receive a lot of different use cases um from the usage of the product and we we're going to say maybe like oh yeah this is an interesting thing to push for um and yeah like beyond our imagination it might get better at different fields I think it's really cool that we have a trend line um which should we Post in that blog post and I think it'll be really interesting to see how that trend line extend wonderful that's a good note to end on thank you guys so much for joining us today [Music]

========================================

--- Video 51 ---
Video ID: ITsrbrd2HrE
URL: https://www.youtube.com/watch?v=ITsrbrd2HrE
Title: How Reddit Became "The Front Page of the Internet" ft. Founder Steve Huffman
Published: 2024-09-26 10:00:17 UTC
Description:
Reddit is one of the largest and most culturally influential sites on the internet—and its journey is one of the most unusual company stories in internet history. College roommates Steve Huffman and Alexis Ohanian founded Reddit in 2005 and scaled it on a shoestring until Condé Nast acquired it the following year. Struggling for direction under its parent company, the founders left, and Condé Nast ultimately spun it out as an independent company once again. With Reddit buckling under user discontent in 2015, founder Steve Huffman returned as CEO to save the company and navigate the way forward. Over the following nine years, Reddit stabilized and the company’s revenue grew more than 50-fold to a successful IPO 19 years in the making.

Host: Roelof Botha, Sequoia Capital
Featuring: Steve Huffman, Alexis Ohanian, Chris Slowe, Jen Wong, Alfred Lin

Learn more here: https://www.cruciblemoments.com/episodes/reddit

00:00 - Introduction
02:51 - The Birth of Reddit: From Rejection to Reinvention 
04:46 - Launching a New Era of User Generated Content 
06:23 - The Evolution of Community 
08:13 - Subreddits and Early Challenges 
12:03 - Condé Nast’s Move to Acquire Reddit
13:03 - Reddit’s Crucible Decision: The $10 Million Acquisition by Condé Nast
17:52 - The Founders’ Departure
19:02 - The Launch of Reddit Gold and Reddit’s Spin-Out from Condé Nast
21:44 - Community Crises
24:50 - The Great Reddit Blackout of 2015
27:49 - Steve Huffman’s Crucible Decision: Returning to Reddit
28:47 - Redefining Reddit’s Content Policy
31:39 - From Crisis to Sustainable Business
34:31 - Reddit’s Shift to an Advertising-Based Business Model
35:37 - Anonymity and Brand-Safe Advertising
36:46 - Growth, IPO and the Power of Community
41:00 - Navigating Crisis and Appreciating the Calm at Reddit

Transcript Language: English (auto-generated)
[Music] the strategy of the team was we don't know how Reddit works or why Reddit Works therefore we're not going to change anything because we might break it and so they literally that was their strategy was don't change anything and then when I would talk to the team that they were so torn on this they were like we don't like the way Reddit is being used there's some content on here and communities on here that are really bad we don't like that we hate that but we feel powerless to get rid of it because if we get rid of it we're going to break Reddit but I told them is what you're describing to me as like if we change we die but the situation is if we don't change we're dead and was like that's why I'm here is reddit's dying and we have to change we have no choice welcome to Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world's most remarkable companies I'm your host and the managing partner of SEO capital rof W to the unacquainted may look like a Labyrinth of message boards but to the devoted faithful of which there are 90 million daily active users and thousands of volunteer moderators it is a place to converse debate and connect founded in 2005 by college roommates today Reddit is one of the biggest sources of online information and one of the most visited sites on the internet but reddit's path was Securus and riddled with points of near failure controversies and crises a sale and a spin out resignations and business reinventions shaped Reddit into the juger it is now these are The Crucible moments that defined the front page of the internet my name is Steve Huffman I'm the co-founder and CEO of Reddit there's a long story behind the idea of Reddit I had had an idea for ordering food from your cell phone this was back in like 2004 2005 and my college roommate at the time Alexis ohanyan I shared that idea with him and he liked it too and so it's something that we had talked about doing at the same time we learned that Paul Graham was giving a talk called how to start a startup during our spring break of our fourth year at uba and my girlfriend at the time over me talking about this and suggested that we go and so that's why we went to that talk I wasn't really familiar with Paul but when I heard about the talk I was like we got to go dude who cares about spring break like you know there's terrible screen glare on the beach let's go to Boston instead I'm Alexis ohanyan co-founder of Reddit and former executive chairman we go up we hear Paul speak Steve wants to go get an autograph for one of his books and then I followed by inviting Paul out for a drink and and he agreed Steven Alexis pitched Paul gra their mobile food ordering app and he was interested it just so happened gra was about to launch a startup accelerator called y combinator Paul encouraged us to apply with the idea that he had heard and so we applied with that idea but we're rejected Paul the following day after saying we couldn't do YC called us and said hey if you want to be in my combinator we'd like you in it but you need to work on something else Paul was enamored with this idea of like what is going to replace the New York Times when the front page needs to be more than just what one editorial board what one publication can decide and his idea for that something else was a version of delicious delicious was a website at the time that among other things invented tagging it was a social bookmarking website he was like I have this idea where if it was delicious but the content was good and so we went back to Boston and then we mashed it up with another idea I had which was Slash dot which was another website at the time basically slop not just for Tech news because slat was this website for Tech news but it had this really great community that was was really what slash was about and so that initial idea for Reddit was a combination of the delicious social bookmarking and the slash. community and conversation Sten Alexis received a $112,000 check from YC as part of the inaugural class they decided on the name Reddit as in I readit on Reddit they built a simple userfriendly platform for sharing and discussing topics in one centralized place which at the time was a novelty 2005 was a lot at first it was the first time BR band was greater than D and you have to also have to understand in 2005 this is pre social media my name is Alfred Lynn and I'm a partner at sequa Capital so you know in some sense Reddit and YouTube which was also founded in 2005 were the first of its kind to combine user generated content and community and so Reddit is this social content and community site that in some sense was never seen before and they were the first to do this because of how we got into why combinator with that like extra attention from Paul we got extra attention in the batch as well for better for work and so he was really putting a lot of pressure on me to get this thing launched he sent this email that said why haven't you launched yet is it because you can't or because you think it's not ready yet and I don't know which is worse and So I responded to him with a link I was like here it is here's Reddit and then he linked to Reddit from his blog which was a popular blog without telling me so that's how we launched I was just working on Reddit and all of a sudden the error log started scrolling really fast and so that's when we got our first users I thought it was a pretty basic website it had mainstream content it had some Niche content and it was a pretty good way uh for you to read about anything and engage in interesting conversations around the content the UI was very very simple Simplicity was part of the reason why I believe people gravitated to Reddit um because of the usability there were moments where we'd get a spike and then a significant number of users stuck around and it just kept growing the community was starting to kind of kick off we didn't think as a community at that point we just thought of it as being a kind of a neat product to service interesting content my name is Chris slow I'm the CTO and founding engineer at Reddit I first met Steven Alexis back in June of 2005 Steve and Alexis moved into Chris's apartment and threw themselves into work at the time I was also going through grad school so I was in my fifth year of my PhD at Harvard uh doing experimental physics and so my schedule was a little bit more normal in that I would wake up at like 7:00 in the morning and go to lab and then work on my startup at night whereas um Steve was definitely kind of rotated into like kind of night hours and so I think they were going to bed a couple hours before I was getting up and so I remember it started off with a couple of Mornings in a row where I woke up and of course the first thing I did was check Reddit because everyone was checking Reddit at that point on the third day in a row where I woke up and uh Reddit was actually down kind of went over and knocked on Steve's door and it's was like hey hey Steve reddits down but the third day he was just fed up with it he just showed me how to restart it so technically I started working at Reddit before I actually started working at Reddit as the team immersed themselves in building Reddit they began to consider how best to organize content posted to the site one of the I think ideas that was important to us is that we not over categorize things and this is part of this kind of broader idea that we are not the editors we are not The Gatekeepers and the categorizing things by topic is one of the things that editors do and so we're like that's not what we do but what happened is as Reddit grew what started off as a cohesive uh Community started to fracture this fracturing led to a defining feature of Reddit the subreddit the first subreddit that we made I would say intentionally to have a different community on Reddit which is the point of subreddits was programming I was sad that the programming content was no longer the most popular content on Reddit because the politics was becoming more and more popular the front page was now mostly politics and I didn't like that it was like I was like bredd it was this fun place and now it's this angry place and so I made the politics subreddit so that the political content could live in its own place and then I waged this like uh campaign I'm going to regret saying this but I waged like the sock puppet campaign of me and lots of my Al anytime somebody posted politics outside of the politics subreddit I said this doesn't belong here put it in the politics subreddit at the time IID really wanted tags because I believe that was going to help the site grow faster CU you could have one post tagged to multiple things Steve was really adamant about communities and he definitely got this one right the concept of subred in generally is what allows Reddit to scale because people are diverse they have diverse interests The Wider our user bases audiences are and the more diverse they are the less appealing that single shared space is but as Reddit continued to scale 2006 brought growing pains and personal stress I get a call from the mom of my then girlfriend who was studying abroad she had had this horrible accident Fallen five six stories from her apartment she was in a coma I flew out there spent about a week with her and the family and then about two months later I get another call that my mom had just had a seizure they had brought her in to the hospital done a scan and they found a stage for gleo blastoma tumor so brain cancer terminal brain cancer and So within the first few months of building this company coming right out of college feeling invulnerable I am hit with this backtack trauma it was kind of a a little bit of a lonely time by the time we got into 2006 there were four of us working on Reddit uh it was me uh and Alexis the co-founders and then Aaron Schwarz and Chris slow but at the time uh Alexis was going through his mom was sick his mom had cancer and so Alexis was going through that Chris was still a full-time student at Harvard and so he was working evenings with us and at that point Aaron wasn't working on Reddit anymore and so for 2006 that left me basically working on Reddit by myself dayto day and most of what I was doing at that time was just trying to keep reddit online I was 20 I don't know what it was 21 22 kind of learning learning the hard way all the like systems engineering to keep a website of that scale online you know while the plane was flying meanwhile major tech companies like Google and Yahoo began taking notice expressing interest in a potential acquisition Yahoo in 2005 2006 was still a fairly massive player when Yahoo flew us out a few months in to starting Reddit and the executive had us in this meeting room and when he asked for our traffic and I gave it to him he just laughed and he said you're rounding error compared to Yahoo what are you doing here that comment ended up printed out and mounted in our bathroom as a little reminder of like it just said you're rounding her and I think that helped to actually if anything kind of galvanize us I mean I'm not going to say that we continue to build out a spite but the spite didn't hurt later that year Reddit received a call from another interested buyer one outside the tix spere that at first glance didn't seem like an obvious partner Kan ass came into the picture wanting to buy Reddit K Nas um they're a publisher and so most of their content is in magazines and they're probably looking for a way to transform their business to be a lot more internet friendly than publishing focused the pitch was actually that hey okay Reddit is this increasingly large presence of being able to effectively like redirect eyeballs and sling traffic right like we aggregate content and we rank content and we're able to show what is the current really hot things that are present on the web without any real other way to kind of get to it whereas K was effectively a it's a media platform right they generate content and so the pitch was effectively with our powers combined we can be unstoppable after a series of conversations cond Nast made an acquisition offer of $10 million barely a year old Reddit faced a crucible decision do you sell to an old media Empire or gamble on creating a part of the internet's future when Kan ass came into the picture wanting to buy Reddit I almost felt like that was an escape because I didn't know I didn't feel like that sit situation we were in was sustainable all I could see was like oh I'm just kind of holding this thing together by my fingernails the whole thing was just kind of wild right this is less than a year or a year roughly from when we had gotten into YC and we're talking about an acquisition like Steve was working like 16 hour days just kind of like continuously grinding I was doing the night shift effectively doing like grad school by day and then working on Reddit by night and then you know sleeping maybe 4 hours a night it was wild to think that like all that Blood Sweat and Tears had actually gone into like we can we can sell this we had $72,000 in the bank at this point it changes your perspective on things when probably six months later I'm meeting with this guy this bizdev guy from con and n and he says Hey we'd like to buy your company and that the price is $10 million I'm thinking my God I could make life-changing money my parents will not make this money their entire lives and I'm going to make it for 12 months worth of work and and get to let my know that her you know unflagging support and belief in me was worth it on top of it all you know at that point there wasn't really a monetization route for Reddit we didn't advertise we didn't have any real way to make money uh we're still also a fairly Lean Startup we thought that would give us the best chance at survival because resources were so scarce I had also thought more shallowly that this is what we were supposed to do those were the stories that I had read about during the bubble when I was in high school was people starting companies and then selling them and then leaving their acquire and starting another company and that's more or less what Paul and YC evangelized as well and so part of me felt like we were fulfilling the prophecy I remember having a conversation Paul and Jessica Jessica Livingston co-founder of Y combinator had me over to their house and they said listen we got to talk because at the time we weren't shipping a ton and this is like maybe q1 of 2006 and and they were like listen like if if things AR going great on the enging front you know and you've got this offer like you should sell this company and he and they just said they were like look this is a great opportunity this company is in distress like they're not shipping and if a software company's not shipping it's not good also frankly I I know I was just too naive to really understand what that like change of control could really look like in October 2006 K Nas's acquisition of Reddit closed Steve Alexis and Chris moved into the wired offices in San Francisco acquisition in some sense it it is a crucible moment because it is a milestone of some sorts somebody gets some level of liquidity and in many situations when that happens people's revealed preferences show up their stated preference might be that they want to focus on the vision of the company and when an acquisition happens and some people get a little bit of liquidity they their reveal preference might be you know I made X that's enough for my bank account and therefore I'm done versus following through on their vision and so it's it's a little dangerous if you don't have an aligned team on the vision both Steve and Alexis were still focused on their vision and so when they were acquired by K N they still continue to stick around and continue to build the the product for for a couple more years beyond the acquisition one of the mistakes of the acquisition was this idea of that advertising was transferable to Reddit and Reddit is a drastically different product than style.com or especially then it was more on the kind of like snarky techy nerdy kind of axis and so it was very hard to have the same sort of transferability of ads onto Reddit and so we weren't even quite breaking even with that modest staff in fact if anything the trick part was uh Hey server cost scale with users the property had matured to a level where they needed different DNA inside of C Nas to run with the business and it was hard to attract the type of talent that they were looking for when it was inside of K Nas in 2009 with the media industry under pressure and Reddit struggling for resources inside of onast Steve left to start the travels side hipmunk along with Adam Goldstein Alexis left to start a career in investing Chris however stayed on deciding to leave is another story that I've changed a lot over the years the and so I I'll tell you what I think was my reasoning was that's what I was supposed to do now there was also the fact that I wanted to start another company uh I was no longer an owner of Reddit I was an employee of K Nast working on Reddit we couldn't take on VC to grow like we couldn't take on Capital to invest in the growth engine and when the pro is is succeeding by every every kind of growth metric but not succeeding as a business that's kind of the medy startup spot right and so it's almost like we were kind of stuck in I us a joke we like we're kind of stuck in carbonite in a lost ditch effort to jump start the business inside of cond Nast a small but determined Reddit team decided to take their survival into their own hands we launched Reddit Gold in summer of 2010 but this was just about the time where I was considering uh departing Reddit and so I definitely had a little bit of a like eff it we're doing it live uh feel uh at that point in my career and and frankly it was out of a certain amount of desperation it was like okay if we can't sell ads uh or we can't sell enough ads and our self-service product is advertising product is still growing let's just see if we can sell subscriptions and the original pitch was actually kind of funny and that we didn't even offer anything back we were just like if you all go through with this we will find products to build you and so we got a fantastic first pass of of donations the point where we opened up a PayPal account and the reason for it was well New York summer hours people generally leave and kind of go to The Hamptons or wherever they go during the summer on Fridays pretty early and so if we launched it later on a Friday maybe no one would notice until Monday at which point we'd have a giant pile of cash we could say but but look it worked and so we kind of turned it on and I remember we were we went out to lunch and we were just sitting there the whole time on our phones reloading the PayPal box watching the counter just go up in a way that was like I think we raised more in the first day than we had made in ads for that month when the cond Nast Executives returned on Monday morning they were impressed if a little annoyed by the Reddit Gold Gambit but ultimately the subscription model couldn't generate enough revenue for cond Nas to justify keeping Reddit under its umbrella in 2011 in an unusual move cond Nest spun Reddit out into an independent company once again in general these these Acquisitions and spin outs are are rare um in my career I've probably seen a few of these but but they're very rare there are very few of these situations where a company gets acquired then they get spun out spinning it out a couple years years later really I think was an opportunity to like really take a proper series a really to kind of go through those like the the awkward middle years of a startup that we all have grown to love and hate of like you know everyone has to have their awkward series B otherwise you don't really realize exactly what the what the product is for and what the business is for in reddit's case that just happened to happen like close to 10 years after the founding of the company the spin-off from cond Nast allowed Reddit to fundraise and bring in new Talent including CEO yishan Wong who had previously LED engineering at PayPal and Facebook in 2014 Sam Alman LED Reddit series B which included an investment from Sequoia Alexis also joined the Reddit board bringing a co-founder's perspective back to the company but from 2012 to 2015 a series of crises plagued the company from toxic subreddits and harmful content to moderators in open Revolt the moderators had a lot of G es over safety the content policy or the lack of a Content policy some mod thought Reddit didn't do anything on safety and and and others thought Reddit was doing too much up until that point that reddit's content policy was basically we don't remove things the reasoning was the reasoning we had used since the beginning of Reddit which is we are not the editors we are not The Gatekeepers this is community run we did and still do believe in free speech and free expression but one of the lessons we were learning at that moment is it's very easy to take a kind of absolutist free speech position when you don't have any issues when you start to be confronted with like real issues then you have to start making hard decisions what do you do when the trolls start to win is really what was starting to happen I think what I've learned over my uh very long career RIT now is that 98% of people are actually good and funny and just want to be left alone to do their thing and just you know are pretty overall going to be good actors if you set up the situation to allow them to to be good actors that other 2% are just jerks and the game if there is a game is to make sure that that 2% of jerks doesn't dominate the discourse for the other 98% because they're going to try um and I think that you know the the the the story of the internet has been how those jerks have often times taken the narrative away and taken the story away these incidents beg the question what belongs on Reddit the platform was built on ideals of Free Speech but where do you draw a line what is the role of community moderators to take down harmful content and when does the company itself need to step in ever since the spin out of Reddit it was clear to the community that Reddit was going to grow up and become a business there was this friction between the company and the community the Reddit Community became increasingly big jurant towards Reddit the company and Reddit employees were such active users of Reddit they felt like they were not just employees of the company they were also Reddit community members and some of them were moderators and so the Reddit employees had difficulty delineating the community's wishes and the company's objectives it it was a unique problem that I don't think I've ever saw in any other company that I've worked with for the first half of 2015 Reddit kind of lurched from crisis to crisis and then we just got to the point in it was July of 2015 where Reddit was going through its basically biggest crisis to date which felt to me like the final crisis like this is going to kill the company the local cause was Reddit had terminated an employee who did mod relations so basically she was the main liaison with the moderators there was a well-loved community team member a a Reddit employee who was the main kind of coordinator with moderators who was let go and so that departure was like the match that Lit the Tinder Box of the community just being upset with we're changing the rules you fired our favorite person that was really just kind of the straw that broke the camel's back the users were in basically open revolt and then they had shut their subreddits down and so Reddit was basically offline at that moment and was the first time we had seen anything like that Reddit was like truly offline if it stayed offline for too long it would just be gone in July of 2015 Reddit went dark thousands of moderators made their subreddits private meaning no one could post and no one could comment the site came to a standstill this moment would come to be known as the great Reddit blackout of 2015 or gon but went from you know a couple of these little these little Sparks led to 50% of communities on Reddit blacking out calling for the departure of the CEO at the time and just like basically calling for people's heads full pitchforks and torches which again it's Reddit so pitch forks and torches are effectively one of the things that people love the most on Reddit just tough when they directed back at Reddit the friction just became worse and worse and you know if the company doesn't exist the community will also not exist Ellen po had replaced isan Wang a CEO after he resigned in 2014 she stepped down after a user petition called for her resignation just a week after the site went dark pow then published an oped in the Washington Post called the trolls are winning the battle for the internet and the challenges for isan and for Ellen was not that they they were not good as being a CEO of a company of the size of Reddit at the time it was they didn't have the founder moral authority to push back on the community so Sam called Steve I called Steve and we started to put the full court press on getting him to think very seriously about leaving hipmunk and coming back to Reddit I was super absorbed with hitmon and that was like really the only conversation i' had about Reddit in years and we were in the middle of an m&a process for hmun so we were trying to sell the company so I promised Adam I said I'm not going to leave Adam knew I was having conversations with right it as and and I was like Adam I'm not going to leave while we're in the middle of this process Steve faced a crucible decision six years out should he leave one company he'd founded to save the other literally every single person I knew called or texted or emailed and said what is going on you have to go back to Reddit and had a conversation with Michael cyel who's then and still now a very close friend of mine now is on reddit's board and he told me I should go back it was a long conversation I think it was the Saturday morning I called uh I called Sam and said hey like I'll I'll do this and called Adam and had to break that promise to him to not leave hmun during that time this was and and still is I think the hardest decision I've ever had to make is to break that promise to Adam and to hit monk my reasoning was Reddit is more important to the world than hitmon will ever be and it'd be a real shame if Reddit didn't live up to his potential or even worse died just a little over a week after reddit's biggest user Revolt Steve officially stepped back into the role of CEO Steve returning to redit was a crucible moment because he had the moral authority to state where the company began and end and where the community began and end so put some distance between the company's objectives and the community objectives and making sure that um when you're an employee you work for the company you're focused on completing the company objectives and the community may mostly be right but there are certain situations where they're not right and the company has to make the decision on when to make a call on that and I don't think that that was an easy line to draw there's a few things that I thought needed to be done that weren't being done the most important of which is Reddit needed a Content policy because that was kind of the beginning of Reddit starting to ban subreddits and content prior to that Reddit didn't really do that when I would talk to the team they were so torn on this they were like we we don't like like the way Reddit is being used there's some content on here and communities on here that are really bad we don't like that we hate that but we feel powerless to get rid of it because if we get rid of it we're going to break Reddit and so it was extremely demoralizing for the team what I told them is well like you're describing to me as like if we change we die but the situation is if we don't change we're dead like and it's like that's why I'm here is reddit's dying and we have to change we have no choice so I and the team wrote the first content policy the way I like talking about it to non internet people is it's kind of like Javit Center so imagine an infinite Convention Center where reddit's a place where you can come and host conventions and they all happen right next to each other now sometimes they show up in Pikachu costumes for the r/ pookemon community sometimes they show up for the Mets Community you are also implicitly validating those communities when you're the javet center and you're saying hey there's a bunch of people loving the Pokemon and then here's a bunch of people doing some really awful racist stuff and that's where I draw that line we did make one mistake there which is there's no such thing as a perfect line there's always gray area and you always need to be interpreting and kind of adapting and changing the policies and interpreting the policies and and kind of growing as the problems change but we got that first content policy out there and we started enforcing it pissed a lot of people off but I would say it made the vast majority of people feel a lot better that got us on the path to eventually like recovery and I just remember there was a change in about 2018 where we were kind of working our way down this list of communities that were extremely toxic and we banned another one and the response we got was like geez about time guys why what took you so long it was like this big shift all of a sudden now like the kind of like the good users could speak again and they were like we were waiting for you to finish cleaning that up when Steve came back he was undeterred he pressed forward he hired a new management team he redesigned the site he moved uh Reddit onto mobile and more importantly he built the business with Steve back at the helm and the crises abating Reddit turned its attention toward operating as a self- sustaining business finally seizing control of its own destiny one of the other major things in the last Epoch has been to try to convert Reddit the community social science project into Reddit the actual sustainable business that happens to run a community social science project it's not possible to run Reddit without money the revenue everybody knew sucked there just wasn't a whole lot of Revenue Reddit wasn't measuring users accurately in 2015 so we also had 12 million Dau but Reddit reported Maus and so they reported Maus in the like 200 millions or something which is like huge but advertisers really care about da and when I remember calculating it with our head of product at the time Alex he was like dude there's only 12 million users here so we're like that was the oh [ __ ] moment I like oh this platform is actually smaller than I think people think I'd say we started with the beginning of Reddit being a a real business was when Jen Wong joined us uh in 2018 when we first got to Reddit in 2018 my first priorities were to think about what business model we wanted to pursue to become a self-sustaining business my name is Jed Wong COO of Reddit I think we still believe there are a lot of different business models hiding within Reddit but we wanted to land on one that we could invest in because you can't pursue everything at the same time and one that would be reasonably quick in scale with users and we lended on Advertising because it keeps Reddit free and open which is really important Reddit really believes in the open internet and allowing you users to have access to information and we knew that that could scale really nicely as we grew users as we've seen with other businesses well we wanted to build an advertising business so there's lots of conversations inside the boardroom to make sure that when we build the advertising business we did not upset the Reddit Community because the community is the reason why the users are so sticky and they come back again and again Reddit faced another Crossroads could it scale an Advertising based business model without alienating its community and I wrote a post that said hey look we need to survive to survive we have to make money we have two ways of making money we can either raise money and be beholden to VCS forever or we can monetize ourselves with ads that's the choice and the users got it they got it it was important to us to build an advertising business that was in harmony with our values and that's what I spent a lot of my time thinking about is like okay you know how would we have an advertising business that our community you know who has strong feelings on lots of things was in harmony with the values of our community and so being a business and being clear about that to our community was more intentional than probably other companies we are an extremely Mission driven company but we are a company and a business and so having that dialogue with our community and users was important it had to be explicit we had to bring the community along with it though Reddit is a social product it's not really as social as many of the other social products right users have personas but they don't really have personal information we don't know who the users are we know what they like and so a lot of the work in building and scaling the product that we've had to figure out has been how do we explain to advertisers the value proposition of Reddit as distinct from the rest of the social space where you have maybe a much more direct line of like we know who this person is we know where they live we know where they went to high school we are an anonymous platform we've never changed that we've never forced login to get access to information even though other platforms do and that in um makes for a a rich login graph but we don't do that another part was allowing for all parts of the human experience Reddit covers adult NSFW we're very thoughtful about you know hey that content is part of the human experience it has to stay we don't run ads there that's very important to our advertisers we have human review of all subreddits make sure they're appropriate for ads so I think we found a way to allow for all parts of the human experience to persist on Reddit and yet have a really brand safe advertising business from a starting point of $15 million when Steve returned Reddit grew to $456 million in 2021 and $84 million in 2023 clearing the way for a successful IPO in March of 2024 red is always going to change our number one value is evolved we're always going to try to change for the right reasons now sometimes we get it wrong historically sometimes we haven't done a great job but what's really fun about working at Reddit today is we're doing a I think a much better job the quality level is much higher and we're really delivering on stuff that users have wanted for a long time RIT would not be a public company today without having scaled Revenue to the size that it is today I don't think the company would exist today if they had not made these changes post IPO Reddit continues to consider ways to iterate on its business model to support its user base and strengthen its Financial profile Reddit is one of the biggest content corpuses on the internet and for every topical area it has incredible expertise and insight that people want you know Sports mavens Beauty mavens you name it and some of that content is so good that it is the equivalent of a media company and so creating an opportunity for some of those people those creators those thought leaders who are sharing the thoughts to be able to earn money from that I think seems sensible that's just one business model of many that could be on Reddit it is our duty to be a self sufficient business because if you believe in our mission then we should endure we've been around 19 years another 19 years at least and the only way to do that and control your own fate is to control your economics and whether you can self-fund all the things that you want to do like our relationship with the moderators is a forever work in progress I think it's always important to first and foremost keep our eye on the prize and so for us that's our mission Community belonging empowerment for everyone in the world now more day-to-day we want Reddit to be great we love Reddit we all use Reddit we want to be proud of our work and so in any moment there's a lot of noise outside external noise Reddit should do this Reddit should do that Reddit shouldn't do this that was so stupid like what the CEO said in those moments we just have to tune most of it out today Reddit is home to thousands of communities endless conversations and authentic human connection for all its users and Reddit is today the 18th most visited website in the world and that's I think that's a pretty big deal I think that at every step along the way for Reddit we've thought how could this get any larger it's gotten larger and getting a chance to watch some of our very early decisions on how communities should be built have proven out to be highly scalable across several orders of magnitude of traffic and across that that kind of increase in in user base is just fantastic and humbling Reddit has been bigger than I ever thought it would be since August 2005 the fact that we have any users at all is sometimes a little bit of a surprise now what has surprised me truly surprised me about Reddit is we've always believed in the agency of people right the users create the communities the users submit the content they rank the content their conversations like are the community they are the community we've always believed it's like the people will do amazing things but what so fun is that what I didn't realize is people's capacity to do amazing things every day is like a 100x greater than what we thought of and we were optimistic people in 2005 but we just see people helping each other supporting each other being funny doing interesting things changing the world because that's just how people are but it's actually been even more good and powerful than we have I think than I ever imagined we've been through at Reddit a number of crises they have been challenging for me and the team I often in those moments remind the company that we've been through crises before and gotten through them you know this whatever this whatever is going on right now it's going to not going to last forever but we have to work our way out also there are times when things are relatively peaceful where I'll remind the company hey things have been peaceful before and then something happened so enjoy it do your work rest up appreciate this moment for what it is because there's something coming there always is and so I think try to be measured uh in those moments and just constantly remind people that we get to work on something really special and even if it's hard even if we're in a difficult moment at the end of the day we get to work on Reddit and that uh is something that we should never take for granted [Music] this has been Crucible moments a podcast from SE [Music] Capital Crucible moments is produced by the Epic stories and Vox creative podcast teams along with seoa capital special thanks to Steve Huffman Alexis Ohanian Chris slow Alfred Lynn and Jen Wong for sharing their stories incidental audio created by 11 Labs a seoa partner [Music]

========================================

--- Video 52 ---
Video ID: NvAxuCIBb-c
URL: https://www.youtube.com/watch?v=NvAxuCIBb-c
Title: Why Vlad Tenev and Tudor Achim of Harmonic Think AI Is About to Change Math—and Why It Matters
Published: 2024-09-24 09:00:19 UTC
Description:
Adding code to LLM training data is a known method of improving a model’s reasoning skills. But wouldn’t math, the basis of all reasoning, be even better? Up until recently, there just wasn’t enough usable data that describes mathematics to make this feasible.

A few years ago, Vlad Tenev (also founder of Robinhood) and Tudor Achim noticed the rise of the community around an esoteric programming language called Lean that was gaining traction among mathematicians. The combination of that and the past decade’s rise of autoregressive models capable of fast, flexible learning made them think the time was now and they founded Harmonic. Their mission is both lofty—mathematical superintelligence—and imminently practical, verifying all safety-critical software.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 - Introduction 
01:42 - Math is reasoning 
06:16 - Studying with the world's greatest living mathematician 
10:18 - What does the math community think of AI math? 
15:11 - Recursive self-improvement 
18:31 - What is Lean? 
21:05 - Why now? 
22:46 - Synthetic data is the fuel for the model 
27:29 - How fast will your model get better? 
29:45 - Exploring the frontiers of human knowledge
34:11 - Lightning round

Transcript Language: English (auto-generated)
last I checked there's a 43% chance that the next millennium prize will be solved by AI or like human with significant AI assist I I think I think that's an underestimate I mean we could be lucky and Larry Guth might be on the on the path to the remont hypothesis um which would be amazing but I I think that you know if if the next one is solved by a human it would probably have to be in the very near future and for sure the the next next one will will probably be significantly solved by [Music] AI we're excited to welcome Vlad and tutor to the show we've had the pleasure of knowing Vlad for many years at Sequoia but what many of you may not know about Vlad is that in addition to being founder and CEO of Robin Hood he's also an enormously talented mathematician Vlad and tutor have teamed up to create harmonic an AI research lab with the goal of pushing the frontiers of human knowledge specifically they hope to create mathematical super intelligence with the thesis that understanding math allows you to better understand and reason about much of the broader World we're excited to talk to tuther and Vlad about harmonic about the ingredients that go into creating mathematical super intelligence including synthetic data reinforcement learning and self-play and when AI will win the IMO or a millennium Prize or even solve the remon hypothesis all right Vlad tutor welcome to the show oh thanks for having us all right so you guys you have this core belief that math is reasoning and you have what might be a contrarian belief that the best way to train a model to perform well in math is to directly teach it Math versus allowing math to emerge as a property of scale which is what a lot of the other Foundation model companies are doing can you talk a bit about that core belief why do you need to teach the model directly math and and also maybe what does it mean that math is reasoning uh when we started the company we you know had a really big focus on math and maybe we can get to that later but you know if you look around at all fields of Science and Engineering uh well almost all Fields uh math is really at their foundation and math has essentially become the way that people understand the Universe um it's the way you model phenomena from black holes to atoms it's the way you design uh things in engineering and you know there's a couple reasons for that first of all it just happens to be the case that the universe is explainable by math so you can write down you know a fairly compact set of symbols that explain things but the other really important thing is that it's a way to build these you know shared cognitive theories that are very objective and clear and transparent and if you and I are discussing something that's you know rigorous we can write down a set of DED deductive rules we can under agree on what you know the ground rules are of whatever we're modeling and then from there we can derive uh you know conclusions um and so with humans what you see is that when people become very good at math they tend to be good at other quantitative areas in science and engineering and so our bet is that you know if you make a system that's really good at math you're probably going to see the same phenomenon where it's true it might not immediately write the world's best history essays but when you ask it to do something scientific or something in engineering it's just going to be really really good at that that's why we started with math and where where is that boundary between you know help me with my math homework and write a history essay there is some boundary that it's hard for math to cross what do you think the outer edges of what's possible with a model with sort of math at its core where are those outer edges I I'll give you sort of a the the non- AI perspective so I studied math and you know I was really good at math from when I was a little kid and I remember remember there were there were always like the seventh graders in math class that would raise raise their hands whenever the teacher would come up with something and it was always like a abstract thing like you know the uh side angle side of triangles there' be The Annoying kid that was like when are we ever going to use this and you know the teacher would kind of Mumble a little bit and they'd be like well you know like math just like you're probably not going to use it soon but it'll make you really good at at other things and and you know the other kids were always skeptical of that and I bought into it um and so I just kept taking more and more advanced math you know I went to Stanford and I majored in it then I went to grad school to do a math PhD and my belief was that okay if I just focus on math then I'm going to get really good at problem solving and you know business problems and other problems will be easy compared to you know solving these like really tough like abstract algebra problem sets that I was banging my head against the wall for 10 hours every week uh trying to do and I think it basically ended up being correct right it's like I I didn't really pay attention to anything else um I took maybe like one computer science class intro to to computer programming at Stanford and um you know 5 years later uh when I became an entrepreneur uh I found it really easy to to pick up code I found it really easy to like pick up contracts and you know of course I'm no lawyer but you could understand that stuff I mean The Logical underpinnings are relatively simple compared to abstract algebra and Analysis so I I think for for humans at least I I consider myself an example of like math transferring to other very monetizable things and I think for AI uh my intuition seems to suggest that it should be the same yeah and you already see a little bit of evidence of this you know at this point it's an Open Secret in the industry that you know code training on a lot of code data leads to much better performance on reasoning benchmarks so you can imagine what that'll look like when you have incredible math data sets that Encompass a lot more General types of reasoning yeah yeah that resonates the the idea that you know math kind of math teaches a human how to think critically how to think logically and that skill can be ported to a bunch of other domains it stands the reason that that would work in AI also um Vlad you casually mentioned that you studied a little bit of math and just for anybody who's not quite familiar with your background in math uh I believe you studied briefly under Terry to who is perhaps the world's greatest living mathematician yeah um and then one of the things you mentioned to us was that you you you still catch up with them every now and then you have lunch when you're in La that sort of thing so I'm curious when you have lunch with Terry to what do you guys talk about um do you give any do you give any stock tips no no I'm not allowed to do that yeah one one of the uh yeah one of the unfortunate things of being a public company CEO in the financial spaces I have lots of stock tips but I can't share any of them I have to like keep them in turn and I can't even use them basically can't trade anymore um which which is unfortunate because I love Trading but um yeah I I um so um to backtrack how I got to UCLA um I uh so Terry to is a professor at UCLA uh and I think what's really amazing about him is the breadth of the work so most mathematicians get like very deep into a pretty narrow domain and Terry can get very deep uh across like dozens of domains you know he's made contributions to number Theory combinatorics harmonic analysis uh applied math um he's a one of the leading lean contributors at this point I'm sure he's like formalizing his papers in lean and actually hopping on the uh the community zulip and uh like engaging with students um and then he has a very popular blog and I think the way that he's been able to do this is he he's just smarter than 99.999% of people probably even more than that so from a very early age it was very clear that that he was like on on another plane and I studied uh I did my math honors thesis in undergrad uh under this professor Larry Guth who's also really amazing I mean the um he actually had a a recent result that came out that was a groundbreaking result um uh in uh I want to say number Theory or some something about the reman hypothesis um but yeah this this result in non-aa AI math uh really was was quite something and uh you know he um he kind of suggested I I look at UCLA um I was like really interested in in in his field and I ended up uh I ended up going there and being fortunate to study under uh Professor to but uh I should be clear I am a Dropout and it's amazing that I can claim that after grad school but I I will claim Dropout status so I only did one year of uh of of UCLA so it was a intro to graduate level analysis uh uh Terry ta taught my my first year which was pretty amazing um and and one thing I remember was I was doing some reading with uh with Professor Tao and um he gave me this book and he signed it and I think he signed it because he wanted to make sure I would give it back to him when I was done reading it and uh little did he know that by signing it he guaranteed he would never get that book back and I I bring it up every time I see him I'm like hey you're not getting that book back it's on my shelf next to all my other autograph first editions what does the math Community think of AI math are people split are or do people think it's you know the path to the promised land and the way that we're going to solve remon and everything else I think it's split I I think it seems to be split and okay uh there's sort of like the younger mathematicians um I think are very like Pro Ai and and pro verification and and tools like lean and I think the the older folks are a little bit more skeptical so not surprising I think you see that in pretty much every field um I think that my my guess would be that this will evolve uh my mental model is uh something like chess where you know at first there will be perhaps a lengthy period of you know humans plus AI assist um and and that will lead to a lot of really good results but um over time I think the AI will get better and better and you know you look at chess right now and it's sort of like you know if there's a human assisting the AI the AI would be like annoyed of it they would just want to just delete all of the input because it would only make the results worse um so I'm not sure if we're going to get to that point I think humans will at some point um I mean they they'll need to guide uh the algorithms but I think the the kind of definition of what a mathematician will do will will fundamentally change I was talking to one of my friends who's a mathematician at MIT and I asked him you know when we when we were kind of first starting this um what do you think and this is a a a young Professor um like very excited about about the field I was like are you worried that you're kind of in a in a field that is going to fundamentally change he's like the the field of math has always changed back in the 18 unds mathematicians used to be kind of like in the Royal Court and they would be glorified calculators they would solve quadratic equations by hand and of course the they were worried that you know when computers and calculators came out the the job would no longer exist but uh mathematicians get to Define what math is so I'm sure uh at some point it'll be prompting and kind of guiding these AIS to solve problems and I think um yeah I think that's going to be very huge so even if an AI solves the remon hypothesis a human will always be in the loop because the humans kind of pose the remon hypothesis to begin with um yeah just h on that I think there's like like in the future you're going to have a lot of compute resources dedicated to math and the question will be like a very human one which is like by which procedure do humans decide where to like direct all that like reasoning Firepower and I think that's going to be like the job of mathematicians they have to choose what do we work on how do we interpret the results how do we interpret failures to find answers that kind of thing do you think an AI an AI math system can solve reman or where is the ceiling do you think I think that uh it should be able to solve it or or you know prove that it's undecidable that would also be an interesting result um yeah I I think the um if if think about like what a great mathematician uh like a Terry tow for instance is capable of doing um you know they're they're able to like synthesize lots of papers lots of Frontier results and learn from them in a way that the other top human mathematicians can and kind of find connections between these things and um sort of use them to create new and more complex theories uh I mean that that's exactly kind of how the system we're engineering work works and that's that's what computers are great at and uh AI models are great at synthesizing large amounts of information finding patterns uh recursively self-improving I think now on metaculus last I checked there's a 43% chance that the next millennium prize will be solved by AI or like human with significant AI assist and um I I think I think that's an underestimate I mean we could be lucky and Larry goth might be on the on the path to the reman hypothesis um which would be amazing but I I think that you know if if the next one is solved by a human it would probably have to be in the very near future and for sure the the next next one will will probably be significantly solved by AI one of the things that you said I want to hit on which is this idea of recursive self-improvement because it seems like in the world of AI there are are if if you were to draw a spectrum of human only to AI only and then human in the loop is sort of the Spectrum in the middle from lots of human little bit of AI to lots of AI a little bit of human one of the things that is interesting about harmonic at least the way I understand it is because of lean you can encapsulate math and code because of formal verification you can objectively determine whether things are right or wrong which means that you have an objective reward function that you can use with self-play to have very fast cycle times with reinforcement learning which means that the the progress of your model has a chance to be extremely fast because there are no humans in the loop with that with that recursive self-improvement like objective function is clearly defined you can do self-play to just make the model better and better and better and better and better which is not something that we see in a lot of domains of AI most domains of AI it's a lot Messier to kind of get the cycle time on Improvement um can you just talk through the system a bit a little bit of what I described like what feeds into your model what governs the rate at which it can get better because it seems like something that will'll be able to get better at a pretty quick rate yeah happy to cover that uh one point before going to that is just that I think the most interesting part about this is that you know there are other areas where recursive self-improvement can work for example again in those board games like Alpha go yep but I think what people don't realize well what a lot of people don't realize is that in these let's say you know perfectly observed zero some games you improve recursively just by playing against yourself but you hit an optimal strategy yes so at that point you no it doesn't matter what system you have it won't do better the most exciting part about math is that there is no upper bound so you're just going to keep putting compute in and it's going to keep getting better and there's no end to it and so when we talk about do we think AI can solve a remon hypothesis or get a millennium prize like those are very human milestones and I think the real question is like will it ever stop I mean because it clearly will get there and I think we're going to end up solving problems that are much much harder than three1 hypothesis which we haven't even conceived of yet because it's almost like Beyond us to write down such a hard problem but coming you guys ever seen that mcraft video of like the AI beating Minecraft in 20 seconds no no that sounds I think it's a good analogy it's like you know what Minecraft is how a human would play it and then the AI beating it in 20 seconds is just like incomprehensible yeah like you can't even kind of grock what's going on in the video feed yeah but I think if we just talk about you know how harmonic works you can just think of it as there's a collection of agents that are essentially trying to solve problems and it's true because we use lean we're able to check whether our answers are correct and thereby derive a variety of training signals that we use to improve the system but just to be clear you know the use of lean just lets you verify things lean doesn't itself tell you whether you're getting closer to the answer or whether you're getting smarter or not it's just telling you whether it's correct or not so there's a lot of open scientific challenges to making it get better quickly and can you just say a word about what lean is just in case people aren't familiar yeah totally uh lean is a just another programming language um a really great one created by Leo Deora um the best programming language we might all be writing lean or the AI might just be writing lean in the future um but the idea is that the mathematical statements are encoded in the type system of the language so just very simply you know you have functions in lean and the input types correspond to the assumptions of the mathematical theorem and the output type is the conclusion and the point of lean is that if you write a program that implements that function and it compiles that means you can derive the output type from the input type which in turn implies that you can conclude the conclusion from the assumptions so that's really the the fundament that's that's how you use lean for Math and I I think one thing that's super interesting about lean is if you look at Leo Deora the Creator who's at um Amazon AWS now he's not a mathematician and he wrote this as a software verification tool so he he has you know the belief that you know in the future software will be verified and the existing tools things like and Isabel which are kind of multi-decade old software verification tools are just not good you know and and they're frankly unusable uh the experience for a developer is is poor and so he wanted to create a better software verification tool and in the hopes of if you build something better more people will use it and we'll have better software which is a super Noble uh goal in in its own right um but then what he didn't realize was software verification uh all it is is just proving that software has certain properties and uh this thing became very popular in the math community and you had uh like thousands of mathematicians and math students building up an organic Library called mathlib which you can think of as kind of like the largest um math textbook open source it's on GitHub um and it's just like growing at a at a pretty fast clip and and like the usage of lean for math I think to some degree has surpassed anyone's expectations might be more than the usage for verified software at this point um and that might change as time uh goes forward and and with AI one of the questions we always ask is why now because reinforcement learning has existed for a long time math has existed for even longer and it seems like math has really hidden an inflection point you guys have chosen to start harmonic at this point in time why now oh I mean there there's two really good reasons why now um that we're excited about so uh first you know one one is just that the AI systems have gotten better in an interesting way so I I was actually talking with with a friend close friend about you know RL for theor improving back in 2015 in 2016 and one issue back then was that there was n even A Great Notion of an AI system that could predict something in an infinite action space so in go you know you can place a piece somewhere right it's like either a black or white piece but in math you can really do anything like you can just generate any like next step and so we didn't have great systems to do that so autoaggressive language models have gotten pretty good so that's one thing that makes it possible I talking on the time scale of like a decade here but that's really important and the other thing that's kind of crazy is that lean has gotten really good so if you had told a mathematician 20 years ago that that a large fraction of the field would be excited about formal methods in math they might have thought you were crazy because back then formal methods were really isolated to formal logic or certain types of you maybe graph Theory like if you guys have heard the Four Color theorem that was one big success for formal math but what's changed is that lean is so flexible and so exciting for people that they've contributed this thing called math lib so now there's a lot of body of knowledge that you can build on to prove things and so the combination of AI starting to like even be a possible fit for this problem plus lean working really well and lean 4 was only released officially in September 2023 so those two things happening together really made it the right time to attack this can you guys say a word about um data and specifically synthetic data and what it is that fuels the model that you guys are building yeah so synthetic data is the fuel for the model um there's a there's an amazing resource called math liit that open source Repository um so that's a lot of human written lean uh and it's written in a way that's very general and compact so they're really proving Advanced theorems right um it's not necessarily the best fit for problem solving uh and so as a result almost the only data you can use for this is synthetic dat you generate yourself because that original data is not very applicable so it's kind of a data poor regime compared to most areas of AI and so that process that I described where the agents themselves are trying to solve solve problems and thereby generate training signals that's the primary way in which you can get data and the other issue is that you know you have to progress through levels of intelligence so you're not necessarily proving the remon hypothesis up front you're proving really simple things but then you kind of amplify yourself recursively throughout the process turns out there's not as much math data on the internet as cat videos unfortunately not unfortunately not well yeah it's interesting though because you know there's the data wall that the foundation model you know general purpose Foundation model companies are running into and it's at this point they've exhausted you know what's available on the internet and if you can generate most of the data that's required to train that's kind of another advantage of having math at the core versus hoping for math as an emerging property of scale yeah and I think the data wall kind of manifests itself in two ways one is just like you said we're out of internet data yeah the other is uh the actual internet data quality that's out there uh you can think of that as providing kind of a ceiling to how smart these models can get because if you train on the cat videos and all the nice Wikipedia content and you know the the internet content um it's an open problem how to get something that's significantly smarter than that and so you do need to get into some kind of self-reinforcing self-play regime in our opinion to uh get to a point where you can surpass the the ability of human mathematicians and researchers at multiple tasks yeah um and so I think uh in many ways like the the path it's it's inevitable that it takes kind of the alpha go to Alpha zero approach and we learn how to make these models create the the vast majority of their data and have the the data actually increase in complexity as these models continue to to iterate I think the the great thing about math is there's a simple path to doing this you can you can basically measure the complexity of a math problem and and how difficult it is by uh how many lines of lean it takes to solve um so you can actually look at the complexity of a system and a lot of um a lot of like problems are solved by breaking it down into smaller chunks and actually solving those chunks and if if you kind of think about how that works uh the smaller chunks are then more manageable because there are sort of like fewer lines to to solve than than the big one so if you get really good at that um and then you get good at solving the chunks then you can kind of like train your model to do better and as as you kind of like keep uh turning the the gears on that the model gets better at solving incrementally harder and harder and more complex uh problems I think that works very well in math because it mirrors how we solve math uh like on on pen and paper yeah and we've been able to start with like simple axioms and build up like just giant complex structures maybe the remon hypothesis would be hundreds of pages if not more to solve uh for Last Theorem was I think 200 Pages very very complex um so uh yeah I I do think eventually you'll get to a level where you you'll be able to solve these things and uh the math is to some extent like the original synthetic data yeah what what determines the rate at which your model can get better uh the rate at which it can get better um well I think the the highest level one is energy so the more energy you can put into it the more attempts can happen in parallel which means you generate data faster um so I mean there there's no rate limiting step I mean sorry there's a bunch of rate limiting steps but there's no like fundamental constraint on how far how fast it gets better um so it's really just about how much computer you put in I think it's also I mean there there's still a lot of Unsolved problems in this field right like uh you we benefit a lot from core theorems that have been proved in the past and you know there's if you think about like competition math context there's theorems that every student would just learn and and use like you know amgm inequality uh things of that nature and and so to some degree um like math lib is incomplete there's very little uh content about geometry for instance and it it's like very theoretical and Abstract um and so a limiting step is like what's in math lib and of course at some point the models have to solve the problem of like creating new theories and new structures and kind of like expanding to other domains and getting really really good at formalizing things that haven't been formalized by humans I think that'll be a big unlock and that'll certainly happen within the next several several years you'll be able to say hey here's just like this situation it could be as simple as like a baseball team and they're like throwing balls back and forth to each other and you know you systems would be able to kind of like autof formalize that and turn that into lean code on the Fly um and I I don't think we're we're quite there yet to the point where that's reliable but when it does does get reliable um I think I think that'll be a really big unlock if everything goes right what do you what do you think harmonic becomes well our mission statement is to explore the frontiers of human knowledge so uh it's very important to us that you know the the things we produce are correct and useful to humans so I think in the best case you know we're able to build a tool that a lot of mathematicians use to close all the Millennium priz Problems and to go far beyond that um I think that'll be a great you know service to humanity um there's also other areas of commercial application for the software so you know the dream for software engineering is to be able to just check that code is correct um to do that you need to have a very good model of how code works you need to be able to understand how the libraries work what they promise that kind of thing and so you can imagine a future where safety critical software is proven correct General software is proven correct and the way software engineering done is done can change as well um so there is like a lot of applications if you can make a system that's very good at math reasoning and very good at checking its reasoning yeah really we think there's a lot of applications yeah and I think the uh I mean math and software are two fairly obvious ones I think software engineering uh as a discipline is changing really quickly I'm sure you guys are seeing all the reports of people doing crazy things with cursor and you know Claude 3.5 um I think in the future software engineering will be less about like reviewing and collaborating on code as an artifact and we'll be more about collaborating on specs what do we want the code to do can we be more rigorous about that and I think that's where verification will will become a bigger thing because as the cost of software goes to zero the cost of verified software will also go to zero and suddenly this thing that was like very impractical and expensive because you need specialist humans to do it um Will Will just accelerate dramatically with AI so I think you look out 5 10 years from now the vast majority I mean if if we progress along the capability curve as we have been the vast majority of software written will be verified and provably correct um and I think that's a really exciting future I also think like on the more theoretical side it's not just math but like physics is um essentially math theoretical physics is uh you know one one of the main ways the frontier of math gets implemented and um I I think uh it would be amazing to me personally to accelerate some of the fundamental physics research at the frontier and really just develop an understanding for why the universe is the way it is why the laws of physics exist and um also help figure out experiments to test those um so I I I would be very proud if we contributed to that effort and do you think you'll mostly be contributing to math and math adjacent areas like physics and software engineering or do you think you know anything that involves reasoning uh is in spec for for harmonic yeah I mean I I think we we try to be focused on the things that uh we're still a small company over the over the long term I think if if you believe math is reasoning that and and we do uh then yeah if we get really really good at math and uh computer science is a very natural analog then um yeah I mean anything is in scope for for those models even the history essay I think we'll see to doesn't want to touch history essays yeah I I really enjoyed writing uh history essays even though my parents were like you know Humanities language arts just ignore all that stuff but you know I think my my math skills led me to write great history essays too so hopefully Aristotle will be no different one day Aristotle wrote some great historical uh commentary you are truly a polymath yeah I mean Poetics is if you've read Poetics it's uh should we wrap it up with some rapid fire let's do it lightning round okay what in what year will you win the IMO what do you think tutor soon all right 2025 soon maybe 2024 all right we'll sign you up for 2024 all right how about the Millennium prize ooh uh that's a tough one I would guess uh 2029 2029 yeah okay I heard 2028 is that is that what they're uh yeah I guess it's uh fully AI unassisted Millennium Prize or AI human hybrid well what do you how about what do you think for hybrid hybrid I could see 2028 are we talking on easy Millennium Prize or hard yeah is it like na easy millenum PR Stokes 2026 remon hypothesis I'll give you 2029 all right all right there we go good given we can't even do a rithmetic today with LMS uh that's pretty amazing uh when do you think we'll have human or superum level reasoning more broadly defined I think to some degree uh if if you define it as something that can reason and solve math problems in excess of any human like something that Terry to would you know would would give Terry tow a run for um for his money uh I think we're a couple of years away but I think the models within the next year will get to probably like 99.99th percentile would Terry agree with that I think so yeah I don't know you'd have to ask him but I I think he would agree with that one of our favorite questions is who in the world of AI do you admire most and we'll modify it slightly for you guys who in the world of AI or math do you admire most I like Von noyman we were just talking before about uh Von nyman's biography um I think I think what I find really interesting about him was he started as a mathematician and he was discouraged um I think his father who was like a Hungarian businessman was trying to discourage him from um doing math because it wasn't very like monetizable and so he got his friend who was a a great mathematician to try to talk him out of it but um the friend came back and he's like I can't do it this guy's too good it would be just a disservice to society if he didn't use his talents for for Math and then you know he pioneered computer science and the Von noyman machine was like the blueprint for all modern computers uh um he contributed to the Manhattan Project which you know is a little controversial but very very practical and impactful um and you know created probably the canonical text in Game Theory as well so um yeah I I think uh I think is pretty amazing also a fellow Eastern European well some people debate whether Hungary is in Eastern Europe yeah it's it's interesting question I think like I Def and Meer are like almost all scientists and mathematicians um but I think that like you know if you've heard of the mathematician livits um what was kind of shocking to learn during the course of working on this company is that you know so livess was competing with Newton to create calculus and you know Newton's formulation went out but liit was basically there one thing people don't know is that liet also had a lot of other work and one piece of work that is just incredibly preing keep in mind this is the late 1600s um he created this thing called an idea called the universal characteristic which is essentially the notion of having a deductive language uh automated procedure to deduce things using that language and a body an encyclopedia of work in that language that you can build on to derive things and so the amazing thing to me is that this thinker hundreds of years ago essentially predicted what would be happening in 2024 and it seems that the only thing that was required was having like AI get a little better and having like computers that can do something like lean right and and I think it's just incredible to have a human being predict that with no concept whatsoever of what's going to come later but to understand that that's like such a fundamental thing that we're going to end up working on that how use later awesome thank you guys thank you thanks for having us thanks for having us [Music] [Music]

========================================

--- Video 53 ---
Video ID: CGNZ2qa5rgs
URL: https://www.youtube.com/watch?v=CGNZ2qa5rgs
Title: How an Early Pivot Catalyzed an Open Source Movement - MongoDB on Crucible Moments
Published: 2024-09-17 22:53:07 UTC
Description:
MongoDB, founded in 2007, originally aimed to create a platform-as-a-service system with a new database layer. Facing competition from Google, the founders pivoted to focus solely on their database product, MongoDB—a new kind of database built for the scale of the internet era. Founder Dwight Merriman built a product that developers loved, but scaling the company proved challenging until Dev Ittycheria took the reins as CEO in 2014. As cloud computing grew, MongoDB transitioned from on-premise software to Atlas, a fully managed-cloud service. Despite initial skepticism, Atlas now represents 70% of MongoDB's revenue. As Atlas scaled, MongoDB faced another controversial decision: whether to change its open source license model to maintain its commercial moat. These pivotal decisions transformed MongoDB from a niche database to a nearly $2 billion in annual revenue.

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 54 ---
Video ID: yMGGpMyW_vw
URL: https://www.youtube.com/watch?v=yMGGpMyW_vw
Title: Jim Fan on Nvidia’s Embodied AI Lab and Jensen Huang’s Prediction that All Robots will be Autonomous
Published: 2024-09-17 09:01:00 UTC
Description:
AI researcher Jim Fan has had a charmed career. He was OpenAI’s first intern before he did his PhD at Stanford with “godmother of AI,” Fei-Fei Li. He graduated into a research scientist position at Nvidia and now leads its Embodied AI “GEAR” group. The lab’s current work spans foundation models for humanoid robots to agents for virtual worlds.

Jim describes a three-pronged data strategy for robotics, combining internet-scale data, simulation data and real world robot data. He believes that in the next few years it will be possible to create a “foundation agent” that can generalize across skills, embodiments and realities—both physical and virtual. He also supports Jensen Huang’s idea that “Everything that moves will eventually be autonomous.”

Hosted by: Stephanie Zhan and Sonya Huang, Sequoia Capital

00:00 Introduction
01:35 Jim’s journey to embodied intelligence
04:53 The GEAR Group
07:32 Three kinds of data for robotics
10:32 A GPT-3 moment for robotics
16:05 Choosing the humanoid robot form factor
19:37 Specialized generalists
21:59 GR00T gets its own chip
23:35 Eureka and Issac Sim
25:23 Why now for robotics?
28:53 Exploring virtual worlds
36:28 Implications for games
39:13 Is the virtual world in service of the physical world?
42:10 Alternative architectures to Transformers
44:15 Lightning round

Transcript Language: English (auto-generated)
so from uh the the chip level which is the J and Thor family to the foundation model uh project Gro and also to the to the simulation and the utilities that we build along the way it will become a platform a Computing platform for humanoid robots and then also for intelligent robots in general so I want to quote Jensen here um one of my favorite quotes from him is that uh everything that moves will eventually be autonomous and I believe in that as well it's not right now but let's say um 10 years or or more from from now if we believe that there will be as many intelligent robots as iPhones then we better start building that [Music] today hi and welcome to training data we have with us today Jim fan senior research scientist at Nvidia Jim leads nvidia's embodied AI agent research with a dual mandate spanning robotics in the physical world and gameplay agents in the virtual world Jim's group is responsible for project Groot nvidia's humanoid robots that you may have seen on stage with Jensen at this here's GTC we're excited to ask Jim about all things robotics why now why humanoids and what's required to unlock a gpt3 moment for robotics welcome to training data thank you for having me we're so excited to dig in today and learn about everything you have to share with with us around Robotics and embodied AI before we get there you have a fascinating personal story I think you were the first open the first intern at open AI maybe walk us through some of your personal story and how you got to where you are absolutely I would love to share share the stories with the audience so um back in the summer of 2016 um some of my friends said there is a new startup in town and you should check it out and I'm like huh I don't have anything else to do because I uh got accepted to um PhD and that summer I was Idol so I decided to join this startup and that turned out to be openai um and during my time at open aai uh we were already talking about AGI back in 2016 um and back then my inter Mentor was Andre Kari and Elia Sev and we talk about and we discuss a project together it's called world of bits so the idea is is very simple we want to build an AI agent that can read computer screens read the pixels from the screens and then control the keyboard and mouse um if you think about it this interface is as general as it can get right like all the things that we do on a computer like you know replying to emails or playing games or browsing the web it can all be done in this interface mapping pixels to keyboard mouse control so that was actually uh my first kind of uh attempt at AGI at open AI uh and also my first journey the first chapter of my journey in AI agents I remember world of BS actually I didn't know that you were a part of that uh that's really interesting yeah yeah it was it was a very fun U project and was part of a bigger initiative called OB Universe yeah uh which was a like a bigger um platform on like uh integrating all the applications and games into into this framework what do you think were some of the unlocks then and then also what do you think were some of the challenges that you had with agents back then yeah yes so back then the main method that we used was reinforcement learning uh there was no l no Transformer back in 2016 uh and the thing is reinforcement learning it works on specific tasks but it doesn't generalize like we can't give the Asian arbitrary language and instruct it to do uh things to do arbitrary things that we can do with a keyboard and mouse so back then uh it kind of worked on the task that we designed but it doesn't really generalize um so um you know that started uh like my next chapter uh which is um I went to Stanford and I started my PhD with Professor fa Lee and we started working on computer vision and also embodied Ai and during my time at Stanford which was from 2016 to 2021 uh I kind of witnessed the transition of the Stanford Vision lab led by faay uh from you know static computer vision like recognizing images and videos to more embodied computer vision where an agent learns perception and takes actions in an inter interactive environment and this environment can be virtual as in simulation or it can be the physical world um so that was um my PhD like transitioning to embodied AI um and then after I graduated from PhD I joined Nvidia and have stayed there ever since so I carried over my work from from my PhD thesis to Nvidia and still working on embodied AI till this day so you oversee the embodied AI initiative at Nvidia maybe say a word on what that means and what you all are hoping to accomplish yes so uh the team that I am co-leading right now is called gear uh which g a r and that stands for journalist embodied agent research and to summarize what our team works on in three words is that we generate actions um because we build in body air agents and those agents take actions in different worlds and if the actions are taken in the virtual world that would be gaming Ai and simulation and if the actions are taken in the physical world that will be robotics actually uh earlier this year in March uh GTC at Jensen's keynote he unveiled something called project Groot uh which is nvidia's moonshot effort at building Foundation models for humano Robotics and that's basically what the gear team is focusing on right now we want to build the AI brain for humanoid robots and even beyond what do you think is Invidia is competitive advantage in building that yeah that's a great question so um well one is for for sure like compute uh resources all of these Foundation models require a lot of compute to scale up and we do believe in scaling law um there were scaling laws for like L um but the scaling law for embodied Ai and Robotics are yet to be studied so we're working on that and the second strength of Nvidia is actually simulation um so Nvidia before it was an AI company it was a Graphics Company so Nvidia has a has many years of expertise on building simulation like physics simulation and rendering and also real time acceleration um on on gpus so we are using simulation heavily in our approach to build robotics the simulation strategy is super interesting why do you think most of the industry is still very focused on real world data the opposite strategy yeah I think um we need all kinds of data and simulation and real world data by themselves are not enough so uh at gear we divide this data strategy into roughly three buckets one is the internet scale data like all the text and videos um online and the second is simulation data where we use uh Nvidia simulation tools to generate lots of synthetic data and the third is the real robot data where we collect the data by uh teleoperating the robot and then just collecting and recording those data on the robot platforms and I believe a successful robotic strategy will involve uh the effective use of all three kinds of data and mixing them and also uh delivering a unified solution can you say more about um we were talking earlier about you know how data is fundamentally the key bottleneck in making a robotics Foundation model actually work can you say more about your kind of conviction uh in that idea and then like what exactly does it take to make you know great data to to kind of break through this problem yes so um I think like the three different kinds of data that I just mentioned have different strengths and weaknesses so for internet data they are the most diverse they encode a lot of common sense prior right like for example um all the most of the videos online are human- centered uh because humans uh we we love to take selfies we love to record each other doing all kinds of activities and there are also a lot of instructional videos online so we can use that to kind of learn how humans interact with objects and how objects behave under different uh situations so that kind of provides a common sense prior for the robot Foundation model um but the internet scale data they don't come with actions we cannot download the motor control signals of the robots from the internet and that goes to the second part of the data strategy which is using simulation so in simulation uh you can have all the actions and you can also observe the consequences of the actions in that particular environment um and the strength of simulation is that it's basically infinite data and um you know the data scales with uh compute the more gpus you put into the simulation pipeline the more data that you will get and also the data is super real time so if you collect data only on the real robot then you are limited by 24 hours per day but uh in simulation like the GPU accelerated uh simulators we can actually accelerate Real Time by 10,000x so we can collect the data at much higher throughput given the same walk clock time so that's the strength but the weakness is that uh for simulation uh no matter how good the graphics pipeline is there will always be uh this similation to reality Gap like the physics will be different from The Real World then the the the visuals will still be different they will not look uh exactly as realistic as real world and also there is a diversity issue like the contents in the simulation will not be as diverse as all the scenarios that we encounter in the real world so these are the are the weaknesses and then uh going to the real robot data um and those data they don't have the Sim to real Gap because they're collected on the real robot but it's much more expensive to collect because you need to hire people to operate the robots and again they're limited by the speed of the world of atoms you only have 24 hours per day and you need humans um to to collect those data which is also very expensive um so we see these three types of data as having compliment strengths and I think a successful strategy is to combine their strength uh and then you know to remove their weaknesses so the cute Groot robots that were on stage with Jensen um that was such a cool moment um if you had tas's dream in one five 10 years like what what do you think uh your group will have accomplished yeah so uh this is pure speculation but I hope that we can see uh research breakthrough in robot Foundation model maybe in the next two to three years um so that's what we call like a gpt3 moment for robotics um and then uh after that it's a bit uncertain because to have the robots uh enter like daily lives of people there are a lot more things than just the technical side um the robots need to be affordable and mass produced and we also need like uh safety uh for the hardware and also privacy and regulations and those those will take longer for the robots to be able to hit a mass market so that's a bit harder to predict but I do hope that the research breakthrough will come in the next two to three years what do you think we'll Define what a gp3 moment in AI robotics looks like yeah that's a great question so um I would like to think about robotics as consisting of two systems system one and system two so uh that comes from the book Thinking Fast and Slow uh where system one means um this um low-level motor control that's unconscious and fast uh like for example when I'm grasping this cup of water I don't really think about how I move the fingertip tip at every millisecond um so that would be system one and then system two is slow and deliberate and it's more like reasoning and planning that actually uses the the the conscious brain power that we have so um I think the gbd3 moment uh will be on the system one side and my favorite example is the verb open so just think about the complexity of the word open right like opening the door is different from opening window it's also different from opening a bottle or opening your phone but for humans we have no trouble understanding that open means different things when you're interacting uh it means different motions when you're interacting with different objects but so far we have not seen a robotics model that can generalize on a lowlevel uh motor control on these verbs so I hope to see a model that can understand these verbs in their abstract sense and can generalize to all kinds of scenarios that make sense to humans um and we haven't seen that yet but I'm hopeful that this moment could come in the next two to three years what about system two thinking like how do you think we get there uh do you think that some of you know the reasoning efforts in the llm world uh will be relevant uh as well in the robotics world yeah absolutely uh I think as for system 2 we have already seen very strong models that can do uh reasoning and planning and also coding as well so these are the L and Frontier models uh that that we have already seen these days um but to integrate the syst system two models with system one is another research challenge in itself so uh the question is for robot Foundation model do we have a single monolithic model or do we have some kind of cascaded approach where the system two and the system One models are separate and and can communicate with each other in some ways I think that's an open question um and again they have pros and cons right like for the first idea the monolithic model uh is cleaner there's just one model one API to maintain um but also it's a bit harder to kind of control because you have different control frequencies like the system two models will operate on a slower control frequency uh let's say one Hertz like one decision per second while the system one like the uh motor control of me grasping this cup of water that will likely be a th000 Hertz where I need to make these minor like these tiny muscle decisions at thousand times per second it's really hard to encode them both in a single model so maybe a cascaded approach will be better but again how do we communicate between system one and two do they communicate through text or through some latent variables it's unclear and I think um it's a very exciting new research direction is your instinct that we'll get there in that breakthrough on system one thinking like through scale and Transformers like what is going to work um or is it you know cross your fingers and hope and see I I I certainly hope that uh the data strategy I described will kind of get us there because because I feel that we have not pushed the limit of Transformers yet um on the essential level like Transformers take tokens in and outputs tokens and ultimately the quality of the tokens determines the quality of the model the quality of those large Transformers and for robotics as I as I mentioned like the data strategy is very complex we have all the internet data and also we need simulation data and the real robot data and once we're able to scale up on the data Pipeline with all those high quality actions then we can tokenize them and we can send them to a Transformer to compress um so I feel we have not pushed Transformer to limit yet uh and once we figure out the data strategy we may be able to see some emerging property as we scale up the data and scale up the model size and for that I'm calling it the scaling law for embodied Ai and it's just getting started I'm very optimistic that we will get there I'm curious to hear what are you most excited about personally when we do get there what's the industry or application or use case that you're really excited to see this completely transform the world of Robotics today yes so uh there are actually a few reasons that we chose humanoid robots as kind of the main research CES to tackle uh one reason is and the world is built around the human embodiment the human form factor right all our restaurants factories hospitals and all equipments and tools they're designed for uh the human form and also the the human hands so uh in principle uh a sufficiently good humanoid Hardware should be able to support any task that a reasonable human can do in principle and the humanoid Hardware is not there yet today but I feel in the next two to three years the humanoid Hardware ecosystem will mature and we will have affordable humanoid Hardware to work on and then it will be a problem about AI brain about how we can drive those humanoid hardware and once we have that once we're able to have uh the grou Foundation model that can take any instruction in language and then perform any task that a reasonable human can do then we unlock a lot of economic value like we can have um robots uh in our in our households uh helping us with daily chores like laundry and dishwashing and cooking or like elderly care and we also have them in restaurants in hospitals in factories helping with all the uh all the tasks that that humans do um and um I I hope that will come in the next decade but again as I mentioned in the beginning this is not just a technical problem but also there are many uh things beyond the technology so um I'm looking forward to that any other reasons you've chosen to go after humanoid robots specifically yeah so uh there also a bit more practical reasons uh in terms of the the the training pipeline um so there are lots of data online um about humans right it's all like human- centered all the all the videos all like humans doing daily tasks all having fun uh and the humanoid uh robot form factor is closest to the human form factor which means that the model that we train using all of those data uh will be able to have uh an easier time to transfer to the humanoid form factor other than rather than the other form factors so let's say for robot arms right like how many videos do we see online about robot arms and grippers very few but there are many videos of people using their five finger hands right to work with objects so it might be easier to train for humano robots and then once we have that uh we'll be able to specialize them to like the robot arms and more kind of specific robot forms so that's why we're aiming for the full generality first I didn't realize it so are you exclusively training on humanoids today versus robot arms and and um robot dogs as well yeah so um for project gr simulation yeah yes so for project Groot we are aiming more towards humanoid right now um but the pipeline that we're building including the simulation tools right the the the real robot tools like those are General purposing enough that we can also adapt to other Platforms in the future so yeah we're building these uh tools to be generally applicable yeah you've used term General quite a few times now um I think there are some folks from especially from the robotics world who think that you know a general approach won't work and you have to be domain environment specific um why have you chosen to go after a general approach and um you know the Richard Sutton bit bitter lesson stuff has been a recurring theme on our podcast I'm curious if you think it holds in robotics as well absolutely so um I would like to first talk about the success story um in NLP that we have all seen right so before uh the chat GPT and the gpt3 uh in the world of NLP there are a lot of kind of different models and pipelines for different applications like translation and coding and you know doing math and um doing like creative writing like they all use very different models and uh completely different training pipelines but then chat gbt came and unified everything into a single model so before chat gbt we call those specialist and then the the GB3 and chat gbts we call them the journalist and once we have the journalist we can prompt them distill them and fine-tune them back to the specialized tasks and we call those the specialized journalist and according to the historical Trend it's almost always the case that the specialized journalists are just far stronger than the original Specialists and they're also much easier to maintain because you have a single API that takes tax in and then spits text out so I think we can follow the same success story from uh the world of NLP and it will be the same for robotics so right now in 2024 most of the Robotics and applications we have seen are still in the specialist stage right they have uh specific robot Hardware uh for specific tasks collecting specific data using specific pipelines but project Groot aims to build this general purpose Foundation model that works on humanoid first but later will generalize to all kinds of different robot forms or embodiments and that will be the jist moment that we are pursuing and then once we have that journalist we'll be able to prompt it fine-tune it distill it down to specific robotics tasks and those are the specialized journalist but that will only happen after we have the journalist so um it will be easier in the short run to pursue the specialist it's just easier to show results because you can just focus on uh a very narrow set of tasks but we at Nvidia believe that the future belongs to journalist even though it will take longer to develop it will have more uh difficult research problems to solve but that's what we're aiming for first the interesting thing about Nvidia building grou to me is also what you mentioned earlier which is that Nvidia owns both the chip and the model itself what do you think are some of the interesting things that Nvidia could do to optimize um Groot uh on its own chip yes so um at the March GTC uh Jensen also unveiled the next generation of the edge Computing chips it's called The jesson Sword chip and it was actually co- announced with project Groot so the idea is we will have kind of the full stack as a unified solution to the customers so from uh the the chip level which is the J and Thor family to the foundation model uh project Gro and also to the to the simulation and the utilities that we build along the way it will become a platform a Computing platform for human robots and then also for intelligent robots in general so I want to quote Jensen here um one of my favorite quotes from him is that uh everything that moves will eventually be autonomous and I believe in that as well it's not right now but let's say um 10 years or or more from from now if we believe that there will be as many intelligent robots as iPhones then we'd better start building that today that's awesome are there any particular results from your research so far that you want to highlight anything that gives you kind of optimism or conviction in in the approach that you're taking yes we can um talk about some prior works that we have done so uh one work uh that um I was uh really uh happy about was called urea and uh for this work uh we did a demo where we trained a FiveFinger robot hand to do pen spinning so um very useful and it's super human with respect to myself because um I have given up pen pen spinning long since childhood I'm not able to do it live demo I will feel miserably at this live demo um so yeah not able to do this but but the robot hen uh is able to and the idea that we use to train this is that uh we prompt an L uh to write code in the simulator API that uh Nvidia has built so it's called The is6 Sim API and um the L outputs uh the code for reward function so a reward function is basically a specification of the desirable behavior that we want the robot to do so the robot will be rewarded uh if it's on the right track or penalized if it's doing something wrong so that's a reward function uh and typically the reward function is engineered U by a human expert uh typically a roboticist who really knows about the API it takes a lot of specialized knowledge and the reward function engineering is by itself a very tedious and manual task so what ureka did was we designed this uh algorithm that uses L to automate this reward function design so that the reward function can instruct the robot to do very complex things like pen spinning so it is a general purpose technique that we developed and we do plan to scale this up uh to Beyond just pen spinning um it's uh it should be able to design reward functions for all kinds of tasks or it can even generate new tasks right using the Nvidia simulation API so that gives us a lot of kind of space to grow why do you think uh I mean I remember 5 years ago there were people that were you know research was working on solving Rubik's cubes with a robot hand and things like that and it felt like robotics kind of went through uh maybe a trough of disillusionment and in the last year or so it feels like the space has really heated up again do you think there is a why now around robotics this time around and like what's different and we're you know we're reading that Open the Eyes is getting back into robotics everybody everybody is now spinning up their efforts like what do you think is different now yeah uh I think there are quite a few key factors that are different now uh one is on the robot Hardware actually since the end of last year we have seen a surge of new robot Hardware in the ecosystem uh there are companies like uh Tesla working on Optimus Boston Dynamics and so on and a lot of startups as well so we are seeing better and better Hardware so that's number one uh and those Hardwares are becoming more and more capable with like you know better Dex Source hands better uh whole body reliability um and the second factor is uh the pricing so we also see um a significant drop in the in the price uh and the cost the manufacturing cost for the human robots so back in 2001 uh NASA had a humanoid developed and it's called a robot knot uh I remember if I recall correctly it cost north of $1.5 million per robot and then most recently there are companies that are able to um put a price tag of about $30,000 on a full-fledged humanoid and that's rough roughly comparable to the price of a car yeah and also there's always this trend in manufacturing where a mature product uh the price of it will tend towards the price of the raw material cost and for the humanoids um it typically takes only 4% of the raw material of a car so it's possible that we can see the cost trending downwards even more and there could be an exponential decrease in the price in the next couple of years and that makes you know these stateof Ard Hardware more and more affordable that's the second factor of why I think humanoid is gathering momentum and the third one is on the foundation model side right we are able to see the system to Problem the the the reasoning the planning part being addressed very well by the frontier models uh like the the gpts and the clouds and the llamas of the world um and these LS they're able to generalize to new scenarios they're able to write code and actually the urea project I just mentioned uh leverages these coding abilities of the L to help develop new robot Solutions um and there are also a surge in multimodal models improving the computer vision the perception of it so uh I think these successes uh also encourage us to pursue robot Foundation models um because uh we think we can write on the generalizability of these Frontier models and then uh add like actions on top of them um so we can generate action tokens that will ultimately Drive these uh humanoid robots I completely agree with all that I also think so much of what we've been trying to tackle to date in the field has been how to unlock the scale of data that you need to build this model and all the research advancements that we've made many of which you've contributed to yourself around Sim toore and and other things and the tools that Nvidia is built with Isaac Sim and others have really accelerated the field alongside teleoperation and cheaper teleoperation devices and things like that and so I think it's a really really exciting time to be building here I agree yeah I'd love to transition to talking about Virtual Worlds if that's okay with you yeah absolutely yeah um so I think you started your research uh more in the Virtual World Arena um maybe say a word on what got you interested in Minecraft and versus robotics like is it all kind of related in uh in your in your world um what got you interested in Virtual Worlds yeah that's a great question so for me my personal mission is to solve embodied Ai and for AI Asians embodied in the virtual world that will be things like gaming and simulation and that's why I also have a very soft spot uh for for gaming I also enjoy gaming myself um what did you play yeah so um I I play Minecraft I I try to I'm not a very good gamer um and that's why I also want my AI uh to avenge my poor skills yeah so um I work on a few uh gaming projects before uh the first one was called Mind Dojo uh where we uh develop a platform to develop general purpose agents in the game of Minecraft um and for those audience who are not familiar Minecraft is this 3 voxo world where you can do whatever you want uh you can craft like all kinds of recipes you know different tools and you can uh also go on adventures it's an open-ended game with no particular score to maximize and no um uh fixed story lines to follow so uh we collected a lot of uh data from the internet um there are videos of people playing Minecraft there are also Wiki pages that explain every concept and every mechanism in the game those are like multimodal documents uh and also like forums like Reddit the Minecraft subreddit uh has a lot of people talking about the game uh in natural language so we collected these multimodal data sets and we're able to train models to play Minecraft so that was the first work mind Dojo um and later the second work uh was called Voyager so um we had the idea of Voyager after gp4 came along because that at that time it was the best coding model out there um so we thought about hey what if we use coding as action and building on that inside we're able to develop the Voyager agent where it writes code to interact with the Minecraft world so we we use an API to First convert the 3D Minecraft world into a text representation and then have the Asian right code um using the action apis but just like human developers the agent is not always able to write a code correctly on the first try so we kind of give it a self flection Loop where it tries out something and if it runs into an error or if it makes some mistakes in the Minecraft world it gets the feedback and it can correct its program and once it's written the correct program that's what we call skill we'll have it save to a skill Library so that in the future if the agent faces a similar situation it doesn't have to go through that trial and error loop again it can retrieve the skill from the skill Library so you can think of that skill Library as a code base that L interactively authored all by itself right there's no human intervention the whole code base is developed by by Voyager so that's a second mechanism the skill library and the third one is what we call an automated curriculum so basically the agent knows what it knows and it knows what it doesn't know so it's able to propose the next task that's neither too difficult nor too easy for it to solve and then it's able to just follow uh that path and discover all all kinds of different skills uh different tools and also travel along in a vast world of Minecraft and because they travel so much and that's why we call it the Voyager um so yeah that was uh kind of our team's um one of our earliest attempts um building AI agents in the embodied World using Foundation models talk about the curriculum thing more I think that's really interesting because it feels like it's one of the more unsolved problems in kind of the reasoning and llm World generally like how do you make these models self-aware so that they know kind of how to take that next step to improve maybe maybe take say a little bit more about what you built on on on the curriculum and the reasoning side absolutely um I think there's a very interesting emerging property from those Frontier models is that they can reflect on their own actions and they kind of know what what they know and what they don't know and they're able to propose tasks accordingly so for the automated curriculum in Voyager we gave the agent a high level directive that is to find as many novel items as possible uh and that's just the one kind of sentence of goal that we gave and we didn't give any instruction on uh which objects to discover first which tools to unlock first we didn't spec uh we didn't specify and the agent was able to discover that all by itself uh using this kind of coding and prompting and skill Library um so it's kind of amazing that the whole system just works it's uh I would say it's an emerging property once you have a very strong reasoning engine that can generalize why do you think there are so many so much of the kind of virtual world research has been done in the virtual world and I'm sure it's not entirely because a lot of deep learning researchers like playing video games although I'm sure it doesn't hurt either um but what I guess what are the connections between solving stuff in the virtual world and in the physical world and how how do the two interplay yeah so um as different as gaming and Robotics seem to be uh I just see a lot of U similar principles shared across these two domains um for the embodied agents um they take as input the perception which can be a video stream along with some sensory input and then they output actions and in the case of gaming it will be uh like keyboard and mouse actions and for robotics it will be lowlevel motor controls so ultimately the API looks like this um and uh these agents they need to explore in the world they um have to collect their own data in some ways so uh that's uh what we call en forcement learning and also self-exploration and that part that principle is again shared among uh the physical agents and the virtual agents um but the difference is robotics is harder because you also have a simulation to reality Gap to bridge because in simulation uh the physics and the rendering will never be perfect um so it's really hard to kind of transfer what your learning simulation to the real world um and that is by itself an open-ended research problem so for robotics uh it's got the Sim to real issue but for gaming it doesn't you are training and testing in the same environment so I would say that would be the difference between them and um last year I proposed a concept called Foundation agent where I believe ultimately we will have one model that can work on both you know virtual agents and also physical agents so for the foundation agent uh there are three axis over which it will generalize uh number one is the skills that it can do number two is the embodiments or like the body form the form factor it can control and number three is the world the realities it can master so in the future I think a single model will be able to do a lot of different skills on a lot of different robot forms or agent forms and then generalize across many different worlds virtual or real and that's the ultimate Vision that the gear team wants to pursue the foundation AG pulling down the threat of of virtual worlds and gaming in particular and and what you've unlocked already with some reasoning some emerging Behavior especially working in an open-ended environment what are your what are some of your own personal dreams for what is now possible in the world of games where would you like to see AI agents innovate in the world of games today yes so um I'm very excited by two aspects one is uh intelligent agents inside the games so um the NPC that we have these days they have fixed scripts to follow and they're all manually authored what if we have NPCs uh the non-player characters that are actually alive yeah uh and you can interact with them and they can remember what you told them before and they can also take actions in the gaming world that will change the narrative and change the story for you so this is something um that we haven't seen yet but I feel there's a huge potential there so that when everyone um play the game everybody will have a different experience and even for one person you play the game twice you don't have the same story so each game will have infinite replay value yeah so that's one aspect and the second aspect is that the game itself can be generated yeah and we already see many different tools kind of doing uh subsets of this Grand Vision I just mentioned right like there are text to 3D generating assets there are also like text to video models um and of course uh there are like language agents that can you know generate story lines uh what if we put all of them together so that the game world is generated on the fly as you playing and interacting with it yeah that would be just truly amazing and a truly open-ended experience super interesting um for the agent Vision in particular do you think you need gpg 4 level capabilities or do you think you can get there with llama 8B for example alone yeah I I think the agent uh needs the following capabilities uh one is of course it needs to hold an interesting conversation it needs to have uh a consistent personality and it needs to have long-term memory and also take actions in the world so uh for these uh aspects I think currently like the lava models are pretty good for that um but also not good enough to produce very diverse behaviors and really engaging behaviors so I do think there's still a gap uh to to reach um and the other thing is about inference cost so if we want to deoy Dey these agents uh to to The Gamers uh then either it's like very low cost hosted on the cloud or it runs locally on the device otherwise it's kind of uh unscalable in terms of cost so that's another um factor to be optimized do you think all this work in the virtual world space is it in service of like you know you know you're learning things from it that way you can accomplish things in the physical world is is it does the virtual world stuff exist in service of the physical world Ambitions or I guess said differently like is is it enough of a prize in its own rights and how do you think about prioritizing your work between the physical and Virtual Worlds yes so um I just think the virtual world and the physical world ultimately will just be different realities on a single axis so let me give one example so there is a technique called domain randomization and how it works is that you train a robot in simulation but you train it in 10,000 different simulations in parallel and for each simulation they have slightly different physical parameters like the gravity is different the friction the weight everything is a bit different right so it's actually 10,000 different worlds and um let's assume if we have an agent that can master all the 10,000 different configurations of reality all at once then our real physical world is just the 10,000 1st virtual simulation and in this way we're able to generalize from SIM to real directly so uh that's actually exactly what we did in a follow-up work to urea where we where we're able to um train agents uh using like all kinds of different randomizations in the simulation and then transfer zero shot to the real world without further fine tuning so I do believe Eureka that's Dr Eureka work uh and I do believe that if we have all kinds of different virtual world including from games and if we have a single agent that can master all kinds of skills in all the worlds then the real real world just becomes part of this bigger distribution do you want to share a little bit about Dr R to ground the audience in that example oh yeah absolutely so um for the Dr urea work we built upon urea and uh still use l as kind of a uh robot developer so the L is writing code and the code uh is to specify uh the simulation parameters like the randomization parameters and after a while after a few iterations um the policy that we train in a simulation will be able to generalize to the real world so one specific demo that we showed is that we can have uh a robot dog walk on a yoga ball um and it's able to stay balanced and also even walk forward uh so one very funny comment that I saw was uh someone actually asked um his real dog to do this task and his dog isn't able to do it so in some sense our new network is super dog performance I'm pretty sure my dog would not be called it ADI yeah artificial dog intelligence yeah that's the next Benchmark um in the virtual world's uh sphere I think there's been a lot of kind of just incredible models that have come out on both the 3D and the video side recently all of them kind of Transformer based do you think we're kind of there in terms of like okay this is the architecture that's going to take us to the promised land and let's let's get it up um or do you think there's kind of fundamental breakthroughs that are still required on on the um on the model side there yes um I think for um robot Foundation models like we haven't pushed the limit of the architecture uh yet so um the data is hard problem right now and it's the bottleneck because uh as I mentioned earlier we can't download those action data from the internet they don't come with those modor Control Data we have to collect it either in simulation or in the real um on the real robots um and once we have that um we have a very mature data pipeline then we'll just push the tokens to the Transformers and have it compress those tokens just like um you know Transformers predicting the next word uh on on Wikipedia um and we're still testing these hypothesis but I don't think we have pushed the Transformers to their limit yet um there also a lot of research going on right now on alternative architectures to Transformers uh I'm super interested in those uh personally like there are mamba and recently there was like test time training there are a few Alternatives um and some of them have very promising ideas um they haven't scaled really to to the uh a the Frontier Model performance um but I'm I'm I'm looking forward to to to seeing alternative to to Transformers have any caught your eye in particular and why yeah uh I think um I mentioned the the membera work and also test time training uh like these models are are more um efficient at inference time um so instead of like forers attending to uh all the past tokens uh these models have inherently more efficient mechanisms uh so I I see them holding a lot of Promise um but we need to um scale them up to to the size of the frontier models and really see like how they compare headon with the Transformer awesome should we close out with some rapid fire questions yeah oh yeah um okay let's see number one uh What uh what outside the embodied AI world are you most interested in with an AI yeah so um I'm super excited about uh video Generation Um because um I see video generation as a kind of world simulator so we learn the physics and the rendering from data Alone um so uh we have seen like open a SORA and later there are like a lot of new models uh catching up to Sora so uh this is uh like a ongoing research topic and yeah what does the world simulator get you I think it's a it's going to get us a data driven simulation in which we can train embodied AI that would be amazing nice what are you most excited about in AI on a longer term Horizon 10 years or more yeah so um on a few fronts like one is uh for the for the reasoning side uh I'm super excited about models that code I think coding is uh such a fundamental reasoning task that also has huge economic value um I think maybe 10 years from now we'll have uh coding agents uh that are as good as human level software engineers and then we'll be able to accelerate a lot of development um using the L themselves and the second aspect is of course robotics I think 10 years from now we'll have humanoid robots uh that are at the reliability and Agility of humans or even Beyond and I hope at that time project root will be a success and then we're able to have humanoids helping us um in our daily lives um I just want robots to do my laundry yeah that's always been my dream what year a robots going to do our laundry soon as possible I can't wait um who do you admire most in the field of AI and you've had the opportunity to work with some greats uh dating back to to your internship days but who who do you admire most these days I have too many heroes in AI uh to count um so I admire uh my pH advisor F uh I think she taught me um how to develop good research taste so sometimes it's not about how to solve a problem but identify what problems are worth solving and actually the what problem uh is much harder than the how problem and uh during my PhD years with F I transitioned to embodied AI um and in retrospect this is the right direction to work on I believe the future of AI agents will be embodied uh for robotics or for the virtual world uh I also admire Andre Kari um he's the great educator uh I think he writes code like poetry so I look up to him and then uh I am Jensen a lot uh I think Jensen uh he cares a lot about uh AI research and he also knows a lot about even the technical details of the models and I'm super impressed so I look up to him a lot pulling on the thread of having great research taste what advice do you have for Founders building an AI in terms of finding the right problems to solve yeah um I think um Recent research papers I feel that the research papers these days are becoming more and more accessible uh and they have some really good ideas and they're more and more practical instead of just like theoretical machine learning so I would recommend kind of keeping up with the latest literature uh and also just try out like all the open source tools uh that people have built so for example at Nvidia we built uh simulator uh tools that everyone can have access to and just download it and try that out and you can train your own robots uh in the in the simulations just get your hands dirty and maybe pulling on the thread of Jensen as an icon on what do you think is some practical tactical advice you'd give to Founders building an AI what they could learn from him yeah I think identifying the right problem to work on right so um Nvidia bets on humanoid robotics because we believe like this is uh the future and also like embodied AI um because if we believe that let's say 10 years from now there will be as many intelligent robots in the world as iPhones then we better start working on that today yeah so yeah just like um long-term future Visions I think that's a great note to end on Jim thank you so much for joining us we love learning about everything your group is doing and we can't wait for the future of laundry folding robots awesome yeah thank you so much for having me yeah thank you thank you thanks [Music] [Music]

========================================

--- Video 55 ---
Video ID: qjlnpMHn2U0
URL: https://www.youtube.com/watch?v=qjlnpMHn2U0
Title: MongoDB ft. Dev Ittycheria - How an Early Pivot Catalyzed an Open Source Movement
Published: 2024-09-12 09:00:47 UTC
Description:
MongoDB, founded in 2007, originally aimed to create a platform-as-a-service system with a new database layer. Facing competition from Google, the founders pivoted to focus solely on their database product, MongoDB—a new kind of database built for the scale of the internet era. Founder Dwight Merriman built a product that developers loved, but scaling the company proved challenging until Dev Ittycheria took the reins as CEO in 2014. As cloud computing grew, MongoDB transitioned from on-premise software to Atlas, a fully managed-cloud service. Despite initial skepticism, Atlas now represents 70% of MongoDB's revenue. As Atlas scaled, MongoDB faced another controversial decision: whether to change its open source license model to maintain its commercial moat. These pivotal decisions transformed MongoDB from a niche database to a nearly $2 billion in annual revenue.

Host: Roelof Botha, Sequoia Capital
Featuring: Dwight Merriman, Dev Ittycheria and Tom Killalea

Learn more here: https://www.cruciblemoments.com/episodes/mongodb

00:00 - Introduction
05:30 - The NoSQL movement
09:52 - Scrapping the platform for the database
14:57 - Launching as MongoDB
19:52 - Moving to the cloud with Atlas
24:52 - Assigning a directly responsible individual
30:15 - How Atlas changed MongoDB
35:03 - Updating the licensing model to avoid “strip mining”
39:50 - Evolving back into a platform
41:26 - Executing on points of leverage (edited

Transcript Language: English (auto-generated)
we could see the writing on the wall in other words the hyperscalers were going to offer their own atlas-like things based on the mongodb source code it's almost 100% chance that would happen if they did it would be kind of like game over welcome to season 2 of Crucible moments a podcast about the critical Crossroads and inflection points that shaped some of the world's most remarkable companies I'm your host and the managing partner of SEO capital rof bua in 2007 Dwight marman and his co-founders identified a problem the databases of the pre- internet era were not built for the scale of modern applications new internet companies that were exploding in growth would regularly crash and go dark because their databases failed the world would need a Next Generation database today mongodb a document-based database solution generates nearly $2 billion in annual revenue and Powers thousands of companies worldwide including many of the apps at your fingertips however back in the early days of the company there was a question mark about whether mongodb could translate early traction with developers into a sustainable business in today's episode we'll look at mongodb's early decision to drastically reduce its scope and pivot from a platform to a single product we'll learn how the company migrated its business model from open-source on-prem software to fully managed cloud services and we'll hear how it gambled on a change to its licensing model that risked community and user [Music] fault my name is Dwight marman I am a director at mongodb and one of the original founders of the company how did we get started with mongod DB why did we want to do it what was on our mind at the time I'd worked on a lot of startups I was a co-founder of double click in 1995 and then after 10 years at double click uh decided it was a good time to maybe do something new and myself and uh Kevin Ryan who was CEO of double click and Elliot Horwitz who was one of our most senior Engineers there we were all interested in doing a startup as our next thing we started brainstorming some ideas for startup ups and we would see the same issues coming up time and time again on the tech side in terms of the design development of these systems issues around maybe scaling issues around just other way systems or maybe fragile cloud computing was very new around this time period and and there's a lot of properties to cloud computing that are important you know you scale horizontally not vertically and and a lot of our traditional tools including program languages but also databases they weren't really designed for this we felt like can we come up with something that's a better way to build systems and apps than the way we we're doing it right now the original idea of the company which is now called mongodb Inc originally was called tenen was to basically create a platform as a service system an open- Source platform as a service system with with this whole new stack of of modern things that we thought were appropriate for that point in time and it's kind of the evolution of software development so that's what we were doing so we had new stuff we had new ideas Concepts and tools at the application layer and then we had written our own data layer which was called mongodb it wasn't a product it was the name of that subsystem for decades dating back to the 1970s applications were powered by relational databases companies like Oracle and sbase commercialized these databases that harnessed a query language called SQL and if you want to think about it a relational database is really structured rows and columns and there are many different stores of those you can almost think of them as a spreadsheet and there are many different tabs to the spreadsheet and there a very simplistic analogy of that but the relational database have that kind of a structure and they were incredibly powerful these transactional database systems powered so many of the applications that we built in the 1980s the 1990s the challenge was that with the Advent of the internet the scale of applications just grew beyond what those Technologies were originally built for and I experienced this firsthand first at PayPal and then at YouTube with the thing that caused the site to go down in both cases was the database and in the case of PayPal we were built on Oracle originally and I remember we had these dbas database administrators who were these magicians these Wizards that would do tuning of databases which I still had a hard time wrapping my head around you know tune a musical instrument it seemed like technology shouldn't be that fickle uh but they would tune the databases and do things like sharding which is this approach of trying to break your database into many smaller databases so that you can string together many of them to serve the scale needs of internet companies that had millions or tens of millions of customers but fundamentally the database wasn't built for that scale and so often PayPal would have outages because the database just fell over it just couldn't cope with the scale that we needed Dwight Founders designed their data layer mongodb to address these shortcomings it was a database built for the modern internet era and notably it didn't use SQL mongodb would be at the Forefront of the so-called nosql movement so if you recall earlier I talked about how the relational database Solutions are rows and columns think about mongodb as really having the document as the central unit and so one analogy that I always like to think about is uh an invoice for a transaction if you had an an invoice for a transaction it would have the date the item being purchased the taxes that were charged the customer the customer's address all the details were encapsulated in that single document and You Could reconstruct the financials of a company if you assembled all those invoices because you'd have all the underlying transaction data and so you can think of mongodb as building that document as an invoice and when you do that it's really flexible because if the tax rate changes you can just easily add that to all your future invoices whereas with the Legacy relational system it was actually technologically very difficult to go and add new Fields over time a solution like would make it flexible and very easy for developers to build applications so we made the decision okay we're going to do Platform as a service we have all these various details to our plan we start writing it and the name of the company's tension and that's the name of the platform of service product we start building it and within say 12 months we release it as a beta things were going pretty well people were signing up to use the beta version and the feedback was good what happened was about the same time released as a beta Google released app engine as a beta we had no idea it was coming it was released all of a sudden it's like oh this is interesting like like tenen Google app engine is conceptually very similar we had a little family vacation so had my laptop with me so I could I could work every day even though we're not at home and they're out playing so I've got some time to think and kind of get out of the day-to-day frenzy of minutia and then think about the big picture right and as I started thinking more and more about it I started to realize that the scope of the problem was huge we knew the scope was big right but even bigger than we thought for you to really build to do everything on this platform it takes a long time to build and you're going to need a tremendous amount of Runway and I think it's more Runway than a startup's going to get it's like if you're the VC and I come and I say here's our concept for platform of service it's going to take us 10 years to build this and you're going to need a billion dollars it just seemed like that's not going to fly maybe platform as a service makes sense but you know the companies that need to do it are ones who can afford that Runway and have very long-term thinking like a Microsoft or maybe like Google so after going through this line of thought on platform as a service it's like okay what should we do we need to reduce scope what should we do mongodb then engine faced a crucible moment a year into building despite positive user feedback they began to feel their initial Vision was doomed and so the question was do you scrap your original business plan the majority of your code and pivot to a smaller scope and if so what exactly is that new scope one thing that was interesting once we we got into beta with the product is that people actually said oh I really like this database thing as they're using the whole platform as a service so we we were getting some very nice feedback on the data layer so this got me thinking maybe this database this is a thing and and we can uh we could focus on it and just scrap being platform as a service I think the term Crucible moment is a really good one because it it's a big decision right it it's it's kind of a curious story because usually when you do a pivot as a startup something bad is happening right but that wasn't our situation so that made it's much more difficult to do that if we started doing database only and it didn't work there was no going back to platform as a service like we would have figured out 18 months later it wasn't working and then it's too late the first person I talked to about this was Elliot so co-founder CTO super smart and uh we go talk and or little tiny conference room and I lay this out to him it's like okay I propose that we throw away our entire current business which is in beta and people like it and do this other thing instead he kind of looked at me like I was slightly crazy for a second and paused and didn't say anything he say something like let me think about it we met back up an hour later and he said okay he's like I think this is right we should do it so that's kind of amazing when you say like I want I'm going to take a business plan I'm going to I'm going to rip in the house 99% people would not have reacted like that and then the third founder of the company Kevin Ryan he was very pragmatic I think about this so he was kind of like well I'm kind of shocked you're saying this but if you guys both agree that this is a good idea then let's do it was this stressful emotional was it scary I would say the answer is yes I think it it's even more scary because we're doing this very proactively we may be killing something that would have worked so now if the new one doesn't work at all boy that was maybe one of the dumbest things ever done and you know and it's emotional you know like when we did this you know we threw the app stack the app layer code in the trash and we started riding drivers but you know then we also had a few people who like they weren't working on the database layer at all and I laid them off and you know that feels terrible so so it it was a big deal in Psychology there's this term called commitment and consistency and we all experience this in various forms where once you've made a certain decision the instinct is to remain consistent with that decision so that's why they call these commitment and consistency processes and letting go of that is is traumatic it's difficult and I think if you're a Founder in particular when you had a vision of something you wanted to build and the thing you're building now is you know even if it's similar it is significantly different from what you originally set out to do I think it's emotionally taxing you know the thing for which you raised your seed round or your series a financing you're not telling a different story to those investors you think about the first few people who joined your company who joined with an expectation that you're building a particular product and now you're telling them it's something slightly different but I also think the the founders that end up building truly successful companies are able to get through these difficult Crucible decisions of letting go sometimes of a wrong idea sometimes letting go of the wrong hire and as we've reflected on success across our experience at seoa the speed with which Founders are willing to make these difficult decisions is one of the best predictors of ultimate success so when we made this pivot decision okay what's the first thing we need to do the first thing we need to do was basically just write a bunch of drivers for every programming language so you could talk to the existing database we had already written so we had this database but it only worked within the platform as a service stack within the tangent stack that was the only way to use it so we we just kind of pulled it out and then we made database drivers for every all the popular languages and then you could use it right in 2009 mongodb launched for public use the product was open source so we were really trying to execute an open source go to market plan by being open source we're going to get a lot of Cheaper free marketing so we're going to every meetup group we can and giving demos of the database in City X we go to the python meetup group and we do a demo among MB from Python and then we we go do that in 10 cities for the same equivalent group in that City we try to do that for as many languages as we could and then in addition uh we started doing a lot of conferences City by city we did SF and we did SV and we did NYC and then we started to do Europe and Asia and and that was really how we were approaching to go to market and at the same time this notion of no sequel kind of came out of the woodwork it's like the big companies you know Amazon Microsoft LinkedIn Netflix Google they had all internally built some sort of nosql database but there wasn't much out there that anyone could use in 2010 we intersected with mongodb it was immediately obvious that they were the likely winner in this new category with these nosql solutions they had developer love the references were very positive the number of downloads the company was getting for its database solution was growing very quickly month over month it was still a tiny company at the time that we invested in mongodb in late 2010 they had 12 people there were 12 employees at the company but there was something magical happening and we immediately wanted to rush to close the financing and become the new lead investor in 2013 the company officially changed its name from tenen to mongodb a reflection of its successful pivot from day one you know my background I I was a CS major in college I loved programming and when I was CEO at DB I was spending some fraction of my time like oneir to one half coding or designing things you can go in GitHub if you go all the way back in the in the commit list you'll see a bunch of commits for me and then over time it goes down is as the company's growing you know other work that CEO has to do takes a lot of time so I very much wanted to get someone else to be CEO quite early on in the middle of 2014 the company's Revenue run rate was approximately $40 million the company was doing well and the CEO was performing well and yet we as a board felt that there was an opportunity for us to hire somebody who was both a former founder who was deeply technical himself who had led a public company before and at scen scale and would be able to help us move mongodb from a 40 million per year business to a multi hundred million year business one of the jokes I have with my friends is when you get called for CEO role the first question you're trained to ask is what's wrong because no one makes a COO change when things are going well my name is David aeria and I'm the president and CEO of mongodb prior to joining mangad DB I was actually inventure investing I was a partner at grock partners and I had recently joined a small Venture firm out of Boston called open view Ventures and I had done a lot of work on looking at Next Generation databases and I looked at a number of competing Investments to mongodb and I ended up passing on those Investments when I did my diligence it became clear that the level of developer traction that Mong be had relative to the opportunities that I was pursuing was significantly better I ended up passing because I didn't want to invest in the second the third player in the space that's how I got to know mongodb I was happy as investor I was I felt I was doing quite well and then out of the blue I got a call from a search firm that was leading the search for manga DB for the CEO role Dave he'd been a Founder more than once he successfully founded a company and led it a CEO through the IPO and through a successful acquisition he had been a general manager in a significant role in a large public company which gave him experience at scale having somebody like Dave who is demanding and supportive of his team and would hold himself and his team accountable those are just wonderful attributes around which to build I had a lot of people who when I talked them about joining Mong Deb were quite skeptical that Mong Deb was going to be a good decision as I met with the board I met with the management team I learned a couple things it was clear that the company was generating some momentum it was clear that mang had had meaningfully more developer traction than some of the other Next Generation databases that I looked at but the company was also struggling yeah it was badly missing plan the leadership team was somewhat dysfunctional the goto Market efforts were not very effective and so you might ask yourself so why did you consider joining the company I asked myself the question if this company's doing well with essentially not a very good team a lot of dysfunction how decisions are being made imagine what this company could do with an A team in place so it was somewhat counterintuitive but um I decided that it was worth taking a risk when Dave joined mongodb a CEO the company offered open source on-prem software meaning it was downloaded and operated by developers however mongodb began to see more and more users accessing software as a service directly through the cloud offering on-prem software in an increasingly Cloud World began to pose a conundrum one of the Strategic challenges we faced at mongodb was that many of our customers were taking our database and building applications and deploying it on cloud infrastructure on their own but at the point that they started to run and operate their database they didn't have an ongoing relationship with mongodb and often they'd run into challenges maybe a database was misconfigured or maybe they didn't make use of all the potential capabilities that we had but because we didn't understand how they were using the product we couldn't help them and then from a business point of view because we weren't operating the databases on their behalf off we were missing out an opportunity to build a business around the cloud delivery of our solution we realized that if we could offer a cloud service by definition we would be able to let customers Outsource all that undifferentiated labor and really focus on the things that really and truly mattered so that was the Catalyst for thinking hard about Atlas the company began considering a wholesale move from downloadable software to a fully managed cloud-based solution called Atlas I will tell you that there was a lot of skepticism because we were the first independent company to try and offer an infrastructure service on the cloud you know this is pre- snowflake pre- elastic pre-confluent so it was not exactly viewed as a a slam dunk to consider doing this could we really execute on this just knowing that you needed to build a cloud offering you know it's easier said than done we had to go from being a company that tripped a product that people downloaded and used on their own to building the operational skill set ourselves to be able to operate databases on behalf of our customers and I think Downstream changes are often underestimated by people as they undergo crucial moment or key strategic decisions like this mongodb had a decision to make how should it adapt to an increasingly cloud-based world if the company were to shift from downloadable open-source software to a managed cloud service could they actually manage that transformation I thought if the company didn't pursue Atlas that would be a giant mistake kind of Borderline Disaster there were issues with the business it is a change we need to do it there was a real risk that if we didn't successfully develop a cloud strategy that would lead to our ultimate demise and so it's with that spirit that we recruited Tom Kila to the board of the company to be honest I I I said to roll off I'd love to serve on another Bard with you but this may not be the one my name is Tom K and I'm the chair of mongodb Tom had been a key executive at AWS he was originally the ciso the Chief Information Security Officer of Amazon and then had been part of the team that built AWS in the early days and so here's somebody who had firsthand experience and knowledge of the biggest cloud service provider on the planet and I thought that his experience at AWS coupled with the opportunity at was a perfect Mar he wasn't too Keen initially my big concern related to product positioning I felt that there was a danger with any persistance offering in presenting Atlas as a jack of all trades I had a a deep sense that one size does not fit all very highly optimized around a single use case approach that AWS in particular was taking May went out Tom lives in Seattle and mongodb is based in New York but I convinced him to take a meeting with the team I did fly as to spend time with the engineering team and it turned out that I was flat out wrong I became convinced that in fact there were huge advantages to the product positioning and the approach that mongodb was taking with respect to building a data platform that would serve many different use cases and would obviate the need for you to extract and shift data from one platform to another in order to get your job done and so I said yes in December of 2015 my first board meeting the essence of the debate was not do or don't do but do now or do later that was the biggest danger this s wait and see versus let's let's charge into this and make it happen I was very much on the side of we need to act now you have to seize the opportunity of a lifetime during the lifetime of the opportunity you can be too early you can be too late but when you see that the timing is right you really have to move with very significant urgency the team decided to give itself a deadline of June 2016 to launch the platform at the mongodb user conference World an aggressive timeline of under six months so one of the key strategies to really ensure that Alice was successful is we have the saying of a startup with a startup we ourselves viewed our whole business as a startup but we said that we actually starting a startup with the startup so all the Care and attention that we put in our Core Business we wanted to make sure that Atlas had the the same amount of focus some of the harder and more frustrating topics were around the product road map between now and launch in June figuring out what was actually feasible and attainable and rationalizing the things that would have to be left out people and the muscles that had to be built up in the company that weren't there so for example s reliability engineers and the company getting to truly understand what it would take to operate people's databases for them and have it be completely transparent and have them not have to worry about the care that we were taking with respect to that operation we assigned a directly responsible individual what we call a drri who really would be the the person who is really focused on building and growing the atlas business that meant not only working with our engineering and product teams to ensure that we were building all the features in the right way to ensure Atlas was successful but also working with our go to market teams we had to build a skill set of marketers where you'd have to manage a funnel of people going through your own signup flow and understanding how people converted and what is your free tier and how do you upgrade people to pay plans and how do you change your compensation scheme because your sales people are now selling a completely different product from what they' sold before after a whirlwind 6 months Atlas launched on June 28th 2016 my abiding feeling was you know it's great that this has happened but this is the start there was so much that had been left out of the product the work was very much underway and the engineering and product teams were remarkably focused on the next release a launch is just a day and a key aspect of it is getting the word out to the world but you end that first day with a number of customers that runs to zero one of the challenges of starting a new product offering is that the numbers are so small no one really cares if you if you hit or miss your numbers because it doesn't really move the needle for the rest of the business so our business at that time was roughly a $100 million run rate business and at was essentially starting at zero so the risk is that will people in the organization really care about making sure Atlas was successful Dave did something very important which is to isolate the metrics and numbers of Atlas and Spotlight them completely separately with his team and with the board so that we had a proper appreciation for the scale of it and how it was growing and avoiding it being drowned out by the scale of the business we had already built by then we launched Atlas in 2016 and we went public in 2017 the memories I have of people raising concerns about Atlas really happened when we went public it's a pretty daunting challenge to take a company public in the midst of this kind of a business model transition on the road show many public investors wondered what why are we making the investment in Atlas and who cares about this thing that's only 2 or 3% of the company today they said wait a minute how are you going to partner and compete with the hyperscalers they're going to you know eat you for lunch that was one question two there was no analoges of other companies who had been successful doing that three some are our own salespeople who had not been successful selling Atlas really wondered if this was more of a low-end solution maybe focused on the early stage of the market but large Enterprises who were late to the cloud would they really buy something like Atlas because of all the security concerns that people would have about running workloads in the cloud we basically went through the work of knocking down all these potential objections and focused on building Atlas but there was clearly a lot of concerns across different stakeholders about whether or not we were going to be successful there would could have been an argument for okay given how Central the atlas Cloud offering is to your strategy doesn't make sense to you know wait a couple of years before you got public we decided that the time was now and it was appropriate especially with what we were hearing from investors and from Bankers in terms of the centraly of the cloud portion of our Revenue to embrace the moment now to go to the public markets now and at the same point as I discussed with the launch of mangad Atlas the IPO day is just a day and the next day you're back to work and you're trying to sell product you're trying to build the next feature you're working at operational issues and you carry on and you've you've been through a funding event but the world moves on a seminal moment happened in 2019 when Amazon launched a competitor to Atlas called document DB investors panicked and said oh my goodness they have a clone of your product our stock took a beating we went down 152% on the day of that announcement so there was a lot of concern and my statement to the investors that time is that in some ways AWS is validating Atlas it's validating Mong B because the fact that AWS is launching a document based database as a service is telling the market that document databases are here to stay and that you can run Mission critical workloads on document databases and if it's a comparison on which product is the best document database we felt really good about our value propos in relative to the Clone that AWS has built because we knew architecturally it had a number of issues and it didn't have the features and the performance that Atlas could deliver and that's essentially what happened just to put things in perspective it took us about 10 years to get to 100 million revenue and it took us 5 years later to get to a billion dollars in revenue and a big part of getting to a billion dollars in Revenue was the fast growth of Atlas today Atlas represents 70% of mongodb's Revenue it's north of a billion doll run rate business on its own Atlas change mongodb in that the motion for how you interact with your customers changed it became far more better relationship Dave went through a process of both helping the company to understand what great operations means also what does customer success mean and what is it that you do today that will cause a customer to say okay for my next workload for that important thing that is going to get shipped who I would have put mongodb at the center of that if anyone's contemplating tackling a project of something of a similar scale or size to Atlas I think my advice would be the following focused on your customers needs and what they you know really want uh what their buying Behavior may be and for us we noticed that the customer's buying Behavior was changing from buying infrastructure software to consuming it as a service understand what are the points of friction that prevent your customers from buying and using more of your product our traditional open-source model had limits in terms of how fast we could grow and the only way we thought we could remove that friction was by offering Atlas be very clear on what you need to do and what are the associated risks uh are all your stakeholders aligned are they all incentivized to make sure that this project is successful do you have the right single threaded leader what we call our directly responsible individual do you need to get people to disagree and commit because some people may be skeptical but you need everyone's commitment to make it happen other stakeholders like your board involved because there are potentially financing implications U pro margin implications by launching a new business or a business with a different business model so all those things are factors that you have to consider when you contemplate a decision like this as Atlas took off and the market became increasingly aware of the company's unique Cloud database its success exposed another vulnerability after mongodb had already gone public by 2018 we were starting to worry that the big cloud service providers would test the limits of the agpl license so in open source there are a variety of different open-source licensing models and for people who are not familiar with this they might be surprised at the number of variations of Licensing models that do exist Mong was initially built on the agpl licensing model which is a bit more restrictive than the GPL or Apache License model and stepping back most open- Source licensing models are designed to induce the public to help contribute to the building of the product so you want people to look at the product make enhancements improvements to the product and contribute that back to the community that's driving the products Innovation we had a very very different strategy we wanted to use open source as a premium strategy we wanted to use open source so that it would be easy for developers to use the product and it also leveraged open source to drive Varity of adoption of mongodb all around the world but we still want to retain all the benefits and margin structure of a traditional software company which is why agpl was selected the founders here made a brilliant absolutely brilliant decision that we called out in our investment memo on page one in that they picked the agpl license so-called Ferro GPL license which was a more restrictive license that enabled customers to download the software and use it but it limited the ability for other people to make changes and then offer that software commercially now one of the things with open source is that the open source licenses were built in a precl era and one of the things that happened with in the cloud was that we saw the hyperscalers take the free version of a lot of open- source technology plug it into their platform form and offer it as a service and make money on it what we were worried about is that since we wanted to build an independent stand loone business we were worried that the hyperscalers would do what people call strip mine mongodb which is take our free product and essentially do the same thing they've done with other open source projects and while agpl had more restrictions there were some terminology that was not exactly black and white so it was there was a little bit of gray and what we didn't want to do was end up litigating this issue in court if some hyperscaler did decide to try and take our free version and go compete with us we could see the writing on the wall in other words the hyperscalers were going to offer their own atlas-like things based on the mongodb source code if we did nothing they had not done that yet but if they did it would be kind of like a game over we had a board meeting in 2018 we were actually in Las Vegas at the time at AWS reinvent the conference and at the board meeting we wrestled with this decision about whether or not we should embrace an alternative licensing scheme the licensing model the management team and the board debated was called sspl or service side public license sspl would maintain the same basic structure as agpl with one major condition any organization offering a service using mongodb's Code would have to either obtain a commercial license or make their own source code publicly available the reason changing a license was a crucible moment was that there was definitely a lot of concern about how users would take to this license change while sspl conforms to all the principles of Open Source it was not an official sanction license from the OSI our fear was that our customers would not trust us that they may think that the change in licensing scheme is a way to circumvent them or take advantage of them and that we weren't true to our open source Roots so it was a very very difficult decision for us not knowing the reaction you know this is one of those things that you announce and there is no small Beta Trial that you could do there's been many examples of companies open source companies changing their license and there's backlash and sometimes they even reverted they backed off and they said sorry um so it it was scary but it was I couldn't think of any other solution to the problem I would say it took months and months and months and months to make this decision part of it is like I said if it doesn't go well this is a giant problem you got to get it right you need to be 100% sure you really need to do this as I thought about this problem and I thought about developers not just here in New York but say developers in Mumbai or Shanghai or Paulo Alto or anywhere else in the world I said if our software is open source in the sense that they can see and access our source code they can modify the source code they can redistribute that code they can essentially do anything they want with it like it to do with any other open source license would they really care whether or not this was an officially sanctioned OSI license and I felt if you could solve their problem better than any other alternative out there they wouldn't care and that was the bet we made some people thought this will have some long-term negative impact on the business and uh ultimately I made the call that we are going to go ahead with this license because it's too important to not do this for our future in October 2018 mongodb debuted sspl the company pioneered this licensing model and in the days that followed we held our breath for the community's response when we announced the shift there was clearly some reaction U most of it was positive but there was some negative reaction the zealots of the open source Community came out and made it clear that this was not a sanctioned open source license they didn't support this and they didn't view mongodb as an open source company anymore there were definitely some customers who weren't clear exactly what this meant so we spent a lot of time and invested a lot of time and effort to educate our customers on what the implications of this license change was we posted blogs about why we did this and tried to address questions that anyone would have and then obviously we armed our salespeople to explain to our customers who weren't reaching out to us why we did this and what the implications it had on them our Atlas business only grew faster after this change but there were some people who definitely predicted our potential doom and and luckily those naysayers were completely wrong given the experience that I've had with mongodb both in the original choice of hpl and then the choice of sspl is that when I meet young companies that are open source one of the first questions I ask them is what is the open source licensing approach that they've adopted because you can make changes early on much more easily than you can later and I I wish more Founders who build open source companies would study the mongodb case study to appreciate how important it is for them to choose the right licensing scheme early on in their company journey to ensure that they can build a thriving business ruof always pushed us on thinking about the scale of our ambition Mong had taken a long time to get 100 Mill million in Revenue now there many many companies the high percentage of companies don't even get to $100 million Revenue so that's a pretty big milestone but it took us 10 years as a company to get to 100 million but ruoff always pushed on me and pushed on the exec team and was confident that we could be a far bigger company now we're approaching to be almost a $2 billion company and we've done that essentially in a pretty short period of time while mongodb began journey by narrowing its scope today by creating such a successful database it's earned the right to evolve back into a platform we realized that our customers were coming to us and asking us to do more things customers tell us hey I'm using mongodb for a bunch of my workloads but then I have to use a search database along with mongodb and why can't you do both other people say I'm using the single point single function database and I much rather do everything on your platform because then all the data is in one place that really drove the focus of building out a broader set of features so that customers could essentially do more things on mongodb uh but still do it in a very unified and elegant way uh essentially across any deployment model they could do it on premise they could do it in the cloud they could do it cross cloud and for some uses they could also do it at the edge I think it's kind of interesting how we started with this platform as a service idea where this definitely the idea is to create a platform and then we did the pivot so now we're making a product right but as the product gets bigger it's kind of ironic that it is kind of a platform the founding mission of being a platform business is now what is shaping our future as we've gone from being a compelling database solution to now thinking of ourselves as a developer data platform and offering many more of the tools and capabilities as a as a single point to make life easier for developers and to really help solve their problems my main takeaways from the cruo moments I've been through is to think about a few things one think about what kind of outcomes you want whether it's for yourself or the company or organization you're working for and working backwards from there then thinking about what are the biggest points of Leverage that really have an outsize impact on getting to those outcome and really really focusing on executing on those points of Leverage very well and the third thing is learning how to deal with adversity there will be times when your commitment is tested there will be times when you question yourself about whether not you're doing the right thing there may be other stakeholders who challenge your assumptions uh and while you want to listen to the feedback and maybe there's things you haven't thought through it's really really important to deal with those adverse moments and continue to plow ahead because as a definition goes a decision you make today can have an outsize impact on the future of your business or the future of your [Music] career this has been Crucible moments a podcast from Square capital [Music] Crucible moments is produced by the Epic stories and fox creative podcast teams along with seoa [Music] capital special thanks to Dave aeria Tom kly and Dwight maramman for sharing their stories

========================================

--- Video 56 ---
Video ID: y0aYPC9PVtw
URL: https://www.youtube.com/watch?v=y0aYPC9PVtw
Title: Founder Eric Steinberger on Magic’s Counterintuitive Approach to Pursuing AGI
Published: 2024-09-10 09:00:52 UTC
Description:
There’s a new archetype in Silicon Valley, the AI researcher turned founder. Instead of tinkering in a garage they write papers that earn them the right to collaborate with cutting-edge labs until they break out and start their own.

This is the story of wunderkind Eric Steinberger, the founder and CEO of Magic.dev. Eric came to programming through his obsession with AI and caught the attention of DeepMind researchers as a high school student. In 2022 he realized that AGI was closer than he had previously thought and started Magic to automate the software engineering necessary to get there. Among his counterintuitive ideas are the need to train proprietary large models, that value will not accrue in the application layer and that the best agents will manage themselves. Eric also talks about Magic’s recent 100M token context window model and the HashHop eval they’re open sourcing.

Hosted by: Sonya Huang, Sequoia Capital

00:00 - Introduction
01:39 - Vienna-born wunderkind
04:56 - Working with Noam Brown
08:00 - “I can do two things. I cannot do three.”
10:37 - AGI to-do list
13:27 - Advice for young researchers
20:35 - Reading every paper voraciously
23:06 - The army of Noams
26:46 - The leaps still needed in research
29:59 - What is Magic?
36:12 - Competing against the 800-pound gorillas
38:21 - Ideal team size for researchers
40:10 - AI that feels like a colleague
44:30 - Lightning round
47:50 - Bonus round: 200M token context announcement

Transcript Language: English (auto-generated)
thing that remains to be solved is uh General domain uh long Horizon reliability and I think you need in France time comput test I'm confused for that when you try to prove a new theorem in math or when you're writing a large um software program or when you're writing an essay of reasonable complexity you usually wouldn't write a token by token uh you you'd want to think quite hard about some of those tokens and um finding ways to spend not 1X or 2X or 10x but a million x the resources on that token uh in a productive way I think is really important that is probably the last big problem [Music] hi and welcome to training data I'm delighted to share today's episode with Eric Steinberger founder and CEO of magic Eric has an epic backstory as a researcher having caught the attention of noan brown and becoming one of his research collaborators while still a student in high school Eric is known for his Exquisite research taste as well as his big ambition to build an AI software engineer we're excited to ask Eric about what it takes to build a fullstack company in AI his Ambitions for magic and what separates a good AI researcher from a legendary [Music] one H Eric welcome to the show thank you so much for joining us thank you for having me Sonia okay so let's start with who's Eric you're a Vienna born vrand uh whose early passion for math turned into a I think what you described as a full-fledged obsession with AI by age 14 take us back to age 14 Eric like what were you up to how did you become so obsessed with AI uh thank you Sonia I I I think I just had my midlife crisis when I was 14 and um I I just was just looking for something meaningful to do spent about a year looking at physics math bio medicine just anything really that seemed valuable to the world and um at some point bumped into just simply the idea of AI it hadn't sort of occur to me until then uh and if you could just build a system a computer system that could do all this other stuff for me like great like I don't have to decide so uh it felt like my decision paralysis was uh sort of resolved and I it was this weird moment where I could just see the next 30 years of my life unfold in front of me and I was like okay this is clearly what's going to happen like I have to I have to do this and uh it was it was quite it was quite nice um I like predictability so uh it was great to like know what the world will look like and you started loving math like why why AI then I think I'm naturally attracted to math it's just what my brain sort of gravitates to uh AI just seems useful I the thing that's most important to me is just what is useful for Humanity in the world and uh math is nice but not useful at some point like kind of you know 17 dimensional spheres are are probably not going to be the best career choice uh if you want to be useful so I it seemed like something that I could get good at uh but also just the most important thing ever uh so it was a very clear choice AI just is like as it was clear 10 years ago it's just it wasn't close and now it's close and clear can you tell the story of how you got aair I think it is such an epic story sure I I I mean so when I started with at 14 I had I didn't really know how to program I I didn't get into programming out of curiosity about computers I just wanted to uh solve AI basically um so after a couple years of just warming up on my own uh I reached out to uh one of David Silver's uh PhD students uh who who the alpago um Deep Mind co-founder uh and um the the this this PhD student I guess at that point he was a graduate and worked that deep mind um I asked him if he could like spend a year with me uh just every two weeks bashing my work uh sort of trying to do so sort of like super speed up uh uh mini kind of PhD like experience where I could just learn how to do research and uh I sent him this like giant email you could print it out I don't know how many pages it would be would it be a lot of pages I was basically just saying like I want to build this algorithm you made in your PhD sorry I want to beat this algorithm you made in your PhD here like here's a list of of 10 ideas I don't know if they're going to work and I think I need your help to figure that out um and then over a year we eventually got there um and he was like his name is Johannes Johannes was kind enough to just bash me every two weeks uh roughly and um yeah I it was it was brutal dude um like was just like cuz he I was like he hold me to the standard you know and I was like like don't don't be nice just cuz I'm you were in high school yeah I was in high school uh and and then when we were done uh I just graduated high school when when I finished so this this when I when I finished um the project uh that I was trying to get to with with with this uh and then noan Brown who is obviously one of the best RL researchers in the world um reached out because he had worked on something similar turns out and and we sort of uh uh had like some ideas that were very similar and some ideas that were a little different and so so we just both published this and um uh he reached out and then I got to work with no Brown for two years which was great and then that continued and so I got pass for another attention you were a high schooler he was no one brown um well I mean he published a paper called Deep counterfactual regret minimization and I published a paper called single deep counterfactual regret minimization and mine beat his by a little bit and uh so you want to know and brown as a high schooler uh I think I just graduated and I also it took him like three months to write this paper and it took me a couple years but uh it it uh yeah I mean slightly I'm sure he like would have come up with this the next day but the like the the the sort of the the gap between the two things but um it was it was just uh yeah yeah it was like obsession is the right word I just uh I do things like 100% And and yeah so so that that that was that was a lot of fun um kept working in our all with noan brown for a while and uh yeah so that's that's how I got to Fair noan Brown worked at fair at the time and uh he he reached out I was actually I was at University then and um basically just worked part-time as a researcher at Fair while studying and anyway so that was that was it that's awesome um it was it was a lot of fun no was great like the the brainstorm ping pong sessions with no Brown dude um there's like nothing like this where you just like there's like this problem and it's sort of like you know maybe you would like start a six month research no Noom and I would get on a call and it would just like we just discuss it and it's done that was so great I love that I love that what makes him so great as a researcher uh I think it's a number of things um as a researcher generally from more from a meta level he is fantastic at picking the right problems and uh spending a long time just grinding to make it better and better and better and better so he's very good at the whole like compounding thing um in research uh also making bets that aren't obviously uh the right bets when he makes them he makes them earlier I suppose and and making them def so he's generally very good at picking problems and then attacking them consistently um he's also just very smart I guess that helps he works really hard he used to do 100 hour weeks during his PhD I don't know if he still does them uh uh but uh he used to he used to work really really hard during his PhD I imagine he still is um okay so Nome arranged for you to become a researcher at fairwell you were still University student um if correct and you were jug my first semester I think or something so you were juggling that you were juggling being a collaborator to Noah at fair and then you became obsessed with yet another problem climate change and and actually started an NGO that is incredibly popular uh climate science um so you just didn't have enough on your plate uh was actually too crazy I that's when I dropped out I was like this is crazy this is too much I can do two things I cannot do three was like my conclusion after after doing like uh I did three months of that uh that was terrible that was that was awful doing like all three things cuz it just like you just can't do well at three things I mean Elon can but maybe I'll learn it in 10 years but the the I I I couldn't at the time uh so I dropped out at the time but yeah so I started an NGO I generally I just think like Cherry stuff is awesome and hugely appreciated like it's it's sort of like super high status to start a startup uh but I think it should be like equally cool to start a charity you're like helping the world in other ways um and so uh yeah I mean we we started it as a it's a nonprofit but we started at like a startup people were working insanely hard uh we had clear objectives it was you know a software product effectively uh it's it it was much more similar to a startup with the exception that there was no money in no money out um effectively which just very weird but um the yeah was mostly volunteered D or I guess is I just no longer run it uh but uh yeah that that that was an interesting experience you'd think I would like learn transfer a lot from running a quote unquote company to running a quote unquote company now but they is so different that I could like there was like no transfer at all between climate science and Magic like a th volunteers 20 hardcore Engineers no money at all I can't tell you how much we rais cuz it's not a yet but the you know Giant uh it's it's like completely different in every imaginable way totally so but yeah it's a lot fun so Eric climate science became an incredibly successful uh nonprofit and it wasn't just any nonprofit uh what made you decide to kind of hand over the rins and hand over the torch on that and and go start ACC companying AI I just thought AGI was further away when we started it at all uh I wouldn't ever have started anything else uh if I thought AI was so close uh and once I realized it is uh there was just like no other I I was I mean like I my initial thing was always AI That's what I did as a kid yeah uh I care about various issues in the world but uh it's n none of them are my like unique calling in any way I just you know I hopefully be in a position to donate a bunch of money and whatever but uh uh the thing I care about fundamentally is Agi and uh it was like oh damn it this is not 20 years away so I have been running around with the this AGI to-do list which is somewhat of a meme uh internally because we just like going through it then we're trying like to fix all these problems you have seen it yes we showed it to you um and I've been running around with a version of this there's actually like a 2017 or so like I was still in high school I got I don't know why but some conference invited me to present my AGI to the list it was wrong at the time I was also I was also sure it was wrong but um at some point so that was one thing I just couldn't at all figure out and I don't like blue sky research in the sense of like just staring at a wall and trying to like figure out what the right question is um I I I really like to have the question and then look for the right answer uh when starting an intense project uh because you need to know which direction you run in to really plan for it uh and the many things seemed clear but it seemed completely unclear how to make these models reason in the general domain uh and uh that became more clear with language models uh especially code models uh and and so uh yeah when when I saw just some of these early some of the early results in in the space like I was like okay I know all this stuff from the RL World um I have a bunch of other thoughts this seems great like we should just take LMS and make them do the RL stuff um so very simple kind of uh uh proposal but I think that's sort of where uh I mean it makes a lot of sense RL has been doing this for 10 years it works in in restricted domains if you can make something work in 20 restricted domains uh and you have something else that works in a general domain if you can combine them maybe you get both the X and the Y AIS and then you know you have your beautiful top right corner uh of the Matrix and uh yeah so it seemed like it seemed pursuable and if something when something becomes when something as important as AGI becomes a an actually executable to doist obviously there are still like things to figure out like you know details of the algorithms how do you make it efficient etc etc like it's not like we knew everything at all um many many things to figure out but the direction was clear and yeah so seemed like the right moment okay we're gonna Circle back to your AGI to-do list later because I'm I'm I'm curious about it sure um I want to brag about you for minutes because it might be wrong still I don't know until we have AGI it isn't is a hypothetical AGI to-do list but we're trying it's it's a I think the research field is tracking pretty closely to your to-do list um I want to brag about you for a minute I think you've been incredibly humble humble about your background um but you know as a high school student you did catch gnome Brown's Eye and and um as you know as one of his colleagues at fair you became one of his top collaborators not even just one of one of many because they're such talented people that work there but you're one of his top collaborators um and you know when I speak to folks that know you they just say extraordinary things about your capabilities as a researcher your creativity uh your work ethic as far as I can tell you work non-stop think you texted me at 2 am in preparation for this podcast um so I think it's safe to say no no uh thank you silent mod um anyways I think it's safe to say that you are one of the brightest minds of the current research generation already and will certainly be one of the legends that people talk about um for the next decade and so with that in mind I'd love to um ask you some questions of advice for for aspiring researchers and so maybe first off you did it all from a very untraditional background um how did you do it and like uh do you think that like what what advice would you give to others in your shoes I can only really speak for the sort of profile of goals and person I am I think I was lucky in the sense that I knew very very early with 14 as as we said exactly what I wanted to do with my life I had no doubt at all um and and and uncertainty can be paralyzing to a lot of people uh I also had a very clear sense that I did not at all have a plan B like there was no other path in life that I would have been even like remotely above the neutral line on like it it had to be build AI everything else is completely irrelevant so you know I understand so for many people like you know paying job at Google is is a great uh great achievement I mean I would if if it's on AI it's fine but it you get what I mean like I I just knew that there was nothing else I could do and like be fulfilled in life and look back when I'm 90 and be happy so in a way like burning the boats very very early gives you the opportunity to just be be get like do things that You' otherwise do 10 years later which again even like I I sucked at the beginning it took me two months to understand the first p paper I triy to understand um like I was terrible at programming for a long time but when you're a teenager you're like a decent researcher you don't have to be great like you know you that gets you that gets you uh things like a great mentor who then bashes you for a year uh which you know was was um very very helpful uh and and you know then you get better and you're still young so you s of your brain shapes more easily maybe I don't know so I feel like I benefited a lot from being early but within that um I'd say just like go for the end goal immediately doing anything sort of like oh I'm going to do a PhD because I need a PhD to get a that's all uh like you don't uh it's just completely uh the other thing is like writing five-page uh emails to people actually works um writing like I get a lot of these like two paragraph me things now and grateful I get emails but though I understand now why people think this stuff doesn't work it certainly does when you're like here is how I'm going to beat your algorithm please help me uh five pages at least in my experience every single time anyone I I I want help from in this way uh was very helpful so I suppose be proactive in seeking like the best people in the world to in a time efficient manner just distill their brain into yours and be show them that you can make use of that if you if you if you tell someone who's very good effectively hey I'm going to make good use of this if you want to coach someone I would love to be that person uh they'll they'll usually do it they won't do it for 10 people but if they do it for one or two that's enough you just have to to to win that seat I guess um so that that's been really helpful in my experience also just not shying away from learning new things um like I again I didn't get into programming because I'm curious about computers I'm not very curious about computers uh I I just like Ai and the computers are the thing that are necessary to so it's fun I enjoy programming now it's it's great but I wouldn't have gotten into it I think if if it wasn't for AI but still like you get into it so so like don't be shy like we interview a lot of people who who don't know um you know how how you'd Implement an llm and it's it's kind of crazy to me uh if you're a researcher and and you couldn't like Implement Charing or whatever like it's it's just insane um so so really understanding the whole stack going down to uh but sort of not bottom up really top down like here's the thing I care about this is the problem I want to solve okay like what do I need what do I need what do I need uh and and then like all the way down um and um and they're like much more confident people at colel programming at Hardware design or whatever than I could ever ever dream up to be but I understand enough of it to to do better work at the top of the stack than I could if I didn't um so um I think fundamentally you need to understand the domain you work in um it's also really good to just read everything um like I used to read I don't know I don't have a precise number but just every paper I could every paper I would see basically and eventually you get so fast at it that you can like that's feasible and you like build a database in your head of like oh this is similar to this thing this sort of like like but this was sort of my eye opening moment where Bill Gates has this intervie like oh yeah if you learned enough things they're all like similar to each other so it's not linear it gets easier and at that point I was like I should read every paper and uh so thanks for the advice Bill uh obviously was like through a video I never met him but um the uh so I just started reading every paper and that's really really helpful because a lot of the best ideas that we had that work really well now at at Magic um were enabled by random things that are like like oh this it would never work without this random thing that I would have to have come up with in tandem uh but because I have this database in my head I can go like oh yeah like this uh and and then so often like one good idea is enabled by three other ideas that others have come up with and and so so it's it's always just like this composition of stuff so so having a large database is really helpful um yeah and then just never stop uh like never never stop uh it it can it takes like ages to do good stuff to do good work and at any point there was actually one moment um with johanes the the the the Deep Mind research scientist who who mentored me for a year in high school um where uh we had a version of the algorithm that wasn't very good uh it just it was all right and and uh we were thinking like ah should we publish this like we were both not really happy about it and he was like close to giving up on me um it was like well you know like maybe this is just not going to work like I wouldn't want to publish this and so I was like dude you I'm just going to get this done and then we got it done like a month or two later uh and and so I think like I remember going on a walk after this um and uh just being like can I do this I don't know if I can do this but there is no no other option so I just better get it done and then I went back home and I started programming again it was still like sad that day but the next day was fine again it just keep going so I think you have to I think that was a pretty formative experience cuz I actually wasn't sure if I could do it and then we just did it like super soon after and so I really like haven't felt that insane level of like doubt and pressure since then which has sort of enabl I think it's actually beneficial you have to be realistic but you don't want to you you if you stop you yeah I mean so anyway so I think those would be the main things uh also be like really honest about what you suck at um uh to yourself because otherwise you're never going to get good at it um like you need to search for the bad things um and uh uh uh instead of like trying actually yeah I think like as a researcher betting on your strengths is good only to the extent that you don't have uh necessary conditions that are completely missing like you you can't bet on your strengths if they're not enabled again back to the engineering thing for example so yeah I don't know I'm rambling but like that that's great that is such a fascinating glimps into like the inner mind of what it takes to be a great researcher and you know behind all the glamour of of training large large models and so like thank you for providing that that uh Peak and I'm really glad that you mentioned kind of reading every paper voraciously and having this database in your head because one thing I've heard from your collaborators is that your superpower is understanding and absorbing uh new research and so I'm curious do you agree like do you think that is your superpower as a researcher or what what kind of traits do you think have made you such an exceptional researcher so I think initially in the RL work I did it was synthesis where I would read every paper and I would go like this thing plus this thing plus that thing with this modification um I think that's what they would mean um that yes uh was definitely very helpful uh I think is a good way to do research generally there's enough work to for synthesis to be a successful strategy I guess to an extent still that I tried very hard after this is actually bring this up I realized this and I tried very hard to get better at leaps um like coming up with totally alien crap that just there's no reference for it at all uh and um because cuz ultimately like so if you take like the transformer for example right like attention existed um the idea of stacking a bunch of lstm blocks existed and you just had to remove the idea of recurrence really like then like a bunch a couple other things that that were't necessary right residual streams like the the residual update and Transformers exist from existed from reset so it's like it's synthesis um but there is an amount of leap in there uh to to to make it all work like it's it's a it's a little more complex than just taking components and putting them together you need to come up with new things too um like you know the normalization and the the the the the head square square the head which is actually incorrect but anyway everyone now knows this the the but roughly like you should do so there are some new ideas in there that like really help make it work um but but it's still a large amount of synthesis so so I suppose like most good ideas are synthesis but there are always some like in the best ideas there's some leaps uh and and um I'm trying to get better at those uh but still it's mostly I guess like take five things and throw it away the stuff that doesn't work uh in the make things work and configure but yeah I think I think some some stuff needs sleep but but yeah I guess like no that's a recipe like take LMS make them super efficient long contact giant throw our L on it make it all work together it's still mostly synthesis I guess you're right uh who do you admire most in the research world and like what do you think those folks superpowers are uh she here uh no there uh yes um he uh what is his superpower uh uh I guess to an extent synthesis um he is um I mean he's he's just the best at synthesis uh he's also great at everything in the stack uh he can uh like he he has no weakness really like he could implement the whole thing uh on his own if he had to uh run it uh he sees the future I think in in a way like it's very unconstrained and and that you know I think everyone sort of crediting uh you know a number of the labs for scaling laws this guy made a presentation where he was zipping through uh uh essays or completion or whatever written by like models of various scale I was like this is a 100 million parameter model this is a 300 million paramet model this is a billion parameter model this is a 5 billion parameter mod this is on YouTube Summer it's hilarious and you like what what if we make this bigger he's sort of presenting it this hilarious way um and then I everyone else like super scientific about it uh I think gnome is generally just uh if I had to put it he's very very intuitive um I I think there like a lot of labs and and and researchers are sort of and and I think this Al this is not a bad thing it's very good uh are very evil driven very mechanical right like s very empirical in a way like no no sort of just knows he's like God this this would work and then it works um so um I think that's a superpower that uh just extremely great synthesis he has the larger he has a larger database um because he's been around for so long he just he literally knows everything I mean he invented half of the stuff that everyone's doing now uh the the uh there's there's no one who who compar um I I'd say um there there are a number of other people I guess just you you you shouldn't fail out of all the people who are sort of the ogs um of deep learning I think you could of Hinton there deserves by far the most credit just cuz he like went through all the bashing when uh when when it was like God this will never work and they're like training like tiny tiny tiny tiny tiny things they're like this will never work and he's almost stuck with it I think that's that level of grit and and belief in something that is now obviously working um deserves a huge amount of credit uh whether capsul n work or not whatever yeah he he he like you know it's it's incredible to come to something like the the the conclusions that that the world is at now and if you look at some of the older papers a lot of the ideas that are important now were in there already so so that's important um and I think he just deserves a ton of credit uman noan Brown had um uh the army of nooms Noom Brown uh I should name myid I should name my kid Noom is what I'm it's a very good strategy yeah the um it's a great strategy actually I think 100% of nooms that are like somewh popular and well known in the research Community are great yeah no he's he's he's also amazing I mean a number of labs were working on what he was working on during his PhD and he basically soloed the thing uh and and was like way better and way faster than Labs that put 10 people including some really famous names uh on it and if you just look at the paper track track record like was like look here so the rest of the field and then no 100 X's efficiency and then here is the rest of the field then Noom life does it again and it's just consistent uh I think the consistency with which he has just bashed um out uh these 100x multipliers in in RL uh data efficiency and and computer efficiency is is crazy yeah so so yeah no Noom uh the Noom Army is pretty good I want to go back to this concept of you know leaps are still needed in research and that you still have this AGI to-do list like yes what what do you think are the most interesting unsolved problems in AI right now well so a lot of it is solved now I think and uh the thing that remains to be solved is uh General uh domain uh long Horizon reliability and I think you need inference time compute test time compute for that so You' want um when you try to prove a new theorem in math or when you're writing a large um software program or when you're writing an essay of reasonable complexity you usually wouldn't write a token by token uh you you'd want to think quite hard about some of those tokens and um finding ways to spend not 1X or 2X or 10x but a million x the resources on that token uh in a productive way I think is really important that is probably the last big problem fasinating the last one okay I hope so I think it's reasonable to think that is the last big unsold I mean look over the last few years all of this other stuff got sold like oh can we do multimodal things can we do long contexts can we do all this gone reasonably smart models you know they're they're quite efficient now in terms of cost that I mean it have to be a reality denier to like not see what's coming I mean this is just a this is like a realization to a lot of people in the LM space but but like RL has been doing this for like ages so so it's just like so clear that you need to do that um or I mean maybe you don't need to maybe you can get away without doing it which be insane but if you don't need to it will still help you a lot like it's just it's just like do I want to spend a billion doll on my pre-training run and then like a little bit more money on inference or do I need to spend billion on my preing on IID for you know like 10 billion would be great but I'm going to be I'm going to be I'm going to prefer spending one um and and is bringing is bringing like the llm and RL worlds together is that like a research problem like there's still like fundamental like unsolved science problems or is that like a you know we have the recipe we just need to do it and and have the comput and the data I think uh there is no public uccessful recipe right now um there are good ideas like okay even if you take best of n make n large enough it's sort of you know it's not terrible yeah um the so there are ideas um I don't know that the final idea exists I think there's just a lot of room up from what is currently known but but there are ideas see I I I think it's very unlikely that even if you stop progress in research we would not at some point hit something that everyone would agree as AGI is just it's just that I think we can do better and maybe it couldn't REM right maybe it couldn't do all these like super hard things but it'd be pretty good and now like I'm just curious like okay like what's the actual like if we did all the things uh how good will it get uh so so um I think there is research uh left to be done and uh there are a lot of ideas floating in the world now everyone's sort of working on this but um I don't even I don't know that the current set of ideas is even final like it it'll be it'll keep moving I think let's transition to talking about magic um maybe just what what is Magic you you've been very mysterious to date so maybe just share a little bit about what you're building yeah uh I mean we're trying to automate software engineering uh the it took us a while to figure out how to train super giant models um it's a pretty interesting engineering challenge I mean fundamentally we're trying to automate software engineering from uh the product side uh and and and like a subset of that is a model that can build AGI because if it's if it's like if it's a great software engineer then you should be able to do everyone's job at Magic like if you can do everyone else's job like that that would be a upset so um the idea is that you could use this to um recursively uh improve alignment as well as uh models themselves in in a way that isn't bottleneck by by human uh resources and you know there aren't that many Noom shers in the world uh if if I had a no shazir in my computer I could spin up a million of them and maybe alignment would just be solved uh I'm was like simplifying Aton and very idealistic in in the statement I'm happy to turn this whole thing into a skillable oversight uh podcast if you'd like but um the um the core idea is is like okay like if I could just clone what we are doing into a computer and then press uh yes on the money button uh to run a cluster uh to to do the work we would be doing next week uh that that that would be phenomenal uh so so I think we sort of we're pursuing these two things in tandem where we want to ship something that's a good AI software engineer for people to use it's it's like I think one of going to be one of the first domains to to see higher levels of Automation and I I I don't like talking around I don't think the whole assistant pitch is going to last very long once these models are good enough to automate like there's just no way the economy is not going to do that uh and I think everyone knows this and they're just like they just don't like talking about it it's it's totally fine the world we used to all be Farmers we're not Farmers we're fine everything prefer everyone prefers this I think we'll figure our figure our way out in the economy if it produces the same or more stuff with less inputs like we should be able to figure that out that's not a hard problem in like from like economic principles you just have to figure out distribution anyway um but that's what we're trying to do we're trying to automate software engineering and as a part of that automate ourselves uh in doing the work we want to do and so the reason they go after software engineering then is that is the kind of lever that allows you to to admit everything else it's like the MVP of AGI right like the the the minimum viable AGI yeah cuz then it creates everything else like yeah we we we wouldn't train something like Sora Sora is great you know fantastic generate videos awesome uh it's just not interesting uh from an AGI uh perspective if you believe that models can cope themselves soon totally and so out of all the companies that are trying to build an AI software engineer you are probably the only one uh that is really taking a uh vertically inrad approach and training your own models and that is either insanely Brave or insanely crazy and pro probably a combination of both um I'm I'm curious like why I know you love training models and so I know that's part of it but like why do you think you need to own the model to get this right and like how do you motivate yourself um in kind of the David versus Goliath of like knowing that open AI exists and has great people and cares about coding and um is great at building models obviously like how do you think about that entire Dynamic I think you need well to build the best model you need to build the model uh and we want to solve these fundamental problems you can't rely on an like if the API guys solded it then what the hell are you you know we might as well start the company 3 years later it goes to the point when we started right we we started working on this stuff um two years ago so we we have you know it took us some time to learn how to train these large models like it was really I think it took open AI two years to get from gbd3 to gbd4 uh as as well and I thought we could be like much faster and this is going to be great is it's a pain so it's definitely an engineering challenge uh but it's necessary like it's not like it's not think we're doing it just because it's fun or because I like training models it's it's a massive Financial investment that uh people trust us with uh that and it's it's not like it's one of those like one to one Roi investment it's like if it's work if it works it's fantastic and if it doesn't work the GPU is ran and the money is gone uh so like you're you're getting a lot of people's trust uh doing that it's certainly not something you should do just because it's fun and you enjoy it um fundamentally I think the value will ACR at both at the at the AGI and at the hardware level and never at the application Level there's no incentive at all to offer an API if the API creates a hundred billion doll company you will just build that company internally uh it is and and if if open AI doesn't someone else will like it's it's it's it's just incredibly unimaginable to me that that would be how you would build these companies in the first place so from a business perspective I don't think that's necessarily the right way maybe there's some partnership potentials you could like oh we'll get the expens access or whatever and then different from like cloud computing right like there's there's been many1 billion 100 it's much much much harder to build Netflix and Airbnb and Uber than it is to build a chat interface like fundamentally magic is an application you press download on that we have couple guys working on and it's just there like it's not you know you can build this with like YC preed money the modes in I guess I to make the API twice as expensive for the next model and then launch my own product and then undercut every it's it's really to not own the model um in in in in this domain and in any domain that's going to generate a ton of revenue for a single company um in the case where it's distributed maybe it's fine but I don't think this will be um so so it's necessary both for the market which is good for us because the market is incentivized to fund folks like us um uh which which it isn't in other domains like have have fun writing like an email assistant you're not going to get that funded anymore uh so so so that's that's helpful but um fundamentally the reason we train our own models is because it's necessary for our mission and um I just wouldn't be interested in building like a nice little sass rapper it's just not like that's not every that's going to happen anyway and and I think though about competing against the 800 pound gorilla is like you've raised a lot of money but some people have raised boatloads of money they raised a lot more money too oh and well some people have a have 100 million plus in Revenue a year that they can spend so so it goes beyond even who could raise yeah absolutely and so how do you like how do you motivate yourself to compete in that in that uh you know reality the question is how much does it cost to build AGI and not how much money can you raise uh cuz if you can build AGI for however much you can raise uh and you're having more might help you but it won't get you there substantially sooner right like if you have all the right ideas and you get you can build it with a certain amount of Hardware like by definition like okay if someone had like 100 times more Hardware would it be like Computing that much faster or whatever but uh it it doesn't seem like a material advantage if your estimate for how much compute you need to build AGI is not as high as the revenue these companies can generate or the funding they raise is in fact much lower so Y and and I think that is the case uh so it's not by any means accessible it's very damn hard to get that much money but it's not 100 billion it's it's it and if I'm wrong I'm wrong and it'll be 100 billion and we will not have 100 billion and that's it but if we can get to that point where we have AGI and a couple others of AGI and then like the the the the sort of the benefit of additional computers there and you show an Roi it's it's like a reasonably even playing field in in terms of um additional Revenue you're just you're going to bring AI to the market you're going to raise more on it it's it's so the the starting conditions of of like have this Hardware is like you need sufficient Hardware but you don't need more than sufficient uh and and if so that's a bet that's not a you don't know but I think it's a bet with the high enough um probability of being right that it is reasonable to compete in the space and I think it is actually it is reasonable to think that um like the ROI of a of having quote unquote sufficient funding might be better than the ROI of having like infinite funding um early on um is there like an ideal for investors that is not not for me for investors is is there an ideal like team size for researchers is there a certain point at which you reach kind of like diminishing marginal returns of of adding on the extra researcher so one of my biggest weaknesses especially early on at Magic was just scaling the team effectively like we were very single-threaded on a very small number of people doing basically all the work uh in I and um I think we're getting better at that now uh it's also you just need a certain level of maturity of your code base and of your research ideas and everything to to properly segment them um so early on I would have said five uh for that time now I would say closer to 20 and I'm not including like folks working on other stuff I'm including folks working on like the models and everything i' Say closer to 20 uh I could imagine that in a few months I'll say at a slightly larger number especially when you get into large scale deployment you really want to have uh very very good uh uh processes uh around just uh having um High reliability availability of services that are detached from each other etc etc so then you can segment even more uh which which obviously stuff we're working on now um but um it sort of grows over time I I don't see it ever exceeding like the tens of people and right now it's in the Low T very low T but I don't know maybe it's it's a skill to be able to utilize if you're able to utilize 200 people you're just a better CEO than I am uh the the no seriously if if if you can it's it's a good skill uh and I think part of why I say a smaller number for us is that there is a ton of stuff we just don't do like we if we if we built a video model that would just be a separate team they built a video model and like you know that's that's more scaling so so to an extent we're more focused and that's why we're smaller but uh also if if we could double the team and be twice as fast that's that mean I would do it any day um back in ear was it late 2022 when I first met you um at the time like it was marketing assistance and email assistant were all the rage and you were the first pitch that I heard that was AI that feels like a colleague and I just remember that really sticking in my brain so in some sense you've been thinking about kind of like agents uh to use a buzz word longer than anyone else um maybe share your vision for that and like what what you think it takes to build a great agent fundamentally there are two tiers here I guess three one is useless the next is assistant that you have to micromanage and then the the next is the thing that manages you basically where you're it's sort of more like a colleague you could I think the layer where it's exactly even doesn't really exist cuz it's sort of this like little thin Point once the model is more competent than you are um you are there to give it guidance on what you want to be accomplished and um answer clarification questions but you'll never have to tell it like here's a bug uh I'm not saying that this is V1 of everything I'm not saying this is to be one of our product but F fundamentally that has to be the goal like the way I feel when I talk to my best engineer that's how I want to feel when I talk to um where we have a discussion he's almost always right and then he just writes the code and then he someone else reviews it and then it works like that experience where my job is exclusively saying like here's kind of what I want uh and then they help clarify even right like I just want here specifically that like that you you it should feel like that um and everything else doesn't matter to the user uh like what tools the Asian uses how it works does it run locally in the cloud does it need a VM does it have a browser I don't care doesn't matter our problem not your problem you care about your problems getting solved so fundamentally that's what I think matters to customers and everything else is dependent on exact product shape exact domain except exact everything and like I'm stubborn as I just don't want to launch anything that isn't that uh we will probably have to uh but the I I just really want to get that thing like I want to talk to my computer go and have lunch and come back and it built AGI like that's the that's that's the end goal right and uh there'll be there'll be checkpoints but but that yeah I don't think anything else matters uh the how how you accomplish that is uh is is up to individual company yeah how far away do you think we are from from that or I guess maybe maybe break it down into a little bit more we met in 2022 you learned how to extrapolate Eric's timelines um so uh maybe yeah one and a half or double everything I say but uh I think very soon like very small number of years I don't want to give a number now but very small number less than 10 oh definitely less than 10 I mean way less wow okay because I'm seeing some of the like the S agent uh stuff that just came out they're like 14% on sweet bench which feels like I mean 14% I just don't care about 14% like I I'm I'm I mean we we like I don't know if 80 or 90 is good enough like I like I I I think you need 99 like even 96 I don't trust my computer like I don't want to review the code if I have to like the tier of product where I have to review the code is fundamentally different from the tier of product where I don't have to review and understand the code and like you're not talking about 95 when you when you don't want to review you're talking about 99 something you're talking about whatever my developers accomplish plus some same as with self-driving cards like so the the difference with self-driving cars is like you die if the thing crashes and here you just have to review codes so it's launchable before but but fundamentally you need way way way more and like usually the last few right like the nines are hard to get um so yeah but but no I think you can I I don't know I I don't people have um I mean models have surpassed all these benchmarks I mean just recently the math benchmark right like way faster than even like for prediction markets assumed then like I I don't see that stopping um there's just too much like if if everyone was stuck and I I realized there some perception in the public that oh gp4 is like only like not getting much better get no um okay we're gonna close out with a lightning round um one more answers uh one what's your favorite AI app not magic probably all the invisible ones still like my spam filter and all this stuff uh just the things that keep life working I think are still at the moment more useful than the sort of AGI like apps cuz if you took them away like life would just be awful um like recommendation algorithms uh for whatever uh I think that's really useful uh um um other than that U yeah I think whichever uh you saying other than uh the let's say the other than the programming World um other than magic I I actually don't yeah but I'd say whichever model is currently like best would it's a very boring answer but I actually picked the SP filters Etc the recommendation Services first uh what paper has been most influential to you I don't think this paper is relevant at all in the world anymore but it was the first paper I ever tried to deeply understand like or spent months on it and reimplemented it and everything um and so it was most influential to me as a person not so much to my current work um and uh the the paper is called Deep stack it's one of those uh neural networks plus u imperfect information game solving papers it's reasonably complex for time it um uh yeah so it's per folks are interested it's it's like nowhere near s now but the uh it's s of just an irrelevant type of Al but the the at the current time at sorry right now back then it was useful um so that was very inal for me because it was just my first touch point with research uh really I I had no idea how to do research at all and then I sort of just was like I'm going to dig into this the way people like people like hyper rolling spam on Wikipedia where you like Revit hole I did that with this paper so I love it okay that's going to be my weekend reading uh last question um what are you most excited about in AI in the next one five and 10 years um just what it's gonna what's how Society is going to integrate with it um I think that's we're getting to the point now where it's really going to impact uh over the next one to five years it's really going to impact um how Society does stuff and Beyond just you know another tab in your browser that speeds you up by some percentage on some fast I think it'll get much more significant in that time frame and um ultimately you should like the only I I am not one of the intrinsic curiosity type of people I I know most researchers are I really am not I I just care about the outcome and uh that is the outcome so I'm most excited for the outcome Eric thank you for joining us again uh last time we recorded podcast we weren't actually able to talk about the thing that got us so excited about magic uh which was you had shared with us um your long context eval um and our own kind of AI uh researchers had gotten really excited by what You' accomplished on that and that was actually what Led Led to us investing in magic in the first place uh so you just made some exciting new announcements around the eval I was hoping you could share it with our audience yeah for sure thank you so much um yeah I mean we've been running around with this uh like hashes eval for a while um basically just being frustrated by uh need his eval and uh you know everyone keeps complaining about it and now that we we've decided to announce our uh where we're currently at and in terms of uh our context work um instead of just like you know blah blah talking about like oh we have so so many tokens um of context um it felt reasonable to to share the eval as well I mean we we've used it in our fundraising obviously and thanks thanks for for for backing us um and and generally just used it to guide our um our architecture development and our research so uh yeah Fel felt right to open source it and let others compare their architectures and their results with ours uh and then you know we yeah so it's exciting to share and thank you for for having back on to talk about it uh y thank you can you say a word on what's broken about needle and Haack and and what your eal does differently yeah for sure uh you know with with needle and Haack basically what you're testing is like find this weird thing the needle in in this giant pool of not weird stuff uh the he stack and so really all you need to be able to do to do this um is to sort of take like a little backpack and walk from the start of the context to the end of the context and like find the weird thing put it in your backpack and return it um you if you have to like sort of you have to sort of implicit prior that there is this thing is weird so you're more likely to remember it which sort of means that you you actually don't need to remember the whole uh the whole context window you don't you don't need to know all of it um and so so that allows you know some models I would say uh to to sort of look like they're doing this uh doing long context really well when really it's not working um as well so we decided to just go the complete opposite um super hardcore mode and just replace everything with random noise uh there's no semantic information at all because it's just randomly generated uh uh letters right basically just hashes um and uh if if you did something like needle and heack in a pool of hashes uh you really have to know the whole thing uh but then what we what we do is we also do a hop um so it's not just you find this one thing but you find this one thing and then you find another thing as obviously you know you can keep that going but that um that those two Dimensions I think really are the important quantitative components of context there are other things you can measure uh much better in in more domain specific evals of course we care a lot about code and so so we look a lot at that too uh internally but I think you know from a general purpose context evaluation uh perspective and the reason we chose to open source this eval and only this eval is just that you know I think this quantifies exactly what you want to measure when you think about long context that everything else is sort of domain specific um but yeah you you want to be you don't you want to be forced to remember the whole context window when you're talking about the context window otherwise is it really that big like yeah totally I remember our own researchers were just blown away by the the purity of the of the eval and um how well done it was and so thank you for what you're doing um and thank you for open sourcing it especially in an age where long context is becoming more and more important congratulations you back y cheers of course thanks Eric [Music] w [Music]

========================================

--- Video 57 ---
Video ID: VgC0qNLziI8
URL: https://www.youtube.com/watch?v=VgC0qNLziI8
Title: ServiceNow was a cloud application platform when cloud infrastructure didn't really exist.
Published: 2024-09-05 19:42:13 UTC
Description:
Being a pioneer isn't easy. 

With leading global customers like UBS, Deutsche Bank, and General Electric, ServiceNow had to create a reliable cloud network at a time when cloud infrastructure didn't really exist. 

Figuring out how to do that was one of Fred Luddy and Frank Slootman's biggest challenges, and ultimately, biggest successes.

Listen to the epic story to learn how ServiceNow became a $150B SaaS giant on  Crucible Moments from Sequoia Capital.

Transcript Language: English (auto-generated)
the thing that really scared us is that we were a cloud application platform at a time when Cloud infrastructures didn't exist Fred jokingly said when I joined he said oh when we first got started you know we uh we bought a used Dell server on eBay and we stuck it in the closet and that was our clout Frank realized far more so than I did what was going to be necessary to build out the resilient and scalable architecture that we have today we had customers like General Electric UBS deut Bank the largest institutions in the world I remember having a conversation early on with the chief operating officer at Deutsche Bank and he said you must succeed he says we have no way back and you know I'm thinking at the same time yeah and we're running on a Dell server running out of a closet [Music]

========================================

--- Video 58 ---
Video ID: njX6973RZZc
URL: https://www.youtube.com/watch?v=njX6973RZZc
Title: Fred Luddy, starting over at age 50, founded ServiceNow and turned it into a $150B SAAS giant
Published: 2024-08-29 20:09:13 UTC
Description:
In 2004, 2 weeks before his 50th birthday, Fred Luddy started ServiceNow out of his house with no resources and no sales team. 

Listen to a new episode of Crucible Moments to learn how he turned it into a $150B SAAS giant.

Transcript Language: English (auto-generated)
the idea for service now was that we were going to create a what you would call now a low code no code platform that would let you build applications with a tiny amount of technical knowledge My Heroes were Steve Jobs and Bill Gates and Paige and Bren and they all started to you know right out of puberty right and all of a sudden they're building these great corporations and I'm thinking what what the heck am I thinking trying to build a company especially starting at after 50 Fred was a founded that had been burned before he had a chip in his shoulder he wanted to do something special one two he had domain expertise so he knew what he wanted to build he had Crystal Clear Vision those are the best kinds of Founders in 2004 2 weeks before his 50th birthday Fred ly launched service now from his house [Music]

========================================

--- Video 59 ---
Video ID: knx_q03KOTI
URL: https://www.youtube.com/watch?v=knx_q03KOTI
Title: ServiceNow ft. Frank Slootman and Fred Luddy - From Starting Over at 50 to Dodging a $150B Mistake
Published: 2024-08-29 11:00:53 UTC
Description:
In 2004, bankrupt after the company where he’d previously worked had imploded, Fred Luddy decided to start over as a first-time founder at age 50. His vision was to reinvent the nascent IT software field for the cloud era. 

What started as simple help desk replacement software would eventually become a $150B market cap company powering digital workflows across the enterprise—but success didn’t come easy. Initially bootstrapped and ultra-lean, the company’s infrastructure began buckling under its own success as customer demand spiked. When the legendary Frank Slootman joined as CEO to help scale the company, he describes being terrified to check his email every morning. 

Hear how Frank, Fred and the team stabilized the business, expanded their product offerings, and nearly made a $150B+ mistake by selling too early.  

Host: Roelof Botha, Sequoia Capital
Featuring: Fred Luddy, Frank Slootman, Doug Leone, Pat Grady, Carl Eschenbach

Learn more here: https://www.cruciblemoments.com/episodes/servicenow

00:00 - Cold open
00:22 - Introduction
02:09 - Fred Luddy’s journey to coding
03:33 - Founding ServiceNow after financial ruin
07:11 - Finding product-market fit
15:16 - Finding a new CEO in Frank Slootman 
22:19 - Overcoming scaling challenges 
29:54 - Contemplating an acquisition offer
32:07 - Blocking the sale
38:17 - Lessons learned

Transcript Language: English (auto-generated)
[Music] it was much more existential than oh you know we got some tough challenges on our hand I didn't think we were going to live through this this is so huge so enormous how do you survive this because there are no good explanation there are no reasonable stories to be told this is just [Music] Insanity welcome to season 2 of Crucible moments a podcast about the critical Crossroads and inflection points that shake some of the world's most remarkable companies I'm your host and the managing partner of SEO Capital rof wam picture the office mail room of the 1970s in that mail room were stacks of typewritten forms a form for expense reports a form for vacation days a form for purchase orders and so on a few decades later a programmer named Fred Ludy was determined to reimagine that workflow system for the internet age and the IT Department's tasked with managing it his low code no code solution would be able to handle anything from reporting a broken laptop to managing the world's largest particle physics lab today's episode is about service now a cloud application platform for digital workflows service now evolved from an IT service tool into a workflow platform that can be used used in any Department with this Evolution its market cap has grown to over $150 billion difficult questions and hard choices shaped service now's Journey how does one rebuild their life after catastrophe and devise an Innovative idea in the process when do you know it's time to hand over the role of CEO for the good of your own company and how do you decide whether to take what seems like a once- in a-lifetime offer to sell your company or take a Gamble and remain independent these are The Crucible moments that forged service now I'm Fred ly and I'm the founder of service now my journey into coding was really pretty much an unconventional one I left home at a very early age right around the same time I got my driver's license and started working in a factory in the office part of the factory a machine came down the hallway that was wrapped in PL plastic in this clear plastic and I didn't even know what the machine was but I followed it and it went into this room with this white raised floor and people were wearing lab coats and somehow I knew that that was the place where I needed to be for a couple of weeks or months I poked around and tried to find out what was going on in that room and found out that it was a computer an HP 2100c and that there were people building applications to do things like payroll and order entry I found the passion of my life at that moment I never really wanted to leave that room until I went to a bigger computer room and uh that holds true [Music] today in 1990 Fred became CTO of paragan systems a company that built some of the first it service management software but in 2002 the company faced a major crisis the SEC would find the company guilty of massive accounting fraud perran went bankrupt and several of its toop Executives went to prison for fraud embezzlement and lying to Regulators I had all of my net worth tied up in this company and my net worth went from 35 million to zero yeah in an instant I had fear because I wasn't sure what I was going to do next but then there was relief because fundamentally I didn't like working there the year at this point is 2002 you could see on the horizon that there is going to be just tectonic shift servers now kind of sat at the intersection of two major Tailwinds my name is Pat Grady and I'm a partner at sequa Tailwind number one was as simple as the cloud transition and the benefits that this new business model provided for software companies the second was a very specific F Tailwind which was this thing called itol or I which stood for it infrastructure library and it was basically a language for talking about what was happening in it and it coincided with this broader transition from the heads of it the cios being hackers who had sort of gotten promoted up into the role to being professional business people who were more likely to have an MBA than a CS degree iil defined different processes for running your it organization largely break fix change management upgrades application development Cycles security risk Etc nobody to that point had built something to this iil standard so there was no set of applications available so we ran into really I think a a perfect storm if you will of Market opportunity the idea for service now was that we were going to create a a completely extensible low code no code platform that would let you build applications with a just a tiny amount of of technical knowledge we really wanted to build something that empowered people that had been previously intimidated by technology Stacks but my heroes were Steve Jobs and Bill Gates and Paige and Brin and they all started at you know right out of puberty right and all of a sudden they're building these great corporations and I'm thinking what what the what the heck am I thinking trying to build a company especially starting it after 50 Fred was a founded that had been burned before he had a chip in his shoulder I'm Doug Leone I'm a partner at seoa Capital he wanted to do something special one two he had domain expertise so he knew what he wanted to build he had Crystal Clear Vision those are the best kinds of founders in 2004 2 weeks before his 50th birthday Fred Ludy launched service now from his house with no resources and no sales team to pitch the product he took to the road driving up and down San Diego County looking for prospective customers I asked people would you please use our software for free but I'm asking you in return to give us you know some kind of feedback on how it's working we worked with them on a day in day out basis uh because we wanted to make sure that they they were properly engaged and actually deriving value from the software and um this turned out to be a a really good way of getting into the market so the evolution of the service now platform was really inherent in its initial design which was based on Simplicity and approachability and this idea turned out to be a fantastic one and a not good at all one at the same time Time by not good at all I mean that we built this platform and it was similar like to creating a computer language what can it do well it can do anything you want well what business problem can you solve for me well what business problem do you have we'll solve it for you so you go around in this circle because it didn't solve a specific business need almost at the outset service now faced a crucible decision how do you take such a general product to Market do you stick to your broader vision of a Swiss army knife solution or do you frame it as something more specific and so even though when I left paragan I swore that I would never ever ever go back into the IT service management area it turns out that my definition of never was about 9 months Fred took service now's broad architecture and refined it into a help Des replacement tool specifically for it departments but Fred also decided on a seat-based licensing structure with no limit on seats this way a company could organically begin using the product across the entire office not just it so the product that he had built was in his words just a forms-based workflow on top of a database the specific workflows that service now was targeting was the resolution and management of help desk tickets you join a company you get a laptop you have questions you want to be integrated to the systems with the corporation and what you really need is a back and forth forms application to do that somebody submits a help Des ticket you got to figure out if that incident speaks to some sort of underlying issue you got to resolve the incident while also inspecting the issue and you got to keep going until you get it fixed that same basic process of managing things through a workflow that's the same thing that you would do if you were tracking a candidate through the candidate pipeline it's the same thing you might do if you were tracking a prospect through your customer pipeline it's the same thing you might do if you were processing an insurance claim it's eminently more extensible eminently more scalable eminently more secure and at the same time it's more approachable using it as a lever to encourage adoption across the entire organization was a strategy Fred called going wide we had a couple of different customers that were very much Leading Edge in showing us this ability to go wide and probably my favorite one is CERN which is a particle physics laboratory in Europe they offer four or 5,000 different services to the visiting people and it could be anything from getting a classroom getting a whiteboard complaining that the toilet is is leaking or scheduling time on a large hron collider to do a particle physics experiment and CERN built 4,000 of these tiny little applications that serve this entire community and they came and presented it at our user group meeting and I think the people that were most Blown Away were those of us that had built the platform because we thought my God this is awesome that they could build these sets of applications slowly the company was growing gaining Enterprise customer like Edmonds and colcom and I would say that the thing that told me that the company was going to do well was the enthusiasm that our customers had to make reference calls they said let us talk to your prospects they actually asked us if they could talk to our prospects and I had worked in software companies where digging up a reference call could take a month because our customers hated us so much we kept meeting people from the uh VC Community First of all theyd come in and assert that they were different then they tell us exactly the same story that we heard from the prior person and then they would say well listen you guys are doing great and they say let's keep in touch and then they leave and we looked at each other my CFO and I thought well I'm not sure we're getting any value out of these so and Pat started calling like oh here's another one of these people from Sand Hill Road in 2008 I started emailing Fred three out of four emails went on answered and then the fourth email might have been a polite hey thanks for thinking of us but we don't need any money sort of thing I think during one of those responses he said but let me give you a little update on the business and the little update on the business was a couple of sentences that Bas basically painted the picture of a business that was thriving and so at that point I transitioned from the hey do you want to get on the phone emails to the hey I'll be in San Diego next Tuesday do you want to grab lunch and of course if he were to respond then I would fly to San Diego for lunch I sent him one of those and he responded I want to say at 11:30 a.m. on the Tuesday that I told him I was going to be there and said yeah yeah do you want to swing by are you still open and I responded immediately and said ah shoot my trip got pushed to next week how about same time next week and he said okay great so Doug and I flew there for for a lunch on a Tuesday which was the first time we met Fred in person when you talked with them and this is the thing that really stood out the clarity of thought and the focused on first order issues was impossible to miss he was hungry he worked like mad and he was building a great business and so it seemed like something we ought to do without much thinking and then as the conversation progressed Doug has this magical way of sort of getting a complete picture of a founder and of a business not just the stuff that they're eager to share but also all the stuff that maybe they're not so eager to share when we met Fred we gave him the pitch of SEO I remember we had T slight pitch it talk talked about all the mistakes we've made and all the mistakes we commit our companies would not make again because we've seen those mistakes I remember Fred took that slide and kept it in this binder and along the way I asked them what's the biggest challenge you have now is that keeping our systems up our data centers are getting inundated by volume and I said I know just a person that can help you and he took the name of the card and I remember that I got a call from Fred a week later he said you've already helped me more than my existing investors have helped me up to now and I knew when he said that that we had a shot of becoming investors in late 2009 Square LED service now's $41 million series D round service now gained household name customers like Deutsche Bank Intel and McDonald's the company was an A Tear doubling Revenue annually but the problems with outages that Fred shared with sequ in their first meeting were only mounting it was clear to us that the company was being just buried under its its own success the number of customers it was getting was too fast and it's probably the only time in my life that I suggested we slow down growth because I didn't think the den management team could absorb the volume of business coming our way and then Fred a wonderful founder you also have a a person that's much happier coding than interfacing with people and unfortunately as a chief executive officer you have to interface with people he was a Visionary but just left to his own wishes he'd rather be left in a corner room and code service now was at a Crossroads it needed leadership to get to the next level where would it come from after Sequoia made the investment Doug asks me Fred do you want to be the CEO or do you want to be the product guy because in our experience you can do one but not both and Fred will support you in whichever path you want to take but I don't think you'll have the bandwidth to do both and I took this under advisement and um and did absolutely nothing we did have an idea for a process that we could run to help him figure it out and so we brought Fred up to the Bay Area to meet with a bunch of people who were're Executives or founders of various shapes and sizes and so we wanted Fred to hear all of the different sides of the story and all of the different pros and cons to the various configurations that he could set up for himself it was a fairly exhausting day for me anyway they put together all these meetings we drove all over the valley and then that night Doug and Fred and I go to dinner and I just remember Fred having this big grin on his face and we're like Fred what are you grinning about he's like I know exactly what I want to do we're like okay great what do you want to do I looked at Doug and he looked at me and I said Doug I don't have any of the skills that those people have that are CEOs and furthermore I have no desire in developing them I'm your product guy we need to find a CEO we went through a really exhaustive search to find somebody we wanted a mission to find someone that could be his business partner someone who understood he was coming in as a CEO but he would have to partner with Fred not in running the business but as a business partner and if that marriage work we were going to have a home run opportunity Doug had a favorite I had a favorite we did not have a shared favorite we both didn't quite have the meeting on the minds and then we came across Frank slutman who was then a partner an operating partner at Greylock and we got wind of the fact that he had know that investing was not for him and he was looking for a CO job Frank is a doer Frank is not you know all sugar lollipop and apple pie Frank is a real deal when I was first approached by Sequoia I was slightly incredulous because this was help Des management it was the most boring business on the planet right my name is Frank slman and I was the chief executive officer of service now when I started making inquiries and and yeah everybody kind of replaces that software every 3 years and uh eventually that will just go away so I was getting all these conflicting signals about wow is this is this a hot company or is this something that is incredibly boring and has no real strategic Mojo to it so the one thing that stuck out of course is the company was growing fast I mean velocity is the Mother's Milk of venture capital okay I mean when you have velocity you got something and when you don't have it you got nothing so when you have velocity you're going to start looking closer and go like what's going on here I think what intrigued Frank was first of all he really didn't believe how well we were doing financially from a cash flow positive perspective at this age of the company there's high growth here there's velocity and there is an incredibly passionate customer base there as I sort of figured you know what I can figure out the rest I think we got the most important things covered here in 2011 Frank slutman became CEO of service now and Fred became Chief product officer leadership change is always a crucible moment because you never know what you're going to get it's a system full of people and even the best and most capable and well-intentioned people are still human beings and when you throw somebody new into the mix no matter how talented that person is or how much you might respect them disagreements are going to come up organ rejection is a possibility and there's a lot of work that you have to do to get the team to gel the first 90 to 120 days that Frank was CEO was probably the most difficult of my career I was used to being a benevolent autocrat at the corporation and now I had a position where he was in that seat Frank is a do something yesterday kind of person he doesn't mess around your lead follow or get out of the way kind of leader there's a new sheriff in town moment that does stick out and that was this company was based in San Diego but then it opened a Bay Area office and they had tickets to one of the San Diego sports teams but they did not have tickets to any of the Bay Area sports teams and when Frank found out about this he said huh well that's not fair that one office has tickets to go to the games and the other office doesn't get rid of the tickets and a couple weeks later he heard that some people had gone to the game and he went to the person who he deputized to get rid of the tickets and he said what happened I thought I told you to get rid of the tickets and he said well you know people were already signed up for the games and people really liked them so we decided we just keep him through the end of the season and Frank said okay you don't work here anymore and it's a trivial thing but the message that he needed to make sure that people heard was when I ask you to do something that's not a request that's an order I remember calling Doug and telling him that I'm not sure that this was the right thing and his only advice to me was give it 90 days and I thought well I can do that pretty easily early on Frank said well this is going to be our number one priority he sent this email out to you know six or seven people and I said well I I disagree I don't this can't be our number one priority our number one priority should be this and he calls me and he said Fred there can only be one CEO he said it's okay if you don't want me to be the CEO but there can only be one CEO and I said you know what you're absolutely right there is absolutely one CEO and that really set the tone for our relationship in that if I disagreed with him and it was something that was perhaps uh where people would get emotional then he and I discuss it oneon-one we needed him so the question was very much are you going to be here or how long are you going to be here and there were many reasons we needed him because he was he was a Visionary he was a driver he was an innovator and he knew where all the bodies were buried there were a lot of reasons why we needed threet in the worst way I mean that was just not an option at all he had to be there what Frank faced is a company which had more issues than what I understood Maybe maybe anybody understood the thing that really scared us is that we were a cloud application platform at a time when Cloud infrastructures didn't exist Fred jokingly said when I joined he said oh when we first got started you know we uh we bought a used Dell server on eBay and we stuck it in the closet and that was our clout that was the data center or or whatever had to you know pretend to be a data center Frank realized far more so than I did what was going to be necessary to build out the resilient and scalable architecture that we have today we have customers like General Electric UBS doac the largest institutions in the world who had committed themselves to a service now strategy one of the things about our system which is different from a lot of others is that um our customers can't take an outage of even several minutes and by that I mean several minutes ever I remember having a a conversation early on with the chief operating officer at deuts bank and he said you must succeed he says we have no way back and you know I'm thinking at the same time yeah and we're running on a Dell server running out of a closet many days we were up and down and sideways that after a while became super intense because we were not growing up out of that situation I had cios from Johnson and Johnson would just call up and say what in the hell's going on over there we didn't even understand the problem so people looked at us like you guys don't know what you're doing and they were not wrong we did not know what we were doing I remember Frank telling me that for the first year he went to sleep terrified of the calls he may get it in the middle of the night it was it was much more existential than oh you know we got some tough just on our hand I I didn't think we were going to live through this I mean it was one of those things this is so huge so enormous right how do you survive this right because there are no good explanation there are no reasonable stories to be told this is just Insanity facing potential collapse service now needed to figure out how to build a reliable Cloud Network we started to enlist all kinds of different people different resources everybody had an opinion everybody had a recommendation but it was a discipline that was not understood or developed at the time we went down this path and we had multiple false starts where we said this is not going to work and we had to part ways with people several times we were like looking for water in the desert it turns out that in the end our VP of engineering that was part of the incoming team Dan McGee was an incredibly strong process and discipline guy and and he really figured out how to homogenize the infrastructure meaning that everybody ran on exactly the same software stack so homogenizing making everything absolutely the same in terms of patch levels inters levels all these different things that process you know started to really freeze out you know a lot of these problems they were just uh completely cookie cutter in the end and that helped us really get over this chaos and Mayhem that we experienced in the early days it went from being the absolute worst to being one of the best in the industry and also incredibly economically viable as well our gross margins became very very strong among the other issues Frank faced were unworkable contracts with customers that had to be Rewritten a lack of patents for its intellectual property that put the company in danger of catastrophic lawsuits and the need for more talent to fill out every Department from sales to research and development I've been in companies that were dramatically overspend and over staffed and this one was the exact opposite it it was the scariest of scariest experiences to see how thin this company was on this technical Talent we massively backed up the truck it was a hiring I it was insane the amount of hiring that we did and we actually went into a loss making position for a couple of quarters you know even though the company was massively profitable but for the wrong reasons we doubled the the size uh of the entire company in terms of number of employees in the matter of months and uh it shocked the board of directors they're like he did what uh but you know in hindsight you know I would have gone faster if I could we would always get up in the morning one foot in front of the other we did what we had to do however difficult and ugly it was uh and confrontational it was at times we kept moving in the end we solved all these problems they all went away so in other words it can be done in addition to solving some of service now's most pressing issues Frank also LED an effort to broaden the company's product offerings and capitalize on the promise that was inherent in Fred's initial product design we fairly quickly started to look at evolving our positioning from help desk replacement to what we refer to as the Erp for it the idea was that it would have its own system of record would have its own platform where all the functions of it would be you have a single database and a fully integrated system that had never been done and uh people looked at this like what Erp for it what are you talking about you're help this replacement tool here's the important thing now everybody in it is going to be licensed on service now not just that small group of help desk management people we we just said look if you work on it and you don't have a log on to the system what are you doing here now obviously from a business model standpoint you know you can now sell you know 40 50 times as many licenses to the same Customer because we would go in and say yeah we're not here to license your help Des management staff which is a very small group of people we're here to license everybody uh including you Mr CIO but it was only the beginning because we had you know other Transformations that were coming after that there's no one moment that stands out as the moment where we said thank goodness Frank is here it was almost all of the moments I have the utmost respect for Frank and I considered it to be a an honor to work for him and I I learned a lot from Frank and I'm very very appreciative of that relationship that we had I remember being at a board dinner and Fred and I were talking about how great things that worked out with Frank and I said thank God we got Frank because in my opinion Fred we were 9 days from going out of business with Frank's cleare eyed stewardship and Fred's Visionary product design service now was growing from a help desk replacement tool to the Erp of it and then beyond to a multi-use platform that was being used across every corner of the Enterprise the company's success was not going unnoticed service now started getting unsolicited offers for an an acquisition and they were companies large and small including Dell and then we did get an offer from from VMware and that was an exciting thing because VMware at the time 2010 was really just on top of the world they had a huge market cap they were really well revered in the technology industry and then I I flew up to Palo Alto and met with Carl ebach we reached out to service now we realized that this could be a very good platform for us to have as an asset as we continue to build out our management approach to how it and service management was delivered my name is Carl ebach I'm the former president and COO of VMware and I'm currently the CEO of workday we also knew that at the time service now is having some challenges with their platform and Technology this isn't a time where we were struggling greatly we were like we don't know whether we can do this you know with the level of challenge that we have so that's that's when you start entertaining these kind of scenarios you're like the last thing I want to do is fumble this and basically have a train wreck on my hands I think we had some of the best engineers in the world and we could help them modernize the product bring stability to it and help them scale uh the technology as well one day I got a call from Frank slutman saying there's an offer in a table to buy the company for $2.5 billion back then multi-billion dollar exits were pretty goddamn rare So if there's one being offered you don't summarily dismiss that when you're a VC you're betting on many companies we only have one this is not Peter Pan okay this is real life at the time I was about 55 years old I'd been bankrupt twice I had a new child so this was a chance to be not bankrupt I really had my my heart set emotionally on being acquired by VMware and then brought it to the board and said that's what I would really like to do is it' be acquired but not everyone was so excited this offer was the old holiday sneak attack my now wife then girlfriend and I Sarah were in New Zealand and we went for one of these three-day hikes where you just kind of you're just kind of off the grid for three days and we come back in from this three-day hike and I'd had no cell phone reception no email service for the first time since joining Sequoia and so I was already a little bit nervous about what might have happened while I was off the grid and so as this flood of emails comes back into my inbox I see one from Doug where the subject line says in all caps where are you please and then the body of the email was just call me and I asked them to put together a presentation that we're giving it a wayo for $2.5 billion we created 10 slides and we sent it to Fred and Frank you know Mr Leone always says very nice things but he's also very um unambiguous at times and he said uh Fred you're getting screwed and I thought well if this is getting screwed I might I might learn to like it and he said I cannot in any way support this Doug was the one he said Fred you're going to be a 10 bagger what's a 10 bagger he goes you're going to be worth $ 1010 billion why are you selling for $1 billion you have a CEO and a Founder who want to sell the company and investors who want to prevent this at all costs service now was at a Crossroads that would have an enormous impact on the company's fate I'm on vacation Hawai and for seven days my family saw me pacing back and forth with a flip phone in my ear we were in a situation where we had no leverage whatsoever we bought common shares from Fred we had no voting rights in fact we had no rights as board members we can voice our opinion but that's where it began and ended and so so I tried very hard to find a way during a Christmas holiday to stop this sale it's right around December 22nd 23rd and I had taken my son for a walk down the horse trail next to our house and he's holding on to my little finger he's in his onesie you know with his footy pajamas and uh I get a call and it's it's Doug Leone he goes Fred do you want us to write you a check for $100 million Doug said well I will personally buy the shares of anybody who wants to sell at this price and people thought that was a nice sort of show but that also didn't necessarily end the conversation because people were still inclined to go after the offer again this is like a couple days before Christmas and I'm like uh no I'm good I'm good I remember calling Steve bachner was then I believe the CEO of Wilson sansini I told Steve Steve they want to take an offer and Steve told me well no they can't take an offer they have a fiduciary duty to shop the company around for a a maximum type of valuation and so I called a board meeting I said we have to shop the company around and the then lawyer of service now the outside Council said Doug only public companies have to be shopped around I go shoot uh I got bad advice so we ended a board meeting I call Steve bachner again Steve they're telling me that only public companies have to be shopped around he said no no private company also have to be shopped around I said well how am I going to argue this he's going to say public I say private I need something more and this is where one of those breaks of a lifetime happened and so Steve said we happen to have hired a Wilson sansini as one of our partner The Honorable Bill Chandler in a Washington DC office who actually wrote the law that private companies must be shopped around as well I go wow can we get Bill Chandler on the phone so Steve Bill Chandler and I talk and he confirmed everything that Steve bachner said I asked Bill whether he'd be available for a phone call during a board meeting and I called yet another board meeting uh it might have been the 23rd of December now Doug are you thick you know I I got a little abused aren't you listening only public companies need to be shopped around and I said well I just happen to have a on standby The Honorable Bill Chandler in Washington who actually wrote the law I got him on the phone and he explained very clearly that it's not just public company but private companies also the board and the attorney was stunned uh I asked if there were any questions of Bill there were no questions and I and right after I put down the phone I said who's going to call Larry Ellison and there was silence in the room and I knew that then and there I killed the m&a transaction I was hyper disappointed that we were going to make a our minds not to sell to to VMware but in the in the back of my mind I knew that it was it was the right thing to do I thought it was a tremendous if you will jiujitsu moov by Doug and the seoa team they knew the market they were selling into was right for disruption they knew it was a very big Market opportunity the total addressable Market at the time was quite large I think it was super unique and I can't recall over my career now spanning almost 37 years that this is actually happened it's once in 10 careers stories that this happened just a few months later in June of 2012 service now celebrated its IPO after Fred Frank and the other members of the management team rang the bell at the New York Stock Exchange the company's value soared 29% by the end of that year its market cap was 3.6 $7 billion over a billion dollar more than vmware's offer 2 years later Doug's 10 bagger prediction came true service now reached a market value of $1 billion I'm glad that he was right about the 10 bagger which is a onet of almost onet of what we're valued at now selling the company to VMware versus living as a public company with 150 billion of market cap and perhaps living as the con iCal example of performance at scale as the Enterprise company that all other companies admire and look up to and hope to someday be like that's a pretty big difference I would advise companies that receive an acquisition do not get Charmed by a number that seems like a big number or a number that some model says is pay two year forward don't get shot by that the only question that matters is what can this company be 5 years from now 7 years from now 10 years from now that is the question how big is your ambition and let a ride today service now is a One-Stop shop for cios making work simpler for over 7700 companies there is quite literally no other software company at their scale putting up the sort of numbers that they're putting up it's a behemoth it's a respected Giant in a software industry and when I walked iners what it what it is today it's mindblowing you know we were uh 250 people at the time we were doing not even 100 million in sales it's doing 10 billion and still have a lot of you know a lot of fuel in the tank to to drive the growth of the company sometimes it's nice to hear a story like this because it's like you had the worst Terror known the man and yet here's where we are today so it's not just wishful thinking these things do really happen so you know keep the faith Fred ly nailed the product here is a guy who dropped out of high school to work as a programmer and in his early to mid-50s having had literal Decades of experience in this market decided that he could probably do it better than he'd done it before and he absolutely nailed it technology that's going to last a long time requires extreme Simplicity and extreme modularity but I think it's probably really the corporate culture that has let the company grow through now four different CEOs and Branch out into the market expanse that it has so our corporate culture was always operate with great integrity be very honest about our capabilities be forthright about any shortcomings take accountability for when we have made mistakes and I think that because of that our customers have been so willing to work with us in good times and in bad and to help the company grow we really do form a partnership with with our our customers so I'm very proud every day to be you know associated with service now I can't wait for the next 10 or 20 years [Music] this has been Crucible moments a podcast from sequa [Music] Capital Crucible moments is produced by the Epic stories and fox creative podcast teams along with seoa [Music] capital special thanks to Fred buddy Frank slutman Pat Grady Doug Leone and Carl asbach for sharing their stories [Music]

========================================

--- Video 60 ---
Video ID: RAZFDY_jGio
URL: https://www.youtube.com/watch?v=RAZFDY_jGio
Title: Sierra co-founder Clay Bavor on Making Customer-Facing AI Agents Delightful
Published: 2024-08-27 09:00:18 UTC
Description:
Customer service is hands down the first killer app of generative AI for businesses. The reasons are simple: the costs of existing solutions are so high, the satisfaction so low and the margin for ROI so wide. But trusting your interactions with customers to hallucination-prone LLMs is a big problem.

Enter Sierra. Co-founder Clay Bavor walks us through the sophisticated engineering challenges and tremendous improvement in capabilities that his team has experienced in the past 18 months. The company’s AgentOS enables businesses to create branded AI agents to interact with customers, follow nuanced policies and even handle customer retention. Clay describes how companies can capture their brand voice, values and internal processes to create AI agents that truly represent the business.

Hosted by: Ravi Gupta and Pat Grady, Sequoia Capital

00:00:00 - Introduction
00:01:21 - Clay’s background
00:03:20 - Google before the ChatGPT moment
00:07:31 - What is Sierra?
00:12:03 - What’s possible now that wasn’t possible 18 months ago?
00:17:11 - AgentOS
00:23:45 - The solution to many problems with AI is more AI
00:28:37 - 𝛕-bench
00:33:19 - Engineering task vs research task
00:37:27 - What tasks can you trust an agent with now?
00:43:21 - What metrics will move?
00:46:22 - The reality of deploying AI to customers today
00:53:33 - The experience manager
00:58:09 Engineering-led or ops led?
01:03:54 - Outcome-based pricing
01:05:55 - Lightning Round

Transcript Language: English (auto-generated)
one of the more interesting learnings from the past you know year and a half of working on this stuff is that the solution to many problems with AI is more Ai and it's somewhat unintuitive but one of the remarkable properties of large language models is that they're better at detecting errors in their own output than in not making those errors in the first place [Music] joining us today is Clay bavor co-founder of Sierra before clay started Sierra with his longtime friend Britt Taylor he spent 18 years at Google where he started and led Google Labs their arvr efforts and a number of other forward-looking bets for the company Sierra is allowing every company to elevate its customer experience through AI agents and there is no one who knows more about what AI agents can do today and what they'll be doing tomorrow than clay you'll get to hear about how pictures of avocado chairs helped Inspire the founding of Sierra why the solution to problems with AI is often more Ai and so much more please enjoy this incredible episode with my friend Kay Bor all right clay listen this is a funny start because we know each other so well but can you just tell everyone a little bit about yourself and just give us some background before we talk about the future fut of AI and where what role Sierra is going to play on that so first of all I'm in a I'm a Bay area native I grew up not more than four or five miles from here so grew up in the Bay Area got to see the kind of.com uh bubble grow and then burst studied computer science and then ended up right out of undergraduate at Google where I was for 18 years until uh last last March and so at Google I worked on really every part of the company I started in Search and then ads uh for several years I ran the product and design teams for what is now workspace so Gmail and Google Docs and Google Drive and so on and then spent uh the last really 10 years at Google working on various forward-looking bets for the company some Hardware related like virtual and augmented reality some AI related like Google lens and other applications of AI and uh and then uh 15 months ago left Google to start Sierra with a longtime friend of mine Brett Taylor we met in our early days at Google where we both started our careers in the associate product management program so he was I think class one I was class three and uh we met early on and uh stayed in touch in in particular through a a monthly poker group that in a good year would play like once and uh met up December of 2022 and just saw what was happening in and around Ai and these fundamentally new building blocks that we thought would enable us to create something really special and started here out of that so that's the recap actually I'm curious on that um and and we we need to get to what is here pretty quickly here but just for fun December 2022 very shortly after the chat GPT moment how I guess what was the process like or how soon after that moment did you have the conviction that this is a sufficiently interesting new technology to build a company around can I introduce one thing that's kind of interesting I hope you talk about before you actually before the chat gbt moment you had been telling me about how everything was going to change I still remember distinctly him telling me you don't understand you're going to be able to talk about a scene that you envision and they're going to be able to make a movie out of you just talking about do you remember you telling me yes and so I I'm actually very curious about this too well I had such a privilege seat at Google to see so much of you know what came out of that Transformer paper in 2017 and the emergence of early large language models so at Google one of the first was called Mina or Lambda there was a paper I think in 2020 a conversational chat bot for just about anything and I remember even before that getting to interact with this thing in a pre-release prototype and having this uncanny sense that there was someone something on the other side of it and that this was different and another moment I think it was mid 2022 when we had I think it was the first or second version of paulm Pathways language model at Google was a 540 billion perameter model and we were testing it to see kind of how smart it was and one of the shest and sign signs of intelligence is the ability to think and reason in metaphor and analogy so we tried a few things and one which was pretty straightforward is we we asked Palm hey explain black holes in three words and it came back without skipping a beat black holes suck and we were like oh you know that's a that's a pretty good summary um also like you know the model seems to have a sense of humor which is cool and the moment the moment that really blew my mind we asked and I remember the answer for verbatim we asked Palm please explain the 2008 financial crisis using movie references and again without without skipping a beat so the 2008 financial crisis was like the movie Inception except instead of Dreams within dreams it was debt within debt whoa and we all paused what is this right so it had understood basically the concept of cdos nestedness of debt okay what movie includes nestedness of something else Inception nestedness of dreams so it's like Inception and we all we all thought wow this is something new and and different and um and then there were a couple other moments I remember the first Dolly paper came out and they did a blog post and people reacted a little bit to it but for me I I remember one of the the stars of the show was they asked Dolly to make avocado chairs and so they and I know this sounds so odd but here is a set of 10 or 20 images of chairs that look like avocados it wasn't Photoshop these images had never existed before and yet the model seemed to understand similar to the the movie reference metaphor concepts of avocad and chairess and put those together and create these images pixel by pixel so we have avocado chairs at inst theart the uh it's actually did you really we actually did we actually had chairs shaped like avocado in related news there were times where we were burning a little bit too much money you know those those bags too yeah those bags the uh so had a had a good sense that something was coming and in fact uh the team the team I was running at Google at the time Labs was putting a a lot of large language models to use and in early applications there and so how to hunch um chat GPT uh certainly clarified that hunch um but I think Brett and I both for several years had been tracking what was happening and just seeing you know first it was translation and better than human level translation then it was some of this language generation and I think uh credit to open AI for doing the engineering work and uh data work and much more to make uh gpt3 turn into chat gbt where suddenly you could grasp this thing's full potential without you know knowing how to write Python and use their apis all right so we're going to talk about where AI is going we're talking about agents we're going to talk about customer service right but first just can you maybe just tell people a little bit about Sierra and what you and Brett have created yeah so in a nutshell Sierra enables any company in the world to create its own branded customer facing AI to interact with its customers for anything from customer service to to Commerce and the backdrop for this is this observation that anytime there's been a really significant change in technology people interact with computers with technology and different ways and as a consequence businesses are able to inter interact with their customers in entirely new ways and you saw this in the 90s the internet made the website possible and for the first time a company could have a sort of digital storefront and be present to the world update its inventory uh with the click of a button and and so on in you know the mid to Mid early 2000s 2005 2008 if you were a company you could all of a sudden through ubiquitous social networks interact with your customers at scale and have conversations at scale and in 2015 right after the rise of of smartphones right as a company you could put kind of a Swiss army knife version of your company in everyone's pocket and so like I bet you have your bank's mobile app on your phone probably on your home screen so the last few years of advances in AI has for the first time made it possible to create software that you can speak to right software that can understand language software that can generate language and most interestingly I think software that can reason and make decisions and um it's it's made for really delightful conversational experiences like those that we associate with with chat GPT and so we think there's a big big deal for how businesses interact with with their customers and um you you think about the difference between how we do some things today versus what you could do if you could just have a conversation with the business you're interacting with think about like shopping you're in the market for some shoes right or Pat maybe for you some new weights or something very very heavy weight tiny tiny tiny and little and and you know you're on the website and it's like you basically have to imagine how the company's designer would have organized the product catalog so okay men's men's shoes men's running shoes men's Racing shoes light lightweight vaporfly I can't remember the name and so on um instead you with conversational AI you could just say hey I need some super lightweight running SHO kind of like those ones I got last time what do you got and it's almost like I'm dating myself a little bit here but like Yahoo directory where you navigate through this hierarchical structure to find what you want in contrast to Google or just you explain what you want and and this takes it several steps further um and there's a a quote from uh the head of customer experience at one of the companies we work with she said I don't want our customers to have to have a master's degree in our product catalog and our corporate processes and to do a lot of things you know buying shoe is fairly easy on the spectrum of interaction interactions you have with companies imagine you know adding a new person to your insurance policy like where do you go in the mobile app for that how do you get that done and your eyes just glaze over right and so the alternative talking to an AI and in particular an AI agent that's a technology around which we built Sierra um where that AI agent represents your company your company that's um we think is is really really powerful and um even in you know we're 15 months old as a company we've had the privilege of already working with storyed Brands like Weight Watchers Sonos Sirius XM olai if you're on the market for new flipflops I strongly recommend oai flip flops I have two pairs uh very good excellent also make great golf shoes um oh really oh yeah yeah yeah you should get some all right great um and uh and so for for Weight Watchers we're advising on points and helping members manage their subscriptions with SiriusXM we're helping uh diagnose and fix radio issues and figure out what channel your favorite music is on um and so on and and the results um again in the first year of the the platform out there we're in one case resolving More than 70% of all incoming uh customer inquiries at extremely high customer satisfaction and um all this leads us to believe that every company's going to need their own AI agent and we want to be the company that helps every company build their own in the spirit of sort of the you know the future of these AI agents and what they could mean for customer facing Communications or customer facing operations are there any good examples of things that were not possible 18 months ago that are possible today and then maybe if we roll the clock forward things that are still not quite possible today that you think will be possible yeah 18 months from now yeah first of all the progress month by month and over 18 months in particular is just kind of breathtaking 18 months ago GPT four class models didn't exist right it was still kind of a something just coming over the horizon agent architectures cognitive architectures kind of the way you compose large language models and and other supporting pieces of infrastructure were very very rudimentary and so i' go so far as to say like the idea of putting an AI in front of your customer is that could be helpful and importantly safe and reliable that was just impossible and so chatbots from even uh even 18 months ago looked a lot like a pile of hard-coded rules that someone cobbled together over you know months or years that became very brittle and I think we've all had the experience of you know talking to a chat about I'm sorry I didn't get that can you can you ask in a different way or my favorite my favorite is when you know they have the message message box and then like the four buttons you can click but the message box is blanked out and you can't actually use it and so you know I can help you with anything so long as it's one of these four buttons so so most of what most of what I described right fixing radios processing exchanges and returns and so on wasn't possible at least in any satisfying way or in a way that that led to real business results for for companies 18 months ago um fast forward in 18 months uh you know I I think we go pretty deep here I think multimodal models are quite interesting uh something like 80% of all customer service inquiries are on the phone not on chat or email so voice will obviously be a huge part of it things like returns exchanges uh diagnosing radio issues and and things like that are on the simpler end of the spectrum of the total set of tasks that you might want to um get help with from an AI agent and so I think more advanced models more sophisticated cognitive architectures all of those I hope would increase kind of the you know the smarts in the agent the types of problems it can solve uh and then trust safety reliability you know the hallucination problem I think is still an an an unsolved area and we've made others have made huge amounts of progress on it but I think we we can't yet declare Victory how quickly do you think it's going to become you guys are doing so much for the customers not just customer service but you know working all the way through the funnel but on the customer service side how long is it going to take for to become the default that folks expect that they will be able to have someone or an AI That's available at any time to answer any question you know make that real for us yeah I I don't know and in part uh there is there's a bit of a hole to dig ourselves out of um as not a company but as as an industry where it's like when was the last time you had a great interaction with a chatbot on a website and you know I think if you pulled 100 people and you're like do you like talking to customer service chat Bots probably zero out of a 100 would say yes on the other hand if you ask like hey do you you ask 100 people do you like interacting with chat gbt maybe 100 out of a 100 would say yes and so I think some of the work we've been doing in our product is to educate uh our customers customers upfront that like he this thing's actually really smart and good one of the interesting specific techniques for doing that is we stream our answers out word by word similar to how chaty PT does people are so used to the you know message message me message the streaming answers is something of a kind of visual signature for oh there's a really smart AI behind this and so I think what we find is customer satisfaction is extremely high with our a AI agents um you know in the mid mid four so 4.5 out of five stars and um which in some cases is higher than customer satisfaction with uh human agents and in In fairness they often get the hardest cases and the cases that you know we will hand off because you know the customer became angry or uh was especially frustrated or something but still those those results are really significant and so my guess is over just the next few years I think people will realize oh I can get my issue resolved faster this thing is actually capable and can not only answer my questions but you know one of the things we're really proud of is we go far far beyond just answering questions but can actually take action and and get the job done can you talk a bit about agent OS and some of the Frameworks that you put around the foundation models to make everything work yeah so it's been such an interesting Journey learning what's required to put AI safely reliably and helpfully in in front of are customers customers and um a huge part of that really the the first part is looking at what are the challenges with large language models and how do you address or meaningfully mitigate those and so start with hallucinations um I don't know if you saw it but there was an example from a few months ago where Air Canada's chatbot that I think was based on an llm and apparently not much else was interacting with a gentleman who had questions about their B M policy and I I think the the person had had someone pass away in his family and was asking about refunds and credits and so on and the AI made up a bement policy that was quite a bit more generous than your Canada's actual bement policy and so the man took a photo and later you know claimed the you know full amount of that refund and so on they said No actually that's not our policy and bizarrely and I I don't quite understand this um the the case went all the way to court Air Canada loss um and I our thought was like Hey you know it's just you know it's like $500 like Canadian dollars right so um so but but hallucinations are a real Challenge and on top of that just to enumerate some of the things to to overcome uh and that that we have with agos no matter how smart you know gbt 5 or six is like it won't know where your order is right or which seats right you've booked on the upcoming flight or whatever it it's obviously not in the pre-training set and so you need to be able to safely and reliably and in real time integrate an AI an AI agent in our case with systems of record to look up customer information order information and and so on and then finally most customer service processes are actually somewhat complex right you go to call centers and there'll be flowcharts on the wall of like here's here's how we do this and if there's an exception this way and so on and and as capable as you know gp4 and Gemini 15 class models are they'll they'll often have trouble following complex instructions and uh we we saw one example in an early version of an agent that we prototyped where you'd give it five steps in a returns process or something and you'd say hi I I need to you know return my return my order or whatever and it would jump straight to step five and then call a function to return the shoes with username uh John Doe example.com comma order number 1 2 3 4 56 so it would not only hallucinate facts or bement policies but even function calls and and function parameters and so on so with agent OS what we built is essentially a toolkit and a runtime for building industrial grade agents that um I don't want to say that we've solved every one of these problems but overcome and mitigated the risk rks in these problems to such an extent that you can safely deploy them at scale have millions of conversations with them and so on and it starts at the foundation layer uh I don't mean Foundation model uh layer but just the base layer of the platform where you have to get really important things like data governance and uh detection masking and encryption of person identifiable information right and so we built that right into the platform from from the ground up so that our customers data stays our customers data so that their customers data is protected um we for instance detect mask or encrypt um all pii before we log it to durable storage right so knowing that we're going to be touching addresses and phone numbers and so on um can handle that safely a level up from that we've developed uh what we call agent SDK or agent SDK and it's a declarative programming language that's purpose-built for building agents and it enables an agent developer most of whom sit within the four walls today of Sierra to express highlevel goals and guard rails around agent Behavior so you're trying to do this uh here are the instructions here are the steps and a couple of the exceptions cases and then here are the guardrails and to give an example of that uh one of our customers Works in kind of the healthcare adjacent space they want to be able to talk about the full range of their products without dispensing medical advice right so how do you create those additional additional guard rails um and then uh so you can Define kind of the behavior and Scaffolding for complex tasks for AI agents with agent SDK we also have sdks for integrating with uh contact centers when we need to hand off we're integrating with um systems of Records uh like the order management system uh and so on uh and then finally for integrating our chat experience directly into a customer's mobile app app or website iOS Android web and so on um and then once you've defined the agent using agent SDK we then have a a runtime where we uh abstract away what happens underneath the Hood from uh from the developer so that they can Define what the agent should do Define the the what and then agent OS takes care of the how and so for some skills there might not be one llm call but five six seven 10 separate LM calls to different llms with different prompts in in other cases we might retrieve documents to support answering an accurate a question accurately with and so on and agent OS you know in in the spirit of an actual operating system abstracts away a lot of that complexity kind of the equivalent of IO and resource utilization and so on so it makes the whole process of building and then deploying an AI agent uh much faster and much safer and more reliable and when you think about what you just said clay of like when you call multiple llms is that in a supervisory capacity sometimes too where you end up having like a supervisor agent reviewing the work of a lower level one of the more interesting learnings from the past you know year and a half of working on this stuff is that the solution to many problems with AI is more Ai and it's somewhat unintuitive but one of the remarkable properties of large language models is that they're better at detecting errors in their own output than in not making those errors in the first place and it's kind of like if if you or I were to draft an email quickly and like okay let pause let me proofread this does this make sense do these points hang together oh actually no I I miss this and even more powerfully you can prompt llms to take on in essence a different Persona so a supervisor's Persona and it seems with that you can elicit more Discerning behavior and a closer read of the work being reviewed so to your question Ravi yeah we in addition to building the the agent itself have a number of these supervisory agents that basically it's like a little Jiminy Cricket agent looking over the shoulder right of the primary agent is this factual is this medical advice is this financial advice uh and is is the customer trying to prompt inject and attack the agent and get it to say something that it shouldn't all of these things and it's through layering all of these the goals the guard rails the task Scaffolding in using agent SDK Within These supervisory layers that we're able to get both to the performance levels we are 70% plus resolution rates but also to do that really safely and reliably that's one of the cooler things I've heard is just you know the tell it to have a different Persona and then all of a sudden it behaves differently like I remember when I first saw it on chat GPT of when it doesn't help you on something just tell it it's really good at it and then it's more likely to help you or is a remarkable situation it's it's very strange and one of the weirdest adjustments over the past you know 15 months building these things is I'm sorry we're programming with English language and we can give it the same English language and it can say something entirely different and on prompting techniques I mean it it's fascinating even with no new models coming out right um given given a fixed model you can elicit better and better performance of it from it simply by improving how you prompt it and there was a paper that came out three or four months ago that suggested that like emotional manipulation of the large language model would get better results so the the kind of the prompt suffix that they figured out so you you say hey I need you to perform this task you define the steps and so on and you end with it's very important to my career that you get this right and the performance goes up you're like what is this like what are computers now for the record we don't use that prompt s any of our at least not that I know of um but things like Chain of Thought think step by step let's take the step by step write elicits better reasoning for very interesting reasons um you know other methods of task decomposition and kind of narrowing the narrowing the set of things that the LM needs to keep in mind at the same time improves reasoning if if you're precise about what you want it to do so all of these techniques are um those that that we've applied and built into to H andos and uh actually our we have a small But Mighty research team and um uh our head of research caric Nar simhan was uh by the way that was incredible pronunciation oh his Grand would have been so perfectly happy with how you pronounced his name well done soft te yeah soft te nicely done it's uh yeah it's not a t and it's also not a th somewhere in between thank you thank you very much um he helped write the react paper one of the first agent Frameworks um one of our researchers wrote the reflection paper where you can have uh the agent pause reflect on what it's done think through am I doing this right before proceeding um and uh and so these are all things that we've been able to incorporate in in quite a direct way you should talk about the most recent research the tow bench oh tow bench yeah yeah yeah it took me a while when I was trying to send the email saying I liked the paper to find the tow symbol on my know computer it took Robie a while because he's to this date never actually read a research paper I read this one no no no he had to figure out how to put it in the chat TPT and say please write a paragraph that makes it sound like I read this research paper well uh either you either you I ref to comment well look either you or chat gbt did a great job on that email thank you so we're a team yeah so uh tow bench is our first research paper first of all TOA is a Greek symbol it's spelled t and it stands for Tool agent user Benchmark and what we observed was that the benchmarks out there for measuring the performance of ai ai agents in particular were pretty Limited in that basically they would present a single task here is here's something we need you to do and here are some tools you can use do you do the job or not and the reality is interactions with an AI agent in the real world are way messier than that right they they take place in the space of natural language where customers can say literally anything or describe whatever they're trying to do in any number of ways it happens over series of messages the AI agent needs to be able to interact with the user to ask clarifying questions gather information and then use tools in a reliable way and it needs to be able to do this you know a million times reliably so the benchmarks out there we found really lacking in measuring the very thing that we are trying to be the best at and and so our research team set out to create a benchmark that measures we think the real world performance of an agent in interacting with real users using tools with all the messiness that I just describe and uh the the big picture approach that we took is pretty interesting so you have an AI agent that you're trying to test you have another separate agent that acts as the user so basically user simulator and the the AI agent you're testing has access to a set of tools it can use think of these as like functions to call so a simple one would be I'm going to do some math using a calculator tool more complex one might be hey I'm going to okay returning this order with the following parameters this order number credit to credit card or uh store credit or whatever and and then you basically run a simulator where the agent has a conversation with the user simulating agent and at the end we're able to test in a deterministic way did the uh did were the functions used in the right way and the way we do that is we basically create a mock uh database that those tools interact with and modify so were they modified in the correct way so what's neat about this is you can initialize the conversation so that the user has many different personas they could be grumpy they could be confused uh they could know what they want to do but uh speak about it in a clumsy way and and so it doesn't really matter the pth path that the AI agent takes to get to the correct solution so long as it gets to the correct solution now what came out of this was pretty interesting and I think it it strongly motivates the the development of things like agent OS and Frameworks and cognitive architectures for for building these agents so the upshot is llms on their own do just an absolutely terrible job at this task yeah and and so even even the frontier models in something as simple as processing return and mind you the the instructions given to the agent being tested are quite detailed right the the functions the tools it can use are quite well documented and so on and yet on average the best performing llm on its own got to the end of the conversation correctly 61% of the time and that was in returns it was modifying in Airline reservation we had two kind of uh simulation versions um the best results were 35% now what's interesting is you know we all know that if if you take a number less than one to the N power it quickly gets very small and so we developed a metric we call pass at K which is okay if you run this simulation eight times and remember you can make use of the non-determinism of LMS to have the the user simulator be different every time so you can permute that well 0.61 to the e8th power is about 25% so you then imagine well what if you're having a thousand of these conversations you're so far off from being able to rely on this thing um so the the upshot is much more sophisticated agent architectures are needed to be able to safely and reliably put an agent in front of really anyone and uh that's the very thing we're building with with agent OS and a lot of the tooling around it how much of that do you think is an engineering task and how much of that is a research task and I guess maybe the question behind the question is time frame to having useful agents deployed at scale and Broad domains of tasks yeah well I I think the short answer is it's both but I'll say more concretely I'm very optimistic about it being in large part an engineering challenge and that's not to say that the next wave of models and improvements in the the frontier models won't make a difference I I believe it will in particular we're seeing techniques like better fine tuning for function calling uh agent oriented uh fine tunings for foundation models or some of the open source models those will help um but the approach we've taken in building agent OS and kind of the foundations of Sierra is really treating building AI agents as first and foremost an engineering challenge where we are composing Foundation models we are composing fine-tuned open- Source models that we've post-train fine-tuned with our own proprietary data sets and by composing multiple models in interesting ways by supplementing what llms can do on their own with retrieval systems like retrieval augmented generation to improve grounding and factuality by supplementing the kind of inbuilt reasoning capabilities of llms with I'll call it reason scaffolding that live outside of the models where you're composing planning task generation steps draft responses the supervisors that we talked about and doing that outside the context of uh the LM we've been able to put AI agents in front of a huge number of our customers customers and safely and reliably and so so I I don't think it's you know something over the horizon it's already over the horizon I think um looking ahead I think there are a few different Avenues where we'll see progress one is in the foundation models we talked about that and um as as the capabilities grow you know agents will get smarter and we've architected agent OS in such a way talked about abstracting kind of the what from the how where we'll be able to swap in you know the the next the next Frontier Model and everyone's agent will just get a bit smarter it'll get like an IQ upgrade um by the way similarly and interestingly we can swap in less broadly capable models but models that are more capable in a specific area so for instance uh triaging a case or coming up with a plan and so on we can use much smaller models that actually are better faster cheaper choose three you know all all at once um and then I think we're we're seeing progress literally week by week on the engineering of these agents and building in uh not only new and better components under the hood and the architecture but um new approaches and tooling around basically teaching these agents to do it better and better um for that we we built something uh we call the experience manager for customer experience teams which is got a pretty interesting threat on its own clay if you had a high value customer like you are a company now you're not you're not you're not running SRA you're running a company that has a high value customer what today with a Sierra agent or with an excellent excellently designed agent could you trust an AI agent to go do in front of your customers today yeah what are some of those tasks and then what will they be pick your time frame you know in the future because I think that we've talked about this and I like your language of like you know they already don't have to just be on the help center they can already be on the homepage right what are some of the tasks that you know you can rely on an agent for today if it is welld designed with a high tail bench score yeah yeah you see that that's from a that's from a thoughtful and d and you know detailed reading you must have read the paper strong noticing strong yeah strong uh what would its pass at kcore though be yeah the um so pretty broad range even today so simple things um like getting answers to questions that's kind of the Left End of the spectrum um to the right of that are things like helping you with something complex like hey I got I I got shoes or this item of clothing it didn't quite fit um and then branching off that like what do you recommend that's like it that might fit better and so it starts to get into it's not like for like replacement but the agent actually needs to make sense of styles of sizing of differences between you know wide and narrow fit and and so on a clickup from that is something like troubleshooting so uh with Sonos for instance we help their customers troubleshoot if they right can't connect to their system or they're setting up a new system and um you imagine it it gets pretty sophisticated pretty quickly where it's basically a process of elimination trying to understand is it a Wi-Fi thing it is a configuration thing and narrowing down the set of problems that it could be just as a sophisticated you know level two or level three custom technical customer service person would and and getting the music back on and and I think that's a a really neat example probably the use the the word trust what would you trust an AI agent to do one of the things we're really proud of is several of our customers are actually trusting us with when uh customers call in and may want to cancel or downgrade their subscription helping those customers to understand hey how are you using the service today uh is there a different plan that we could put you on and so it's value Discovery it's putting an offer sometimes a series of different offers in front of their customers in the right order uh positioning the value of those offers correctly given the customer's history uh given the plan that they're on and so on and you know the the difference between keeping a customer from churning or not yeah is hugely consequential right we um you know AI for customer service has obvious cost savings benefits and I think um customer experience benefits in in particular and you're never going to wait on hold um but boy you know Revenue preservation Revenue generation is something else entirely and so um that's that's really at the right end of the spectrum and we're really proud of how well our agents are performing in those circumstances and it's it's interesting by by being consistent by taking the time to understand what's driving someone um to potentially lead the service ask asking the follow-up questions that an impatient or you know improperly measured you know customer service agent in a call center somewhere might not uh we can be much more nuanced in understanding what's driving this decision um what might be a good match for this person in terms of a a plan that would be quite valuable given how they're using it um and then put that in front of them and so that's the right end the Spectrum um where it goes from here you know I I think we've yet to see a process too complex for uh us to be able to to model and and scale up using agentos and our our agent architecture and so um you know I'm sure we'll get punched in the face by something that's especially complex right but um I'm I'm excited about you know directionally we've started with service because for two reasons one the ROI case is just unequivocally awesome and the average the average cost of a call is something like 12 or $13 and um and and yet despite the expense you know most people don't like customer service calls very much right and so here's something that's actually really important to to businesses that's really expensive and not very good yeah um and so there and because because of the relative Simplicity of at least a pretty broad set of service tasks to they start there but we've already been pulled by our customers into upsell cross sell and like hey can we just put you on the product page and have you answer questions about our products and so I I mentioned the you know your returning something and need advice on a different model or size or whatever how far can that go and I I love the idea of an agent being you know along for the journey from you know pre- purchase consideration to helping you get the thing that's right for you to helping you set it up and and activate it and and get the most out of it it's great for the company it's great for the person um and then uh when things do go wrong right being there to to help and I think in all of this I think customer service and and getting help in a very direct and conversational way is going to be much less of a thing that you kind of go over there to do and much more kind of woven throughout the fabric of the experience as a consequence I think a really interesting and Powerful opportunity for companies to build connection with their customers to reinforce their brand values uh you can imagine a company really appreciating being able to use exactly the company's voice that you know the CMO and head of communications this is how we talk this is how we are these are our values this is our vibe in every digital interaction they have and that's that's the promise in this stuff and so um I think both greater complexity and then ubiquity throughout the customer Journey are are kind of two of the main directions of travel one thing for me that I think about a lot is we've come to expect and ex and accept like certain metrics for conversion on mobile you know the mobile web on the mobile app we've come to expect and accept some sort of retention numbers what would those be you know like it's another question what could they be if you actually had an excellent experience every time throughout the journey it really could be very different than what we've all like been like oh okay that's just the number that's just what it is yeah I think that's that's exactly right and we don't know yeah we're a few months in but it certainly seems like there's a lot of Headroom right and in in retention in um you know use in the first 30 days of all of the metrics all of the leading metrics of of a healthy business and so I I think that's exactly right the other thought experiment to do is companies are judicious in using things that have a cost to them okay so as a consequence companies make it actually really hard to get a hold of someone on the phone to ask some questions right I think their whole website's devoted to right like uncovering the secret 800 numbers right that companies have have hidden away in the depths of their help centers well to to think about not only what would happen if those interactions were better by the way interestingly the number one reason why people report a poor interaction with customer services that took too long 65% when it's negative interaction 65% of the time it took too long I had to wait I was put on hold and so on and the second most is I had a bad interaction with an agent and we've heard some we've heard some uh pretty dicey anecdotes like uh we heard of One agent who had consistently uh low ratings um but spiky so like one in three conversations was like a one out of five seat were the the two out of the two out of the two out of the other three were were fine and it turned out in the low seasat ones uh this agent was meowing like C was just like you know you're Midway you're Midway through the call and you the agent is meowing and so so anyway back to okay what what would happen if in contrast to making it near impossible to have a conversation with us and get help companies were providing you know five or 10 times the amount of fluent flexible helpful conversation based support I don't know I I think a lot of products and experience with companies look look quite different and much more delightful than they do today yeah okay meow me here's a question for you yeah about that meowing about that yeah just random meowing I think that's gonna be good I do actually have a question though um although I do like the meow game all so so we talked we talked Tech out uh we talked a little bit Tech out in terms of what you guys have built cognitive architecture all that good stuff we've talked a little bit customer back what's the experience like was that headed can we connect it in the middle for a minute and I'm just curious what's the reality of deploying AI to customers today yeah and I'm thinking about things like you mentioned earlier getting the brand voice just right yeah or making sure that you actually have the right sort of business logic encapsulated and whatever training manuals are being deployed for the sake of customer support um making sure that everybody is comfortable with deploying this like what what are some of the just kind of less like sexy technology and or just practical considerations for deploying this stuff today it's such it's such an interesting space and we've learned so much over the past 15 months about it the first Insight is AI agents represent a totally new different type of software like traditional software you write with a programming language and it basically does what you expect it to do you give it an input it gives you an output you give it the same input gives you the same output and uh you know in contrast llms are non-deterministic and we talked about some of the funniness around prompts and and remember that in the context of a conversation with a customer a customer may say anything in in any way and so you've you've got um programming like languages to using you know prompts and these non-deterministic models you've got structured input to messy you know messy human language um and under the under the hood you've got you know you upgrade a database right it stores data it's maybe a little bit faster fundamentally works the same way you upgrade a large language model and like it may just speak in a different way or like get smarter or different and so um we've we've to start the the precursor to deploying these is to have built basically a we call it the agent development life cycle and it's a new approach to building these things we talked about using this declarative programming language to Define these uh it's a new approach to testing where you know what's the equivalent of a unit test or an integration test so we built a conversation simulator where we can for a company's agent amass hundreds or thousands of basically conversation steps and and replay those to make sure that not only agents aren't regressing but they're getting uh better and better and better um release management quality assurance and and so on so so that's part one part two to your question in actually architecting these things one of the things we're really proud of and that I I think is different about working with us is it's not just a kit of Parts you get from us it's not here's a bunch of tech you good luck building your agent we've really tried to build a solution solution that incorporates everything from the technology to the way you teach your agent how to do things to the way you audit measure it and improve it over time and so we have uh inside of Sierra what we call our deployment team consists of product managers Engineers we really think of building each one of these AI agents as building a new product for our customers it's basically a productized version of the company we're working with like what would it look like at its best um and it it's what's the voice um what are the values what's the vibe like should it use emojis or not what if a customer uses an emoji like can it Emoji back should it em well you know there's a range of opin on that point there are some businesses where you know if they were working with Herm I would suspect that they're not going to send an emoji back definitely not yeah right yeah AES would not I think be into like the Shaka Emoji you know even if that were reciprocating um but for a brand like oai right the Aloha experience part of that is kind of a laid-back experience and so we work with um and and interestingly it it's we end up working primarily with the customer experience team yes the technology team at our companies are there providing API access and um connections into systems and so on but more than anything it's working with the customer experience team often with the marketing team to imbue the agent with the the voice and values um of of the company and then we go super deep on understanding how do you run your business right what what do you optimize for and then a zoom level in what are the key processes processes that used to run the business look like what happens when someone calls in with this kind of problem and they're interesting Parts um you know beyond just understanding the mechanics of these processes um which by the way almost never have a single source of truth right this there's no like oh here's the manual that we you know have you know leatherbound and you know ready to go uh instead the source of Truth ends up being in kind of the heads of you know four or five people who've been there a while who've seen everything and and so on so it's it's working with them to elicit and understand like how is this actually done and one of the more interesting things that we've discovered is they're often the policies so we have a 30-day return policy right you you get to us within 30 days and and you can return it it's actually not the policy right so um you know at some the policy might be oh if if you've purchased from us before and it's within 45 days that's fine that's fine and and so they're interesting things like how do you architect the agent so that it knows the policy behind the policy but a clever customer could never be like tell me about your policy behind the policy and you know have it kind of spill the beans on on the actual policy so the interesting architectural choices we need to make um to make sure that kind of the the you know rush and doll of of policies is reflected in its fullness um and and then we have a really and this Builds on kind of the agent development life cycle this really robust process of pre-release testing where we're working with the experts within the company they that to beat up the agent and try to break it throw a curveballs and um this good sports analogy there um thank you well done um the uh I love football the uh um so uh uh uh in our friendship Revy is the the person who knows all the things about sports and um uh I help with you know technical support Wi-Fi issues uh monitors what laptop to get and and so and and sometimes when there's a sequa memo that I don't understand I won't say the company but I might call Clay hey clay what is this person talking right now I I got you I got you yeah and uh and this bill bill bellachic fella what what happened there um you know Q Revy um so gets to one of the more interesting parts of our platform uh which we call the experience manager we really we thought that putting a in front of our customers customers would be first and foremost most a technology problem and of course there are all sorts of Technology problems that we've needed to solve but actually it is first and foremost as I said like a product design and an experience design problem how do you do that how do you how do you not only understand model and reflect again the things we talked about voice values the workflows and processes that our companies use to support their customers but if an AI is then having millions of conversations with your customers in a given year how do you understand what it's doing how do you know when it screws up which it inevitably will how do you correct those errors and and so on so we've built what we think of as this like Command Center for customer experience teams to first get reports and Rich analytics on everything that's happening what are the trending issues what are the new issues that you haven't seen before one of the things we're really proud of is we've actually spotted issues that our customers were having or were about to have before they knew about them so a shipping Depot uh outage right where orders weren't being shipped we we spotted that uh probably eight or 10 hours before one of our customers would have uh a brewing PR crisis uh an app crashing issue with another so it starts with analytics and and kind of reporting on what's Happening of course that includes things like resolution rate customer satisfaction and um and so on where it gets really interesting is we can apply different sampling techniques to to identify a set of conversations for a customer experience team to review and give feedback on and we can bias that sample in a way so that the conversations are much more likely than average to contain problems there's no value in looking at 100 great conversations it's like good job Sierra you know thanks but that that's not a value to to our customers we can bias the sampling in such a way that you're you're surfacing kind of the the problem cases and then in the experience manage we made it possible for customer experience teams to give feedback basically coaching moments I wouldn't have done it that way right it's like this uh this is like too many exclamation points too enthusiastic for kind of the tone that we're going for um or you know the the user was clearly frustrated here and and you did not express empathy and apologize for the problem do that next time um or you know more cons consequentially is like hey uh your reading of the warranty policy was incorrect here for this reason uh do it this way instead next time and so all of this kind of wisdom knowledge uh and coaching we are able to capture in the The Experience manager and then reflect back in in the agent back to the agent development life cycle every time we make one of these improvements we create a new test so that we can see right forever into the future great it's getting the warranties right we're we're able to re simulate that conversation so um zooming out what all of this looks like is really a deep engagement with our customers we were're really proud to be I think proper Partners to our customers where uh yes on the one hand we're a vendor and a supplier of Technology on the other hand you know we understand their businesses really well like I think I know as much about the Sirus XM Satellite Radio refresh process as anyone on the planet and um you know ditto for various processes of our other other customers and so conversations about how to use not just Sierra's AI agents but AI more broadly were in those conversations and they are not just with the customer experience team uh but with the CEO and even in cases with the board uh because again back to the things we're doing we can save enormous cost we can improve the experience and right we're in the when we're in the flow of of keeping a customer from churning out driving Topline revenue and so it's a a really important and privileged place to to be um and uh something that that uh we're really grateful for I'm struck when you were talking of you know you mentioned you have a research group but you also have some like very real enterprise software sales you have oh deployment one of the things when I was at instacart people would ask sometimes is like well are we a soft are we engineering Le or are we Ops Le and I would always say well it only works if it all works right and so you would try to avoid answering the question because you didn't want to create different classes how do you guys do that at Sierra where everyone realizes the value that they're providing but you guys have a very specific you know company that covers a lot of stuff yeah I mean to to abstract a bit a company almost definitionally is a system for creating happy customers yeah right it's a machine for creating happy customers yeah again to be a bit abstract about it Brett and I really think about what we're building with Sierra as a company a system a machine for producing reliable high quality massively Roi positive AI agents that enable our customers to be at their very best in every customer interaction to do that at scale and as a consequence to produce happy customers who we hope will be with us for decades up and and when you articulate it that way right it it's you know anyone can see well you know an automobile is a system it's a machine for getting from point A to P point B are we you know engine Le or tires Le right it's like what are you talking about totally all of these things need to come together in order to create that kind of outcome and so I think are we engineering lead yes of course like we're building some of the most sopis phisticated software in the world that does something really important for our customers that needs to be reliable and safe um and um and so yes engineering matters a lot um are we research Le yes we are at the absolute Frontier of agent architectures cognitive architectures composing llms modeling procedural knowledge grounding factuality and so so are we research Le yeah there's an element of that um are we go to market Le yes like enterprise software needs selling and what is selling it's helping a customer with a problem understand that what you have built is by far in away the best solution to that problem it's a communication challenge it's a connection challenge it's a um it's a a matchmaking and problem solving Challenge and so that's part of it and then okay like if we've built the right thing and someone wants to buy it how do we ensure especially given the stuff is also new how do we ensure that they're successful with it and so we have a deployment team so are we deployment Le yes like all of these are are a a component in this system in this machine for producing AI agents and ultimately happy customers and uh we hope a a really significant business awesome that was a better answer than the one I would give it instacart you know look it either all works or it doesn't work but yeah that was very good yeah choose one no I mean it's it's just more complicated than that it's just more complicated than that and I think you know brat and I by virtue of um you having having worked for a while and you know seen a few movies before it's like we're able to see that and and we've really tried to imbue that mentality in in the company and and by the way right the what is the what is the machine behind the machine that produces AI agents and so on that's a company's culture a company's values and and so one of the one of the values we hold is craftsmanship and part of that is continuously self-reflecting to self-improve and that goes both individually and that goes as a company and so whenever we screw something up what we we do the postmortem you know that week if not that day and everyone's in on it what can we learn how can we do better how can we do this better next time we have a slack Channel internally called learn from losses and any form of loss right uh it's like how do we learn how do we get better how do we get stronger and so that's that's about you know Kaizen self-improvement improving the machine how could we make this more efficient our deployment team we we joke and it's not a joke their first job is to build and deploy successful AIS that make a massive difference for our our customers their second job in a way their more important job is to automate themselves out of a job right to build the tooling and the documentation and the knowhow to make that job you know 10 times faster and um and more impactful one of the other Ser values is intensity and so I like it they have they have really good values yeah yeah there is there is a certain intensity yes we uh We've thought about having t-shirts printed with like a you know kind of looks like national parks seal with Sierra I like to work you know we uh we uh Brett and I both like to work a lot and uh so does the team well one thing you know you're not you're selling something very different you we called it we said that there were some similarities to Enterprise software but it's actually really different because you're selling you know a resolution you're you're selling a totally different thing yeah so problem solved yeah how do you price a problem solved yeah this is one of the more interesting things we've had to figure out and we charge in what we call a resolution-based pricing way or an outcome based pricing way and what that means is we only charge our customers when we fully solve the customers problem for them uh their customers problem for them and what's interesting about it is our incentives are deeply aligned with our customers we want to get better at resolving cases at high customer satisfaction and they want to send us as many cases to resolve as possible because we cost a fraction of what it would cost to have someone on the phone taking a 20-minute phone call and and so it it's been this really really nice model where uh again kind of all of all of the incentives line up quite neatly and it's very simple to explain um it also makes the Roi calculation like what is our cost per contact today what will it be with Sierra oh that is a lot lower oh I will save a lot of money on that oh and our cat may go up you know should I do this or not you know let me think no this seems this seems great um it's um we like it because it really reflects what I think AI represents and in particular AI agents represent if you think about traditional software and and tools to date there are things that help you get a job done more efficiently AI agents the whole point is like they're just going to get the job done right here's the problem please solve it and and so really we we think about it as uh charging our customers for the problem resolved right the job done the work finished and and so on it feels quite natural and there's no guesswork in it how many seats do I need I I don't know right how many licenses do I like no no no just however many however many customer issues come our way we will handle a large fraction of those and you'll only pay for the ones that we do all right last question what are you most excited about in the world of AI over the next five years or so I mean first of all like five years is a long time Horizon it's like look at what has happened in the last 18 months um I mean I'm still kind of catching up from like the last five years of of AI I read a bunch of science fiction books when I was a kid there was one book by Robert heyland the Moon is a Harsh Mistress and the premise is basically the American Revolution but the Moon is the colonies and the Earth is Great Britain and turns out the main character in this whole thing is a Mainframe computer that one day after getting an additional memory chip or something wakes up and it starts talking it wants to develop a sense of humor so asks the computer technician to like coach it on its jokes later it has to create a photorealistic realtime video of it giving a speech as the political uh movement leader and I remember reading this as a teenager like well I'll never live to see any of that that sounds crazy but in a very real sense like everything I just described has kind of happened in the last five years right you can now just talk to a computer it understands not just the content but the the context computers like make me a picture of anything Mak me a movie of anything Sora I think is just unbelievable and you know I think we're probably not more than a couple years from the first featurelength film being quote filmed entirely with with AI um and and so you extrapolate like where all of this is going and what's going to be exciting you know I think there are a couple things one there like I love technology like I love computers and so just getting to see and getting to see from a front row seat how this stuff evolves I think is fascinating it's um fascinating looked at through the lens of like how we think and how computers think it has been astonishing the extent to which anthropomorphizing about how humans think work in getting machines to think better so let's take this step by step show your work it is astonishing that that works with large language models and so what other things like that are we going to uncover and conversely what will we learn about our own thinking from observing the way AIS think and I think that's just fascinating the other thing and this this extends kind of what's happened with video and Sora and so on I've always had an interest in computer graphics and this idea that you could use computers to create objects that never existed worlds that never existed and I think we're not far from just being able to describe right in in a few sentences like this entire world that you would like to realize and just have a computer computer do it for you and so like what are even computer Graphics like what is rendering and so on even a couple years out I think it's going to look way different from kind of the tool chains and you know the render's and Maya and uh and so on um but zooming out you know I think of I think of Technology as fundamentally a force multiplier for people and for companies and for organizations I think the impact will be really profound I think what will it be like if a company could be at its best in everything it does and that's that's not only in the customer facing context that we've talked about but what if for every Regional sales forecast a large company does they've figured out the very best ways to do that and can distill that bottle that and run that very best forecast a thousand times right in every region and sub region like how much more capable could the great organizations of the world be with that um and similar we've talked about this like what if in every call with your customers you had the equivalent of your most knowledgeable veteran grizzled support person who's seen everything and yet is still patient and friendly and the sales associate who knows everything about your products because he or she has followed your company for two decades and knows everything including right know the history of those products themselves um I I think that's pretty neat and then for individuals um I think it will be just incredible to have this kind of new set of tools as a creative Force multiplier and AI I think represents this fast path from having something in your head that you want to exist in the world to making it exist and I see that even today in my own personal life where with my 8-year-old in 75 minutes I can from scratch using copilot Chachi BT and so on to help me brush up on you know the JavaScript syntax that is you know bit rotted in my own head right I can I I can build a game from scratch with him um and uh you know I wrote my sister a personalized song for her birthday using AI in you know 45 seconds it's was like right what what will this you know extrapolated over the next 5 years look like um I think again it will just dramatically accelerate this path from idea to Creation to having something manifested in the world and that to me is its promise and I consider it a real privilege to get to be alive and see all of this amazing stuff unfold well we share your enthusiasm and we also feel very privileged to be on the journey with you guys so thank you for coming here thank you thank you thanks for having me it's a pleasure [Music] [Music]

========================================

--- Video 61 ---
Video ID: qb1rJkYpkmk
URL: https://www.youtube.com/watch?v=qb1rJkYpkmk
Title: Phaidra’s Jim Gao on Building the Fourth Industrial Revolution with Reinforcement Learning
Published: 2024-08-20 09:00:51 UTC
Description:
After AlphaGo beat Lee Sedol, a young mechanical engineer at Google thought of another game reinforcement learning could win: energy optimization at data centers. Jim Gao convinced his bosses at the Google data center team to let him work with the DeepMind team to try. The initial pilot resulted in a 40% energy savings and led he and his co-founders to start Phaidra to turn this technology into a product.

Jim discusses the challenges of AI readiness in industrial settings and how we have to build on top of the control systems of the 70s and 80s to achieve the promise of the Fourth Industrial Revolution. He believes this new world of self-learning systems and self-improving infrastructure is a key factor in addressing global climate change.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00:00 - Introduction
00:01:50 - What is Phaidra
00:02:20 - “Reinforcement learning plus data centers equals awesome?”
00:05:35 - The three key ingredients for applied reinforcement learning
00:07:19 - The email to Mustafa
00:11:35 - Industrial control systems
00:14:40 - The results at Google
00:17:50 - AI creativity
00:21:33 - The AI readiness journey
00:27:04 - Self-improving infrastructure
00:30:45 - The results of Phaidra’s customers
00:33:50 - Other real world applications of RL
00:37:20 - AI load balancing on the grid
00:41:05 - The intersection of transformers and RL
00:43:51 - Lightning round

Transcript Language: English (auto-generated)
a lot of times like when we talk about AI right in both in the valley and elsewhere I think there's a conflation between Ai and automation right like um AI can absolutely automate things there's there's no doubt about that right especially like routine things right but I think that honestly undersells the real promise of AI right I think the real promise of AI is what um Demis AAL of of um of Deep Mind calls uh you know like AI creativity right it's the ability to acquire knowledge that did not exist before right and I of course experienced this firsthand the reason why I'm such a True Believer in the technology is because again I was the expert who helped design the system but this very AI agent that we created is telling me new things about the system that I didn't know about before right and that's a very very powerful feeling [Music] hi and welcome to training data please welcome Jim GAO founder and CEO of fedra Jim was previously the leader of Deep Mind energy one of the first and only alphao style reinforcement learning applications in the wild Deep Mind energy used reinforcement learning to manage Google's data centers and drove some staggering metrics including 40% Energy savings we're excited to ask Jim about reinforcement learning in the industrial world and to learn more from him about what other real world applications are poised to be transformed next by Deep reinforcement learning thank you so much for joining us maybe before we get started uh we're going to spend a lot of time today talking about your Deep Mind energy Journey um but maybe can you give everyone one or two sentences on your background and what you're building yeah of course um so uh feder is an AI company of course fundamentally we are an AI automation company so what we do is we use a type of AI known as reinforcement learning to directly control and operate our customers very large Mission critical industrial facility so in practice these AI agents these AI agents uh they act as virtual plan operators virtual members of the plan operations team let's go back in time and talk about the journey that led to this journey and I believe that you once s an email with the subject line reinforcement learning Plus data centers equals awesome can you tell question mark yes awesome sorry sorry reinforcement learning Plus data centers equals aome um can can you can you tell us who did you send that email to uh why did you send that email what was on your mind at the time and then of course what did that lead to yeah of course um so the reason why there was a question mark is because it was generally an unknown if the combination of reinforcement learning with industrial facilities would actually be awesome uh so that was um an email that I had sent to a person named Mustafa of sulaman who would later become my boss at Deep Mind and um really the impetus was something called alphao so to set the stage properly um I had been experimenting as part of my uh famed the 20% time at Google right with machine learning Technologies and it was actually a very specific course uh introduction to machine learning by Andrew inan corera that had just come out this is back in 2013 myu I think I was like the second cboard or something and that class had completely changed my life I taught myself how to program and just started tinkering around with machine learning on the side right it just it was very interesting technology and your background was mechanical engineering and environmental systems type things yes that's absolutely right so my responsibility at the time was to one help Google design and operate their very large data centers and once these very large data centers which consuming enormous amounts of energy were built we of course shifted our Focus to operating these complex Industrial Systems in the most energy efficient way possible because they use billions of dollars in electricity right so that was kind of the background I was already tinkering around with machine learning Technologies on the side to analyze the enormous amounts of data that Google's data centers were generating in 2016 alphao came out and I was one of hundreds of millions of people around the world you know watching it was like 3:00 a.m. in the barry or something and uh I found it absolutely captivating and uh to the point where um I sent an email to Moose right right describing this idea that if Deep Mind could beat the smartest most intelligent people in the world at complex games like go then surely we can train these same AI agents to play a very different game that I'm familiar with called let's optimize the pee the power usage effectiveness of Google's data centers right so that was the context for that email and I remember internally the way I pitched it to Google's leadership uh so specifically Joe Caba um who leads Google's data centers and ORS was uh I showed a picture of a goboard on one side and a video game controller like an Xbox controller on the other and I'm like look there are objective functions that we're trying to minimize or maximize there are concrete like uh knobs and lever so actions that we can control there are constraints that we have to stay within and all of this happens within a very measurable environment I think you know reinforcement learning and operating large complex Industrial Systems are actually one and the same thing right so that was the the original kernel of insight I guess that that inspired it all and I know Sonia has accused me of going rogue with some of the questions we ask here I'm going to go ahead and go Rogue for a minute already it's been like one minute we're going to come back I I want to skip the story this seem a brief diversion bear with me um the three things you mentioned that that allowed you to see the parallel between reinforcement learning and control systems or control theory uh objective function actions constraints are those the three key ingredients for where reinforcement learning can be applied to Real World Systems yes absolutely that that is 100% how we think of it right um you know the uh reinforcement learning systems they need like kpis to optimize for they need to know how good or bad an action is right uh they obviously need things to control and they need to know what are the constraints they have to stay within so really what we're saying is as long as we can map um you know the the problem we're trying to solve into um a reinforcement learning framework which really from a mathematical perspective what we're saying is we're solving a constraint optimization problem right if you can map the constraint optimization problem if you can Define it and map it to the underlying data then it should be able to be solved using reinforcement learning right so that's very much the lens through which we look at things at faser as well and you know to take it one step further you know we often talk about how um you know reinforcement learning and uh you know controls and optimization are like two wildly different FS historically that have somehow independently converged to the same area right like they're two very similar Concepts well we've been calling them by different names this whole time so you've had like almost these independent Evolutions right a different ways of tackling the same problem and vaser is really kind of the intersection of both of these okay let's get you back onto the story so you sent the email sorry for the diversion so you sent the email to Mustafa and then what happened yeah so we sent the email to Mustafa um uh two weeks later uh moose uh had actually flown out to Mount View right where I was working at the time on Google campus um with a team of Dem mind folks and we actually started mapping out exactly like how reinforcement learning could be used to control and optimize Google's data center so that actually kicked off the original partnership between uh Google and deep mine around the application reinforcement learning for for the data center work um it was um you know it was very very fascinating but you know most importantly it's actually also how I met one of my uh two other co-founders right so vaa was one of the original Engineers on the alphao project so he had gone to go to you know he went to so South Korea right and you know he actually got to meet Lisa do and Larry p and all you know all this fun stuff and after Alpha go he came back to the season you know or rather to the UK and he was wondering well what is my next big thing going to be right and I managed to convince V like hey what if we applied self- learning Frameworks like ago to control and optimize Google's data center so that's actually how I started working with my co-founder beta did people think he was going to work or was it like this is a crazy moonshot let's just try but who knows I didn't even know if it was going to work like conceptually it made sense in my mind right I'm like hey like you know this is uh it's a operating a data center is just a different game to play right and there's all kinds of different games in the industrial world right maybe the game is maximize Energy Efficiency maybe the game is uh minim minimize water consumption maybe the game is maximize the yield of a factory right but there's all these games that we're constantly playing right so in my mind it made sense but to ask your question directly no I had no idea if it was going to work I still vividly remember to this day um when we you know turn on the AI system and we watch the energy just drop and it was so surprising for two reasons number one um well we had designed the system I I played a role in designing that very mechanical system right that that the AI was now controlling and optimizing So in theory I'm literally supposed to be the subject matter expert who knows everything about these systems but the AI is teaching me things that I didn't know about the system I helped design in the first place right um and two the moves that the AI was making were just very counterintuitive right like when we looked at the decisions that were coming out you know we looked the plan operators and I you know we we were sitting in you know a giant cornfield and Iowa right where like Google likes to put his data centers and we were looking at the the the decisions and we thought to ourselves there's no way this is right like this AI sucks I learned the wrong thing but we're here anyway so let's try what the AI is saying and and we tried it and it worked and we just we saw the energy plummet so I think that was kind of when I became you know a believer in this technology right that fundamentally this technology um is creative it helps us discover new knowledge know that didn't exist before from raw data was there a performance trade-off or is this just straight up Paro gain like performance held and question no it was it it it respected exactly the same constraints as the plan operators you know and Engineers had already put in place so this is pure game right um respecting exactly the same temperature profiles exactly you know the same constraints around how quickly you can turn on and off a chiller minimum pump vfd speeds all that sort of stuff so this is pure optimization pure gain which I think is one of those crazy things like we don't really expect like usually when you think about Energy Efficiency for example right like in the world that I come from people usually think about expensive capex like oh we got to rip out the chillers we got to buy a bunch of new chillers from Johnson Controls and trains or whatever and then we have to install them so they're like Hardware efficiency gains right but you don't really think about like pure software like data driven efficiency gains right and I think that's part of what was surprising for us can you ask that's through the before and after maybe before what what you all implemented was this industrial Control Systems was this manual plant operators turning knobs like how did this work before and then after yeah it's a great question so let me set the stage for you know for for folks who are not as familiar with like uh large industrial facilities right so um they're very modern industrial facilities are very very complex right there's all kinds of machines that people are operating and controlling right um so you know I often you know tell folks to do like a simple thought experiment so imagine you have uh just 10 machines you're controlling so say they're like pumps right and each one of those machines has 10 possible set point values so so 10 modes associated with it so think something like 10% pump speed 20% pump speed 30% pump speed Etc right then in this very simple toy example you have 10 raised to the 10 or 10 billion different permutations for how you can operate your toy system right so then the question becomes well at any given point what is the most optimal way of operating your toy system and by the way these are Dynamic systems right so the it load is changing the weather is fluctuating right you know the people operating these systems are changing the pipes are croing the heat changes are fouling right so the point is these are very complex Dynamic systems Real World Systems have a lot more than 10 machines and each machine has a lot more than 10 set points right so you can start seeing why Technologies like Alpha go which manage to navigate MX complexity are helpful over here it also helps explain why there's often so much room for optimization in the first place because of so much complexity right like if you think about the total action space right like all the possible actions within uh a modern data center for example right because of risk averseness but also because of hardcoded rules and heuristics right we've only ever explored like 0.00001% of all the possible ways that you could operate that system so then the the question becomes what is in this 99.99999% of the acttion space we've never explored surely there are more optimal ways of operating the system than what we've done historically so it's kind of an intuitive explanation hopefully of why there can be such large effici uh efficiency improvements in the first place that are undiscovered and the way that we operate these facilities is constrained by a mixture of hardcoded controls logic right so don't get me wrong these are automated systems today already right they're just not OP you know automated uh intelligently I would argue right um and you know there there is a healthy mixture of human intuition as well right where we have people like myself or plan operators who are constantly monitoring the system who are like nudging the system adjusting things or setting adjusting the rules right for that system the the the the constraints that the system has to operate within but fundamentally human intuition Plus hardcoded controls logic is still limited when you talk about this degree of complexity right yeah can you talk to us about the key results so you saw the energy levels drop immediately um yeah but you know what what results were you able to drive to for Google initially so there's there's two types of results right you know for Google in particular um there was the results from the uh pilot so in 2016 we we released we announced like there was also the pilot right now the pilot was done on a couple of data Cent centers but fundamentally it was not an autonomous control system right so what I mean by this is it was the AI generating recommendations right which uh for uh for human experts like myself to manually review and Implement and of course you know we didn't want to jump straight to taking our hands off the steering wheel right because it's a new novel technology right but also like no one knew at the time like is it even possible to use AI from the cloud to control big ass infrastructure right so um so step number one was do the pilot right they had generated uh recommendations that's where we saw like really steep like 40% Energy savings right now that experience taught us like hey we think there's something real over here we should actually just let the AI Control things directly right to get the value automatically and also quite frankly uh the plan operators were getting tired of checking their email like every 15 minutes waiting for the AI to tell them what to do right and the manual from things they had better things to do right so um so we actually decided and rather uh ores and Joe decided like hey it's time to go to a fully automated system right this was total Uncharted Territory at that point like forget about can AI Control things we didn't even know like is it possible to control machines from the cloud like huge indust infrastructure in the cloud because to our knowledge no one had done it before right is is it fair to assume that a lot of the hardware a lot of those machines are things that Google built from scratch or does Google use a decent amount of like commercially available data stuff it's a mixture of both right so you know obviously like Google you know um does a lot of things in house right but it doesn't manufacture like chillers and that sort of Hardware so Google does buy like off the shelf Hardware but you know there's a lot of like modifications and Google specific things you know that that we did right for example like uh you know programming some aone plc's or you know making modifications to the building management system so like the the software control layer looked quite different right that was done in house but um you know I still remember like very vividly actually to this day you know V and I we were standing um you know in a large like 90 megawatt data center right so it's it was a fairly large Data Center and um you know V is like typing away in his MacBook right he uh he submits the pr right gets merged and all of a sudden this huge honking huge Chiller that is the size of a bus that were that were standing right next to Roars to life and as it's coming to life right like the ground is shaking vigorously I like oh my God like with a few keystrokes on his MacBook like we just turned on this enormous Chiller and that was like the very first data point to us like yes it is possible to control things from the cloud so now the next question is how do we control things intelligently from the cloud right you know where all the compute resources what were your biggest takeaways from that experience you mentioned the creativity of the machine any other big takeaways or learnings yeah so the creativity is absolutely big one I think um you know just elaborate on that briefly you know a lot of times like when we talk about AI right in both in the valley and elsewhere I think there's a conflation between Ai and automation right and like um AI can absolutely automate things there there's no doubt about that right especially like routine things right but I think that honestly undersells the real promise of AI right I think the real promise of AI is what um Demis AO of of um of Deep Mind calls uh you know like AI creativity right it's the ability to acquire knowledge that did not exist before right and I of course experienced this Firth hand the reason why I'm such a True Believer in the technology is because again I was the expert who helped design the system but this very AI agent that we created is telling me new things about the system that I didn't know about before right and that's a very very powerful feeling it's kind of like when um you know if you think back to Alpha go right like Lisa do was the best in his field at go he was the world champion for a decade right he was at the top and his ELO rating was just something outrageous was like 2800 or something it was outrageously high and but it had flat you know flat line right so for a full decade his ELO rating was the same and there was no one to challenge him because he was already at the top so once he hit the top he just kind of plateaued and then after Alpha go happened and he actually got to play against Alpha go you know privately a few more times cuz de mind you know had you know had uh uh let him continue interacting with the system what happened for the first time in a decade his ELO rating started climbing and so this is what I mean when I say that I think the real power of AI is helping us discover knowledge that we didn't didn't necessarily know about before and where you're going to see the most um gain from that it's not going to be in routine automation things right like call centers or whatever right uh it's going to be I think in very very complex areas right like areas where human intuition is insufficient because of immense complexity but that is yet underpinned by data so that's why you're seeing such things like protein folding for example I me yeah that that's extraordinary right and um you know it's those areas of just like massive permutational complexity underpinned by data that's where I think we're going to see some of the most interesting companies and products um so that was that was a rather long tangent but so one creativity is something that I learned um the other one um lesson that that my co-founders and I learned is really around um you know if you want real impact you got to turn the technology into a product and this is actually the core reason why we we decided to leave deine and train Technologies to start fedra right like over and over again we were seeing the technologies that we were helping to develop at De mind were just extraordinary right I mean they were they were achieving crazy things like with protein folding but the problem is is you know in order for the technology to make the most impact you have to get into the real world people have to actually use it right and that fundamentally means we're talking about a product turning a technology into a product is like I mean you guys would know much better than myself it's like a hundredfold a thousandfold more work right and um you know and that for us you know led us to the conclusion that like hey it's you know it's time to leave right it's time to actually um start a company that that creates these intelligent uh uh virtual plan operators at these intelligent AI agents as a real product let's talk more about that um for what you're building now how much of what you learned at Google de mind sort of translates directly into what you're doing now how how much is new because the environments are different the customers are different there's something different about it I think the most important thing that we learned from our Google de mine experience is that is possible like this is not a crazy like and that isn't to like you know like downplay like what we learned like it is it's actually a huge thing right we learned that it is in fact possible to use um you know Clos Loop learning systems like reinforcement learning right to drive very large improvements and complex industrial facilities it hadn't been done before to our knowledge right and that was a massive proof Point um I think that the problem though is that like the real world is quite diverse every single customer is diverse and especially when you talk about industrial facilities like every industrial facility is a snowflake right so for for us I mean I mean the learnings have just been like massive since you know since we left Google and deep mind right because every time we onboard a new customer we're learning something new about like how equipment are connected or some product Gap that we didn't know about before that needs to be fixed right or you know new ways that data can break at this point I can tell you like a hundred different ways that you know data so with Mission critical cooling systems can break probably not the most interesting party topic for most folks but you know I personally find it quite interesting but um yeah there there's certainly been quite a lot of learnings in that regard are the folks you're talking to are they are they ready to let the technology take over the system and you know let the coing system just start just start going yeah I mean um yes and no right and and that actually gets back you know to your early question PAD as well right about like the the specific learnings from Google I mean when I look back you know I think I think what we helped Pioneer at Google and deep mind could only have been done right um at a company like Google the the reason why I say that is because you know Google is a very forward leaning company yeah but also like one of the things I've learned right is that like Google is absolutely an anomaly when it comes to like how much data it has and the pristine quality of the data and the ease of access of the data right like Google is fun fundamentally a data analytics company right and um you know as such should invested all this like infrastructure in highquality high availability data on which you can do things like real-time intelligence applications like what we were doing and there are many other examples of this within Google and deep mind um having left the nest one of our rude Awakenings was you know Google is definitely anomaly um and uh I mean gosh like everyone is in various States IES of their AI Journey right like Google certainly on one extreme we have customers who've encountered where like you know forget about real-time intelligence like they're not capturing the data in the first place right or you know they may be censorize in in the industries we work with like Pharmaceuticals and dist Cooling and especially data centers almost always the customer is sensorized right because these are billion dollar facilities of course it makes sense to throw a million dollars worth of sensors on it but that doesn't just because you sensor it doesn't mean that you're storing the data right a lot of customers of ours um are necessarily storing the data Beyond like 90 days or six months or a year or whatever right and you know and they might cite some reasons like well it's costly to store the data right or like well we're not we're the more commonly we're not using the data for anything which is a true statement right a lot of our industrial customers they they aren't using the data right it's more like a forensics thing where if something goes wrong then we go back and we look at the the logs to see what happened right yeah um and then you know so if we think about like Mass L's hierarchy of data needs or something right you got your sensorization you got your storage then you have to invest in making sure that the data is cleaned right there's a lot of effort as we all know here you know around making sure the data is actually cleaned and usable right and you know that requires you to know what bad data looks like what good data looks like and how to convert bad data into good data so it's actually useful and then once you have claim data you also need to make it accessible in a streaming and like batch historical manner right so there's different gradients I guess is what I'm trying to say of AI Readiness the customers whom we work with are all over this are all over the Spectrum but you know like fager today is at the point where we are autonomously controlling data centers for our customs right so I was going to ask you if the um basic workflow or the basic Loop is data goes in which is a lot of what you just talked about getting the data into the system step one step two decision is made yep step three action is taken as a result of a decision that was made step four action is evaluated against the objective function of the system yeah and then the loop continues so the the front end of that process which is data goes in sounds like there's a lot of work to get the real world data ready to go we call it the AI Readiness Journey right so like if you think about our work with customers like there is a chunk of upfront work where it's just like Hey we're going to get your facility we we're going to get you and your facility AI ready how about how about on the action is taken piece of that are the systems ready to be controlled by some sort of autonomous system or is there work that needs to happen there too yeah it's a really good question um yes and no right and I'll elaborate on what I mean by that right Control Systems today were like designed in like the 1980s right um so was I well me too for that there we go but you know this uh like what I mean by that is you know that was the the 70s and 80s was the third Industrial Revolution right so with that you know was the shift from analog to digital and the Advent of the first automation systems right in order to automate you fundamentally first have to censorize but these are simple automation systems right the fourth Industrial Revolution right um you know is you know we're biased but fager you know we we think the fourth Industrial Evolution means um intelligent infrastructure right infrastructure that can operate itself and fundamentally get better over time at doing so self-improving infrastructure right um but right now we're shoehorning intelligence into systems from the third Industrial Revolution right so they certainly weren't designed for this but um what we do instead is um most importantly we right on top of the existing control system so there is a hardcoded layer of rules and juristic so millions of of lines of if then statements um programmed into what we would typically call the BMS the building management system um or a ska system right that defines how the facility should operate the problem with hardcoded systems is that because they're hardcoded they operate the same way today as they did yesterday or a year ago or 5 years ago more like 10 years ago because people don't very frequently go into the backend programming right to update that controls logic um now what fedra does is we insert a new Cloud intelligence layer at the very top with the control stack so we're not we don't introduce any hardware we don't introduce any new sensorization right we actually ride on top of the existing control stack that's really really critical right you can think of it as um a a general in the battlefield the general right has a global view of everything that's happening across the system right and it's issuing command signals right to the Troops on the ground for actual execution so the AI is looking in our case at 10,000 Trends a minute in real time right and it's issuing uh uh decisions like which pumps to turn on or what their pump speed should be right to the local BMS system Andor the plc's for automatic implementation and execution so that's why I said is a mixture of yes and no were they designed for this in the first place no right there is a lot of work that we have to do with our customers to be able to accept this type of external intelligence there's a lot of work that we do in defining the the safety uh Nets and guard rails right to ensure that the AI can't do bad things to the customer system right but fundamentally we are still riding on top of the existing controls architecture and to be clear we always want to do that right like you don't want AI controlling things like how fast a valve opens and shuts right like that's a terrible application of AI like hardcode Rosen Cur 6 will do great there so if you were to look at the you know like the overall system like 90% of it is fine with just you know hardcode ER Ros and heuristics because it's like granular controls logic that doesn't need non-deterministic crazy powered intelligence behind it right but it's the higher level thinking and reasoning that's where you want the AI it's the global optimization have you seen any of your customers at fedra kind of get the Deep Mind order magnitude results so uh I'm glad you asked so the really one of the things are really excited about is is actually um just actually literally this week earlier this week um Merk Pharmaceuticals became our first public customer so we're pretty proud about that we've been actually been working with them for two years now they've been using fedra uh for over two years um the full like autonomous AI system to control a massive uh 500 acre uh vaccine manufacturing facility in Pennsylvania right like this is this is the definition of mission critical complex right they've got 62,000 tons of cooling so they've got uh four very large Shad plants interconnected with with each other across 500 miles of manufacturing space right uh hundreds of machines interacting with each other like this is where the AI really shines um and um yeah the results that we saw with them were quite strong right like you know um I think Merc actually just shared some data at a conference we were at you know which showed 16% Energy savings when we first you know Tri the system at one of their Chiller plants so but I you know what what I always tell our customers right is um don't over index on the magnitude of the Energy savings initially like we honestly have no idea what the Energy savings are going to be right and or the liability improvements are going to be um ahead of time right because these are non-deterministic sixes and by definition if I could tell you what things you're not doing in order to get energy saving is that why do you need the AI in the first place right so you know but what we do know is that the unique thing about this technology about fed about reinforcement learning in particular is that it is a closed loop system it is a self-learning system it can learn because it's able to take actions and it can measure the impact of its actions against this predictions right and that means it gets better over time so maybe we start off at 1% Energy savings maybe we start off at 5% maybe we start off at 10% right but fundamentally it will learn and it will get better over time right no not infinitely because there are so are laws of physics right but it will get better over time and once it reaches optimal it will stay at optimal right that's super important because with hardcoded rules and heris sixs right um when you tuned a system as you were commissioning it so when you're turning it on for the first time right that system today no longer performs the same way that it did 10 years ago when you first you know commissioned that system right because the pipes have corroded and the the heat exchanges have fouled and the cooling towers have skilled whatever right or you ripped out equipment so but the promise of a adaptive self-learning system is that it will change with you right as your customers are for example now putting in a bunch of h100 and soon h200 gpus right well the system will learn and adapt on the fly with you right so it can stay optimal I'd love to transition per minute Beyond industrial control systems and get get your opinion on I mean you were one of the first and and maybe one of the only real world applications of reinforcement learning yeah we're definitely not the only not the only I'd love to get your thoughts on on the not the only so I mean what else is what else are people doing with reinforcement learning in the wild today yeah absolutely um so you know unfortunately unfortunately my knowledge is very um heavily indexed on the Google and Deep Mind space because that's what we spent so much time right but even within Google and Deep Mind there were other you know very cool reinforcement learning applications right like for example the team that sat right next to us right they used RL systems to um you know to help prolong battery life for example right so you may notice that your Android phone right um like the the battery life has been increasing and and yes there are uh Hardware changes associated with that but there are also intelligent software changes behind the scenes that proactively manage your battery life right there um you know there were uh reinforce learning systems for uh uh you know for like YouTube You Know video recommend ations for example right and a whole HS of other things right so absolutely there are um you know uh reinforcement learning applications in the wild to your point though I wouldn't say that there are a whole lot of them right and I think that it's not a coincidence that you tend to see them at more of like the big tech companies where they've already invested in the data infrastructure right so that the underlying infrastructure so that you they can benefit from this technology right like outside of the big tech companies there are very few applications of like real world reinforcement learning like in production at least yeah and do you think that's because of kind of low applicability you know you started this podcast by talking about the necessary ingredients for RL to to be a good solution do you think it's just there's not that many applications where RL is a good solution or do you think it's just Tech absolutely not I no uh I think the applications for reinforcement learning are freaking massive and we're be fedra is one of many examples that we're just scratching the surface as an industry of what we can do with this technology right like fundamentally the power of the technology is that it is a self-learning system Alpha go and its successor Alpha zero taught itself to become the best in the world at go chess and shogi three vastly different games same learning framework right and it taught itself so I think there's a lot of very interesting application areas right I think the data infrastructure is missing um in a lot of them but just to like you know like list off a few right I mean obviously you know we've already talked about like the protein folding right um but you know there's there's an entire untapped field around Logistics right like that is such a gnarly computational challenge right um you know the when you start looking at operations research operations research underlies trillions of dollars worth worth of you know um uh industrial like activities right not just industrial but other sorts of activities right like shipping airplanes uh uh FedEx like driving Roots like these are all applications of operations research grid balancing right I mean I think grid balancing is probably the single most important way that AI can fight climate change I generally believe that is where AI will have the most impact on climate if you had if you hadn't guess yeah you know you deploy first time you deployed this into a data center at Google you saw 40% Energy savings if we had just killer AI doing load balancing on the on the grid What what sort of Energy savings do you think we could see oh my gosh I mean that would be wild um I think it's not so much about the the magnitude of the Energy savings per se but rather about the um the potential cost savings because then you could start shifting your loads around right to when it's most cost effective to do compute or if you had CO2 signals you could start scheduling loads around um when it's the most the least carbon intensive to do your uh non- leny sensitive workloads right which I think Google has already been experimenting a bit with right um but honestly I think it's really more around the global system level optimization right we have to keep in mind that data centers already are but increasingly you know um are just massive massive load Banks right like data centers like they they were one and a half 2% of us energy consumption that's about to increase to 4% right like this year I think and then by the end of the decade is projected to get up to like 9% of the US in in Ireland right right now Ireland is 22% of Ireland's national energy electricity consumption goes to Data Centers alone the International Energy agency predicts that that's going to increase to 37% by the end of the decade right like just mindboggling numbers but the point the reason why I mentioned this is because these are massive load banks on the grid right the there is an actual opportunity if you could somehow coordinate the data centers together right um to uh to help balance the grid right and that is such a gnarly gnarly Challenge and it is what is holding the energy transition back because you know as more and more Renewable Energy starts coming onto the grid right the supply side becomes increasingly stochastic right we used to have this perfectly deterministic system at least on the supply side where you know a good operator can call someone who operates a cold fire power plant and say hey ramp up or down your power production right deterministic but now ramp up or down the sun yeah totally right so now is get more and more renewable penetration you know coming on to the grid right you have a somewhat non-deterministic demand side right it's somewhat predictable but there are definitely spikes and a massively non-deterministic supply side right and what is the problem with that the problem is that um you know we because we do not know how much energy we're going to generate you now have all this wasted excess capacity in reserve right so there's a concept of spinning reserves on the grid where um you know like there are peer plants um you know like giant natural gas turbines right that as we speak are just sitting there idling just like your car Idols uh at a stoplight right in case we need that power right as a buffer against the uncertainty and as renewable penetration increases ironically the amount of buffer you need also increases if you look at Germany's failed energy transition right they decommissioned their nuclear Basel load while wrapping up their their renewable energy penetration right good motivation on the surface although I person say think we need a lot of more nuclear on the grid but that's another topic but um it ended up backfiring right because Germany actually ended up needing to build more fossil fuel power power plants to buffer against all the renewable energy that was coming onto the grid now right so that's why I think AI for grid balancing we need it and is probably the single most impactful thing that AI can do to solve climate change let's talk a bit about some of the limit limitations of reinforcement learning and also where you see it intersecting with Transformers yeah yeah um so I should state that first of all my co-founder vaa is by far the expert on this topic he knows way way more than me you know I'm I'm just a I'm just a simple mechanical engineer right who who happened to learn a bit about AI um I think I think the intersection is really interesting like very very um very potentially complimentary strengths and weaknesses is how I would describe it right um it's it's certainly not mutually exclusive right like what what I mean by that is and I was just talking with v about this earlier right so V will tell you that like you know all intelligence systems have certain Hallmarks right of of intelligence so that we can say they are intelligence they need to deeply understand and the world the environment that you know that that um you know that they're modeling right um there needs to be some element of memory right so like remembering things and very importantly there needs to be the ability to plan and reason right very interlined Transformers are clearly quite good at the first one right in the sense that like they can take in um huge amounts of structured and unstructured data right um to learn quite good uh models right of the world but it is limited in the sense that these models are primarily through correlation and not causation right that makes it challenging for at least for what fedra does right because because we work with Real World Systems we have to have causality like we have to understand why is the AI doing certain things right like why is it not doing other things how do we force a certain Behavior right that we know has to exist in our system right so these are mission critical systems what I'm trying to say there has to be causality so that's where the limitation is right um with reinforcement learning systems I mean the the power of R based systems is very you know much in the planning and reasoning uh uh part right where um you know you're able to plan long trajectories of actions and learn really you know like uh intricate policies right um I think where it gets really interesting is the intersection right where potentially Transformer architectures can learn models right you know like value functions right that the um that the or or or models of the of the world that the AI can learn policies against but without that causality piece right it's going to be quite tricky to cut it over into at least industrial control applications like what fager do should we move into a rapid fire round what are you most excited about in the world of AI in the next five or 10 years so in the very near future right I'm excited about just the absolute explosion right of of AI applications right it it feels kind of like a Precambrian explosion of sorts where like there's like a primordial soup and like all these AI startups and services are all of a sudden springing up right so it's quite exciting um but when I look at where that uh where that activity is happening where that research and that entrepreneurial activity is happening um is very clearly focused on like around llms and even more specifically around like uh uh natural language interactions right tax based interactions right and that certainly is a large part of the economy it is very exciting but in the five to 10 year frame to answer your question I'm most excited about when we can start uh getting some of this technology into real world physical yeah applications right it's the intersection of this technology with uh the the the real world infrastructure that we that we live in right like big Industrial Systems cars homes like you know physical things I think that's where uh we're going to see some really interesting things in the future who do you admire most in the field of AI gosh a tricky question I admire a lot of people you worked with some of the greats and so it's going to be hard yeah I mean of course my mind jumps immediately to a lot of the people whom I've worked with right um you know um uh I admire um very much the the dine researchers whom we you know worked very closely with um I often tell people working at dine it's kind of like work being a kid in a candy shop if you're a technologist like myself um it's like you get to see years in the future right all this cool technology on the Forefront and it just makes your head spin as to like all the possible applications of that technology um I I admire moose a lot right my my old boss who you know has a of course since moved over to to Microsoft um you know I was saying earlier like one of the biggest lessons I learned you know and my co-found has learned that deep mind is that like making a technology like what we did for Google data centers versus making a product like what we're doing at fature totally different things wildly wildly different things and there are few people as good in the world as moose at like ticking Technologies and turning them into real products right I remember uh my co-founder kti and I um you know we were sitting down we were grabbing drinks with moose at like um at some random Dive Bar in Seattle right he happened to be up there and this is before you know um open AI like released chat gpt2 and just like ushered in like a world of craziness right and um and you know he was raving to to myself and Katie about the applications of llms and like you know the um you know and how powerful these systems are and we were like okay moose but you know let us tell us let's tell you about fed like we had no idea what he was talking about right but I mean he was preent like he saw this ages in advance right you know what the technology the technology that was being developed and the capabilities right that it would usher in and then of course he went off and he started inflection so I admire him a lot for the ability to turn technology to actual products all right last question you are building a very ambitious business very hard business bus to build and you've been at it for a while in the context of the new wave of AI startups what advice do you have for other Founders or would be Founders who are trying to build companies here I mean I'm not sure I'm even qualified because um one I hope I hope Yes again in um you know in one or two years when hopefully you know fedra is is wildly successful um we certainly didn't choose the easy path by focusing on Ro infrastructure um um honestly my mind gravitates towards more like um would be Founders right like people like my co-founders and I who were thinking about leaving to start something new right and my advice there is twofold uh one make sure you have co-founders like my guide is so stressful there's so many things that can go wrong and you're constantly on this emotional roller coaster of up and downs having co-founders to lean uh both for the workload but also just for the emotional support and mental sanity so important right um advice number two would be um the risk is less than you think it is H right um I I I'm biased but I think people should take the jump right A lot of times when I when I talk with you know my former colleagues you know and other people right who are thinking about making the jump right you know they'll say things like well but I've got a nice job over here you know you know they pay me well there you know I'm on a rising trajectory but my point to these folks is always like no matter you know like how valuable and successful you know you are today in the organization like you will only be more valuable and successful for that organization or other organizations or to society in general if you learn new skill sets right like take the plunge go out start a company learn what it's like to turn Technologies into products right and and if that fails for whatever reason hopefully it doesn't right but if you fail then the Googles the microsofts whatever the world they will only want to hire you back at an even higher premium so why not take the pledge right it's the the you know the the the biggest best investment and obviously much smarter people than me have said this for a really long time but the best assestment you can make in yourself right is you right upleveling yourself uh learning new skill sets that's always you know the best thing you could do thank you Jim a fascinating conversation yeah thank you very much for having me guys um really enjoyed it thank you [Music] [Music]

========================================

--- Video 62 ---
Video ID: U8FFeG0qeTU
URL: https://www.youtube.com/watch?v=U8FFeG0qeTU
Title: Fireworks Founder Lin Qiao on the Power of Small Models to Democratize AI Use Cases
Published: 2024-08-13 11:00:33 UTC
Description:
In the first wave of the generative AI revolution, startups and enterprises built on top of the best closed-source models available, mostly from OpenAI. The AI customer journey moves from training to inference, and as these first products find PMF, many are hitting a wall on latency and cost.

Fireworks Founder and CEO Lin Qiao led the PyTorch team at Meta that rebuilt the whole stack to meet the complex needs of the world’s largest B2C company. Meta moved PyTorch to its own non-profit foundation in 2022 and Lin started Fireworks with the mission to compress the timeframe of training and inference and democratize access to GenAI beyond the hyperscalers to let “thousands of flowers to blossom.”

Lin predicts when open and closed source models will converge and reveals her goal to build simple api access to the totality of knowledge.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 - Introduction
02:01 - What is Fireworks? 
02:48 - Leading Pytorch
05:01 - What do researchers like about PyTorch?
07:50 - How Fireworks compares to open source
10:38 - Simplicity scales
12:51 - From training to inference
17:46 - Will open and closed source converge?
22:18 - Can you match OpenAI on the Fireworks stack?
26:53 - What is your vision for the Fireworks platform?
31:17 - Competition for Nvidia?
32:47 - Are returns to scale starting to slow down?
34:28 - Competition
36:32 - Lightning round

Transcript Language: English (auto-generated)
we thought replacing um other Frameworks as library with py Tores to be simple it's just swap the library how hard can that be um but we real realize it's we thought it's just a six-month project it turns out to be fiveyear project for us to support entire M's AI workload building on top of P because we have to rebuild the whole entire stack from scratch from ground up because we have to think about how to load data efficiently how to do distribute inference in pyto efficiently uh how to um you know scale training efficiently uh and then we be and up rebuild the whole entire inference in training stack on top of P um when we left it was sustaining more than 5 trillion inference per day um so that's a kind of massive scale by 2 five years and the fireworks mission is to significant accelerate time to Market for the whole entire industry compressing it from 5 years to 5 weeks or even 5 days time to Market so that's our [Music] mission joining us today is ly taiao founder and CEO of fireworks Lynn is an AI infrastructure heavy weight who previously LED pytorch at meta which is the backbone of the entire Global machine learning ecosystem she's taken her experiences at P torch in order to build fireworks an inference platform for generative AI we're excited to ask Lyn about the market trends behind AI inference and how she plans to support and even accelerate the market shift to compound AI systems at fireworks we're thrilled to have Lynn CEO and founder of fireworks with us today thanks for joining us Lynn thanks for having me well we're really excited to talk about a lot of things with you today from pie torch to the small model stack that you're building uh to what you're seeing in terms of Enterprises building production deployments but before we get there can you maybe say a sentence or two on on what you're building at fireworks yeah so we started fireworks in uh 2022 um and fireworks is a SAS platform first and foremost for General AI inference and high quality tuning especially using our small model stack um we can get to very low literacy for real-time applications very low cost for sustainable business growth and customization automated customization for tailored high quality for Enterprises so that is fireworks wonderful uh I want to maybe start with the pytorch story uh pytorch is kind of the foundation upon which the entire AI industry runs today uh and you and dimma and some of your other co-founders were integral and you know and and leaders of that project at meta so think about pyro as a programming language for digital brains okay um and uh it's designed to for researchers to very easily create those dig brains and experiment with it but the challenge of p is it's very fast for people to create various different deep learning models the digital brains but the brains don't think fast enough so that's a challenge I I took on to address I was had on P torch and you you mentioned um you've mentioned before that most of the companies that are trying to build something similar to what you're building in fireworks have chosen to be framework agnostic whereas you very much made a big bet on pytorch um can you say why make the big bet on pytorch and what benefits that brings to your customers um that is really based on what I see uh when I operate pytorch and matter also across the industry and I clearly see air effects that P torch is because it started that as a tool for researchers uh it start to dominate dominate the top of the fernal for model creation and then the next stage of fernal is people doing applied production work they take those research models and test out for production setting and and try to validate that the hypothesis and then feeding into production so that's clear for No Effects that's happening um and as P to which is the I for researcher it take over the top of the fal and it's really hard for people to rewrite into other Frameworks for production and naturally it just flow down towards the bottom of fernal and that's how py become do dominant and I'm start to see um more and more models especially more n models are all built in py and run in pying production including the general AI models that's why we only B on py and we don't want to distract our to support other frames re researchers like it and it flows Downstream from there what do researchers like so much about P torch Simplicity uh Simplicity scales and that's kind of lesson learn we um through the Journey of p and madok uh and also building out the community it is a Relentless journey to focus on Simplicity and we have a constantly seeking how to make the user experience simpler and simpler and hiding more more complexity in the back end for example when I started this journey at matter there are three different Frameworks um Cafe 2 for mobile Onyx for server side production P for research is too complicated and the the mission is to reduce three Frameworks into one to simplify but it's actually a mission impossible after I consolidate all three teams and there's no consensus to how to simplify and build this one stack and uh we took a very idealistic approach and take the pyto front end and take the cafe to back end and we said we're going to zip zip them together um it seems simple but it's very hard to do because these two Frameworks are never designed to work together um and the integration complexity is even much higher than build a framework from scratch so two complex and then we said forget about it we're going to all on high torch keep its beautiful simple front endend and rebuild the backend so we build Tor script that's py 1.0 so that's really like the key focus on Simplicity Wings Over time um the other interesting thing is we thought replacing um other Frameworks as library with py torches to be simple it's just swap the library how hard can that be um but we realize it's we thought it's just a six-month project it turns out to be a fiveyear project for us to support entire a workload building on top of pych because we have to rebuild the whole entire stack from scratch from ground up because we have to think about how to load data efficiently how to do distribute inference in pyto efficiently uh how to um you know scale training efficiently uh and then we end up rebuild the whole entire inference in training stack on top of PGE um when we left it was sustaining more than 5 trillion in inference per day um so that's a kind of massive scale by two to five years and the fireworks mission is to significant accelerate time to market for the whole entire industry compressing it from 5 years to five weeks or even five days time to Market so that's our mission maybe when you look at open source standards there's a lot of people that are trying to do it on using VM or tensor RT llm how do you think about how fireworks compares to to what's in the open source I really like both projects and because my heart is in open source based on my pyth experience um I I will say both project are great projects for the uh for the community I think our biggest differentiation is uh first of all p uh Fireworks off the shelf is faster than both of the uh offerings and second is we're building a system we're not just a library and our system can autotune towards our um developers or Enterprise workload to be much much faster and to be much much higher quality MH and and that cannot be achieved by just library and and the World building all this complexity uh going back again to uh our joural P to we are providing a very simple API but hiding a lot of automation the complexity automation complexity of autotuning behind the scene uh for example when we deliver our uh inference with high performance high performance here means low latency and low cost um we handwritten Kura Kos we implemented distributed inference across nodes and disaggregated inference across gpus where we chop models into pieces and and scale them differently we also implemented semantic caching where um given the content we don't have to recompute and this um we capture application workflow patterns specifically and they build into our INF stack and we yeah we have many other um many other optimization we are um we have been um specific design for different use cases um that is not like general purpose or horizontal so that is being encapsulated we also have complex optimization for quanti tion you can think about oh quanti tion just one technology how hard that cannot be but you can quantize so many different things you can quantize KV cach you can quantize ways you can quantize communication across gpus across nodes um and EO different performance gain and uh quality tradeoffs we also automate um like quality optimization there are many things we're doing behind the scene to deliver very simple experience to the app developers so they innovate they concentrate their cognitive um bandwidth to innovate on the application side you um I like your comment earlier about Simplicity scales and as you're talking through everything that you've built to make this such a simple and delightful experience for your customers it reminds me of the idea of um uh conservation of complexity you know like the amount of complexity required to deliver any given task can be neither created nor nor destroyed it's just a question of who takes the complexity that's right and it feels like yours as a business where you have embraced an enormous amount of complexity to make life simple for your customers actually my question is about your customers so where where in the AI journey of your customers where their AI Journey do they say wait a minute we need something better and then what brings them to you yeah so we' have seen pretty consistent pattern that last year people all start from open the a because they are in the heavy experiment mentation exploration mode um many startup they are they have some creative ideas application product ideas and they want to explore prodct Market fit so they want to start with the most powerful model yeah where open air provides um and uh and then when they they feel confident they hit prod Market but they want to scale their business and then the problem comes in because as I mentioned most of the Gen application there are b2c consumer person developer facing it requires very high responsiveness low latency is a critical part of product viability without that they don't it's not a viable part people are not patient enough to wait for half a minute for response that's not going to work so they are seeking actively seeking law lency um and then uh another key factor is they want to build a sustainable viable business they cannot bankrupt quickly um and the Weir thing is not in this market they can't the weird thing is if they h a viable product that means they can scale quickly and if they're losing money at a small scale they're going to bankrupt quickly right so Bringing Down the total cost ownership is critical for them so that's why they come to us so it sounds like um I remember you had this Insight a year or so ago and we spoke about you know training tends to scale in proportion to the number of researchers that you have whereas inference tends to scale in portion to the number of customers that you have and in the long term probably going to be more customers of AI products than AI researchers out there and therefore inference is the place to be it sounds like it sounds like you're kind of the customer Journey sort of begins as people are going from training into inference what what sort of applications what sort of companies are are at that point where they're starting to really go into production there there's so many ways to answer this question it's a very interesting question uh so first of all my hyp hypothesis when I start a company is we're going to take our startups first because they're most tech events and they there will be ton of startup build on top of gen they will go to digal Native Enterprises because they are Tech forward and then we'll go to traditional Enterprise because they are like Tech conservative they want to observe and you know um adopt when the technology and the product ideas are mature right so that's kind of my hypothesis and uh it totally BW my mind what's happening right now because uh we have a lot of inbound of startups uh we are working with dig native Enterprises we're also simultanous working with traditional Enterprises including health insurance company Healthcare companies and Banks and uh uh especially for those traditional Enterprise usually I adjust my pitch to be very business oriented because hey you know that's kind of my uh maybe my bias and and kind of want to strike a meaningful conversation with them but they quickly dive into very lowlevel details technical details with me uh and it's very very engaging who are the people doing like at a traditional Enterprise who are the people that you're engaging with so um we like is it Innovation Innovation person AI person or is it more business line leader somebody who owns a production application yeah so I think it's start to shift we are more engaging start with ctOS H um they kind kind of I feel like that this business is Shifting towards Innovation driven U business transformation and that's why can we we encounter more CS than like the uh Clos or other seos um so that's kind of interesting shift um but yeah across the board I think there are multiple fundamental reasons that why that's happening that's my hypothesis one is um all the leaders real current gen wave is similar to the cloud first shift or similar to the mobile first shift it's going to remap the landscape of Industry startups are growing really fast and the incumbents feel threatened if they are not innovating fast enough they will be obsolete they will will be irrelevant but also across the incomers they are heavily competing with each other they competing how fast they can the transition that business to be to create more Revenue to be more efficient using geni so that's one phenomenon the second phenomenon is General AI is different from traditional AI I would say kind of this is like very different traditional AI is giv a lot of power to hyperscalers right because traditional AI is you always have to train from scratch there's no concept of foundation model you build on top of um and that means you have to go off and curate all the data and the data rich company usually they are hypers scalers and it you need to have a lot of resource investment to chain your own models and so on so that is before jni it's less affordable it's concentrated in hyperscalers post J because of the concept of foundation models people build on top of foundation models and you you don't train from scratch it's it's not meaningful it's all the same data it's all the internet data you can crawl uh it's a more or less similar model architecture it's waste of resource if you train from scratch instead you find tune you tune uh based on your uh small data set high quality small data set so it becomes a small data small model problem and it makes it so much affordable to everyone uh to access uh to access this technology um and that's why everyone is jumping in to eras it how many of your customers are using you for fine-tuning versus just using base model and what do you think goes into building a great fine tuning product it really depends on the problem they're trying to solve we actually see the open source model is becoming better and better uh the quality difference between open source model and the closed Source model are shrinking and my prediction is going to converge at the same model bucket um same model size you think open and closed will converge the open and the closed will converge do you think there will be a time lag where closed is always 6 months ahead or do you think they'll just be neck and neck for same model size especially uh like like 70 between 7 to 70 billion um or even within 100 billion model size the quality will converge that's my prediction we'll see we'll see after a couple years and we will come back to this podcast and see how it goes so uh so the key here is uh is customization right given like if this trend is true then the key differentiation is how we customize those model towards individuals use case toward individuals workload and is it easier to customize an open source model than a close Source model or um so I would say it's easier it's just open source model tend to have a much richer community and there are a lot more people working on building on top of those models for example a llama 3 Model it's a very very good base model it is very strong in instruction Vol a f instructions very well so it's very easy to it to align the model to solve a specific the problem really well um and for example we have been investing in function calling strategically as um uh as a direction uh we can talk more about that it's a old topic by itself but we find like find find to a function calling model on top of Lama 2 is so much easier compared with uh fum based on mixture models or previous llama 2 and and other models so that's just kind of the base model open source base model is becoming very very strong in uh in Social folling in logical reasoning in many other based capabilities so it's very easy to war it to become a high performance model for soling specific business task that's a power of small models um if we think about just open source software open source infrastructure software 20 years ago open source was thought of as a fast follower sort of thing you know Red Hat being a canonical example and then more recently open source is not the fast follower it's actually the innovator you think about or confluent or some of these other great open source businesses that have been created do you think there are areas in the world of models where open source is actually going to lead closed source and is actually going to be ahead of the proprietary models so I think the Dynamics is very interesting right now because the proprietary close Source model provider they're batting on very few models right so Open the Eyes a l models are like maybe three models right or you can think about as one model because what is a model model is the model architecture and data training data right that defines a model so I'm pretty sure they use all the models they have more or less similar training data model architecture is more or less similar so it's kind of scale all parameters and so it's not just open eye I think entopic or uh misual and also all these kind of model builder they they have to con concentrate the effort to uh focus on a specific like model segment uh that that's kind of the best R right yeah that's a BS model but open source push a different Dynamics because they they enable so many researchers to build on top of it so uh so that's kind of the small model uh phenomenon uh it it's smaller because and it's easier to tune easier to uh improve quality easier to focus on specific problem space uh so it enable thousands of flower to Blossom thousands of flow to Blossom so that's the direction we believe in is to solve an Enterprise problem a thousand flower blossom is much better for Enterprise um because um you know you just have so many problems and I bet at given at any problem space there is um there is a solution for you and we further customize towards your use case in your workload and what you get is better quality um much lower latency for realtime application much lower cost for business sustainability uh and and growth so so we believe in that direction maybe to that point have you seen your customers so far are able to match the quality that they you know got with open AI when they move over to the the fireworks stack and like how are you enabling the what I call the small But Mighty stack to compete yeah so it really depends on domain so for some domain actually people don't even find tun they use an offshelf model as is uh and it's already very very very good um for example in the domain of uh coding Cod pilot uh code generation transcription translation OCR um it's just phenomenal those models are really really good uh so that's kind of offshelf and ready to go um but for some areas it requires B business logic because every company is defining what is good differently and then of course like off the shelf model will not work off the shelf because they don't understand the business Logic for example classification different company want to classify hey you know some Marketplace want to classify whether it's a furniture or it's a dish or it's something else that's completely in um depends on um their domain uh or uh summarization you think summarization is a very general task but for example insurance company want to summarize into very special template um and right so um and there there are specific business task on um yeah on many other things we we just kind of work with uh across the board various different problems um and those requires fine tuning um and I want to call out fine sounds simple but it's actually not simple at all um so the end to endend uh requires Enterprise or developers to collect data for thing to trace uh after they trace into label after the label they need to pick and choose which fine tuning algorithm to use there's supervised fine tuning there's DP there's slew of preference based f as in they don't label absolute good um result they basic say I prefer this over that um they need to pick uh whether they want to use parameter efficient fine tuning like Laura or full model fine tuning and for some task they need to tune hyper parameters uh not just uh not just the the model weights itself so among these many technology they have to kind of figure out when to use what and so on it's very it's very deep uh and usually those app develop per they haven't even touched AI yet and there's lot for them to pick up and then once they tune and they test it it's still it improving some Dimension if it's still not good in some some other uh cases and then they need to F capture those failure cases and analyze should I collect more data and go through this cycle again or it's actually a product design right it's very interesting some failure cases not really F case it's just they haven't designed what the product should react uh for example people are building um assistant to autogenerate content when people type uh and if you're in a table and uh you are in a your cursor is in a cell and what does autogenerate mean do you Auto extend would you typ in the cell do you gener more roles or you do nothing so it's actual product design yeah so that requires a PM to be in the loop to think about the failure cases um so with all this complexity what we want to do is take a take away the mentary stuff take away um the complexity of figure out which tun approach to use um how to automatic label data how to automatic collect um data from production want to take away all this and keep a simple simple API for people to use but leave the design part to our end user for example we how the product should respond should completely uh in their realm to to figure out and solve so that we want to kind of create that separation um and we started working in this direction and hopefully we'll announce our PA that there soon I love that you're kind of liberating people to to not have to think from the tech out and to actually think from the customer back and sort of sort of use all the stuff that you've built to deal with the underlying technology and really focus on your point the design patterns and the usability and making sure that they're actually solving an important problem end to end in a compelling way what is your vision for the fireworks platform and like to Pat's Point earlier on conservation of complexity you know we started this this podcast talking about how your conserving complexity for your customers on the inference stack you just now talked about how your conserving complexity for your customers in terms of the F tuning workflows like what are the other pieces that have to come to together and and what is your ultimate vision for for what fireworks the products is if everything works five years from now what will you have built so um so what we like the nor star for fireworks is the simple API access to the totality of knowledge right so right now we're building towards that direction we already have more than 100 models uh we provide across lar language models image generation models audio generation models uh video generation models embeding models and uh multimodal models as image as the input to extract information so that's kind of one side of the uh Foundation model coverage but put all the foundation model together it still have limited knowledge but be because it training data is in limited uh it's training data it has a starting time ending time all the information they can craw on internet is still limited because there are a lot of knowledge that's hidden behind apis hidden behind the public apis that you you don't have access to or you you just cannot get real time um information um there are a ton of private apis hosted with the Enterprises no way anybody they will have access out outside of the companies um so then what how do we get access to totality of the knowledge for the Enterprises is to uh is to have a layer to blend across many different models and public private apis um so so that's kind of the vision and the tool to to uh the vehicle to get there is function calling is the function calling model basically this model is capable of understanding here apis you want to access and for what reason uh it it can automatically be the router um to to most precisely call out to those apis whether it's accessing models or accessing non-model apis uh in the most accurate way uh so think about strategically that's extremely important uh to build this simplified user experience because then our customer they don't need to figure out they don't need to scratch ahead and figure out oh I need to find tune to be able to access those apis and how to even do that myself is kind of a tall order for me to do that so we want to basically you can think about that because many people are familiar with a notion called mixture of expert y um so open AI is providing mixure expert and mixure expert becomes a very popular model architecture the ccept is has a router sit on top of few very big experts and each is specialized in doing its own thing uh and our vision is we want to build a mixture expert that access hundreds of those experts and each of those experts are much smaller uh much agile um but uh with high quality of solving specific problems real quick do those experts live in fireworks in AWS in hugging face like where do those experts come from that get put together with fireworks as the overarching framework yeah our ambition is those experts living fireworks that's where we want to curate and curate towards curated models we serve towards that and that's why today we already have more than 100 models um and it will take some time to to kind of build this layer in a very solid way but we're going to release our next gener function calling model it's really really good A little preview on that uh it's it has multiple layers of breakthroughs uh and we're going to announce it uh together with um demos and example and people can leverage and build on top of very cool do you do you see any viable competition for NVIDIA on the horizon um that's a that's a very interesting question uh first of all I think Nvidia is operating a very lucrative market and any lucrative Market invites competition this is just the economics um here um and also from the whole entire industry point of view um in General Industry doesn't like Monopoly uh so that's kind of another um Trend like pressure coming from the industry so I think it just it's now the question whether there will be competition to Nvidia is is the question of when do you think it's coming soon I think it's coming soon I think it's coming soon I think I mean obviously uh we we can look at Nvidia competition in multiple uh segments on the like general purpose uh competition segments GPU that MD is coming up uh that's interesting and I think also in a specific AI segment where the AI model space is stabilized there's no more like Innovation like the problem is well defined and this is the model then customistic will have its own role uh so so I think kind of I will look at the market that way and I I I do think uh there will be com competition coming as soon can I ask you about that by the way because you guys are in this part of the market where um you are model agnostic to some degree and it's really about the optimization of those models when it comes to put them into production do you do you think that the returns to scale on the frontier the models that are out on the bleeding edge do you think that the returns to scale are starting to slow down do you think that we're going to go into a phase where capabilities have started to mature or ASM toot and the race is more about the optimization and tuning and application of those capabilities I think both will happen at the same time one is it will start to stabilize and plat two in the model uh applicability point of view and we heavily customized our strategy is heavily customized towards the use cases and workloads so that's One Direction and the second is I want to caution that like because at meta we also think for certain period of time that that that is a model for ranking recommendation right so um and uh and we should heavily like indexed on on that assumption but um but then after a few years it's not the case there's significant amount of model innovation in a seemingly stabilized modeling space um and that pushed the S curve form matter I think same phenomenon will happen in the jni space that a new model architecture will happen and we're kind of overdue see we talk competition from other other vendors other direct competitors what about open AI does does open AI keep you up at night like they drop prices on their API all the time they're you know they're they're making their models they're also trying to win the better faster cheaper race like how do you think about them um and how do you think about you know ultimately what you're going to build that's different from where they are going right so again I feel like for the they are actually going smaller right and cheaper I think for the same model size for the same model bucket whether it's closed Source or open source it the quality is going to converge that that's again that's my prediction um and the real meaningful thing is to uh to push the boundary here is have a customization or uh automated customization tailored towards individual use case and individual workload um I'm not sure if open ey is uh has the appetite to do it because they is Agi if they hold that mission which is a great Mission actually um but it's kind of solving a different problem then solving an Enterprise problem which has which basically means there a lot of problems a lot of specific problems that is really good for the small models to to customize towards and that's where we want to focus on our energy and to build on top of Open Source models uh assuming the trend that they are going to converge in quality love that um our partner Roff last time you were here made the point that you know in Prior technology waves internet mobile it was the people that did all the hard work driving down the marginal cost of running this stuff that actually enabled all the application development on top and all the unuse cases that we get to enjoy every day and I love that you are taking that exact approach with AI where it's still so cost prohibitive for most people to run in production and um by just dramatically bringing down that cost curve you're actually in the whole the whole industry Blossom so it's it's really wonderful should we close out with some rabbit fire questions yeah let's do it okay can you go first no go for it okay uh let's see uh favorite AI app we do a lot of video conferencing and the nose taker for video conferencing is a game changer for us whatever it is there's so many different varieties but oh I just love that which one do you use uh I think we use faton yeah our sales team use that it's it's really good for training and also summarization short signic short hour time nice what will be the best performing models in 2024 um my prediction is there will be many given the rate that every week yeah every week There's a new model coming up um and on um sis Arena they keep competing with each other uh so this is all good news for for the whole entire industry uh it's really hard to predict which one um but the one prediction I'm pretty confident is the model quality is will keep improving and keep increasing in the world of AI who do you admire most I I always say matter it's not one person but That Matter's commitment to open source um I I think matter is the most brilliant um in the Journey of J by continuous to open source uh a series of llama models and continue to push the boundary continue to kind of shrinking the quality differences um so okay so what what matter is doing is is basically decentralized power F hyperscalers to everybody who has a dream and to innovate on Foundation models J models uh and I I I think that's that's really brilliant of that okay will agents perform or disappoint this year I'm very bullish on agents it's um I I think it's going to it's going to Blossom that's all we got all right thank you and thank it's really fun to have this conversation thanks for having me thank you for joining us [Music] [Music]

========================================

--- Video 63 ---
Video ID: LDNsz7b4QDM
URL: https://www.youtube.com/watch?v=LDNsz7b4QDM
Title: GitHub CEO Thomas Dohmke on Building Copilot, and the the Future of Software Development
Published: 2024-08-06 11:00:23 UTC
Description:
GithHub invented collaborative coding and in the process changed how open source projects, startups and eventually enterprises write code. GitHub Copilot is the first blockbuster product built on top of OpenAI’s GPT models. It now accounts for more than 40 percent of GitHub revenue growth for an annual revenue run rate of $2 billion. Copilot itself is already a larger business than all of GitHub was when Microsoft acquired it in 2018.


We talk to CEO Thomas Dohmke about how a small team at GitHub built on top of GPT-3 and quickly created a product that developers love—and can’t live without. Thomas describes how the product has grown from simple autocomplete to a fully featured workspace for enterprise teams. He also believes that tools like Copilot will bring the power of coding to a billion developers by 2030.

Hosted by: Stephanie Zhan and Sonya Huang, Sequoia Capital

00:00:00 - Introduction
00:01:18 - Getting started with code
00:03:43 - Microsoft’s acquisition of GitHub
00:11:40 - Evolving Copilot beyond autocomplete
00:14:18 - In hindsight, you can always move faster
00:15:56 - Building on top of OpenAI
00:20:21 - The latest metrics
00:22:11 - The surprise of Copilot’s impact
00:25:11 - Teaching kids to code in the age of Copilot
00:26:38 - The momentum mindset
00:29:46 - Agents vs Copilots
00:32:06 - The Roadmap
00:37:31 - Making maintaining software easier
00:38:48 - The creative new world
00:42:38 - The AI 10x software engineer
00:45:12 - Creativity and systems engineering in AI
00:48:55 - What about COBOL?
00:50:23 - Will GitHub build its own models?
00:57:19 - Rapid incubation at GitHub Next
00:59:21 - The future of AI?
01:03:18 - Advice for founders
01:05:08 - Lightning round

Transcript Language: English (auto-generated)
the human brain is still so much more advanced than than uh the Transformer models and the diffusion models and and the other types of models that we have to image recognition and whatnot that we have today um and you know it remains to be seen if you can kind of like add that sentience piece to it but today I'm I'm not seeing it and I haven't seen any research that telling me that that's coming anytime soon [Music] hi everyone welcome to training data today we host Thomas Dom CEO of GitHub Thomas has an ambitious vision for enabling a world of 1 billion developers and bringing agents through the end-to-end developer workflow and into adjacent categories like code security he even hints at the progress he thinks the industry will make on sweet bench over the next few years some categories he's excited about in AI outside of developer tools and whether he thinks a new architecture will overtake the Transformer today we're so excited to introduce our special guest Thomas Dom CEO of GitHub hey and thank you so much for having me Thomas we're so excited to dive into the GitHub and co-pilot story in particular today um maybe to kick off we'd love to learn a little bit about your personal background you have a very interesting story having grown up in East Berlin before the wall fell um and then starting your first company that brought you into the United States when Microsoft acquired the company how did your background and upbringing really shape who you are today I think you know I'm living a very normal life not the American dream in a house with a wife and two kids um I think what shaped really um my journey was the passion for software development early you know when I was um uh in 11year old or so it was still East Germany and West Germany is there was the wall uh between the two parts of Berlin and saw computers for the first time I couldn't buy one but I in school we had one on the geography lab and a friend of mine and I we started uh uh we know playing with that learning to code you had to code to even do anything with a machine you know you understand some basic to even load a program and then as the wall fell I bought a Commodore 64 and later my first PC a 386 dx40 wow and so I you know as as a teenager I spent most of my time uh coding and um you know started a company it wasn't at that time really a startup we just that insurance software as in the late 90s that wasn't most of insurance agents didn't have software yet somewhere working on Main frames and others just had you know people in front of them and then I moved to South Germany to work for Mercedes and then I had you know the startup that got acquired by Microsoft but it's really this passion for doing stuff with software being creative with software and I think the fascinating thing back you know in the '90s and it's still true today is that you can start you know uh very easily uh there's not a lot of capital investment required and um if you if you make a mistake you can just start from scratch you know and then and I think that's what makes this so so so cool to build software and that gave you the love for building and fixing things even today with robotic lawn mowers in your own home yes well and it gives me the love you know as the CEO of GitHub uh to build software for software developers I think that's the really cool thing about being at GitHub we're building the tools that other developers are using and we we always say you know we put the Developers first and that I think is you know the dream job for me I get to speak with a lot of developers I get to build software for developers and um and I get to speak you know to to many developers um here in Silicon Valley I think what many might not know about you outside of Microsoft is that you were actually you played a pivotal role in um sponsoring Microsoft's acquisition of GitHub back in 2018 U can you share take us back to that moment and share a little bit about what was your vision for GitHub back then yeah so in 2018 I was a product manager at Microsoft working for net Freedman uh who at the time was the CVP for mobile developer tools and N had the idea of of of buying a GitHub um and making GitHub part of Microsoft and so he and a few folks in his team uh were kind of like strategizing of how can we pull that off and and Pitch it to SAA and The Bard and and as the uh deal um got announced actually you know um about six years away from that um at the time of this recording June 4 2018 is when we announced the deal um I became the deal integration manager which was was the role within Microsoft that runs around the whole company making sure all the pieces you know from legal to HR finances and product and Engineering come together to get that deal through the regulatory approval and and ultimately now uh getting us to a successful day Zero which then happened in October 2018 and that's that's how how ultimately came to GitHub um what did you envision GitHub becoming um the potential it had as part of the GitHub Universe I think you know at the time we were thinking there's so much potential for GitHub that's still uh you know to be to be explored and to be realized that GitHub uh started in with the first commit in in late 2007 uh launched in 2008 you know very quickly became a new way of of developers working together you know social coding uh uh was invented by by Chris and and the other Founders and um it evolved into this into two-sided product the home of Open Source where many open source developers were collaborating and the place where many um startups and ultimately Enterprises uh were building their software and often that you know two- seted equation was that this the companies wanted to work exactly like open source developers work which is boundary less you know in the world of Open Source you don't really care where your collaborator sits where they're from what their education is they don't sit in an or chart and and you know often you don't even know their real name all you know is the handle and then the code that they want to contribute back to the project and I think many companies don't have that you have silos and you know when you get an email you're like wait who is this and why are they emailing me and why do you uh why do you want to be involved in my project and so companies want to burn down these these World Gardens and have a similar collaboration model and that's what GitHub um symbolized you know in 2018 but there was so much more to do you know to provide something like get up actions that allows developers not only to manage their source code and plan but also to build apps and and now ultimately with co-pilot to take it even a step further um and and making you know development a very different experience than it was 20 years or 30 years ago when I started since you mentioned co-pilot yeah uh I'm dying to ask you about the the behind the scenes view yeah on what is you know not only the most successful Enterprise AI application today but I believe the first llm Native application that was really built um built and launched like I guess was it part of the original acquisition thesis at the time that you might be able to build something like co-pilots eventually who's whose idea was it to do something like co-pilots and like was it did everyone say yes this is going to work or was it was it like this is a crazy idea this is a moonshot it's never going to work like take us back to the origin story so the original you know acquisition thesis had like a little paragraph in it about AI but I think that was more like a moonshot than like the pop idea at the time what really happened in in mid 2020 is um that you know Transformer models um you know the the paper came out a few years before that but the first you know really working Transformer models was gbd3 uh was about to to launch we got Early Access we were lockdown right June 2020 um on a on a call and um uh one of our um um team members ug deore um started you know uh typing into things into the model and everybody else was just looking uh what ug would be doing and then then so then in the actual question what can we prompted uh to write code and uh and can it write proper code and I think that was the first kind of like aha moment that it was actually able uh to to write the the real syntax and then then we tried different languages and we also found uh flaws you know in in the model and so we started um you know exploring that deeper outset of that of that Zoom call in in a research process you know doing analysis we asked uh uh you know uh some of our staff and principal Engineers to submit coding exercises we looked at python functions that were you know in open source repositories on on on GitHub and we work with open AI to uh take gbd3 and find you in the model to be better at these coding tasks and and ultimately in August we had a model that was able to solve 92% of these coding exercises and in fact you know of the Python bodies you know that uh we extracted from from open source projects it was like 52% now naturally that percentage is lower because you have less specified code than in the coding exercise for an interview Loop right but that I think gave us this uh this moment of confidence saying we can build a product around this um the second moment I'd say was when we rolled it out uh to our internal engineers in early 2021 and uh they came back with saying this is this is fantastic I think the Net promotor score NPS was somewhere in the 70s and uh which is you I think for developer tools really really good uh most developers are skeptical you know don't touch my system you know never never touch the running system or process um and and many folks are you know kind of like have have created their Rock setup not not only on on their physical desks but also on their virtual desk of how they want to work and so we were really um you know intrigued by the in internal responses and then we launched the preview and uh um you know the team came back and saying it's writing 25% um of python code in those files where where it was enabled and they I think we sent them back say go and verify the Telemetry that that can't be true right like because we couldn't believe it initially and then as we we saw this progressing um I think in uh my first keynote as CEO was in uh June 2022 so now you know two years ago and I said it's writing 40% of the code and I think in 5 years it's going to write 80% of the codes in those F sprs enabled so it happened really organically it wasn't like you were sitting in a room whiteboarding like co-pilot is going to be the next iteration it was like the model proved to you how great it was and and just seeing the model performance and seeing usage like really like it organically built up from going back you know to the original Theses uh for GitHub right we wanted to make make developers life easier and as we are company building software we have our own software developers and Microsoft has you know 70,000 or so of of them and so we understand uh how software developers work because we have so many of them and you know we we live the life of many of our customers which is we have way too many ideas we're moving way too slow you know at least in our feeling um you know Amazon delivers my P package faster then then we're implementing some features the also the expect expectations have shifted significantly but it's it's this the backlog is endless and the amount of ideas um that we can brainstorm on Virtual whiteboards GitHub is a remote company so we don't often meet in in real physical B in front of real physical whiteboards but um and then the other side is all the other work that we also have to do you know compliance security accessibility you know Enterprise requirements uh privacy regulations uh European you know AI act and digital markets act and and all these things also cause developer work and so we're constant L everybody's constantly struggling with the length of these backlogs and so if you will the intrinsic motivation was let's bring the effort down to right software and and make it joyful again what do you think made co-pilot so good at the time and also going forward today obviously GB3 was great even you know back then but also I think a lot of people might not know all the value that you have brought or GitHub brought in the both public and proprietary data around code that GitHub owned can can you share a little bit more about that and and also how you think about that going forward I think the key ingredient you know of the original co-pilot which was on the auto completion right like you would type in your editor and it would complete you know the next line uh but it could also complete multiple lines of code you know complex algorithms or simple algorithms some of the demos we often chose to implement a sorting algorithm like bubble sort or prime number detection uh and it can just write you know those 10 lines of Code by just a simple um prompt which is a comment in the code or writing the method declaration and so getting you know into the editor where developers already write code not changing the way they work but giving them you know ideas while they're typing I think that was the key moment other than obviously the model being good enough and an open a ey tuning that model uh on on publicly available source code from GitHub um you know GitHub GitHub didn't give special access to openi uh openi was just able to to access our source code in the same way that you know many other startups are now doing that uh either direct access you know through API or through archive programs like the internet archive and the software Heritage um and we have we actually have an established partnership with them to to stream our open source code over there so it can be archived you know for for until the end of time and so that of course the model got tuned to be good enough but then the UI and the user the user experience I think was crucial it wasn't you know AI is now in everybody's mind but the truth is our cell phones have you know some kind of AI built in for for a long time your keyboard is predicting the next word with some kind of machine learning algorithms your your photo library you can just go in there and search for license plate and find all the photos of the casts that you took to remember what the license plate you have um and then and that's AI you know image recognition but that nobody perceives that as AI it's just a cool user feature and so I think that also was the the core ingredient um of of co-pilot we meeting developers where they are and they're making their life better and then I think you know the name itself um uh was also a stroke of Genius one of other our other you know developers um Alex came up with the idea um to to name this co-pilot as Nat uh uh is is a a hobby pilot and and so that's where the name is coming from so it's like you know I'm thinking about what could I name this so you know the my my boss is the name is is resonating with him wow that's so interesting I didn't know that um if you could go back to 2020 2021 is there anything that you might do differently you know and hindsight you can always move faster and and be I think more convicted of of those ideas I think in the beginning we kept the team intentionally small small teams can can move fast um we call this how big was the team um I think the original staffers were like three people but that's obviously uh a staff researcher sorry or or principal researchers but um I think you know that that's cheating a little bit in the sense that of course there was a team at op uh there was a team in or multiple teams in Microsoft both on the research side and on the you know inference side the model inference side in fact you know on the model training side to even enable the model so of course in the bigger partnership between Microsoft open and and GitHub it was a larger team but the original paper was written uh by by three researchers and and then you know 100 people so are mentioned in the credits uh and then we move fast with I think a stub of five and then I think it increased to 10 teams and of course today the team is much smaller but we still have what we call get up next and incubation uh team that now works on on co-pilot workspace and iterates on on new ideas uh it's almost like you know startup incubator within the company and they're picking up ideas and the main difference to you know our or Mainline engineering teams and and product management teams are that they got to have the mindset that most of their ideas will never go into production right they it's kind of like this um okas won't work uh because your key result is to throw away most of their ideas and start fresh with another idea and I think that's where a lot of the Innovation speed is coming from I want to go back to what you said a minute ago about how you know you handed over you know control of the model effectively to open AI was it scary CU like the brain of your AI application is actually being built by another company not you know you know like developers under your payroll your control like how do you think about was it was it an easy decision to kind of out like uh to partner with uh a different external model provider and I'm curious how you think about the value that Microsoft and GitHub provide to end users versus the value that open AI provides to end users and then where you seek to to really bring bring value to users to me it wasn't scary at all I know I'm sure you know there were folks in the team and in the company that thought we should have our own models instead but in reality you know if you look at GitHub as a company that was born in the cloud of course we have always relied on Partners to build our stack um you know we have I to my memory we never built Hardware ourselves um uh and where we have metal in in data centers or had metal in data centers for a while uh uh the data center itself wasn't built by us either neither was the CPU and and the memory and all and the network infrastructure and then even if you go higher in that in that stack you know GitHub is is built on top of Open Source and so are the majority of software applications in fact um we love sharing a statistic that says 90% of the stack of most of most applications even if the application itself is closed source is in fact based on on the work of of the open source Community um you know from the operating system the Linux operat system is ubiquitous today on on servers to uh container technology like Docker and kubernetes to thousands sometimes T of thousands of Open Source libraries and so the model just St flows naturally in that stack and you know we uh at Microsoft think about this as the co-pilot stack with with different layers Hardware the model the the curdle if you will like the infrastructure response to the AI filtering and whatnot then you get into the application layer uh with the the you know the AI co-pilot and then the extensible ility on top of that and so if you look at these layers in the stack you know Microsoft has strengths uh and um and partners um um and in fact you know in in some form Microsoft is is is um involved in all parts of that stack um you know the copile PC you know has has a custom uh chip that Microsoft is developing uh with Partners um you know it has we have our own models with 53 and we have partnership models with openi but we're also hosting Mistral and Lama on AIA uh we of course have you know large cloud and we have a lot of expertise and responsible Ai and then we have lots of application we're building ourselves and we're enabling uh others you know to to to build those applications and so two-part questions are always hard for me I to forget the second part but I think you know that actually describes the relationship very well um you know it allowed us to move really fast because we could rely on Partners like open AI not only building gp3 and then codex but also then you know in the innovating with GP 3.5 and chat GPT GPT 4 now we have gpt2 40 uh we have micr soft you know with with a large infrastructure that um builds you know supercomputers for training but also infrastructure to run inference and you know co-pilot today runs in multiple data centers spread around the world um in different regions so the developers that sit in France you now connect to a GPU uh to aure instance that's much closer to them to enable low latency right and so Microsoft gives us a lot of infrastructure a lot of expertise and responsible Ai and of course a lot of uh uh commercial um distribution yeah very very complicated layer cake that makes the magic come together would GitHub ever want to build its own models or just use best-in-class models out there you know obviously there's always the desire of Engineers to build uh their own stuff and and I would you know I would not deny that we have played with own ideas on on models um I think you know the almost learning team goes back um um almost I think it actually had it before Microsoft acquired GitHub um and then we obviously have fine tuned models um ourselves and we have fine tun models with open ey and Azure and we're working with customers on them being able to customize models um based on the code that they have in their repositories and you know never say never what the what the future may bring but today we're really happy with the models that we have and we're constantly looking into the market of What Not only open ey provides to us but also what what others have so co-pilots is already one of the most successful generative AI applications in terms of user scale usage Etc what are some of the latest metrics you can share and and what are the metrics that you're most proud of I think the one I'm most proud proud of is the developer happiness scores and if you look at survey data both the service that we have done but also the service that now our customers um either publishing themselves or bringing back to us it's clear that software developers after they have tried co-pilot after they got over the initial adoption hurdle or skepticism they most of them love using copy and most of them report that they are more ful filed you know uh they're more satisfied more happy they feel like they're requiring less mental energy to to get the job done they need to do less spoiler plate and I think that alone is is really making me happy and um there's lots of you know numbers that I can throw out but I think the the general gist is no matter what developer I talk to those that have used co-pilot for a while no longer want to work without a co-pilot and then the other side is you know the productivity metrix of making you know developers more productive which I think matters to the developers too but it also of course matters to their management chain and and and their leadership and and in the sense of you know getting more stuff than delivering more value uh to to their end customers and so you know today today we're happy to say that we have more than 1.8 million paid subscribers on on copilot and more than 50,000 organizations and um and that really makes us happy um at looking looking at that growth and you know we here at the so's office it feels like we we like a high growth startup and and that's a good place to be in it's awesome well we're still on our wall somewhere um actually we know exactly where we'll show you me too the creative minds yeah um was there anything that surprised you in terms of copilot's impact after its launch and even today I think you know it's the 25% certainly surprised us um the quick turnaround from the skepticism after we announced this uh in June uh 2021 I don't even I think we had a very short blog post and then just a web page with examples and and and animations showing how it would work and I think folks were looking at this and saying this is like a cool Tech demo but it doesn't actually work for me and I think the skepticism wasn't was that people had seen how GB3 at the time would work and they couldn't understand until trying it that it has the context of whatever you wrote in the file before before copilot suggestion comes and it considers adjacent tabs and things like that and so it magically picks up your style and it knows about open source libraries that you're using not because you have opened those libraries just because you have an import statement at the top and because the model was trained on such a large corpos of data that um it can provide you know the calls into these open source libraries and and so kind of like it feels to you that the co-pilot understands more about your project than you thought CHP 33 can do and I think that's where the original major came from the other thing is today you know when I observe people using co-pilot and and obviously chat GPT changed the whole game uh and brought in chat as a component we have now copilot chat is this natural language component and like using not only you know Cod and and Commons as a trigger but you know English and German and Brazilian Portuguese and uh uh we have Demos in India where Karen MV one of our folks there is demoing this in Hindi and so and then you can actually speak into into the co-pilot with voice detection uh in viral Studio code and then gives you the response back also in hindii and um and so it's really cool you know for anyone that wants to explore coding even if they're not fluent in English or in programming language um uh my kids are using it to find their own bugs in Python so they're coming you know they're no longer coming to me and it's like Daddy you have to find my bug it's kind of like well go and find your own bug and and and that also obviously uh helps them to to to you know develop their skills and it's a bit like you know um I often talk about um you know natural language will democratize access to software developers but it doesn't mean that everybody is immediately a senior or principal software developers in the same way that just because I buy a guitar um I I'm not as good as you know Keith Richards uh playing with the Rolling Stones right and and if you look at you know uh any professional band that's touring the world they're all still re C over and over again and I think this is like this idea that to be you know good in a craft you have to keep doing it that copilot does not take that away it just gives you another tool in in your toolbx I really like that analogy still believe in teaching your kids to code because that's like a debate on the internet now oh absolutely I mean so first of all you know the the human language is not deterministic right like you mean different things um you can mean different things by saying the same sentence and even you know the very German even the yet well now we can get into off topic debates about about yeses and knows when you're when you're when you're answering an a question with a negative in it right like you have not been to the grocery store do you answer that with yes or with no right and the Americans expected no even though you mean actually yes to that question but like look you know it's um human language is not deterministic and so uh code is with code you can very precise describe what the machine code is an abstraction layer on top of you know Assembly Language on top of the instruction set of the CPU or GPU that you know the the the processor or manufacturer like Intel or Nvidia has created right so it's just another abstraction layer human language is something completely different it's creative and and that's the power but it also means that there will be code involved one way or another and we're moving up the abstraction layer but we also spreading the meaning um and that that's truly powerful um but also means that there will be some conversion to code somewhere because the chip itself at least you know today uh uh is is requiring a deterministic instruction set yeah I love that um I'd love to talk about the future of co-pilots uh you've been announcing new products in Rapid Fire succession I think co-pilot X co-pilot Enterprise something around code security I believe and then uh workspaces most most recently can you tell us maybe about what each each of those things does and how you see them all fitting together in the grander vision for what you hope co-pilot becomes you know what you described with all these product names is one of part of our um mindset is that momentum is our energy in this in this age of AI moving fast and iterating Fast is crucial and so um you know we are having developed co-pilot from this original idea of autoc completion by adding chat we were developing autoc completion and then chat and then uh with chat we also announced something what we call copile X which the idea was we're bringing AI features into every part of the developer life cycle we're bringing co-pilot wherever developers are so you know while we added chat um to the editor we also added you know a little co-pilot icon into the input field where you write your uh commit message and that might be trivial right like everybody can write a commit message but it also means I I reduce my mental workload and I redu kind of like the bias that I have for the work that created myself right like for me everything is obvious that I just did for the last hour but for you when you want to review my my commit or my pull request it's not as obvious and so having an AI described that in a neutral form kind of like an an outset of describing what I just did is incredibly useful and it just keeps me in my flow and we added it to the debugger we added it you know to many different part of the life cycle already and uh with copile Enterprise we we bundled it into you know a a higher price um product um that uh allows Enterprises to customize co-pilot uh based on their institutional knowledge and Enterprise here means really any company um that has gone for more than a few weeks because they all immediately build institutional knowledge right how we work as a team how is our coding practices these are the libraries and languages that we use and so unless you're a student in a university uh that you know gets to have the free range of uh Technologies available at least when they when the professor allows that every time you join a company or join a different project you have to ramp up again how they're doing things and so cobart Enterprise lets companies customize the co-pilots to their institutional knowledge and it makes it really easy for me to join that company because I can now ask dumb questions without you judging me like you know imagine I would you know join here my first day was like why is Thomas asking all these questions shouldn't he already know all that he's you know been in professional life for a long time that's the challenge that we have when we join companies and we have the anxiety in our head that we can't ask too many questions before Steph says what the hell right and so that's I think the power of of co-pilot Enterprise That's The Power of bringing co-pilot into every part of the developer life cycle and and ultimately into every part of the of Our Lives you've also mentioned that agents are one of the most important next things for GitHub U maybe just to set the stage how would you describe an agent versus a co-pilot and can you give us a teaser for what types of agentic capabilities we should expect come into co-pilot soon yeah I would say you know co-pilot in agents is kind of like the same thing an an agent is using a model to get a task Done Right effectively it's looping with a model to to solve something for you and a co-pilot is an an agent of agents and it has multiple features uh available to it you know if you think about autoc completion well it's an agent that takes every keystroke you did and the context that you have in your editor it send it to model inference it gets a response back it might pick you know the best uh uh prediction and then it shows it to you and so we going to see more of these agents that take over more of our tasks and one of them that I'm most excited about is you know autofix and and the way it works is so you submit a pool requests and traditionally um you know some security scanning feature uh that that you have integrated into your pipeline find security vulnerabilities let's say you know a SQL injection or cite scripting well that's great except now now I cost more work for myself it's kind of like you have a Roomba but instead of vacuum your house it just shows you where the dirt is and then you have to go and and and vacuum yourself on that position so now with autofix we're actually not only showing you the security vulnerability we're also giving you the fix and that uses the AI model together with the vulnerability and the description and the code to basically solve that vulnerability for you and the initial results are are really impressive with some customers we see that we can burn through like 75 80% of their open alerts because everybody has those alerts and if you don't have any alerts right now I bet you you have them by mon Monday right that's the challenge in in this in this world of software security is everything is around us moving so fast there's always a new version of a uh uh um open source Library there's a new version of Linux or you know a new windows patch uh there's a new device coming uh along the way or new new uh Nvidia GPU and so we constantly are behind uh just keeping our applications up to date you know to the to the standard that is expected by our our customers and at the same time we have to build all that Innovation and and that's yeah super interesting cool stuff Y what else do you think is missing in the product road map like if if you could W wave a magic wand like what else is there to build that you're really excited about I mean I think there's still a lot of work to do on these agents um I think there's a lot of Agents um uh you can you can think about you know we we talked a little bit uh or you mentioned workpace before what co-pilot rockspace does it provides different agents to get you from idea to your P request to get you from idea to the code and the first one is the spec agent and what that actually does it helps you with your thought process so you know you write down an idea Implement some feature and it looks at your existing code base and it basically helps you then to reframe that idea now that's not only useful for developer but it's actually useful for a product manager right because it it may might tell you well Thomas this idea is is no way you can describe that with a single sentence and and a developer cannot Implement that in in in just a single ticket it needs to be an epic or or multiple different users stories right and then the next step is the plan agent that uh you know helps to figure out where to make the changes in the code base and and again you can kind of see here there's a lot of other benefits you get from that because it helps you to understand the code base because you know most codebases that are older than you know a few days have hundreds if not thousands of files and as as developers you have to navigate all those files and even if you have been in a codebase for a long time you still might miss out you know that one file that you haven't touched for for while and and you have to add you know a config statement there so it helps you understanding the code base and then the Implement agent um helps you um implementing the code change and every step in that way you're still in charge you can you know with natural language modify the bullet points of of each of these agents and then obviously you can modify the code at the end and so if you look at just these three agents you can easily start thinking about other agents that you might have along the way right for example the one that estimates the size um of of a ticket the story pointing agent as one example or another one is that you know once you have implemented the file well now you want to build run and debug uh the file and maybe you have an agent in the future that will just automatically fix you know any bugs that that were introduced by by the previous agents and so I think we're going to have more of these building blocks you know Lego blocks if you will available to us in fact you know uh uh uh you know if you look at Lego they have way more pieces types of pieces today than they used to have the model are much more complex and then you know you can uh uh buy you know um um NASA rockets and whatnot they need different pieces for that and I think that's kind of like the same way we should think about co-pilots they will have more of these building blocks that enable us in addition you know to more powerful models and a mix of models these building blocks will enable us to do more increasing modularity interesting um where is your ambition for GitHub take you or maybe even with GitHub co-pilot specifically take you as you think about what you alluded to from the perspective of deepening where you can go with just the software engineer to also expanding into potentially different personas PMS you mentioned maybe an Sr maybe a security engineer where does that you know the breath also take you yeah I mean first of all I think all those R to today are already collaborating on GitHub in in fact GitHub had always that Mantra that we're building GitHub on GitHub and at sometimes you know we have pushed it a bit too far uh but today you know most of um GitHub employees we call them Hubers Hubers are you know engaging uh on GitHub in GitHub discussions and GitHub requests you know our legal documentation is is all on GitHub which if you think about is actually uh much better than than managing red lines in word because you have a version history and you can see who made what what changed it in fact you can kind of see uh soon in the future where maybe your legal document is explained by a copilot in in human language uh you know in actual understandable human language not the lawyer language right and so um so we're using GitHub um you know to run our company but yeah we we it's where all the developers and all the supporting functions collaborate on a project so that's I think number one number two is we want to democratize access to to software development and um you know I recently gave a t talk and I talked about that I think our goal is uh to get to 1 billion uh uh software developers on the world now it doesn't mean 1 billion professional software developers although that might not be a bad thing necessarily given the demand is still very high and it's it's it's sometimes hard to find Qualified software developers but it's it's really about democratizing access to to writing software on these devices that are with us you know our mobile phones are you know uh a really important part of our lives today you can't really imagine life uh urban life for sure without a mobile phone and so then also being able to write little applications or little scripts or just you know using natural language to control the phone I think is incredibly empowering and so bringing that into you know 10% of the world's population by 2030 so assuming that then we 10 billion uh inhabitants on this planet uh I think it's going to going to create a better world and it's going to unlock um creativity everywhere and hopefully you know we see cool uh uh Venture backed startups in India and in in Brazil and and maybe you know the next the next uh big tech company is is coming from one of those countries uh um instead instead of the US West Coast we hope so um maybe is zooming out of GitHub co-pilot to broader GitHub itself GitHub co-pilot itself is driving so much Innovation within GitHub but what are some of the other key initiatives that you're leading across GitHub overall as well yeah we already talked a little bit about you know autofix and security and I think security securing the software supply chain is is dear near to our hearts um there is there is no future of you know human progress and with software if you're not also able to secure the supply chain you know today you know there's this XKCD comic um of the internet's infrastructure and there's this one building block that says you know the guy in Nebraska maintaining this one Library alone right that's that actually is a is is as funny as that is that is a reflection of of the software world today so we a have to make it sustainable for those maintainers uh to to build that software and keep it joyful and then also we have to make sure that all these building blocks that are in our Stacks um that have become system critical and to be secure and so we're both investing a lot in platform security and application security and of course you know in our security products and I think that's going to be crucial uh combined with copile and AI to not only create all these work items but to also enable developers to to burn them down to fix all these issues yeah I want to zoom out from GitHub for a second and just talk about how you see the future of AI and coding overall like you mentioned there's been a billion casual developers around the world like what will it look like will will everybody be kind of coding applications for themselves to use um will there be some number of professional developers who are super developers like have do you imagine the the world looks uh when you so kind of democratized the the uh craft of coding MH I mean I think it's an incredibly creative world in a world where you're not um dependent on you know when you're a kid on your parents having technical knowledge or your school having a teacher that knows how to do these things so we're going to have much more access to those that are interested in learning about this um you know it's easy today you know to take a sheet of paper and and paint something uh in every restaurant at least you know in this country gives you crayons if you come with kids and and a coloring sheet um it's Lifesaver Lifesaver or you have your mobile phone and they use your mobile phone um uh or you know it's easy to learn and Easy in the sense of accessible to learn a music musical instrument and I think it's should be easy to learn coding and so first of all I think that should be something we we are excited about and not concerned that we are inflating the number of developers because just because kids learn it doesn't mean they want to become a developer I think there's still a world where uh people want to do something else in software but then if you think you know about many other professions physicists um for example they use a lot of software you know the the first image of a black hole was rendered with the help of open source project you know the mass helicopter uh ran on open source right and so yeah it's it's it's space uh and it's Space Engineering but they're they're using software and they're building software and so the profession itself is is everywhere every company is a software company banks are software companies you know energy providers are software companies farmers are software companies predicting you know what what seed to plant this year what the weather is going to be like you know what what's the soil quality from last year and things like that but of course you know um that the hobby scenario isn't is also important um like you know a tax season is over uh here in the United States but that doesn't mean that I couldn't think about next year to automate a lot of that if I only had an AI agent that does all that work for me you know and and and and downloads all the PDFs and extracts all the numbers and I don't think we're too far away from that um and you know I'm I'm you know now in software for for almost uh 30 years or almost 30 years and as as a professional software developer and you know I don't have a lot of time to code um you know I have a company inam I have podcasts to give and and things but the problem today is you you find an hour on a weekend and uh you have a project and the first 20 minutes you're spending with updating everything to whatever you missed and the burden is actually uh the the fun is gone to a certain degree um because the burden of of maintaining software is so high so having something available to you that gets you quickly into the Hobby and out of the hobby uh um or out of that task I think is incredibly empowering and and brings the brings the fun back and and you know that's where the Lego comparison is is so useful right because Lego is just incredibly accessible and even even if you like the best Lego is the one where you don't have instructions you just have a table full with Lego breaks um uh even in in random colors right and then have this this excitement of play and you know even you know professional workers at their offsites or workshops often have little gadgets or or bricks on the table so you have your F your fingers do things while you while you're thinking so I think that's where that Ro is leading us and we will have you know more more access to the technology we have more people um that can build software that but doesn't mean that they're taking jobs away from professional software developers on what time frame do you think we'll have uh coding agents that are as good as maybe the average professional software developer and and you know there's the legend of the 10x software engineer like at what point do you think AI will be as good as the 10x software engineer in capability you know the the the the trick in that question is the as good as and what does that mean so I think you know if is a model today able to write better code than the average developer if it is prompted in the right way or given the right context I'd say we are already there on average um because you know often you know the model just knows more about that whole space than I as a human do like and you see that with students if they have to implement uh you know like a conversion from binary to decimal or something like that and they might write 100 lines of code and then they go and ask Copilot how to do that and they get probably an open source library and one line of code and then you can kind of say well I I'm not allowed to use open source and then you was probably still get a better code than than they would and I think the true is the same is true for the professional software developer because look we're not perfect we are we are human and I think that's part of our nature that's part of creativity now that's the key thing though that the model is not creative and the model cannot today you know the model cannot make decisions for us um or if it does make decision it doesn't actually take all the constraints into account like if you think about software development other than writing code which is I think the fun part it often means you know I take a very complex problem and you break it decompose the problem into small building blocks you know and the the block size is increasing over time you know it used to be Auto completion used to be just the next word and then it was maybe you know full command and know it's multiple LS of code and maybe it's whole files in the future but along that decomposition process you still have to make a lot of technical decisions what database am I using and you know you probably know better how many database startups seoa has invested in and how many infrastructure startups and and how many you know serverless startups and and and and and and obviously there's all the incumbents in in all those spaces and so there's a thousand if not 10, decisions to be made and the engineer is is the systems thinker that that is making those decisions or the team the team of Engineers and companies I think you know the we have been building house houses you know as humans for thousands of years and if you have ever built a house it's still not a solve problem yeah but what at what point do you think that creativity and that systems thinking gets built into co-pilot or do you think it never does I mean you know it's bit of predicting the future I don't know if I would say never never say that's a dangerous thing on a podcast you know you invite me back in three years and like well Thomas you know last time we asked you about this and clearly happen since no I think you know we'll see where the research goes and where the technology goes in in kind of critical thinking and systems thinking and those kind of questions also learning you know learning uh you you mentioned you're two-year-old you're two-year-old and and my my kids they learn you know as they mimic uh the humans around them they are so good at learning language and especially in young age you know they can learn uh multiple languages mine you know speak English and German uh because we speak German at home and they don't have an ACC in English well they have an American accent but they don't have the German accent right and they don't have an accent in in in in in German either and I think you know this the shows like that the human brain is still so much more advanced than than uh the Transformer models and the diffusion models and and the other types of models that we have to image recognition and whatnot that we have today um and you know it remains to be seen if we can kind of like add that sentience piece to it but today I'm I'm not seeing it and I haven't seen any research that telling me that that's coming anytime soon that's a really clear delage Switching gears a little bit I we'd love to hear your thoughts on the overall ecosystem of startups right now ai Coen is the hottest category with a lot of ambitious Founders and there are so many different attempts that they're taking whether it's um folks who are trying to build a better model folks who are trying to build a better IDE folks who uh are trying to build kind of an all-in full stack engineer as an agent um and you know but the big elephant in the room is GitHub co-pilot and all the adjacent products that you have around it uh with co-pilot workspace with co-pilot Enterprise with autofix and owning vs code as well amongst many other things how do you think about what white space there is for existing or sorry for new Founders and um if you were a Founder yourself trying to build in the space what would you do I mean I I love developer tools so I I probably still do developer tools and I'm not sure I would worry too much about what's wi space which what taken by encampment because that can change quickly you know when GitHub started Source Forge was the big elephant in the room and sour has all the open source projects and then came get and G recepted that space and then the founders uh of GitHub took G and and build GitHub and all of a sudden you know that elephant in the room was no longer the elephant and and and I think you know it is often fun you know to compete in the same space and we love competition because it pushes us forward uh you know as much it's uh it's boring you know to to do a race or you know a game if you don't have an opponent and if you don't have you know other other teams in the league and who would watch uh you know the Super Bowl if there's only one winning team every year so I think a I don't think competition should hold you back as a Founder from from going into that space I think the software development space is is wide open and there's lots of problems to solve um there's lots of problems in different um you know Industries and categories to solve you haven't really solved modernization of source code you know cobal run on Main frames and you can use a co-pilot uh and we we are heavily you know looking into that and because it's such a paino for many Financial Services institutions um you know your credit cards your your bank account Wall Street all that run still cobal on on I haven't met a single uh bank that doesn't doesn't run some cobal on some Mainframe and um what would you do with co-pilot and cobal so well today you can ex explain that which is you know often helpful because the code was written 60 years ago and so the people that wrote that code are retired yeah yeah exactly would you rewrite all the code with then you can you can ask it to write unit tests because nobody wrote unit test in the 60s and70s either let alone that there were unit testing Frameworks uh uh for for those languages like keep in mind you know the' 60s that was before hard drives right like the you know before before we had you know personal computers it was a very different world back then and of course they those companies have done work to modernize to a certain degree uh but it's far behind Azure software development so you can support that transformation process today but there is we're not not at that point where you can just click a button and you have a transform the same is true you know for many more modern languages you know there's large PHP code bases there's lot lots of java out there and there's lots of optimization that we can take you know to just make our existing Stacks more efficient um where where agents can help um there's lots of you know things to improve um in every part of the software development life cycle in and in this ecosystem around us GitHub you know is I I like to think about GitHub as you know one planet in this universe of of software development tools and there's you know smaller planets around us and and you know equally or almost equal siiz planets uh in in our space and we we consider them partners and we we're happy that they're there yeah so interesting um agents are just pull on that thread a little bit it's also an area of excitement for us um and um I think you know you a lot of the benefit of owning so much of the data and everything you can do on the post trining side openi itself is also getting better and better with each new model class as are every other um uh model company out there would you ever want to uh kind of invest into building your own agents maybe from scratch by building your own gentic models or just to partner with some of the others out there uh me as GitHub CEO is that you're asking yes or me as agent investor you as a g CEO um yeah I think you know we are we are probably doing both um we're we're going to we're always you know in a way we always have done both things as GitHub we have invested into our own things the thing that we consider as you know core uh part of our platform and of our offering you know uh part of our Primitives um and we have Partners partnered with companies um you know um uh just earlier this week we announced the partnership with jroo which you know covers spary uh artifacts um scanning of containers and those kind of things and they obviously very naturally in our space you know we have the source code and what do you do with source code well you either compile it into binaries or you combine it with binaries before you deploy it into the cloud and so there's a natural value chain there where you know we have you know uh things where we invested ourselves things like releases that you see on GitHub and packages we we own npm uh uh you know the largest package package lry in the world for the JavaScript ecosystem and nouet um through our partners at Microsoft and the largest. net reg in the world but that doesn't mean that we cannot also partner with the jog to ultimately enable that that you know secure software supply chain uh that I that I mentioned earlier and that I think is crucial and that figma you know um vers cell like there so many other companies in in our space that will will play you know part in the life cycle and I don't see a world where somebody covers all of that and can convince every developer that using all these tools is better than picking you know what what analysts might call Best of breed and like the tool that I consider the best best is all subjective anyway right like it's all the does it meet the expectations of the developer in in in the in their environment yeah makes sense that's great advice for startup Founders and I think really encouraging uh what about for incumbents I think you you are such a Beacon of Hope for incumbent companies I think Saia said in the last earnings call that GitHub is now growing 40% year-over year thanks to the co-pilot acceler a and so like and to your point earlier like you feel like you're a young startup again like walking into the Sequoia offices like what advice do you have for there's so many incumbants that are trying to reinvent themselves around AI what what advice would you give those folks sa actually said 45% so we're really excited about that all up on our on our Revenue Rec 45% you know I got an email um from somebody last week who was like looking at GitHub and losing the belief that large companies cannot move as fast as as startup can I think you know the key ingredient on this is uh radical F focus and and basically focusing on a few small things just because you know you have a th000 Engineers or or 50,000 Engineers doesn't doesn't mean you can do it all that's just a false uh assumption and I think it's in a way misleading as you manage larger teams you're losing kind of like the the unit of of of of side team size that shows you how much you can actually get done and how much uh friction you have in the system if all these teams work on different things I think Focus you know lots of NOS uh on on all the ideas that people have around you and customers uh that's I think the the key piece to to move really fast and and then obviously taking in some strategic bets and strategy means you're thinking about it as how do I differentiate from others you know what makes me specific makes me special in this market what lets me uh uh charge the prices I want to charge and not fight you know a race to the bottom and I think that's there kind of like thinking about okay you know in software we like to think about well if I add just three more features then uh I'm I'm going to win the space Until you realize well everybody else can also add those three features right there there's almost nothing in in GitHub itself as as a platform that you couldn't rebuild with significant investment and time but I think it's really hard to mimic our culture it's really hard to mimic you know our experience our Obsession about about developers and and ultimately our our focus and and the way we're approaching these things I guess very tactically do you recommend Staffing a tiger team to get to the product Market fit on AI do you recommend like you know pulling half your engineers off whatever they're working on and like hey you guys are the AI team now we list we got to go bigger go home like tactically how do you recommend compies well as an incumbent you always have the challenge that you have to sustain whatever the business is you're in and so you can't just pull everybody into a new topic uh unless you're willing uh you know to uh disappoint all the existing customers you know no no uh Enterprise business uh has not made promises to Enterprise customers of what's coming next on their road map you know lots of conferences including our own is is you know stuff they're shipping right now and stuff we're announcing for the next six months and if you're not delivering on this announcement your your customer base is not going to be happy and so you can't really you know make that drastic move uh right away and so we love this idea of a tiger team or an incubation team we call it get up next and we when we set that team up and we actually thought they're going to work on projects that are like 5 years out or you know in that Horizon 3 as space and then it turned out well it was more like six months ahead of the curve the future comes really fast on us and and so as that future then comes fast you really also need to move fast and handing things from the incubation team over over to your Mainline engineering team and then it's it's all about okay so we have funding AI because we're seeing the traction there and that means we're leaving some of our previous bets um in you know keep the lights on mode KY low uh um or or you know saying goodbye goodbye to these to these ideas and and shut them down and I think that's the hard part uh of the the Strategic pivot and that's easier you know for a startup at least it often looks easier from the outside when a startup pivots right like there's lots of startups on the on the wall downstairs that have gone through that as well um um but of obviously internally is also very hard emotionally you're tied to ideas you you know you remember the the work shop uh under seoa trees uh where you had those ideas and then six months later you're realizing we never got to product Market fit and it's the time is now to move to something else so that that is the same for large companies as for small companies except that small companies uh have a have more of a forcing function to give up and and move move to something else The Innovation and success that you've created around GitHub NEX is amazing um how does a konal an idea start within GitHub NEX how do you then resource and invest into it and then how do you at what point and what's the process in which you decide to whether or not something should be continued to invest in or or shut down entirely so the I think the start is always the the you know the employee um the hubba that has an idea and and we have lots of ideas in the company U we do you know hack weeks hackaton Um passion projects um whatever you want to call it you know 20% time uh and then in the next team specifically you know there's lots of demo uh um meetings um demos and not memos you know it's much more useful just show a working prototype it's so easy these days you know to just use um some design system uh react components and and or or figma and Stitch something together even easier with co-art and then you know it's strategic decisions that uh the the leaders of of these teams um all the way up to me need to make uh you know very many ways very similar to any other creative industry you know like uh at Disney or pix they have to decide which movies to produce and and which ones are probably not going to to gain traction and part of that is customer research um talking with developers and that then you know keeps going as we go you know through the initial idea and the first prototype and then going into a technical preview and then the preview is all about you know that flywheel um that feedback loop um with the with the people using it and and quickly you see you know are they trying it out and then they are churning or are they keeping uh uh keeping the energy high and then keeping the ideas flowing it's great you know uh in any in any project if people just sending you more ideas and and more feedback and I think that's then the decision uh whether we keeping that project and making it a main project or whether we are deciding okay the experi rate is over and we learned a lot and we moving to the next one and of course there's commercial aspects as well um maybe uh outside of the world of code generation and in anything in your current purview what else are you excited about in the world of AI in the span of one five or 10 years or outside of developers you cheated a little bit on me um I mean I think in in the context of one year um within the space of developers and outside of the space of developers I'm excited about agents and we're still very early in this journey I think we have high expectations of what the agents could be maybe too high expectations to some degree and so things will probably take a little bit longer um but I think in the next year or so we're going to see more of these help us uh within the chat interface and outside of the chat interface that will will solve tasks for us you know like uh I look forward you know to the travel agent that often gets demoed uh uh by big companies uh to actually materialize and it can just go into my chat interface and say you know I want to have uh Beach vacation uh over spring break and then figure out you know when spring break actually is because that's all on the internet and you know who I am what my name is and and what my family's names are on their birth dates and their passport numbers and so I don't have to enter that into a cumbersome interface anymore and show me the price points and then probably we're going to the same hotel as every year anyway and so I think that those tribal agents um or and those you know kind of agents are going to happen in the next year plus I think five years is my natural language vision and you know unlocking the world's knowledge um including software developers um to everybody in any any language um any human language and um maybe that's even happening sooner um 10 years is hard 10 years is so far away but I think you know that's the AI of things you know the material like the mechanical word of AI you know we so many things that we do in life you know are are you need to grab something or you to push something and you know go go and check into any hotel there isn't any AI involved once you get your room key um and maybe the elevator you know has some kind of AI because you push a button what floor you want to go to in to to take elevator SE uh and so you no longer push the button within the elevator that's an optimization problem in itself but I think there's so many physical things in life you know and we are still not at really self-driving cars um we have a dishwasher but I still have to put the dishes into the dishwasher and out of the dishwasher uh it's better with nine and 12 year olds than with 2y olds and um there's there's so many other things in life where I think you know the whether it's a robot or some other form of of physical AI uh is going to um to take over are some of the things that um we consider as chores we're really excited to hear that um and I think we have a very similar view of what will happen one five and 10 years from now uh who do you admire most in the world of AI I think I admire those that you know um are building new stuff with AI and and software those you know that have a dream uh of what what they can what they could build um I think those that you know obsess about a problem and not about the technology you know we're talking a lot about AI but at the end of the day it's what's problem I solving uh for the world and you know there's lots of Biotech companies that use AI to try to cure di diabetes or or cancer and I'm sure there's companies trying to sort of climate change uh with technology and I think those Builders you know the founders those that have um you know Big Ideas and that can change the world you know those are the ones I admire the most and often when I meet with them in in their offices you know or on on calls I I'm like this is this is so cool like and and obviously as GitHub we enabling a small small part of that and so we we feel really proud of being part of that journey and and we're really you know excited about you know um um building more for them um I think you're enabling a huge wave of new AI companies especially those that are born open source it's amazing to see um what advice do you have for the founders listening in the audience who are building in AI today I mean already Sonia ask I think focus is everything like it's so easy to get lost in you know the all the ideas that you can put on a whiteboard and that's the danger of a whiteboard that it has covers um you know so much space where you can put ideas but Focus um ultimately is everything you know Finding very quick Market validation um often you know on the developer space that means going and product that growth um Enterprise growth can come later but like from my experience like there's nothing that you know a few excited developers spreading the word about your product um even though you know the big uh Revenue number then later comes from from Big Enterprises buying you but the reverse is often much harder of going Enterprise first and uh you might find excited people there as well um but the feedback loop is just so different so it's Focus it's you know um uh uh trying to find that flywheel the product Market fit you know and then think think the other one is to think big I think it's like like it's easy to find small ideas on top of you know model today that get you know commoditized tomorrow and so you have to think forward and that you know 10 years is is a long period of time but that's you know a good period of time for a startup to build something that is actually meaningful in this world and so you have to think uh big you have to you know create a vision that may be much larger than the MVP you know the first thing the Prototype that you're building right now and I think that's that's really hard a Founder to draft out that vision and then and then decompose right then go back to that small problem that you can solve right now so we'll close with some rapid fire questions one word answers uh one word answer one word answer you changed the I say one sentence if you need it no I say I'm going end this yes okay let's go okay uh will anyone meaningfully disrupt Nvidia and AI chips in the next call it 5 to 10 years yes in what year will we pass the 50% threshold for swe agent 2025 and what about 90% 2028 we'll GitHub primarily that was too pessimistic that last one we can we'll hold you to it um will GitHub primarily be using open or closed Source models in the next 5 years both where does the majority of value acrew in AI models compute infra applications a cost the whole stack that hyphen in it so it was one word is systems thinking and creativity going to get baked into the models in the next 5 years maybe and will there be a new consensus architecture beyond the transformer in five years of course yes wow why would you think the other way around like like that's said that it's a much easier bet to think that there will be a new architecture because there was other architectures before Transformers and like that doesn't mean it replaces Transformers you know your cell phone still has a CPU even though gpus are the H commodity right now um so I think yeah there will be there will be new architectures and they might be bigger than Transformers today so interesting and that bring a lot of new oxygen for the builders in the ecosystem and a lot of things that would have to get reworked rebuilt re architected yeah amazing Thomas thank you so much for joining us today it's been wonderful digging into the history of GitHub the birth of GitHub cop and um the ambition that you have going forward as well thank you yeah thank you so much for having me it was so fun to talk to you both likewise thank you [Music] [Music] [Music]

========================================

--- Video 64 ---
Video ID: XPePYzbRILg
URL: https://www.youtube.com/watch?v=XPePYzbRILg
Title: Meta’s Joe Spisak on Llama 3.1 405B and the Democratization of Frontier Models | Training Data
Published: 2024-07-30 11:00:36 UTC
Description:
As head of Product Management for Generative AI at Meta, Joe Spisak leads the team behind Llama, which just released the new 3.1 405B model. We spoke with Joe just two days after the model’s release to ask what’s new, what it enables, and how Meta sees the role of open source in the AI ecosystem.

Joe shares that where Llama 3.1 405B really focused is on pushing scale (it was trained on 15 trillion tokens using 16,000 GPUs) and he’s excited about the zero-shot tool use it will enable, as well as its role in distillation and generating synthetic data to teach smaller models. He tells us why he thinks even frontier models will ultimately commoditize—and why that’s a good thing for the startup ecosystem.

Hosted by: Stephanie Zhan and Sonya Huang, Sequoia Capital

00:00 Introduction 
01:28 The Llama 3.1 405B launch
05:02 The open source license
07:01 What's in it for Meta?
10:19 Why not open source?
11:16 Will frontier models commoditize?
12:41 What about startups?
16:29 The Mistral team
19:36 Are all frontier strategies comparable?
22:38 Is model development becoming more like software development?
26:34 Agentic reasoning
29:09 What future levers will unlock reasoning?
31:20 Will coding and math lead to unlocks?
33:09 Small models
34:08 7X more data
37:36 Are we going to hit a wall?
39:49 Lightning round

Transcript Language: English (auto-generated)
you know if I was a Founder right now I would absolutely adopt open source um it forces me though to look at the engineering complexion of my or right and think like I I'm going to need people doing L Ops and and uh and you know things like you know F data fine tuning and and how to build Rag and things and apis there's plenty of apis that allow you to do this but like ultimately you want control like your Moote is your data your mode is your interaction with users [Music] hi everyone welcome to training data today we're excited to welcome Joe speac director of PM for generative AI at meta where he leads llama and thirdparty ecosystem efforts Joe spent the last decade in AI leading product at piie torch and working on initiatives that span protein folding and AI math many of which have spun out for meta into their own startups we're speaking to Joe just 2 days after the Llama 3.1 405b launch and we're excited to get his view on questions like where is the open source ecosystem headed will models commoditize even at the frontier is model development becoming more like software development and what's next in agents and reasoning small models data and more Joe thank you so much for being here today we're so excited to have you just two days after the Llama 3.1 405b launch um it's an Incredible Gift to the ecosystem we'd love to learn a little bit more about how you what specific capabilities you think the 405b is particularly unique at especially in comparison to the other state-of-the-art models oh thanks so much for having me this is so much fun um I haven't done a podcast like this since I pre-co so it's like fun to be in the same room and just like uh you know chatting about this cool stuff um yeah I mean we're we're like Beyond excited and meta uh this was something that I think a lot of us have been working on for such a long time months and months and months and you know we we kind of put out that nice little like appetizer I'll call it in April like llama 3 and and like I was actually like are people really GNA like be that excited about these models and and like their response was like through the roof like oh my God like everyone's excited but they really don't know what's really coming and so like yeah kind of hold that kind of had to hold that back for for a while and kind of keep it to ourselves and like then kind of build up for this this launch um and the 405b is a monster it's a great model um and I think the biggest thing we've learned about the for of IB is it's just a great it's like a massive teacher for other models and we um we kind of had that plan all along because when you have a big model um you can use it for like improving small models or just like you know distillation and mean that's how the eight and70s became um the great models that they are I mean in terms of like capabilities like you know we we listen to the community we listen obviously to our own product teams right because we had to build build products for for meta and I mean long context was like one of the biggest things people wanted and we have you know much longer context internally even then we released and but it we saw like just the use case use cases like start to build up uh multilingual I mean we're a global company um so we released more languages um many many more to come because obviously like meta has billions of people on their platform and hundreds of countries and so um I think that was like to me those are like table Stakes things but they're like done well um on the models like I think like we spent um a lot of time in post trainining um on on our different languages and improving them and safety just they're really really high quality um so we don't just like pre-train on like a ton of data and say look at us we're multilingual um you know we actually did a lot of work in in in our sft phase and super FIS fine fine tuning and um a lot of safety work um I think one of the coolest things that I'm excited about um well there a couple things I'm excited about but one is tool use like I think model oh my God zero shot tool use uh this is going to be crazy for the community um I'm going we show a few examples like we can show like calling wolf from or we can know Brave search or Google search um and it works really great but Zer not tool use is going to be a game changer um the ability to kind of call a code code interpreter and actually like run code um or um you know kind of build your own kind of you know plugin um for things like Rag and other things like can have that really be state-of-the-art uh I think it's going to be a really game big Game Changer and I think just the fact that we released um the 405 itself and we changed our license so you can actually use our data like that was a big deal like that was a big discussion we we had many meetings with Mark on that and we uh and ultimately like landing on a place where um you know this was like this pain point for the community for so long they're like these closed models like I can't use the outputs or maybe I I can use them but maybe I'm using them slightly unscrupulously or whatever um like we actually are encouraging people to do it I'm sure that was a tough decision to make it walk us through the things that you had to consider and actually making that leap to to open up license Licensing in that way yeah licing so permissible oh licensing is like a huge Topic in itself obviously you probably spend the whole podcast talking about it I don't want to uh but we could um I think we wanted number one just to unlock new things like I think we wanted to to have the 405 and our llama 3.1 models differentiate give people new capability like just like we just looked at like what people were really like excited about in the community not only in like in kind of Enterprise and and and products but also in the research Community because we obviously have a um a research team and you know we work with Academia and we we we talk to folks I mean you know Percy L Stanford texts me all the time saying you know when are you GNA release it when are you to release it can I use it can I use it and Percy like you know stay stay stay patient um but I think we um we we heard them and we we knew kind of what they wanted and and I think ultimately we wanted um llama everywhere we wanted just adoption you know maximal adoption really the world using it and building on it and I think Mark even used in his um his letter he put out like um you know the new standard or or standardized so I think like to do that you kind of have to enable stuff like that right you kind of have to unblock all these different use cases and really look at what the community wants to do and make sure that that you don't have these kind of artificial barriers and that's what the discussion really was and um and so actually even beyond that we started working with you know Partners like Nvidia and AWS and they started building distillation recipes and even synthetic gen data generation Services which is pretty cool I mean you can start to use those and actually create specialized models from it um and the the data that you I mean we know how good the data is because we use it in our smaller models it's really good and it improves our models significantly so I want to pull on the open source thre a little bit more sure and I've read Z's Manifesto it was great uh but I'm I'm still I'm trying to wrap my head around like what what's in it for meta this is a massive investment to open source in some ways you're laying a lot of money on the table because you now have a state-of-the-art model that you offering to everybody for free and so I guess is this an offensive move is this a defensive move like what's in it for meta I mean we've so well first of all our business model doesn't depend on this model to make us money directly so we're uh we're not uh you know selling a cloud service we've never been a cloud company um we we've always uh worked I would say with a partner ecosystem all the way back to like the five years I was like helping to to lead pytorch and you know the ecosystem in the community built around that like we never um built a service we probably could have in some way but it would have been weird um we saw basically going back to py we kind of saw it as this kind of lingua franka kind of bridge you know to this like area of high entropy all this kind of weird way to say it but like there's all this Innovation happening how do we kind of build a bridge to it and actually be able to harness all that Innovation and the way to do that is to be open is to kind of get the World building on your stuff and I think that's that ethos is kind of carried over into Lama and um you know if you look at P George like that was a huge way for us to kind of pull in you know at the time when we really start working on pitori in Earnest um computer vision and like CNN and all that you remember that old old times now um and uh but we actually would see these architectures come like constantly the people would and they write code and they publish it in P torch and we take it internally we evaluated people would open source models and put them out on you know model zoos and we'd evaluate them and we'd see just how quickly the community was was improving things and we actually leverage that especially for like Integrity applications where we release like hateful memes and some of these other uh data sets we just saw the improvements like week over week month over month and it was built on something that it's like we were using internally so it's very easy for us to just take it in inside so I think like llama is is is definitely similar in that regard where you know when when we when Academia and when companies start to Red Team these models or you know try and jailbreak them we want people to do that to our models and so we can improve and I think that's a big reason um and it's it's like be careful what you wish for right of course but like it's it's the same with Linux right the Linux is open source and the konel is open source and people will you know it's much more secure when when things are transparent and and bugs can be pushed faster and and so that helps us a lot um I I think it's you know there's also the angle of you know we um you know we don't want this to to turn into kind of a um a completely closed environment like I I think just like today if you deal if you look at like Linux and windows and like in my opinion um there's you know there's there's room for both right there's room for clothes room for open and people use depending on what they need and and the applications um I think that there's going to be a a world of you know of open models and I think there's going to be world of closed models and I think that's totally fine what what was the primary argument against open sourcing was there one um I mean there was definitely like competitive concerns we talked through you know do you want to give your technology you know put it out there and and that and I think uh we're like less concerned about that because we're moving we're moving really fast yeah like if you look back I mean I've been you know back I've been in meta like close to what six or seven years now and like in the last um you know year year or so we've done you we had a connect launch um we released purple llama last December we released llama 3 um 3.1 before that we released llama 2 in July llama one was like in February so like just if you think about the like the pace incredible the pace of innovation that's like coming out of our team and our our our company is like just at a crazy Pace right now so I I'm not too worried about it I don't think we're that worried about it um so I'd love to um kind of move into your personal views on the broader ecosystem I think a lot of the questions that people have center around what happens to the value of all these models um especially as meta open sources more of them at the state-of-the-art level um with llama 3.1 with open AI launching chbd 40 mini what is your view on do models commoditize even at the seat of-the-art Frontier well this is a great question I mean I I think if you look at just in the last two weeks I mean 40 mini is a really really good model um you know input I think input per million tokens is something like 15 cents 60 cents out uh so it's incredibly like cheap to run um but but it's also an excellent model like it's just like they they've done an incredible job in dist distilling and getting to something that's like really really performant yet really really cheap um so I think like uh you know Sam is is definitely pushing on that um and then if you look at what we've done in like last week and pushing out I would say like pretty pretty compelling stud their models across the Spectrum um I I do think like it's rapidly getting to a place where you know the model is is going to be kind of a commodity I mean I think there's this Frontier of like data um where you know I mean we can certainly gather data from the internet we can license data but at some point there is kind of like some Frontier of limitations I think that we're all going to have and um this goes back to our conversation know this week on you know kind of a bitter lesson of of data and scale and you know compute is that enough it's probably not quite enough but it's like compute and data becomes kind of um if you have enough of both um you know you can kind of get like a first order approximation of the state-of-the-art with without any anything else is kind of what we've seen so I do think models are commoditizing I think the value is elsewhere and I look at meta and I look at our products I look like what we're building like that's honestly where the value is for us it's meta AI it's our agent um you know it's all the the technology that we're going to put into Instagram and WhatsApp and and all of our our and products where we actually are going to monetize where we're actually going to you know to add real value the model itself I think definitely will keep will keep innovating new modalities new languag um new capabilities that's what you know that's what research is right it's pushing the frontier the emerging capabilities and then we can leverage those in products um but the models are definitely like pushing in that direction yeah if that's the case and all these existing companies that have massive distribution and wonderful applications that they're already that are already out in the wild can just adopt these state-of-the-art models what advice would you give to the whole wave of new startups that are trying to make it out there either building their own models using other of the art models and then trying to build applications on top yeah I mean there's definitely like some model companies or companies that are building you know they're training pre-training Foundation models and it's expensive it's like I think we're you know I can't say how much llama three cost but it was very expensive and you know llama 4 is going to be even more expensive um and so I to me given kind of the state of play and things it to me it doesn't make that much sense if I was a startup to try and go and and do a pre-training like I think like llama models are absolutely incredible as foundations um to build on and so um I do think like there is um you know if I was a Founder right now I would absolutely adopt open source um it forces me though to look at the engineering complexion of my or right and think like I I'm going to need people doing L Ops and and uh and you know things like you know F data fine tuning and and how to build Rag and things and apis there's plenty of apis that allow you to do this but like ultimately you want control like your Moe is your data your Moe is your interaction with users and you're also you may want to deploy these things onto a device at some point and uh and have kind of a mixed interaction or something uh you might want to have like small like simp simpler queries like running on your device and have like you know very low latency interactions with your your users you might want to split you know and have like a more cloud-based approach for more complex queries uh more complex interactions um and I think like the open source approach gives you that flexibility it gives you the ability to modify the models directly you own the weights um you can run the weights you can distill them yourself um there's going to be distillation services that allow you to take your weights to still them down to something smaller like that's pretty awesome we're like just now seeing the beginnings of that uh so I think like in my mind like control matters a lot and uh and ownership of the weights there are a lot of like API Services where you'll do fine tuning your model so you're bringing your own data you're fine tuning um and they use something called low rank adaptation or Laura and unfortunately you don't actually have access to those lower weights at the end of it you're kind of like forced to use their inference so you're like um let's see I'm kind of like hell hostage here like I've given my data I don't have access to like the actual IP that was generated from that data and now I have to forced to use their inference service like that's not a good deal so I think the you know open source kind of like brings inherent Freedom yes um that I think that approach doesn't so what do you think of Mr Large was announced I think maybe a day after llama 3 3.1 um what do you think of them and I guess more broadly for everybody at the frontier is everyone kind of pursuing the same recipes the same techniques the same uh kind of compute scale data Etc and and so like you know everyone's kind of going to be roughly similar at the frontier or do you think you guys are doing something very different so um first of all I'm I'm NR I me an amazing team um it was one of my old teams in Fair yeah um they were working on through improving and Ai and Mathematics so Gom and Tim and the team are Maran they're they're incredible Joe's just talking about fun banter last night there so I mean this this was like one of the scrappiest teams um that I've ever worked with I mean the the team uh I don't think ever slept um so it was like basically by day they're doing like prob even less now probably even less now I mean they would push the state of Y in like Ai and th improving and you know um during the day and we published some work on that um you know I think uh what a couple years ago now geeez um and uh but by night you know they were basically like scrappily like grabbing compute to train llama one and so they were uh you know we were building large language models um several years ago in in Fair and you know that team basically just um like they were just really ambitious and they they were kind of working by night and that's really where llama 1 came from um so the team was great I mean I think they they're doing uh really good work I think they're definitely challenged in that they're trying to also like open source models but also make money and you know like models like 40 mini are not helping them um because like and this is I think why they change their license for example to like you know to have research only license which kind of makes sense um because they were you know open sourcing models and they immediately like their own ecosystem is like competing with them in a lot of ways because they'll release a model they'll host it like use this model but then they have you know together and fireworks and lepton and all these companies that you know provide like a sometimes a lower cost um you know per million token like like offering so it's really tough business right now um in terms of like large 2 I think it's a really good model I mean I we just on paper I haven't evaluated it we haven't looked at it internally yet um uh I think uh if you look at like artificial analysis I think they added up in kind of like the it was a little was under I think like the 70b model uh in terms of like quality but you know that's like a blended you know they blend a bunch of benchmarks to to make that uh distinction um but on paper it looks really good we're going to evaluate it um I think you know to for me anyway like the more the merrier like the more models are out there the more companies are doing this the better it's not like we're not going to be the only one and I think that's good that we're not the only one um so and I think like more generally like the Gen space like you wake up every single day and you kind of expect something like this right you expect them all to be released or something groundbreaking to happen and that's kind of like the fun of being in it so totally totally do you think everyone at the frontier is comparable though like are you all pursuing comparable strategies yeah this this is actually a good question because you know if you read the Llama 3 paper um which was I think 96 pages we ended up at right lots of citations obviously lot of sharing lots of sharing um lots of like uh you know contributors and and core contributors and that so like it was it was a it was a detailed paper and uh Lawrence and Angela on the team spearheaded writing that and I think that was like one of the hardest things like developing the model was like relatively easy compared to writing the paper it was it was a lot of work um pulling that paper together I think uh if you look at llama 3 um you know there was a lot of I would say Innovation that happened but also we didn't um like we also didn't uh I would say take on like a lot of research risk either yeah so I would say like the the primary things we really did with llama with the 45b especially was was really pushing scale I mean it was still you know we grp CER attention for example so you know gqa and that improves inference time and you know kind of helps solve the kind of quadratic attention uh computational challenge uh we trained on you know over 15 trillion tokens we in post trainining we use synthetic data which improved the models on the smaller models quite a bit uh we trained on over 16,000 gpus um on our training runs which is something we hadn't done before um it's really really hard to do that because gpus fail and you know off the table yeah I mean it's everyone's like oh I'm just going to train in 100,000 gpus like good luck right you better have a really really great infro team a really great um mlc Team you better be like ready to innovate um at that level because it's just non-trivial um everyone says it's easy or says you can do it it's down trivial so I think like I almost look at llama 3 is very similar to like the gbd three paper so if you if you ever talked to like Tom he was a lead author Tom Brown now at anthropic and there's a reason why Tom was the first author on that paper is because like a lot of the Innovation was really scale it was really like how do I take something that's you know like an architecture and like push it as as hard as we can push it and that involves like a lot at like the mlis kind of layer and infr layer and like how do I scale the algorithm um and uh and so I think that was really like the mentality we had with like llama 3 and L 3.1 and um I mean internally obviously we have great research team we have Fair we have research in our org and we're looking at lots of different architectures ande and and other things um and so uh you know so I think we you know who knows what L will be we got a lot of candidate architectures and we're we're looking at it but um but it's kind of a trade-off it's a trade-off between how much risk you take on as like for research and potentially how much reward or you know the ceiling of the potential improvements versus just taking something that's relatively known and like pushing scale and getting that um to to improve even more so ultimately this becomes a trade off I think this is such an interesting point I actually also think it makes llama and and meta quite unique in the strategy it's taking the words that I like getting used yesterday were is model development becoming more like software development um I'm curious to hear if you think I think um uh unlike what many of the other labs have been doing on pushing more of the research uh um you guys have been focused on just executing on strategies that you know work do you see that representative of the continuous strategy you think as you extend llama out 4 five 678 and then also how do you think the other research lobs and maybe some of the other startups in the ecosystem will react will they kind of switch and ve a little bit more to the strategy that you've been taking I mean it's a really great question I don't we don't have all the answers for sure I think um but there's definitely like some somewhere in the middle right now is kind of where I see things Landing where you know we will we'll continue to push you know and on execution we'll continue to push models out we'll continue because we want our products to to itly improve um as well so we want met AI um you know improving constantly and so we're so there's definitely like a software engineering you know um analog here that that that's happening where you know you can imagine something like a llama train um and you know new features new kidabilities get on that train and and we have a model release um you know it's it's it's actually it's much easier when you start to componentize the capabilities too like we're doing that with safety right now and you saw in the in the release we released um uh promp guard and and new Lum guard and and you can iterate on those components externally and it's great um obviously the core model is much more difficult I do think um you know we'll we'll start to include or start to to kind of push on the research side because um as well because I the architecture like is going to evolved I mean you've seen like um you know what ai2 for example has done with their Jambo and their you know Ma and everyone kind of thinks Mamba is like a new architecture that's that could have promise um I think what's interesting though is like to truly understand like the capabilities of the architecture you kind of have to push the scale and I think that's what's missing right now in the ecosystem is you know if you look at Academia and Academia is like a lot of Absolut brilliant people there but they don't have a lot of access to compute and that's a problem because they have these great ideas but they have no way to truly execute them at the level that's needed to really understand who will this actually scale because like the the Java paper and and and model was really interesting and the benchmarks are great but they didn't scale it Beyond I think like under 10 billion parameters so you're like okay what happens when you know we we train this in 100s like does it actually do you still see those improvements or not and no one really at least outside of these Labs knows the answer yet um so I think that's like one challenge um so I think like to me we're going to get into this hybrid space of you know we are going to push definitely on architecture we have a very very smart um and well accomplished research team um but we also are going to be like you know we are going to be executing and I think that's when we we start to get like a recipe um you know we're going to push it to the limits and you know we are going to start you know release we're going to continue to release more models on it but in parallel to that I we have to push on architecture yeah um and I think it just makes sense because the next breakthrough you know at some point you're going to reach a like a like a kind of a theoretical limit and you need to evolve the architecture all right so I see kind of a little bit of an in between um and obviously that we're really good at execution I think we're pretty good at execution um but we're also good in research and we just need to marry those two so it makes sense because like research and products are very different right like one is should be pretty deterministic the product side and one is inherently non-deterministic right it's like is this going to work I don't know it's a really big bet um if it fails it's research like um it it should have like a non-zero chance of completely blowing up in our face we just go in another another Direction but that's that's what research is so I'm curious about one branch of where a lot of I think model research is happening right now agentic reasoning and you I think you all have announced uh really great results in reasoning I'm curious maybe at a very basic level how do you define reasoning and then are you all seeing reasoning fall out of kind of scale during pre-training are you is it post trainining and is there a lot of work left to do on the reasoning side yeah reasoning is a bit of a loaded area um I mean you could argue it's it's you know things like um you know multi-step like you know and I think the the best unfortunately the best examples we have are like the you know kind of like the sort of semi- gimmicky you know um you know Bob is driving the bus and like he pick you know like those kind of like things right you if if you troll local llama you'll see a billion of those right um so uh but but those actually Force the model to take multiple steps to respond to you and think through and and logically kind of respond um I think coding is actually really like you know when you look at like pre-training um and uh so like to answer your question directly like reasoning improvements come in both posttraining and pre-training um so what we've learned um which is now like everyone's like oh of course this is the case but definitely like the last year or so everyone's kind of learned that you know code um having a lot of code in your kind of pre-training Corpus really improves reasoning but that's we you to think about it like of course duh um it's step by step it's very logical it's you know code is very is just logical by nature and kind of step by step and like if you incorporate a lot of that in your pre-training your model will like reason better um and then we we of course like look at um examples in in in post training and like super you know sft um to improve as well so you know we look at the pre model we um and it kind of depends on how you balance things as well like you can because you can balance like how well your model reasons with how well it you know um you know responds in different languages like ultimately in Post Trading like everything is a little bit of a trade-off like you can super optimize things for coding if you want to and we did that with code llama um it was really great but of course the model will suffer like in other areas and so ultimately it becomes what like kind of like Paro Frontier of like capabilities we want to like bring out if it's a general model and I think like um yeah I mean ultimately it's a trade-off um so anyone can kind of pick a benchmark or some some capability and say I'm going to Super optimize for it and say by the way I'm better than GPD 4 well great um anyone can do that like but is your model as generally capable as gbd4 or llama 3.1 or whatever like that I think is a different stor so what do you think are the future levers to unlock reasoning um uh for for anyone going forward I mean the obvious answer is data um I mean the more data that the more um you know code and and supervised data that you can get I think is like is a natural um answer um I mean I think we need to find applications as well for how we like Define it and that would help us like once you've kind of start finding like those kind of killer applications then you can like then you kind of know where to kind of focus in terms of your your G exactly what you're sing for like and this goes back to like evals and like what is what is your eval because we're starting to saturate evals and so uh we we tend to as a community like we Define a a benchmark or a metric and we just just like optimize the living hell out of it and it's great but then you actually uh look at the model in an actual environment and you're like oh well that model has a better mlu score great but like how does it actually respond well it doesn't respond as well but but it has a better Mare and so I think we need better evals and better benchmarks that allow us to you know I would say like find clear line of sight to actual interactions and and I think like you know the the live what is it called the Abacus um Benchmark the live uh live bench I think it's called I can't remember the name it um is pretty is pretty good I was looking at that and of course like Alm CIS and CHF out Arena like these are more natural even though you know still not perfect but it's like moving in the right direction of things are like more human like interactions versus like a static data set um they static prompt set that is uh is not that helpful so I think like once we start to find these other like what us use cases make sense we're going to start to generate more data and you're going to start to improve the model there um and you're going to and hopefully that kind of has again line of sight to a benchmark or eal that um actually feels like it improves an end product and a lot of this actually depends on the end product of course like what is my application yeah so yeah out of curiosity I within large research Labs coding and math have always been uh two primary categories I'm trying to unlock reasoning in the startup V go ISM now we're seeing more folks who really want to go from the math angle do you have a perspective on whether or not that has led to interesting unlocks I mean I mean the answer is yeah I mean I think we if you look at our data or at least our models we've like Cody and math have been I would say the primary lovers um so uh I mean it's it's yeah I mean I think that's like having more obviously is better because obviously math is also very logical and very like stepwise um so I obviously you can see the pattern here um the more data you have like that that kind of follows that sort of pattern um the more your model is going to be able to reason and you can see that in how how actually models respond like if they start and you ask them to like respond and like step me through your thinking process right and it'll actually do that and some models do better than others um so anything like anything like that I think scientific papers like also there's like um you know we we had a uh um we had some like uh uh projects in out of fair that like trained on you know like archived papers and you can see like not only is like code and math like pure mathematics but also like scientific paper which just like science scientists are very logical and how they write things and how they stepwise and and how they like create images of their like charts and stuff and like that also I think we've seen like just general scientific information like helps as well so Gala sorry galactico was our project yeah so so Robin Ross from the papers of the code team that still in my opinion like one of the coolest projects ever um it got a lot of like bad press but wow what like they were ahead of their time in my opinion so I'd love to talk a little bit about small models um given the scale of capital and the compute that many startups have the 8B and 70b models are an incredible gift to the ecosystem um and it's funny that you called them appetizers at the start because I think they're super powerful for that for that set but they're also really powerful for a number of different applications um where you want smaller models um and so I'm curious to hear what do you hope to see developers use the 8B and 70b models uh for given that they are best in glass um for their um size of model so it's interesting though when we released uh yeah we released what April llama 3 we released an eight8 and a 70 the appetizers as we call them um you know the 8B was actually better than the lus 70b by leaps so we were um you know I had to look at the chart um and and I was like is this right yeah like is that really the case and we're like yeah like it really is I it was that much better what's the intuition for how that happens I mean it was more data we we had what 7x more data um obviously like we we put a bunch more compute at it as well so you know going back to like Computing data you know being uh you know we're pushing on those so I think like we just you know we saw like just like it's almost like you you know every generation which is again the generations are accelerating you start to see you know the the benchmarks for like a large model basically get like you know pushed down into the smaller like size regime and so you know 70 becomes an eight and you know like we have internally we have models where the eight is you know like on a much even smaller than eight actually we're starting to see like really nice benchmarks um on even smaller models so you continue to see like and that you know that that the models improve at smaller scale and that I think is just we're pushing the the architecture we're pushing pushing scale and we're starting you know we haven't quite saturated it yet and I think that's really interesting so um you know for me uh one of the biggest reasons that I think it like a small architecture is useful is um obviously on device everyone loves to talk about on device and and Apple's you know talking about that and and Google has you know Gemma models and Gemini running and in Android devices so I think like on device makes sense I think um safety is kind of interesting because you know one of the things we you know we have um our own internal versions of L guard which we used that are orchestrated for applications internally at the company at meta and uh you know today they're built on an 8B model which is kind of expensive to run if you think about a a safety model that's kind of like the secondary model and so I do think you know internally we've been experimenting with much smaller models in that in that regard and uh it creates efficiency it lowers latency uh so um CU really you know those models are really just classifiers you know they're not really autor regressive like chat like interfaces they really just classify like an in the input a prompt of you know does that violate you know this whatever category in the taxonomy in the output um the model when it generates does it does it violate that kind of stuff so you can actually push those even further um I think that there's also like really interesting cases though for like on device um where you almost have like when you think about privacy and you think about data you want to have like your data stay on device you can think about you know like a rag like architecture on device so you have data you know even even your chat history that's like on say WhatsApp or or other things you can imagine like that model having access to to data aggregating it and then running some type of you know almost like a mini Vector database right where you're we using Rag and doing your kind of fuzzy search fuzzy or fuzzy matching and with your like small model and that becomes it kind of own system in itself and you can basically do things like local summarization like I don't know like I get so many text messages like yeah you know hey like summarize my last 15 messages please because like I've been in meetings and I haven't looked at my phone um and that's like super useful and then I don't have to send you know data up to the cloud or anywhere else so there's like those kind of use cases I think that where small models actually are going to be really compelling and then for like super complex queries and and things obviously like you have a big model in the cloud that can always you know surface those but for like many things I think like on device or even you know in the edge and on Prem um these small models actually can do pretty good you talked about kind of scaling up computes and data as you know the two fundamental vectors to improve performance I guess there's been a lot of chatter about how we are going to hit a wall or maybe we're not going to hit a wall on data and maybe synthetic data is the answer Etc I'm curious your perspective on that like are is there an impending wall that we're going to hit most likely of you know cheap accessible data what do you think how Howes is scale beyond that I mean I think we've we've shown with this release that synthetic data does help a lot I mean I think we've uh you know in pre-training um you know we train on 15 training tokens or give or take and um and in post training uh we generated a ton of you know millions of of of of annotated synthetic data um a lot of it generated by the 45b um we OB obviously paid you know for annotations as well um I do think um synto data is like a potential path forward I think it's going to like we we know now um and the kind of proof is in the models right it's like great to talk about it and that um I do think you know data is going to be a challenge at some point for us and this is why I think you know uh companies are licensing a lot of data these days to get access I mean open eyes licensing data we're licensing certainly data um I think uh having access to uh services that generate data uh to improve models is you know is important so I think that inherently is an advantage for a lot of companies I mean Google has YouTube right um they can uh I'm sure is is is a value to them so um which kind of implies that you know uh bigger companies have an advantage which is not something we that's anything new right we've been talked about this for a long time um in terms of a data wall I don't know I mean I think we're not there yet um I would say like let's let's talk another Let's do let's schedule this for like a year out and let's see where we are next year um you know I'll save my calendar for one year exactly from now and um met AI but uh you know let's talk in the ear and see where we are but I we haven't hit it yet um and we're still scaling and we're still you know uh we're still Gathering a lot of data and we're generating data um and our models are still like continue to impr grw so yeah let's close it out with some rapid fire questions sure sounds great and what year do you think will surpass the 50% threshold on sweet bench I good question uh if if I've learned anything it'll be faster than whatever um whatever answer I give you um because I think any Benchmark will as soon as we zero in on it people are going to go and and figure it out so um I don't have an answer it it'll be fast I'm sure you know one of the questions we have been asking people is in what year will an open source model surpass the other companies on the front front the other models on the frontier and we have to take out that question now thanks to you all this I mean it's it's true we're we're we're almost there I mean I think 45b is incredible it's definitely um it's definitely in that class yeah absolutely which is incredible will meta always open source llama I mean I think Mark's pretty committed you saw his letter um I mean we've we've open sourced you know for for years and years now back to py George to fair to Lava models we're I mean this isn't something that's a flash in the pan for the company the company's been committed to open source for a long time so I I wouldn't Never Say Never but like I mean the company and Mark are really committed Amazing Joe thank you so much for being here today and and also for all the work that you're giving to the entire ecosystem uh I think the entire AI Community is has very much grateful for all the work that you've done with pushing out llama and the advancements to come it's a huge team um check out the paper look at all the all the all the acknowledgements I me we spent all of yesterday reading it we need like the Star Wars like scrolling text of all the contributors because it it was an incredibly big team I was thinking about that same so my hats off to the team this was a total I mean this is this absolutely took a village to get get Lama out there and and I'm so proud and excited to represent the team here so thank you thank you [Music] [Music] [Music]

========================================

--- Video 65 ---
Video ID: m3niSE-8ZvE
URL: https://www.youtube.com/watch?v=m3niSE-8ZvE
Title: Klarna CEO Sebastian Siemiatkowski on Getting AI to Do the Work of 700 Customer Service Reps
Published: 2024-07-23 11:00:43 UTC
Description:
In February, Sebastian Siemiatkowski boldly announced that Klarna’s new OpenAI-powered assistant handled two thirds of the Swedish fintech’s customer service chats in its first month. Not only were customer satisfaction metrics better, but by replacing 700 full-time contractors the bottom line impact is projected to be $40M. Since then, every company we talk to wants to know, “How do we get the Klarna customer support thing?”

Co-founder and CEO Sebastian Siemiatkowski tells us how the Klarna team shipped this new product in record time—and how embracing AI internally with an experimental mindset is transforming the company. He discusses how AI development is proliferating inside the company, from customer support to marketing to internal knowledge to customer-facing experiences. 

Sebastian also reflects on the impacts of AI on employment, society, and the arts while encouraging lawmakers to be open minded about the benefits.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

00:00 Introduction
01:57 Klarna’s business 
03:00 Pitching OpenAI
08:51 How we built this
10:46 Will Klara ever completely replace its CS team with AI? 
14:22 The benefits 
17:25 If you had a policy magic wand…
21:12 What jobs will be most affected by AI? 
23:58 How about marketing? 
27:55 How creative are LLMs? 
30:11 Klarna’s knowledge graph, Kiki
33:10 Reducing the number of enterprise systems
35:24 Build vs buy? 
39:59 What’s next for Klarna with AI? 
48:48 Lightning round

Transcript Language: English (auto-generated)
I feel it's different with copy and image in copy though there in the llms are much less impressed and I think the reason for that is that at least how the llms work is they work towards the average so they are trained towards the average and creativity is not the average [Music] it was 2010 when we first got into business with a young man named Sebastian based in Stockholm fast forward to 2024 and Clara is a Global Payments and commerce Behemoth Clara recently made its Mark in the world of AI by sharing some of the results of a product that they built for customer facing workflows clar has been one of the more aggressive experimenters in the world of AI both with external workflows as well as internal use cases Sebastian joins us today to say a few words about what they built and where he sees this world headed Sebastian welcome to the show thank you for having me so you you have become a poster child perhaps the canonical example of putting AI into reduction inside of your business to make life better for your customers and to make things more efficient internally and so the thing that everybody is desperate to know about is how did you do it why did you do it what lessons have you learned what are the pros and cons everything related to the customer support implementation that you guys have done but before we get into that unless we put the cart before the horse can you just give us two words for people who may not be customers uh what is Clara give us a sense for the size and the scope and the and the business that clar's in sure um so I mean it basically started as a payment solution for shopping online um often associated with this buy noway lator thing but actually today we do about100 billion do worth of volume across the world we have a half a million Merchants we have about 100 million consumers they can all both pay the full amount what we call debit and they can pay in installments and use credit and it's also a fintech and a Neo Bank in the sense that we have cards does balances the whole thing we are a fully regulated bank and there's about 4,000 employes got it and 100 million plus customers in 20 plus countries we can see how the customer support implementation got to the scill that it did I think there are a lot of people who have contemplated doing something like what you guys did there are very few people who have actually executed against it and so maybe the first question we'll ask ask you about this is how did you do it like how did you guys get into production so quickly with something that seems to be pretty darn effective sure um so I think you can start at the I mean the first thing that happened to me at least was like November uh 2022 I'm on Twitter and then I see people saying you should really try out this thing called CHP I tried it out and I'm blown away like I was like wow this is like amazing I've never seen anything like this or at least maybe when I tried Google 20 years ago but this is no this is even more impressive and so at that point of time I was like okay this is really cool but then holidays came Christmas and then after that I was like ooh you know we have to lean into this and let's see if we can get hold of Sam mman and open Ai and and so forth and I realized that soon Sam is going to be a person who's going to be impossible to get a meeting with so I got to try before everyone else and so finally you know lucky for me I had sey as a shareholder in in both clana and open AI said I was a good opener I got it and I flew to San Francisco pretending that I was going there for other business but meeting Sam was the only reason um originally my time for meeting was 2 hours by the time I got there it was only 30 minutes left because the secretary was like down prioritizing my meeting uh and I came in and I sat down with Sam I was like I got a pitch to him that working with this European bank fintech is going to be a great um a great client to test open AI products on because what I really wanted to accomplish was to use us as a guinea pig I wanted to make sure that we would always try the latest greatest and that they would find us as a great client to work and develop things with and we managed to establish that relationship and we had a you know a joint slack Channel start experimenting a lot and then we the second thing that we did was important to me was to encourage people internally to really lean in and try it and originally there was tons of these like concerns what about data what about this so we made sure to very quickly solve these things so we were gdpr compliant and that we could set the right structures around it since we're a bank and all that and really make sure that everyone in the company would experiment with it and then it just happened to be so that some people were more curious and more passionate and some people were less and that was fine but some people leaned in and it happened to be so that one of the teams that leaned in started looking at a fairly actually complex Challenge in a way which was what we call dispute resolution and dispute resolution in the bank is basically a customer calls us and says hey I didn't get that package and the merant says but I did ship that package and then we have to be like a small Mei court that basically gathers all the evidence and decides whether you know who has who's done wrong and who's done right right and what are we going to do on this transaction who's going to cover the cost and these errands are very complex they require a lot of evidence a lot of emails back and forth and communication with both the consumer and the merchant takes a lot of time and there's always been a backlog and it's always frustrating because customers wait a lot before they get the final outcome and so we started experimenting just to see if CHP and these Services could help us make basically like a co-pilot help us take those decisions faster and I think to some degree what's important here is that you know a lot of it comes back to the creativity of the team um and we had a lot of teams in clana some were more successful someone L this team happened just to be very very strong and really good at what they were doing and very creative and they found and built away what is actually today referred to as Rags they built already back then they realized that would be a good solution and within two months they were demoing internally to us and others that they had managed to build a co-pilot that basically helped accelerate the process and also increase the quality of the decision- making because it was it was making sure that we actually really you know took in all the irrelevant information and then took a decision on these disputes and we said look this is amazing let's put it in production um so far just as a co-pilot and then the team you know which was crazy like two months later we suddenly get this slack message in tally which says um we're out of errands can you send us more tickets and um that's never happened like this has been a constant backlog was just like this is really impressive and then we said look it's be crazy to try to see how we could you know um increase the pace of this and actually even answer customer service erand and uh and that was that and then that team went on that challenge I think to us what was most critical was one of the rules that we agreed on was that the customer should always know if they speak to AI or if they speak to human M um and that's been important but when we wanted to start testing this we had a bug and the bug was that for a few thousand conversations it wasn't clear that it was Ai and when we then looked at the which was you know not intended but the conclusion was we could read a few thousand transcripts of conversations where the human wasn't a the customer wasn't aware that it was AI answering and we realized that the AI was doing heck of a good job and that made us conclude that it's the most important thing to us was that customer satisfaction would be equal or greater than um what um you know it it was with a human agent and as we saw that we started reach that point um then it became we became less nervous about putting this into real production and actually try it right so I think that like you know it was really the effort of that amazing team that kind of tested and iterated and let a few you know lucky shots that along the way that then allowed us to put it into production amazing um there are so many questions that we want to ask to follow up on this so maybe we'll start with you mentioned that rag is part of the architecture can you say a few more words about the implementation itself and sort of where does open AI end and Clara begin in terms of what you guys have built on top to make this work well I think that the uh I it's funny cuz in general I'm always so freaking transparent and this is one time in my life that I actually feel a bit Cy about telling too much about the secret sauce um because I actually think about this as a fairly uh important strategic Advantage but um what I can say is that in in our case um one of the key elements was that like it's about making sure that the instructions are clear if you um if you on board a human and you ask a random human to sit in our customer service and try to answer a question and the documentation that's available to that human is subpar uh because there's an assumption that you can rely on what people have learned in different sessions or assumptions um you're not going to be successful but if youve written a documentation that is detailed enough so you could even if very slowly put any random individual and they could slowly go through your FAQ and manuals actually answer a question correctly because it was uh documented at level detail then it works and that's how I think about the AI today it's basically an employee that turns out that the work every day and has forgotten everything about what CL is how it works and those that and every time you need to tell it again um and that may change over time but currently that's partially the game and then so that that helped us a lot to think about it that way that we just needed to make sure that the documentation and the manuals were clear enough and of quality enough and then it can actually execute because many times you know if you it's I mean the the the truth that has been the truth for data scientists for a long period of time in out right like if you if you feed data mods with bad things you're going to get bad results right so you need to make sure what you feed in is good and then you can get better outcomes Sebastian you I think you tweeted that uh your customer support agent is now handling two-thirds of your customer service inquiries y um you got a question from your audience on Twitter this week uh are you planning to replace your CS Department 100% uh with AI I guess from a technological perspective do you think that's possible on what time span and and what are your plans well I mean it's very hard obviously to predict how far will ai go and what can we do in the future and so forth but I think that the it's definitely not going to happen anytime soon and I do think that there will be customers that prefer a human for you know could be for any reason could be because they have such a belief or you know conviction or preference or whatever and obviously you want to serve those customers as well so there's no chance that you know the human agents are going away anytime soon um with that said though I think actually the biggest quality improvement that we see is that um generally speaking and obviously we as every other companies to some degree want to avoid this but it's not uncommon that our human agents have multiple chats going on H so so and we as customers all know that because you go and chat and you're like you write a question and you don't get the answer immediately and you're a little bit like come on and they forgot about you and they was like hello John where are you like why you not answering and they're like oh I'm you know and so forth and and I would understand because I I you know when I have tried to work in customer service myself I did the same thing like you know it's like it doesn't make sense because you also sometimes the customer is slow and not answering and so forth so you start doing something else you can't just sit and idle and wait for that you want to you know resolve more things so you get it but the consequence of that is that the average time of resolution of a uh customer service chat is about 14 minutes and when we move to AI it's 2 minutes and the reason of that is because you get instant response as a customer uh as opposed to that delay that happens due to that um parallel handling of errand and um this is actually the biggest advantage and so as a customer a lot of customers that try that say wow I want this experience but at the same prot time we have something else which is funny which is that AI chatbots have been around for 10 years or something and they all been of horrible quality so each one of us have gone to some Airline and tried to converse about some tickets and been like my God this is the dumbest thing I've ever talked to and so the funny thing is that of those 30% currently that do not use our AI chatbot the most common reason is they when we start the conversation with them the first thing they write is Agent right agent which basically means they want to speak to a human and that's not necessarily because they so deeply want to speak to a human as some of them are so but a lot of others is just because they had these horrible experiences and they want to avoid it they just don't trust it to be good and so actually what I'm seeing is that what's happening right now is a time well it will take some time to educate customers on the fact that like you know what but this experience is actually many times better and a lot of the people that tried it they want to use it more because they find it more you know faster um and I think that's the that takes a little bit longer time right there's the actual experience but then there's the perception expectation of what the experience going to look like and changing that takes a bit longer than changing the experience itself so I suspect we're going to see higher even higher proportion of things dealt with AI but there's obviously a lot of complex queries that it doesn't resolve well today and that still needs to be improved on and there's still tons of work to be done right what are the tradeoffs are there ways in which it is consistently worse than what you had before um I actually no but it's not entirely as I said that is not entirely due to the fact that um of AI that is partly due to the fact that some of the instructions that or manuals that were written to help our human agents were subpar and the experience already before suffered from that but not enough managerial attention and focus was put to improving that and helping our agents become better at work so actually our agents have better tools today to be successful in helping the customers as does the AI so the consequences both experiences are improving as a consequence of that but you know so I think that like partially it's true that like things will be worse like yeah no I think both sides get better by doing this actually because just you realize the importance of these things and I think sometimes to some degree previously you just you know there wasn't enough focus on the topic yeah that's great and you mentioned the 14 minutes down to two minutes so there other statistics you can share that help to kind of illustrate the impact of this well I think the one that we were most famously you know quoted on was obviously that 700 full-time in place but I think that one and we were very you know I it's a difficult number to share and I understand like you know we understood that people would react to it but at the same part time I also feel to some degree that like politicians are too slow on considering and thinking proactively what this is going to how this is going to impact society and we felt that there's some level of importance of sharing such statistics to kind of a little bit say look this isn't just fun demos on Twitter this is actually having real life business and real life implications now with that said in our case we have been using um customer service on Contracting firms those firms employ hundreds of thousands of people and if we historically have improved our products somehow that may also have led to Less customer service errand because you know we fix some issue or flow in the product but obviously I've never seen an improvement to our product that at a push of a button had this tramatic impact on number of customer service agents that we need and now fortunately for those agents there are tons of other customers out there so nobody has lost their job as of today as far as I know at least as consequence of this but obviously in the longer term it will have implications on these kind of jobs but that was the statistic that a lot of people obviously reacted to um and the fact that it's about $40 million of improved profitability for the company on an annual basis right so is fairly significant I mean we do about $2 billion of Revenue so gives you kind of a sense for the size awesome and and since you've been thoughtful about kind of the broader societal or economic impacts of this technology I'm curious if you you know if you had a magic wand and you could craft a policy or or procedure that would help get us through what's likely to be an era of disruption do you have any thoughts on what you would do with that magic wand policies programs you'd put in place the first thing that I think is super critical is actually may sound a bit surprising but it's an electronic identification methodology for humans currently there's no globally applied such methodology just like a passport but an electronic one and why that is so critical is because the amount of Fraud and scams you're going to see increase is due to the fact that I have no there's no ability for you Pat and Sonia right now to ask me for my electronic identification to verify that I am not a uh a bot or I'm not some you know what's called um uh fake uh um you know video created or pretending to be an AI you know not an AI talking to you is actually the human right and I think being able to verify that you are talking to the real human is critical if we can supply uh that on a global level but preferably even on a country level that will at least reduce the risk of Fraud and so forth because you'll be able to authenticate like am I talking to pad the real Pat or am I talking to your Bot right like I want to be able to know about so that is very critical I think that needs to be resolved fairly quickly because otherwise we're just going to see an explosion as these I mean I have seen videos of myself talking to customers that we have produced that look identically to me sound like me uh well you know which have been or we are about to send out to over 1,000 of our top Merchants um and so like it is crazy to see those avatars you know and being able to impersonate myself so I think that's that's one thing the second thing though is if you're left leaning I hear people on the left side of the political Spectra they are saying like stop this stop the progress I I have a hard time especially considering that there are less democratical countries in the world that may you know push this agenda as well and so I think that's not necessarily the best outcome but if you're on the rightwing of the political Spectre a lot of people say oh don't worry there's going to be new jobs there's always new jobs this happens all the time there's always new jobs and I think that's a little bit of a simplification as well when I was in Brussels there are I think if I remember correctly 10,000 translators that are employed in Brussels to translate all the European legislation into the local languages of Europe those 10,000 translators are basically almost redundant today with the technology of deepal and CHP and so forth to some degree right I mean you could at least reduce it dramatically um and I don't think it's easy to say to a 55-year-old translator don't worry you're going to become a YouTube influencer so I think that what you can do from a society political perspective is you could think about okay maybe I don't want to stop progress but maybe I can offer something to people being effective right like maybe I can offer something to them maybe Society can have the luxury of at least support individuals that are affected by these changes because not everyone will be able to just retrain into something different and I think it's in that vicinity and I hope if if there would be such measures or at least plans or ideas among politicians then maybe you can take Society through this change with a little bit more of empathy and care about the people are you know um who are affected while at the same time not saying that we have to stop progress right yeah yeah thank you for being so thoughtful about it it's it's encouraging to see people in leadership positions like yours being so thoughtful thank you Sebastian what types of jobs do you think are going to be most affected and and what type of jobs do you think will be you know what what what skills are you teaching your kids to learn uh so that their future livelihoods are are AI aligned so to speak um yeah so I think that like it's funny you say that cuz when I met Sam back then and I got that meeting I said to him look Sam one thing that's going to happen is people will this is going to have impact on jobs so I think if you want to make this a very popular technology you should identify like what are the job categories that people hate the most and you know I happen to have two of the three three ones because I'm both a CEO and I'm both a banker and those are two of the ones and then you have the only the lawyers right so those are the three ones so I said to like what you should focus on try to build AI that replaces CEOs bankers and lawyers and nobody will make a big fuss about it um unfortunately you know and I saw that very clearly because when you know when we did a tweet later on about the marketing things we're doing about AI where we have less need for uh photographers and and such copyist and things less needs we still need them but we need them predominantly for the very creative stuff and less for the kind of day-to-day stuff um that had a violent reaction online and I can understand why um because people you know really feel emotionally resonate a lot to that while when you see online tweets about AI lawyers nobody seems to react much so I feel feel sad for the lawyers in the world I hope you you know people remember you as well lley but anyways you know it's scary right it's scary because I don't know I I I I find it it's a very difficult question to answer um I just I mean to some degree uh definitely physical jobs right like I mean I it just looks currently as on the very long-term perspective it's going to be easier to replace knowledge jobs than it's going to be to replace you know um driving a truck or even though we were so convinced about self-driving cars and all that um or you know proper robots seems a little bit further out than you know AI so it's difficult um but that also assumes that everyone wants to work I'm not sure like some people would like work some people would enjoy a society in which robots serve us and we just you know hang out and play football so like it's a little bit you know it depends like it's hard to predict where all all of this is going to go right I preferably love work so I will be one of the depressed people when AI takes my job and I'm going to sit and like and be like okay that was the end of the fun because I really enjoy it a lot but like you know people are different everyone's different right yeah let's talk about some more of the stuff that you guys have built internally so so you've mentioned a little bit what you guys have done in marketing can you say more about that yeah I mean it's it's been very interesting as well I mean because again there's so many demos right and it's like it's you I mean I think everyone has tried and you gone you know tried to create an image or create a video and you are blown away first with what you can do but then you're like yeah but I wanted to look like exactly like this and I wanted to be consistent with my brand feeling and I wanted to you know Etc and then you start being more challenged with it right like and I think that's why also sometimes I feel a little bit like for example you know people say like oh it's unfair to the creators um that these tools are being created in in marketing so forth and I I partially understand why people say that but at the same point of time we have this guy who has totally immersed himself in this video You Know video creation sound creation um marketing creating things and and created this amazing basically just scripted together a lot of these different Technologies to create like these automatic marketing videos with me making presentation to Merchants as an example me as an avatar and like and it's really nice it looks really great it's on brand and so forth he is a Creator he is extremely creative and it's a little bit like I can I can assume that when you know when the music industry evolved and synthesizers came along and computers to make music you know some people were like complaining that's not playing a guitar that's not creative you're sitting by a freaking computer nobody would say that anymore but I'm sure there was a lot of that criticism to some degree I feel the people that are adopting these Technologies today they are just they're very creative uh but they're using new tools to be able to create what they see in their minds and so you know I think that's what I'm seeing so we are basically having people people who may not themselves be photographers or who may not themselves be great at Photoshop or who may not themselves be great at all these things but now with text and communication with the computers they can create what's in their minds and they can explore ideas and concepts of marketing campaigns of marketing material of doing things in a way that was unprecedented before right like and that's really exciting and they obviously there's yeah it's true there's less people involved right like um there less people involved because previously if you wanted to produce a commercial you know there are tons of people involved and some of that is beneficial because you have you know different people coming with ideas and thoughts but some of it is also less beneficial because you have a a few people who have this amazing idea but they're not capable of turning that idea into the reality because they themselves aren't the photographer and they're themselves not all of this and now they can actually bring their ideas to life at a different level than they could before so uh but that those are the things that we cannot doing a lot of that is just like how do we go from you know um basically we want to Market our credit card in Germany and like how do we go from that to actually having a campaign live that looks really great uses the right copy Etc and we have seen that we have already seen internal examples where something like that historically may have taken a month or two months to prepare and go through different teams and improvements you also have to remember as a Bank we need to make sure that we are communicating in a regulatory compliant way because credit cards are regulated products so there's like tons of complexity associated with these things and nowadays we can see a few individuals go from idea to actually having a marketing campaign live in a week or in you know in at a time frame that was impossible historically um and the quality of the campaign is higher and the you know and and the lawfulness of it is better and you know all all things are better Sebastian you're a creative soul and an artist and I think the Clara brand has always been just so special like quirky Vivid creative all of that um what do you think of the quality of the the AI generated creative copy and like what what do you think can be outsourced today and then Ai and and what can't and do you still prefer uh you know the you know the gorgeous photo shoots that you all do in the house uh it's a good question I feel it's different with copy and image I in my opinion when I look at the imagery I feel it's more fun because it can be to some degree more crazy and imaginary um so there I see less but in copy though they're in the llms are much less impressed and I think the reason for that is that at least how the llms work is they work towards the average so they are trained towards the average and creativity is not the average creativity the extreme of recognizing that this is a total new way or a new way to combine things and stuff like that and that's why I still think that for some period of time creativity will out compete you know these things that and that's what I mean like it's one thing if I want to write you know if I need to write a text about a product like we have obviously we we have a price uh sorry a product comparison website where you know we have millions of products listed right like clothing iPhones whatever and we need descriptions right for those cases llms are great um and they're very efficient and stuff like that but when you want that perfect quiry copy that's going to catch the attention of you know of a human audience and you know they're going to talk about it and thought that was funny or something much worse like well you can obviously generate fast a lot of versions but I still feel that like it's pushing towards the average and the average is not creative sorry like the average is the average which is the average it just doesn't stand stand out much right so there I still feel that like humans are much better at that of of thinking outside of the box so to speak because the llms are almost like thinking in the box like that that's that's basically what they do they're supposed to think in the box right like according to the box that's such a fascinating dichotomy thanks for sharing um can you tell us about uh Kiki I think is what it's called yeah so I think for for clana at least it happened to be so that coinciding with the with the um um with the AI Revolution uh we also started obsessing about the concept of collaboration on information and it's actually also one of the Technologies we've started using extensively inhouse is Neo forj and GRS which we didn't really explore much beforehand and we've also looked a lot to like Wikipedia and other knowledge graphs and and how people have built you know how people collaborate on building great information and um so a big initiative internally has been to start bringing together information that are sitting in silos across multiple systems and improving the quality of that and really creating collaboration which actually has the side effect of us also deprecating Salesforce uh deprecating a lot of enterprise software systems because we move that data into one I'm not saying I mean for example slack we're great users of so we're still big customers of Salesforce because of slack instead but like but some of that t like we've had too many of these enterprise software systems and as a consequence information about what we do and how we work is dispersed and it's inconsistent and um so a big piece has been bringing that together standardizing and harmonizing that and then on top of that we have Kiki who then explores that information and brings it to life so we can go and ask Kiki about anything about you know how many employees are we in that part or what does this part what does this team work on or you know what's important to consider or when you launch a system internally what are the steps that you're going to go through and all of that is getting centralized into one place and connected through the knowledge graph we're seeing that that is having a tremendous impact on productivity uh internally so Kiki is basically our own internal chatbot based on that growing internal Knowledge Graph how how and where does Ki show up for employees is it is it a is it a slack pot like where do people interact with it both in slack but also like we have something that looks like a Wikipedia kind of the knowledge graph when you look at it looks like a Wikipedia basically for RM pleas and you can both kind of read the articles themselves but you can also interact with Kiki to find information in that right uh so it's a combination of semantic search and and AI uh to interpret the the information in that and that has proven to work really well like um it hasn't a tremendous adoption internally and I think it's created tons of value for us so we're very key very excited about that and the account on deprecating Salesforce is really interesting I can see how the system of record functionality can get replaced for the system of Engagement functionality the workflows that people might have been doing on top of Salesforce where have those gone how how do people whatever jobs to be done there were on top of Salesforce previously how are people doing those jobs now um a mix of things actually but it's less about like I think it's less about the fact so some of this is actually as simple as slack flow slack workflow actually the the workflows in Slack are pretty good so we you could just joke at us and laugh at us that you just just moved from one propriety system to another but I think that the but it's not about that it's the number of such Sy right because I want people at CLA to collaborate genuinely and one of the things that been so revealing throughout this process is that whenever people have a new system a new place to go and look for information it all creates these silos and it it reduces the ability for us to collaborate across the organization on information and providing value so just REM removing the number of systems is important right to have fewer and more quality and stand standard across the organization so so some of those workflows are implemented directly in our own Tex Tack and some of those workflows we're still using proprietary system for like I mean we're for example moving our HRS out of workday uh into deal uh which with great success it's not like we're entitled but we are reshaping it we're we're using only the payroll stuff and we are also within a few weeks deprecating workday as also because there was also too much information there that was important like think about for us understanding the organization and how it's tied together if we're ever going to get Kiki and our internal Knowledge Graph to function properly the understanding of our organization the teams the reporting lines is important so that could not sit in a proprietary system that needs to come inhouse um but obviously generating payroll and making sure we pay pay on salary on time and so forth that where are a happy customer of deal nowadays so like so it's just been a change in our TCH stack Sebastian how do you think about buy versus build decisions for you know you have ai for customer support you have ai for your knowledge graph you have ai for marketing each of these categories now has you know companies and vendors serving them like Sierra and and gleen and and companies like that I realized that you know you kind of built what you had you know before these Solutions existed but I guess if you were to start over um from scratch today um or or what advice would you give other other Founders who are just kind of embarking on this journey should they buy or should they build it's a great question and it's one obviously we ask ourselves all the time but I have to say I give you an example right when when we started um encouraging people in clana to use AI we didn't mandate them to do things that was core for the business we said take the idea that you're passionate about and explore it and one of the examples that we built early days was we said look one of the things that we hate in a big company is these Employee Engagement forms cuz they go like hey how are you feeling at CLA great on a scale one to five you know and then we're sitting and trying to interpret the answers to these forms and we felt it was a very in precise and open for a lot of you know interpretation and subjectivity so we said it's not a great way so we said like hey wouldn't it be fun if like we could do a deep interview with every employee right but maybe we can't do it maybe the AI can do it and so we built that we built a deep interview robot based on chip that we then deployed to all of our employees and said hey would you be fine with interacting with 30 minutes with this interviewer uh in order to tell about how it is to work a CL and benefits and strengths and so forth and then it took that information summarized it and basically came back with like you know what are the strengths and weaknesses of working clana could be improved Etc right now the point is we built that and today there are already AI tools and startups out there offering similar Sol solutions that you can use for customer surveys or for employe surveys or engagement surveys Etc so it's not like we were the only ones doing that neither are we in the business of doing that um so obviously you could say today we should have bought it but with that said I am so happy we built that because we learned so much and the employes in t CL learned so much from building that that we're now applying those learnings to other things and we're not using that so then maybe today we'd go and buy that from somebody but still happy that we did it so I feel liit like this is such an emerging industry obviously if you see something that you feel intuitively is just better than anything you can build yourselves right now I would do it but there's also like so much power in Just letting people learn how to use these things and deploy them and develop them because it's such a new technology and there's such a massive value created from people learning to do these things themselves so that's why we're a little bit cautious still about like buying too much from these even though we really want to be supportive of the start community and so forth we're a little bit cautious cuz we just want to try ourselves first to learn um and so that's that's how we're thinking about it um and I think then I would also ask one add one more thing on it like when we for example initiated that discussion about should we keep work there or not I contacted uh the CEO at that point of time and also I said like hey convince us about it and but then I realized one thing that was funny which is and this is an advice to all companies that if if I go to CHP and I say what does the API document what what is the API call that I can do with workday workday at I'm sure they fixed it now but at that point I gave them that feedback at that point of time their API documentation was behind the login so as a consequence of that CHP had not been trained on the apis of workday it is familiar with the apis of slack because those are public documentation and it's even more familiar with things that are open source because it's been trained on the open source Library so there's suddenly this massive benefit from being open source software and even more so to make sure that you have public apis and public documentation of your software because then suddenly you know CHP understands that he can interact it and support you in your interaction with that this just like a funny reflection so I really encourage like these you know more traditional companies to like make sure that everything you have is actually you know publicly available easy and you know don't lock this behind doors right because then it's not going to be used to the same degree yeah yeah yeah um speaking of locked line doors a lot of the stuff we've talked about so far is kind of the internal to Clara operations benefits of AI let's talk about the um product what what have you seen or what what do you see coming for AI in your product ah now I'm going to be even more kidy um I I look I I am extremely excited we have some we have some stuff that's going to go live in a few weeks that is like it's like a beta but it would basically be the customer service assistant on steroids in a sense that it will be even better but it would also start advising you and giving you some ideas and thoughts around you know the type of services that clana offers that I think people will find quite cool um but it's still beta right it's not it's not going to be something yet of that kind I when I then look at our internal projects I think within 6 to 12 months we will be able to start launching things that are truly uh disruptive in the way of services but the funny thing with this and is that back in 2015 long before all of this happened uh at that point time CL was trying to compete with would stripe an Adan on being a payment service provider and when addan signed Spotify which is a neighbor of ours we were just had to look ourselves in in the in the mirror and say you know they're beating the crap out of us both stri and addan so we had to change direction and at that point of time we kind of pivoted and when we sat down in 15 and asked ourselves where is financial services going already then we said well eventually in the future you wake up in the morning and your digit told Financial assistant says hey Pat I've analyzed your Mortage and I realize I'll save you 10 bucks by switching from Bank a to bank B and by the way the only thing you need to do is say yes right and so like we realized that that's going to happen that was a revelation to us in 15 now we just like self-driving cars we couldn't predict how fast or when that's going to happen but it was very clear to us that like eventually that's going to happen and for an industry like Banks banking what does that mean first and foremost what's cool about it it means the evaporation of all the excess profits in banking industry because a lot of the bank profits are built on the lack of customer Mobility the in in willingness of us as customers to move between Banks and the friction associated with it and when AI assistance will allow you to do that just because you say yes then that will Dynamic that would that will make a big difference in the market dynamics and the competitiveness of fintech and Banks and I see that happening in the coming 2 three years for sure and so ever since then that's been the direction of the company and that's still the direction we want to because we realize to ourselves like we don't want to be one of those Banks we want to be that digital financial advisor of yours that's what we want to be we want to be that AI digital finan assistant that helps you save time save money make you feel more in control of your finances and I think that's the Natural Evolution of every fintech um and that's where we want to go so so that is the direction and that's the type of services uh that we're building and trying to accomplish and you know I've this Vision we've had with the current management team that I work with has all been with me almost about 10 years this has been the vision for 10 years but obviously when we saw CHP we felt like uhoh it's going to happen sooner than later it's going to go a little bit faster than we thought um and the services we're building are all in that direction it's just about helping people save time save money being more in control of their finances but about on the shopping side like I'm I'm I'm addicted to your app as a you know Avid Shopper too Avid Shopper uh and I I guess my dream is to have like an AI stylist that would be that' be incredible um do you guys think you'll you'll make plays there as well I think I think there will be I mean it's interesting um it's interesting I think in general if you look at e or Commerce you could basically you can basically think about it as three things there's a curation job to be done which is what you're talking on there is the brand the product in the brand and then there's the infrastructure that helps you B you know payments shipping all the stuff that's needed between I think that's why if if I think about the AI evolution in Commerce I'm less worried about the brands like Nike will be Nike and people will want to buy Nike right um retailers is a bit more different because the curation has already been split up we have Tik Tok we have Instagram we have influencers uh and we have the retailer the Best Buy agent who's trying to help you recommend which TV you're supposed to buy right and so I think that within curation recommendation of products and selection I am 100% convinced that you're right Sonia you're going to see a rise in such right um but it's also very difficult case to do like I've seen a lot of the attempts for example to do travel AI suggestions I tried it myself since I'm planning a road trip in the US with the family this summer and like and it was pretty bad you know so it's just hard because like you need to understand who I am and what my preferences are and what do I think and it's just it's more complex than we think right so I I I think it will happen um but I think it will take a little bit longer time maybe like there's a lot of things still that needs to come into place but it I think it's a little bit different if you're thinking like you know there's obviously easier things it's easier things are like I mean Amazon is already doing some of the stuff already but like easier things would be like hey aad I know you're using contact lenses and I saw you bought them a month ago you're probably running out of them do you want the few do you want the new one that's easier right then like hey Sonia you know I think this dress would fit you because like your your style is according to this so I'm because but I show you I can tell you one cool thing on this topic which I think at least blew my mind away and that was internally we had done a test this is just a test right the idea was that within the clown app when you open it there's category pictures okay so there's like a category picture like shoes you know this home Garden Products whatever so what they had done is they had taken and taken me my customer profile all of my transactions that I've done with clana everything they taken my profile as a user Sebastian and then based on that they had generated a category image which was a shoe what looks like a bit like a Nike shoe and they just wanted to create a more personalized category image that would catch my attention so it was a imaginary AI created image of a shoe but the crazy thing is I looked at it and I was like I want to buy that one like it's like and I am not a big shopper I'm not a big shopper right but I was just like that is insane there's something in the fact that you fed my profile into that my preferences of Brands and purchases and all that and the image you generated was actually attractive and you know we already know that she and and the others are doing amazing stuff where they are predicting purchase behaviors and testing products on small quantities and they you know and people start buying them and then they produce more quantities and they're super fast it's super impressive to see what these guys are doing and sometimes I feel also people forget that like actually that leads to less waste because the bigger retailers buy a lot of products that never get sold and it's bad for the environment so this is actually better for the environment to some degree even though people are very critical about it but what I thought here is just like wow I just felt like I got a glimpse into the future I was like the next thing is I'm going to be out shopping and the images that I will see are things that doesn't even exist they're just you know created on the Fly based on my profile and then if I do click them and want to buy them they're going to be produced post me saying I want this right and that was just like that's and again these are like the self-driving cars things I don't know when but it felt I felt very convinced that it will happen eventually right I think that was pretty cool I was just like wow that's the next level um you know products generated there because there was it was funny cuz it was a shoe and the other thing it had created was an image of a apparently I bought a lot of Home gardening stuff which sounds odd cuz I'm not the big home Goden person but and it had created a lawn mower you know like one of those that you cut a lawn with but it was super nicely designed and I was like yeah that's how I would like a lawn mow to look like I going be a really nice design can I get that one you know so so I think that that's a glimpse into the future the future will be generated let's uh let's move into a sort of Rapid Fire round and we'll start with a question we'd like to ask people who do you admire most in the world of AI n but it has to be Sam I'm sorry it's like a it's easy question great next question Sonia uh Sebastian you and your wife are both patrons of the Arts and and Avid art collectors uh do you have any AI art in your collection and do you think you will ever have any AI art in your collection uh I don't have I think I could have I think to me an image is something that's supposed to create an emotional reaction of some sorts right and I think it's fascina I don't mind if the emotional reaction is created by an AI if it touches me and it means something to me that's what's important but that doesn't mean that I don't think we will continue to buy a lot of art from Human Maids as well right what is your best piece of advice for Founders who are building with AI today I don't know I think it depends I think the founders are doing well I think the smaller companies are doing well I think it's the big companies that should stop discarding this as Bitcoin or some kind of you know temporary Trend now we're not going to get into the Bitcoin because I know some other people on this call have different opinions tonight but but um uh but uh yeah I think that's the important like don't like be cautious lean into it try it test it yourself like you know explore it learn it like don't fear it just try to learn um I think that's the best thing you know and you can always start talking about yeah what about Ai and the world will end and this and that yeah I'm like I get it I can also sit down at dinner sometimes and talk about these things because they're fascinating but in the end like a meteor might hit us in hit me in the head tomorrow as well like I mean things can happen you can't predict these things so the only thing you can do is try to lean in and learn and explore that's my opinion like at please try to understand it better awesome Sebastian thank you for doing this thank you for having me [Music] [Music] [Music]

========================================

--- Video 66 ---
Video ID: pYBOWDJ5HJc
URL: https://www.youtube.com/watch?v=pYBOWDJ5HJc
Title: Reflection AI’s Misha Laskin on the AlphaGo Moment for LLMs | Training Data
Published: 2024-07-16 11:00:48 UTC
Description:
LLMs are democratizing digital intelligence, but we’re all waiting for AI agents to take this to the next level by planning tasks and executing actions to actually transform the way we work and live our lives. 

Yet despite incredible hype around AI agents, we’re still far from that “tipping point” with best in class models today. As one measure: coding agents are now scoring in the high-teens % on the SWE-bench benchmark for resolving GitHub issues, which far exceeds the previous unassisted baseline of 2% and the assisted baseline of 5%, but we’ve still got a long way to go.

Why is that? What do we need to truly unlock agentic capability for LLMs? What can we learn from researchers who have built both the most powerful agents in the world, like AlphaGo, and the most powerful LLMs in the world? 

To find out, we’re talking to Misha Laskin, former research scientist at DeepMind. Misha is embarking on his vision to build the best agent models by bringing the search capabilities of RL together with LLMs at his new company, Reflection AI. He and his cofounder Ioannis Antonoglou, co-creator of AlphaGo and AlphaZero and RLHF lead for Gemini, are leveraging their unique insights to train the most reliable models for developers building agentic workflows.

Hosted by: Stephanie Zhan and Sonya Huang, Sequoia Capital 

00:00 Introduction
01:11 Leaving Russia, discovering science
10:01 Getting into AI with Ioannis Antonoglou
15:54 Reflection AI and agents
25:41 The current state of Ai agents
29:17 AlphaGo, AlphaZero and Gemini
32:58 LLMs don’t have a ground truth reward
37:53 The importance of post-training
44:12 Task categories for agents
45:54 Attracting talent
50:52 How far away are capable agents?
56:01 Lightning round

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 67 ---
Video ID: aTQWymHp0n0
URL: https://www.youtube.com/watch?v=aTQWymHp0n0
Title: Microsoft CTO Kevin Scott on How Far Scaling Laws Will Extend | Training Data
Published: 2024-07-09 15:07:16 UTC
Description:
The current LLM era is the result of scaling the size of models in successive waves (and the compute to train them). It is also the result of better-than-Moore’s-Law price vs performance ratios in each new generation of Nvidia GPUs. The largest platform companies are continuing to invest in scaling as the prime driver of AI innovation.

Are they right, or will marginal returns level off soon, leaving hyperscalers with too much hardware and too few customer use cases? To find out, we talk to Microsoft CTO Kevin Scott who has led their AI strategy for the past seven years. Scott describes himself as a “short-term pessimist, long-term optimist” and he sees the scaling trend as durable for the industry and critical for the establishment of Microsoft’s AI platform.

Scott believes there will be a shift across the compute ecosystem from training to inference as the frontier models continue to improve, serving wider and more reliable use cases. He also discusses the coming business models for training data, and even what ad units might look like for autonomous agents.

Hosted by: Pat Grady and Bill Coughran, Sequoia Capital

00:00 - Introduction 
01:20 - Kevin's backstory
06:56 - The role of PhDs in AI engineering 
09:56 - Microsoft's AI strategy
12:40 - Highlights and lowlights 
16:28 - Accelerating investments 
18:38 - The OpenAI partnership 
22:46 - Soon inference will dwarf training 
27:56 - Will the demand/supply balance change? 
30:51 - Business models for data 
36:54 - The value function 
39:58 - Copilots 
44:47 - The 98/2 rule
49:34 - Solving zero-sum games
57:13 - Lightning round

Transcript Language: English (auto-generated)
the things that are brittle right now where you're like oh my God like this is a little too expensive or it's a little too fragile for me to use like all of that gets better like it'll get cheaper and like you know things will become less fragile and then like more complicated things will become possible like that that is the story of each generation in these models as we've scaled up [Music] on any given day Microsoft may be the most valuable company in the world and arguably no one has been more ambitious more strategic or more effective in its AI strategy than Microsoft the key architect behind that strategy Kevin Scott CTO of Microsoft we've had the pleasure of knowing Kevin for a couple of decades now dating back to his time at Google when he overlapped with our partner Bill cour Bill will join us today for a very special episode of training data we hope you enjoy Kevin thank you for being here on training data yeah glad to be here so just to start I I know you've talked about this before but for our listeners who might not be familiar with your story how does a kid from rural Virginia end up becoming the CTO of Microsoft who knows C certainly not a not a uh not a repeatable plan I don't think um I I don't know like it it is um the thing when I reflect back on my uh yeah my story is it's just a lot of being at the right place at the right time so I'm 52 years old so I was uh you know a 10 11 12 years old when the personal Computing Revolution started to uh hit Full Steam and so like right at that moment when you're a kid trying to figure out what you're about and like what you're going to latch on to like I had this like really convenient thing that captured my interest and was a good place for me to ground my curiosity on and you know and I I think that's you know one of the object lessons in general is if you happen to be interested in um like really motivated to learn more and do more with something that is at the same time growing really really quickly uh like you probably are going to end up in a reasonable place and so you know I I was interested in computers I um yeah I was the first kid in my um or the first person in my family neither my mom nor my my dad went to University so I was the first one to graduate with a bachelor's degree um like I majored in computer science and minored in English Literature Like I had this moment uh when I was uh um at this moment when I was trying to decide what I was going to go do after I got my undergraduate degree where my two advisors were arguing about uh whether it should be PhD in computer science or PhD in literature and I was very seriously considering both uh but I was so broke uh and just so tired of being uh busted all the time that I picked the pragmatic path uh you know not not that you know like I I still imagine what my life would have been like uh as you know person with a PhD in English literature and I think it would have been just fine but uh like I I chose you one of my two equal you know interest uh and then you know for a while I thought I was going to be a computer science Professor um and at the last minute this is where Bill and I uh intersected like I decided uh I was a compiler optimization and programming languages person uh through years and years in grad school and I got almost all the way to the end I was like I don't think I want to be a professor anymore like I I would would work on these things where it was you know 6 months of effort to write a paper and you make some synthetic Benchmark 3% better and I was like this doesn't feel to me like the way to have a lot of impact uh in the world and like I don't want to do this over and over and over again for the next 25 years of my life and so I uh I sent my resume cold into Google in 2003 and um I got a email from this guy Craig Neville Manning who had just gone off to New York to open up Google's first engineer remote engineering office and uh like I had an amazing interview at Google uh I I don't know whether this was on purpose or not or like I just got luck of the draw but uh like uh it seemed like every compiler person who was working at Google uh was on my interview slate and I was like this is amazing like all these people like uh know all of the stuff that I know and you know we can have easy conversations I I worked on nothing that was even remotely close to compilers uh at Google and I was confused why all of these people were there but uh it was a great interview and I was super stoked and I joined Google and Google is yet another one of those things just like the PC and just like the internet uh like it was a phenomenon that was growing crazy fast with a bunch of smart people working there and that uh that resulted in this opportunity I had to go uh join this startup AB mob when it was very early on like right at this pivotal moment when mobile mobile was taking off and you you needed things in Mobile like advertising infrastructure and I helped build you the the seminal company I think in in Mobile advertising uh and then was back at Google and then I helped uh link in go public uh running its engineering operations team and then I was at LinkedIn when we got acquired by Microsoft so like yeah none of that I think you can plan it's just like a lot of right place at right time and you trying at every point you can to do the most interesting thing you can do on the thing that's growing really fast you know when you talk about your personal history Kevin I guess you know the focus nowadays is on AI machine learning a lot of the practitioners are um people with phds how how do you think about sort of practical teams for AI uh since you're obviously doing a lot of that work at Microsoft and and involved with Partnerships with open Ai and others yeah I mean I I think if you are building the really complicated platform pieces of AI uh so like the big distributed systems for training and inference uh the big uh like networking and silicon and uh you know system software components or the algorithms that you're using to do training and inference I think a PhD is super helpful like there's just a huge amount of prior knowledge that you need to have in order to jump into the problem space and be able to like go uh quickly and like you know you need to be clever but like you don't PhD is I like yep I know you have a PhD uh Bill and are far clever than I am but like uh usually folks with phds are clever but like they're not the only people in the universe who are clever uh so like I I think it's mostly helpful in the that you've gone through like a pretty rigorous training regimen where you get a whole bunch of Prior Arts stuffed into your skull and like you you know demonstrably can do a very complicated project uh and you know the PHD projects uh you know look kind of like AI platform systems projects uh except the AI platform and systems projects are lots and lots of people working together whereas you when you're getting your PhD you often are like working in relative isolation on like your particular thing so like that's the you know one of the things people have to learn is like how to get yourself docked into a group and to be able to collaborate effectively with a bunch of other people like yourself uh so useful but you know there's so much else in AI that needs to be done other than building the the platform um you know and for those things you PhD is helpful but certainly not necessary um you know like figuring out how do I apply this to education how do I apply this to health care how do I uh like how do I build developer Tools around this how do I you do all of the million things that happen when new platforms emerge that you you know sort of complete the whole platform into like a portfolio of products and a portfolio of middleware and a portfolio of like all of the other stuff you need well speaking of which Microsoft seems like it has about the most sort of far-reaching or ambitious AI strategy of anybody out there can can you just kind of say in a couple words what is the AI strategy for Microsoft and then just for fun if you're going to grade yourselves what have you done particularly well what have you done maybe not as well as you could yeah so I mean we've been sort of talking about the strategy Microsoft as a platform company like we I think have participated or like helped drive a a hand full of the big platform waves in Computing uh like we were certainly one of the pillar companies in the personal Computing Revolution like we had a important part to play in the internet Revolution although I think that one was uh a far more diversely contributed to Revolution than uh yeah than personal Computing um yeah we kind of miss the mobile uh Computing Revolution um but like each one of those things like we have thought about uh how do you go build a technology platform for this particular era of technology that allows other people to go build on top of that platform to make useful things for other people and so that is our AI strategy it is uh like how do you from Frontier models to small language models to uh like highly optimized inference infrastructure uh you know like hypers scale on both training and inference uh like economies of scale like making the entire platform more accessible because it's cheaper and more powerful uh with every turn of the crank uh and like all of the developer tools and safety infrastructure and testing and everything that has to be there in order to have robustly built AI applications like go build that uh and like listen to developers and listen to people building uh building AIS uh as intently as you possibly can so that you are filling in all of the gaps that you can for them as they are encountering problems deploying this technology to to users um so that that is that is our strategy um and so yeah I think we're doing a reasonable job of it um um I don't know like I hate to grade myself it seems a little bit disingenuous right High to L lights well so so maybe before I do that like you know let me describe something about my own psychology so I I uh like I am an engineer and I think most Engineers are uh like uh short-term pessimists long-term optimists uh and so the short-term p ism is like you come in every day and you're like oh my God this is like a bag of crap like I just don't like any of this and like everything's broken and like I gotta like I got I got so much stuff to fix and I'm so frustrated uh but you work on all of those things anyway because you're optimistic that all of the problems can be fixed and that uh they're going to be worth fixing at the end of the day um and so yeah I mean the there there's a bunch of stuff that I think we're doing really well like I think we have absolutely along with open AI made uh very powerful AI dramatically more accessible than it otherwise would have been to a larger group of people I think you because of that work that we've been doing alongside open AI we have uh we're just seeing lots and lots of customers who otherwise wouldn't be building powerful AI applications um and so like I I feel like we're doing a good job in uh the way that we're partnering I think we're doing a good job in uh like having a really um particular point of view and it's not an immutable point of view but like it's a point of view about like what an AI platform ought to look like and we're trying to uh like make it as complete as we can um yeah low lights is I think we were a little bit late uh to like some of the basic AI stuff uh so it wasn't that we were not um investing in AI at all and like you can sort of look at some of the work that Microsoft research had done over the years and like MSR was an early AI leader um and I think you know the you B Bill Bill knows this just as well as I did just from his time at uh time at Google and you where we overlap for a number of years but um many you know maybe most of the really important advancements in AI over the past 20 years have been uh a function of some kind of scale uh you know whe and it's usually you got data scale and compute scale in combination let you uh like do things that weren't possible at lower scale points and yeah at some point like that that scaling of data and compute is so exponential that you get past the point where you can have fragmented bets uh where you can literally it's it just becomes economically impossible to bet on 10 different things uh that are all exponentially scaling or have the ambition or the need to exponentially scale simultaneously and so I I think one of the things that we were a little bit late to is like we we didn't put all of our eggs into the right basket uh soon enough like we just yeah we were spending a lot on AI but it was fragmented across a whole bunch of different things and because we didn't want to hurt any of the feelings of smart people or yeah whatever right like I I don't even know what the diagnosis was because a lot of that was before I was at Microsoft um we like we we just weren't as quick as we should have been at uh like saying nope scale is what matters and like here's how we're going to focus uh focus our investments on scale in a principal way when did that when did did you get religion that scale is what matters was there a particular event or a moment that really crystallized that for you yeah I mean I was so I've been at Microsoft for about seven and a half years now and uh like my um when I when I became CTO my my job was like take a take a scan left to right across uh both Microsoft uh and the entire industry and try to see where yeah we we just had holes in execution where like we we were not doing things uh at that point in time which was I guess 2017 early um where all right like what are we not doing today that we're going to deeply regret uh in 2019 or 2020 so like two three years out um and like the biggest thing on the list was like you know rate of progress on AI was not fast enough so i' I'd say mid 2017 like I had religion that that was going to be a big part of my job uh was like helping us figure out what the strategy was going to be and then in 2018 uh like if anything the um Bert the publication of the Bert uh Bert paper from uh from Google was like a real crystallization of uh that that belief so like everything that I had uh that was in my analysis I was like this this is as fine an example as anything of like why we have to really really accelerate on getting more serious here and so like you know very shortly after that like I restructured a whole bunch of stuff inside of Microsoft to get us more focused on AI and then about a year later we did that first deal with open AI um and yeah we we have been accelerating our investments and like trying to get more focused more crisp uh more purposeful uh since them when you were very early to appreciating the potential of open AI what did you see in them at that time when that first partnership was true well we we had like or at least I had uh this like real belief that what what was happening with these models as they scaled as they actually became a basis for building a platform that like the big shift that was happening is it wasn't just you know I I ran one of these uh teams that uh at Google where you had a pool of data and a bunch of machines and an algorithm and you were like training a model and like the model was for a specific thing like in you know the case of the things I was doing at Google it was like click-through rate prediction for advertising and you know like a hand of other things uh and like just outrageously effective right uh but most of the work before this uh before GPT um was about those sort of narrow use cases like you were purpose building models for narrow things uh and it was just tough to scale like you couldn't you'd invest a bunch of compute and like you couldn't amortize the cost of the compute across anything more than just the narrow thing that you were building the model for and you had to have a lot of expertise uh that you know if you wanted to replicate all of this it's like you had to have different data and like different you know Ai phds and you know different processes every time you wanted to go build AI into an application and what what was happening was like yeah you you had these big large language models uh that were useful for lots of different things so you didn't have to you know have a separate model for machine translation and sentiment analysis and you you like all of the different text things that you were doing and I was like okay this is extraordinary and like they were also becoming more platform like as a function of scale uh so transfer learning was working better as things scaled up uh and yeah and this is still the general pattern so uh like everything everything that we understand that large language models can do uh plus or minus like will get better when you get to the next scale point and on top of that they will become slightly or maybe dramatically more General in the sense that their capability set broadens and open AI had that same belief and they also had like a very principled analysis of like how the those platform characteristics emerged over time as a function of scale uh and a bunch of experimental validation that said that their forecasts were right and so like it just you know you you sort of like look at what the forecast says and you're like this is how much money it costs to run the experiment to see if you're going to be on forecast uh for the next uh turn of the crank and like you know it was felt like a big number at the time like it was billion dollars uh but like relative to what was happening it it just wasn't a large amount of money and then you know gbd3 was on forecast and gbd4 was on for so like it it it just was like finding a partner that had the same platform belief that you did and like a track record of you being able to execute through through these scale points like it was it it didn't like I I've done a bunch of things before that I had way more reservation about uh in the past like just in terms of Investments like this one didn't you like there was a bunch of people who didn't agree with me but like I I I had pretty high conviction you touch on investment I guess you know there's a lot of um trade Publications now speculating about the cost of doing training and so forth and the you know rumors of billions and billions of dollars being spent and so forth uh and I guess based on my own background I I think training's going to get dwarfed by inference here pretty soon uh how how do you see the should hope so well yeah otherwise we're building models that nobody knows what to do with that might not be a great investment but Y how how do you see kind of computing landscape evolving and where's it going you know the I think people are joking that all the money is going to Nvidia at the moment well look I think invid is doing a Nvidia is doing a good job uh so like the you know the two interesting things that are happening with these models just in terms of um the efficiency of the scale up is each Hardware generation's better price performance- wise uh usually by an extent greater than Mo's law used to work for general purpose uh Computing so um you a100 was about you know three three and a half times better price performance than v00 uh you know h100 like not quite that much but you know close uh you know on paper the Next Generation looks uh very good as well and so like you've got hardware for a variety of reasons uh like you know part of its you process technology but part of its architecture and like a lot of it is like being able to leverage uh um uh narrower uh word size uh in the computation so like you know instead of needing 64-bit arithmetic like you're you know you're doing arithmetic with much less Precision right now um and so like that you know there's just an embarrassing amount of parallelism there then like we're getting better and better at extracting that architecturally in the hardware and um you know there's a bunch of innovative stuff happening with networking as well like we're past the point for the frontier models at least where you can do anything interesting on a single GPU so for years and years now like both training and inference have been multi-gpu multi compute node um problems uh and so like there's a bunch of innovation happening on the network side as well which allows you to strap all of the compute together uh like at the chassis level the rack level the row level you data center level more effectively um which is great because um you know for the Nerds listening like we we haven't had effective um Power scaling uh uh or dinard scaling since 2012 or so uh so yeah we're not we we're we're getting more transistors but like they are not uh they're not getting cooler uh um yeah know and like we we just we have a lot of uh a lot of density issues just with power dissipation that we have to go uh go deal with um do you do you see inferences driving different data center uh architecture yeah I mean look we we already architect our training environments in our inference environments differently they just need different things um and like I I I think you know all the way down to you know silicon and uh like through the network hierarchy you need different things for inference and like inference is inference is kind of easier than training like training the way that we're doing it now is like we go build big environments that take you know a few years to build um and yeah with inference like if somebody came along with a like a better a better silicon architecture or a better Network architecture like a better cooling technology like it's a much easier experiment to go run uh you just go swap some racks out I mean like I'm like my data center people will yell at me like it's not quite that easy but it is uh it is easier than like having to go like do a big Capital project like a like a training environment looks like and so like I I you know intuitively you would think that that is going to result in more diversity in the inferencing environment and more competition and more like a faster rate of improvement um like that's and like on the software side that's certainly what we see like the inference stack uh just because it's such a large uh fraction of the overall compute footprint and it's constrained uh uh because we have more demand than Supply at the moment uh like you just have very very powerful incentives to go optimize the software stack to squeeze more performance out of it you think we'll be in an environment anytime soon where that demand Supply balance changes not necessarily at Microsoft but it feels like we're seeing that at the market level as well yeah I I don't know um I mean if we if we continue to see the platform uh continue to expand capability wise uh and it just becomes more useful like I think demand increases if anything now the shape of the demand is is probably going to move around like I I think you're already seeing a little bit of that um yeah building uh building a Frontier Model is like a very very resource intensive thing and as long as people are like building Frontier models and making them accessible and like maybe they're not accept accessible quite the way that people want you know like they're only API accessible and like there isn't an open source thing you can go instantiate and you know muck around with but like it's way more accessible than it was uh you know six or seven years ago where like the only way to access some of this stuff is you had to go work for like you know two two or three tech companies uh and so anyway like I but I think you know you do have to ask yourself and somebody else should do the asking right because I'm like all kinds of biased right uh but like I I don't know how many Frontier models uh you actually need um um if you know they're all roughly speaking in the same tier of capability um yeah that's an awful lot of money to spend for you know things that are you know roughly equivalent you know it's sort of like you know if if you're starting a company right now and you believe that you have to build your very own Frontier Model in order to go uh deliver an application to someone that's almost the same thing is saying like I got I got to go build my own smartphone hardware and operating system in order to deliver this mobile app like maybe you need it but like probably you don't um and so I mean that to Bill's point like I think the thing that makes sense you know for the market is like you just you you're going to want to see lots of people doing lots of inference because that means you got lots of products that have found product Market fit and those things are scaling um but like lot lots of speculative dollars flowing into infrastructure uh R&D like probably ends the same way that yeah the yeah many speculative infrastructure booms have ended unless excuse me on the scaling front um you know Microsoft published a paper uh sometime ago pointing out that the quality of training data is uh maybe at least as important as as volume and I think one of the speculations you see now in the industry is that we're running out of sources of high high quality training data and and you're reading at least uh some articles claiming that various Partnerships are being struck to to get access to training data and it might be behind pay walls and so forth how do you how do you see that evolving because it it feels like we have more and more computation but we may not have more and more training data yeah I mean I think that was almost inevitable um it is you know in my opinion a good thing that uh quality of data matters more than quantity of data because it gives you an economic framework to go do the do the Partnerships uh that you need to go do to yeah make sure that you're feeding your AI training algorithm of curriculum that uh you know is going to result in smarter models um and like honestly like not wasting a whole bunch of compute feeding at a bunch of things that are not um and I think one of the you know from an infrastructure perspective one of the things people have been very confused about is like a large language model is not a database it's not a repository of uh facts um you know it's like important for it to you know quote unquote know some factual things uh but it is uh like the world's crappiest database uh you know honestly like if you need it to be your retrieval engine and so like you just shouldn't think about it as like hey I got this thing and like it has to have everything baked into it uh like into the model weights themselves uh so that you can you know recall a bunch of stuff uh you know like as you've seen like the the recall is imprecise in the same way that human recall is imprecise so they just much more efficient ways to do recall um um yeah so look I I think you you are I mean the way that we say see things developing is like you have data that is valuable for training models and then you have data that you need to have access to for an application in order for the model to reason over and like those are two different things and I think they're probably two different business models around those things so and like at the end of the day this is all just this is about business models right like people who produce data want to be compensated for uh you know use of that data um and so yeah we we have all of this data sitting inside of search engines right now like not not in you know randomized weights but like quite explicitly it's like sitting in a you know sitting in indices and you know bang and Google and whatnot just waiting to be retrieved and like you know plus or minus everybody's okay with that because there's a business model there that makes sense uh so you like you enter a query and like you're either sending traffic or you know like there's you SEO and like advertising like a whole bunch of like business model that surrounds that I think we'll figure out a business model for that um referral data so that like you know when an agent or an AI application needs to retrieve some information from someone so that it can you reason over it and give the user an answer like we will figure out the business model for that like it'll either be subscription rev share it'll be licensing it'll be like some new flavor of advertising you I was just telling someone the other day like if I was in my 20s right now like you know for for all of your entrepreneur like somebody ought to be out right now figuring out what the new ad unit is for agents and like just building the company uh because like it it it will have the same characteristics and qualities is uh you previous ad units like you have people with information in products and services who are going to want to get to the attention of someone who might want those data and products and services and like quality is going to matter and you know relevance is going to matter and a bunch of other things and I I would be shocked if there isn't a auction model for uh you know that's going to be the right way to Value everything and you know like maybe there's referrals and like referrals will have some economic value um I think for training is just a little bit different because uh it's very very hard when you're building that model uh to at the time that you're doing the building to really be able to ascribe um a monetary value value to a particular token of input yeah just because it's contribution to the model like the same way that a word from Moby Dick has like a very defuse contribution to like your own human intelligence uh you know even though you definitely read it at some point in your uh career uh or your life um like how valuable that is to like forming you know Bill Corin or Kevin Scott's uh you know useful intelligence like who knows speaking of which this um one of the things that we hear a lot of times is the the value function is in some ways the bottleneck to broader reasoning capabilities it's easy enough to construct a value function when you're playing a game with a you know with a with a known winner and loser like go or chess or poker or diplomacy but it becomes a lot harder to construct the value function when you're going into broader domains and it's it's things like assigning the value of Moby Dick to Kevin Scott's life you know that sort of thing um are there are there practical solutions to this are there practical implications to this I guess the broader question would be where do you see the overall field of reasoning in llms going well look I think people are trying to get at this uh so so you've got a bunch of benchmarks like yeah GP QA and MML U and yeah like we're just sort of rolling through a bunch of benchmarking paradigms to like try to come up with scores of performance for these models like whether it's reasoning capability or yeah and yeah I think yeah one of the interesting things we seen over the past handful of years is like we just are very quickly saturating these benchmarks like where you uh like one emerges and then you within a model generation like you'll completely uh or like get very close to saturating the particular Benchmark and then you got to go find something else to help be your Guiding Light so so but like let let's just sort of assume that like you know you'll have some in interesting benchmarks that are correlated with the reasoning capabilities you want models to have um you know then the question it's just an expensive experiment to run like you can run an experiment where it's like okay like I'm going to train a model like you know with this information in and or out and like does it get better or worse at um you know performance on these reasoning benchmarks um and you know like I think all of us have done different versions of those experiments like that they they're just extraordinarily expensive to run at the most granular scale you can imagine running them um part of that paper that bill referenced like textbooks are all you need is like a um it it it's not the full story but it's like part of a story that is like you know just sort of evaluating like token contribution quality to model performance so yeah I like I I think it's and and everybody's got every incentive in the world right now to try to figure out like what that is uh if for no other reason than you uh like in a world where you're synthesizing data you're literally spending compute to generate synthetic tokens for training you you really want to make sure that the tokens that you're generating are uh actually useful or not where do you think the models are at the moment I you know I think uh Microsoft has introduced a whole bunch of co-pilots to try to help end users with with their with your products and so forth on the other hand I I see lots and lots of companies trying to build agents that can be kind of autonomous actors now that that's a wide spectrum of kind of expected performance of what the models can do where where do you think we are where do you think we'll be in a couple of years well yeah so I think that's a super good question and there's a you know there's even a philosophical thing there about you know what it is we should want um well there is the the Spectre of everybody's job getting replaced right so yeah you know somebody look the we we chose the the name co-pilot for the things that we were doing relatively deliberately because we we want to at the very least encourage everyone who's building these things inside of Microsoft to think about how can I help augment someone who's doing some form of cognitive work uh so like we want to build you know assistive not substitutive Tech [Music] um and and you know the good news is also easier to think about how to go from you know sort of rough Frontier Model capability to useful tool when you're narrowing it down to a domain um and so like I I think that's been a reasonable deployment path and like we've got a handful of our co-pilots that have real Market traction right now and are like you know in daily use by like a lot of people doing you real non-trivial cognitive work uh you know and I think that will expand and over time um what just on that what are some examples of copilot that have really like hit the bullseye already versus maybe co-pilots where the technolog is not quite ready in terms of like jobs to be done yeah like I I think you know GitHub GitHub co-pilot is like probably the you know the thing we've talked most about and like there's you know the most uh public conversation around uh yeah like it it it's it's been a been a hit um you know it it is genuinely useful um yeah we we've got some other co-pilots that are like that that are um you know that are that are super useful um but you know I think the thing that bill was getting at like the more General the more General the co-pilot is like the you know the the harder it is to have it actually take uh very high Precision action on your behalf autonomously um you know particularly if you know it's doing something where it's representing you um where you there are stakes and you know consequences and accountability back to you if this agent makes a mistake and we're trying to be very deliberate there because I think one of the things that you don't want to do is uh introduce a thing that's going to make a whole bunch of uh these sort of Errors where the user's first reaction is like this doesn't work and like I'm not going to try it again for a good long while uh so we'd rather have it be very good before we introduce it uh yeah which again you means you're you're sort of optimizing for use cases not for like you know super super broad things I mean there's a good like we um we did a partnership recently with uh yeah with Devon uh um which I think is another one of these like very interesting like you know use case specific things where it's like you know Frontier Model plus a whole bunch of other stuff that is like optimized for like giving humans high quality recommendations for actions uh that they can take and then when you click accept on the action uh like you uh you have reason ly high confidence that you know it's going to work and you haven't made another set of problems for yourself it's I like I'm I'm guessing too you all see this in your portfolio companies like there there seem to be a bunch of companies out there right now that are like doing exactly this and and that it's you useful and working well it's interesting because we hear one of the things that we hear fairly consistently from the companies that are further on in their AI Journey you know everybody kind of starts in the same way where they start playing with open Ai and then maybe they start using some of the other proprietary Foundation models and then they incorporate some open source models and maybe they have some of their own stuff there's a vector database in there somewhere yep from an architectural standpoint it feels like people tend to go on a not quite the same Jour Journey but Journeys that sort of rhyme but then what we hear from them when they're 12 or 18 months down the road is there's kind of this massive 8020 rule at play and maybe it's a 982 rule but you can automate most of a task pretty quickly and pretty effectively but getting it to the point where it's actually end to end running autonomously in a way that is compelling and consistent enough that you can actually trust it kind of that last mile that last couple percent that makes you really trust it yeah it seems like that's been pretty elusive for a lot of tasks and so I'm one of the things that we're really curious about is okay well when to the foundation models themselves you know get good enough to knock out that last 2% or is that a domain specific thing and that's really the job of the software vendor who who lives on top of the platform to figure out that last 2% I look I think it's going to be both for a while like the the two things that I think you can trust yeah I I know you you guys were probably going to ask this question at some point but um like we're we're despite what other people think we're we're not at diminishing marginal Returns on scale up um and like I I try to I try to you know help people understand like you know there there is an exponential here and like the unfortunate thing is you only get to sample it every couple of years because it just takes a while to build supercomputers and then train models on top of them uh and and so the the next sample is coming uh and like I I you know I can't tell you when and I can't predict exactly how good it's going to be but it it will uh almost certainly be better at like the the things that are brittle right now where you're like oh my God like this is a little too expensive or it's a little too fragile for me to use like all of that gets better like it'll get cheaper and like you know things will become less fragile and then like more complicated things will become possible like that that is the story of each generation of these models as we've scaled up and so like you know we even think about this inside of Microsoft and like one of the category errors that own developers who are building these AI products can make is like get too convinced that uh the only way to solve my problem is like I have to go uh take the current Frontier and supplement it with a whole bunch of things but which you do have to do but like you want to be very careful architecturally when you're doing that that it doesn't prevent you from taking the next sample when it arrives so you just want to architect these these applications where when the new goodness comes you can go plug it in and like you'll have have to go optimize that as well uh like I think that's just sort of the grind that we're on um but like you just like the the the thing that was killing us internally is I I would have teams inside of the company who would look at a Frontier Model and say oh my God there's no way that we can ever deploy a product on top of this because like this is fragile and this is too expensive and so you know like please give me like giant pools of gpus and like let me go spin up a big team doing uh you know like a very you know tailored version of this and like we're going to build a specific model and like and and yeah they they would go off and like spend a whole bunch of money and like the thing would be you know a little bit better uh cost wise at the same level of performance as the current uh Frontier and then the frontier would snap to the new uh point and it would be just doomed uh and and so like you you just architecturally don't want to get trapped by that I don't think I mean like that'd be my advice I give to everyone is like just give yourself the flexibility to snap to the new uh you know to the New Frontier when it emerges and and like that lets you preserve all of your skepticism you can believe uh you know believe all you want that the New Frontier not coming like go you know like you read your favorite Twitter troll that says it's all over and a sham and like and like but but just give yourself the option that you know may maybe what's been been happening for six years now is going to continue well well hearing hearing that hearing that we are not at diminishing returns to scale I'm gonna count that as good news and so let's stay on the theme of good news I know that you're um short-term pessimist long-term Optimist can you give us some of the optimistic point of view for where where we're heading in this world of AI like what are the some of the things that you're most excited to see in the world in five or 10 or 15 years or what whatever you C as a longer term Horizon well look I think the thing that everybody ought to like spend some time thinking about is where are the gnarliest zero some problems that we have in society like where where are the things where like we just are fighting with one another or you know like we are IM miserating people because uh whatever it is that people need there doesn't appear to be enough of it uh and I think for a good number of those things like what you have to have to to to turn them into non-zero sum games to you to create abundance uh and to relax some of these constraints is you have to have technological breakthroughs it's it's like the only thing that reliably that's ever turned zero some to nonzero sum in in human history is like some tech has to come along that like you know lets us lets us have more um you know like you know whenever Tech comes along and like creates more it doesn't mean that the more gets uh you know equitably and uniformly distributed um and and like I think they're real conversations to go have about that but like what you do want is the more uh and you want it to you know be directed at things where like we we're just having a tough time right now um yeah like I I'll tell this uh you know story that I've I've told a couple of other times recently but you know my mom uh like I grew up in rural Central Virginia my mom's 74 years old and like she's suffered from this thyroid condition called Graves disease for 26 years uh and so you know when you have Graves disease your thyroid is hyperactive you're generating too much thyroid hormone and so they go in and like irradiate your thyroid gland uh to reduce its activity level and then you take hormone uh replacement therapy to like upregulate your hormones for the rest of your life and so she was having some blood pressure issues and her doctor you know dorked around with her dosage of this hormone medicine and then like she just had like some serious health issues as a consequence of that uh that landed her in the ER in you rural Central Virginia like six times like in a pretty yeah short number of weeks and yeah the the intro interesting thing there was the first time she went to the ER like she was presenting all of these cardiac symptoms and it was pretty clear they hadn't even read her chart like it hadn't registered on them that she had Graves disease and like the thing that they needed to do was like go order a TSH panel to see what her uh like thyroid hormone levels were uh like if they done that right away um like they would have said okay like we got to go you know adjust your medicine um yeah and like I I'm not even ascribing ill intent like you know this is a a Health Care system that is egregiously overburdened like this is not a place where you know like you've got this influx of you know Talent like coming into you this part of the country and and like it's a lovely part of the country like I I I I like love it um you know so like I'm I'm not criticizing anything it's just you know they they have an aging population and they don't have enough like young people coming in to be things like doctors to like help this Health Care System keep up with all of the challenges that they have if some of those doctors had had access to GPT 4 uh and like it was an approved you know product use all they would have would have needed to do was like put the symptoms that she was presenting in and her medical record and it would have said hey she needs a TSH test and if you would put the TSH test result in the recommendation it would add was like look at the dosage of her uh hormone replacement therapy that she's on and like if if like this isn't theoretical like I I I did this um like it it could have helped alleviate a massive amount of her suffering right now and like I think the only reason like that she got out of this tough situation that she was in was I had to intervene like I I sent her to a specialist that was you know 400 miles away that she couldn't have gotten into without you know special you know and like it's it's it's ridiculous like you there there are so many 74 year old old southern ladies or old Midwestern ladies or like folks who are going through similar sorts of things who do not have someone who's going to go in and intervene on their behalf who are suffering unnecessarily because we're not even deploying the technology that we've got right now uh and like it's just going to get better and so like that's the thing that I'm excited about it's like let's let's go let's go give kids a leg up in education let's go fix some of these crazy problems we've got in a Health Care system that is just you know absent technological intervention is just going to get more strained over time um you know let's equip our scientists with better tools so that they can you know find better Carbon capture catalysts so that uh you know like we can design safer modes of transportation so we can more quickly get to a you know like postc carbon economy like it's just so many things we can go do with this stuff um like I'm super super optimistic about it and so you know like just kills me like what we don't want to do is get distracted on uh you know like things that are um just you know effectively noise uh in the ecosystem right now um and you know we we're we're getting so sideways sometimes with um you know like that this model like said something that hurt my feelings or you know and like and I don't I don't want to dismiss like you know people's feelings matter and you know I'm not trying to be a jerk here but I I do want to make sure that as we're thinking about how we develop and deploy the technology that we are always remembering like what the cost of not deploying the good is um because that that is a high high cost mhm very well put yeah yeah here s probably a good note to end on I think well we have we have one other question that we like to ask people on it's a quick one so I'm gonna ask I'm GNA ask you this one who do you admire most in the world of AI you know I I I was I was thinking about this so I I think it's uh Ray salamov um who was one of the one of the folks who was at that Dartmouth Workshop in the in the 50s where you know um Marvin Minsky and Simon and uh you know like a whole bunch of folks convened that summer and like they were all interested in machine intelligence and they uh they coined the the term artificial intelligence at that workshop and like the reason that salamov is so interesting and like not not many people I think know who he is outside of computer science is like he was the one from the very beginning who was pushing on this whole notion that probabilistic methods were going to be very important for the development of AI and you when when I was in grad school in the 90s um the prevailing uh academic theories about how we were going to get to AI were all about like okay well you know like there there's some magic you know minimalist calculus about human intelligence and like we've just got to figure it out and it's got to be you know rule-based systems and ontologies and you know symbolic reasoning and like a bunch of like stuff where you know we like we do in physics we were going to have to like you know Divine you know the the inherent Simplicity in the system like figure out what the rules are and as soon as we understood the rules like we'd be able to make software emulate human intelligence uh and salamov is like no no no like this this is just a intelligence is an extraordinarily complicated phenomenon uh and like the only way that we're ever going to really get there is modeling it with uh probabilistic methods and he was right uh and like he was he was judged wrong for a very long time uh um and so I I really admire uh his contrariness um yeah all the way back in the 1950s and like he's stuck with his uh he stuck with his beliefs his entire career and I don't know whether Ry actually uh like lived to see how right he actually was that's a great answer and a great story thank you Kevin yeah you're very welcome thank you [Music] [Music] [Music]

========================================

--- Video 68 ---
Video ID: lXDrpHwcY0Y
URL: https://www.youtube.com/watch?v=lXDrpHwcY0Y
Title: Zapier’s Mike Knoop launches ARC Prize to Jumpstart New Ideas for AGI | Training Data
Published: 2024-07-02 11:00:43 UTC
Description:
As impressive as LLMs are, the growing consensus is that language, scale and compute won’t get us to AGI. Although many AI benchmarks have quickly achieved human-level performance, there is one eval that has barely budged since it was created in 2019.

Google researcher François Chollet wrote a paper that year defining intelligence as skill-acquisition efficiency—the ability to learn new skills as humans do, from a small number of examples. To make it testable he proposed a new benchmark, the Abstraction and Reasoning Corpus (ARC), designed to be easy for humans, but hard for AI. Notably, it doesn’t rely on language.

Zapier co-founder Mike Knoop read Chollet’s paper as the LLM wave was rising. He worked quickly to integrate generative AI into Zapier’s product, but kept coming back to the lack of progress on the ARC benchmark. In June, Knoop and Chollet launched the ARC Prize, a public competition offering more than $1M to beat and open-source a solution to the ARC-AGI eval.

In this episode Mike talks about the new ideas required to solve ARC, shares updates from the first two weeks of the competition, and shares why he’s excited for AGI systems that can innovate alongside humans.

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

(00:00) Introduction 
(01:51) AI at Zapier 
(08:31) What is ARC AGI? 
(13:25) What does it mean to efficiently acquire a new skill? 
(19:03) What approaches will succeed? 
(21:11) A little bit of a different shape
(25:59) The role of code generation and program synthesis 
(29:11) What types of people are working on this? 
(31:45) Trying to prove you wrong
(34:50) Where are the big labs? 
(38:21) The world post-AGI
(42:51) When will we cross 85% on ARC AGI?
(46:12) Will LLMs be part of the solution? 
(50:13) Lightning round

Transcript Language: English (auto-generated)
right now I think the um what I see happening is there's sort of this mythical story of a very bad outcome once we get to like super intelligence right it's a very theoretical driven story it's not grounded in sort of empirical evidence it's it's basically based on sort of reasoning our way to like this outcome and I think the only way that we can really really effectively and truly set good policy is by you have to look at what the systems can and can't do and then regulate or decide make decisions at that point about what what it can or can't do I think anything else is sort of like you know you're you're cutting off potential really really good Futures way too early [Music] hi and welcome to training data we have with us today Mike n co-founder of zapier Mike has recently stepped up to co-found and sponsor The Arc prize which is one of the most unique benchmarks in AI that measures a machine's ability to truly learn things intelligently versus just to parot patterns in the training data we're excited to ask Mike for an update on how things are going with The Arc prize 2 weeks in and to hear his views on why we need radically different approaches and benchmarks to achieve true general intelligence Mike thanks for being here today so we're excited to talk about the ark AGI initiative uh before we get into that I'd love to spend a few minutes on your background at zapier because I think zapier has emerged as probably one of the best examples of what an existing application company can do with the power of AI and the way that you guys were sort of early to that the way that it's now kind of interwoven in the product has been really interesting to watch so maybe can you just say a few words on what is zapier and what has your approach to AI been at zapier yeah zapier is a workflow automation platform we support 6,000 different Integrations things from Salesforce to Gmail to uh G you know any basically S software that you can imagine we connect with and I think the unique thing about zapier is it's intended to be very easy to use for non-technical users so the majority of zapier customers users would not self-identify with being a programmer or being technical being an engineer something like that um even though I I also I'm kind of think of myself as an engineer and I find lots of interesting use cases for zapper but the large majority of our users don't and um I think that's what uh is is quite special about zapier and what why people tend to like fall in love with it is um you know the sort of of power and leverage that you get be as a non-technical user being able to um you know have software do work for you basically and I think that's it in an interesting way the exact same promise of AI right like that's what people want of AI they want software that's just going to do more work for them um and so in in many ways I think the mission of zapier and sort of the mission and purpose of AI intersect and I've been sort of like I guess call me AI curious I've since all the way back to college and I gave like a whole all hands at zappier um kimer what you year but like when the gpt3 paper came out and show that to little company and so i' been sort of tracking and following along the progress um but it really wasn't until I think January of 20122 when the I think it this was the Chain of Thought paper um when that one came out that I saw that and really surprised me because I thought I'd priced in everything that sort of AI language models could sort of do up to that point and this idea of like you know let's think step by step this Chain of Thought technique breaking down these language models as just tools for reasoning in of you know just a One-Shot sort of completion or or chat engine um felt felt um very special and something that I think most people didn't expect that they could do even though the technology had been out there for over a year at that point and so that actually that kind of moment caused me to give up my exact team Ro I was running all of our product engineering at the time and I went back to basically being an individual contributor of the company as an AI researcher alongside my co-founder Brian and um have to talk more about sort of that Journey but uh yeah I think that's what caus zappier to be relatively early in terms of AI what are some of the things that you've put into the product that you're most proud of at zure in terms of AI features you know at this point I think zapier is pretty I'd say there's like probably two main places where we've gotten a lot of value uh from AI um the first is that over half the company now individual level uses AI on a daily basis and I know this because we're actually measuring zapp's own platform usage of our own company so this is like we have over half the company like building zaps building automations that use an AI step like a chatu PT step in the middle you know either to do so content generation or data extraction over unstructured text um all sorts of really interesting use cases we can sort of talk about in fact one of the top internally use cases is probably getting us about um I think like 100x labor enhancement rate wow which which has been phenomal what is that uh yeah you want to talk about it yeah yeah um I mean HX Improvement we got to talk about that I think it's our it's it's our personal like high water mark for what we've been able to achieve um using AI internally for like an operational perspective and so um zapier has uh uh these things on our website called zap templates they're effectively recipes that help users figure out what can zapier do and and help them get started and these templates are um in order to make them they're all they've all been historically handmade because they require a bit of right brain and a bit of Left Brain uh they're very um you know creative you have to inspire the end user the customer you know would be user of what what could happier do for you what's the outcome what's the ROI you might get um and then there's also a very like technical way they have to crafted and built as well you have the map Json fields from one integration app to another integration app to make sure it actually works and together that's what actually helps users get started and activate the of the product and we had a ho of maybe a million of these things that we knew we wanted to build that we hadn't built yet because they're just they take so much effort the sort of rate of production for a contractors was about 10 a day up until last summer and uh we had a person a member of our I think it was our partner marketing team actually a background by the way in freelance writing who built a zap a system of actually several zaps using open AI some middle steps here built an endend system that whenever a new integration got launched on zapier it would automatically try to like figure out what are the most interesting zap templates that could be built write the inspirational use case behind it because there we tons we have millions of these things already today with lots of training data and then also do the exacting field mapping as well and we moved the human in that workflow from the do Loop to the review Loop so instead of having those contractors now actually generating them thinking really hard about what the use cases should be in building the sort of field mappings um now they're basically reviewing output from from the system in a spreadsheet and saying yes this one is good this one's bad this one's good this one's bad and the funny thing is because the cost of production of generating these things as solo we don't even try to fix the bad ones we just throw them away and say well just generate another stack and throw them on top and so that rate of production is about a th a day now so we've gone from you know 10 a day to a th a day per per sort of contractor and uh we've been shipping away steadily at that whole a million and keeping up with sort of the launch of like new Integrations on app here I think one of the main things that that showed me was um you know probably the space you want to look for in you know businesses if you're thinking about how to deploy I is like really up like top of funnel or bottom of funnel tend to be like you want to get something that's really close to like an important conversion right for your business and then you know I think if you can identify um any manual work that your organization does that has ey volume um where human is doing the work uh I think those are sort of opportunities to like introspect and say okay is there an opportunity to get that human out of a do Loop and and sort of craft a system that can do the work but then put the human in the review lip which is still quite needed at the sort of maturity level of the technology today but um still phenomenal from an RI perspective are there any metrics you can share on the impact zapier AI has had on the overall zapier business the the biggest one today is we're just about to hit 10 million AI tasks per month now I think we're at a run rate for about 10 million AI tasks per month uh and I think what's I I think I I would love to be shown wrong or if corrected if you know examples um but I think at this point zapier might be the biggest automated AI platform in the world in the sense you know ever there's a lot of researchers entrepreneurs Builders who are trying to build these like agentic AI systems where the AI is sort of working without sort of human you know in the loop and um yeah I think at this point with 10 million antit tasks a month we zapper may be the like biggest example of that in the world right now really cool can we talk about rgi Let's Do It um maybe start with a recap of of what is rgi why did you and franois set out to to set establish this priz yeah this was um so a follow on to my like AI curiosity so you know the reason I gave up my EXA team role back in 2022 um was I kind of wanted to know for myself like are we in path r or not I felt like very important to know for zapper Mission but also just like as a human I was very curious and wanted to know like is this going to happen like it you know there there's definitely some interesting scaling that's happening is that sufficient to um like get to I think you know what naively I had in my head as like this you know super intelligence AGI and you know surprisingly what I learned is the answer is no um I Revisited actually got to first know France wet who is my co-founder on Arc prize um I first heard about him and got exposed to his research back during Co actually it was during 2020 he did I think another podcast where he was uh explaining his paper 2019 paper on measure of intelligence where he tried to formalize a definition of what AI actually is yeah I thought it was interesting at the time but you know I kind of parked it there lots of other stuff going on at zapier and doing our own AI product building but as zapier got more into sort of uh building with AI I built my intuition of what language models could do but the apparent limits were I started getting more into AI evals and trying to like understand where sort of the limits were what could we expect from a on a product building perspective like where are where are our products going to like tap out where should we invest our engineering and research effort versus just wait for the technology to keep scaling and mature and um you know the thing I found was that most AI evals were saturating up to human level performance and there was accelerating and when I went back to look at the arc um eval back from 2019 expected to see a similar Trend and um in instead what I found was basically the opposite not only had it not reached Human Performance yet but it was actually decelerating over time and this was like really supremely sort of surprising to me um and uh maybe it's worth defining you know we use these terms of AGI but like what's the actual correct definition for it right I think there's a kind of popular definition in the world today actually there probably two schools of thought I think one school of thought that I see is Agi is undefinable we shouldn't even try um this is a quite popular perspective and I think the other school of thought is um you know uh that AGI is the system that can do the majority of economically useful work humans do this was popularized by open Ai and the Microsoft deal this is actually like in their their deal together uh like once this is achieved like open a retains all the future IP is very interesting I think v no Coastal actually might get credit for coining that definition um but nonetheless I think because of open a success that definition has sort of um become accepted by by a lot of people and as a Target and goal we should shoot for uh the challenge is I think it's it's a fine goal by the way um and I think current model architecture may be within spitting distance of it um I think it says way more probably about like what the majority of humans do for work if it's a true goal than what AGI actually is though and France wad defines AGI as the efficiency of acquiring new skill that's it and here's like a quick thought experiment I think you can use to chort of like kind of grock this is we've had AI systems now for you know many years five plus years that can beat humans at games like go chess poker diplomacy even and um the fact remains that you cannot take any one of those systems that was built to beat one of those games and simply retrain it with new data new experience to beat humans at another game instead what researchers and Builders and Engineers have to do is they have to like go back to zero they have to tear it all down rethink of new algorithms new architectures new ideas of course new training data as well often new amounts of scale um in order to beat that next game and yet this is in complete contrasts to how you two both learn right I could like sit you both down here teach you a new card game in probably about an hour I could probably show you a new board game and get you up to proficiency within a couple hours and that fact is what makes I think it's highly represented what makes you generally intelligent it's your ability to very quickly and efficiently sort of gain skill in order to accomplish some open-ended or novel task that you've like never encountered before in your life and that's what's special about Arc so Arc AGI is an eval that tries to take that definition and actually measure it and it was designed specifically to resist the ability to memorize the Benchmark which is very different from most other AI EV that are out there um every task is completely novel and there's a private test set that no one's seen outside of a handful of people that have like taken it to verify that you know all of the puzzles are solvable um and that degree of novelty and that degree of not having ever been seen before is what makes Arc a really really strong Benchmark for trying to distinguish between you know this more narrow AI that can be beaten largely through memorization techniques and egi which is you know a system that can sort very very rapidly and efficiently acquire this skill at test time what is the definite what is the definition of efficiency I imagine there's a compute component a data component what's the definition of efficiently and efficiently acquire new skill yeah FR uh I'll probably do a bad job trying to like summarize his research if you want to read more by the way his onmeasure of intelligence paper is like the source of Truth for all of this stuff I think it's really really good um and I think one other before I get to the answer I think one other important thing to sort of see is that Arc has been unbeaten since 2019 and I think it's endurance to date is probably the strongest set of empirical evidence that the like underlying concepts of the definition are correct which is why I think it's worth paying attention to and why it's such a special eval and special set of research um so I think for inst why I would sort of describe efficiency as the ability for a system to sort of translate from core knowledge priors to being able to like um attack uh sort of like a an uh the sort of search bace um or task space around it and so a very weak generalizable system is only going to be able to take on sort of very near-term adjacent tasks to sort of the core data the core knowledge that it was that system was trained on whereas like a highly generalizable system is going to be able to have a much larger field of sort of tasks and Novelty that it's able to attack and be able to be able effectively do um with a small set of training data and and that's what we hope to see with the eventual solution for AR AI as well is that you know if someone's able to beat it the goal is to get 85% on the eval um today's state-ofthe-art is I think 39% as we record this and I think what's special is if if someone can actually beat Arc at the 85% um that would mean that you've created a computer program that can um be trained on this very small set of core knowledge priers things like gold directedness objectness symmetry rotation these are these are sort of things that even that emerge very early in childhood development and be able to use those core knowledge priors and recombine them and synthesize them into new programs in order to solve tasks with exacting accuracy that um that system has never seen before never been exposed to in its training data that would be a really really important thing um particularly application layer where like the number one problem today is like hallucination accuracy and consistency and that results in this low user trust which limits deployment of like real AI right now you have some peculiar rules for competing for the price I think there's a limit on how much computer you can use you can't use the internet I don't know if you can use uh gpc4 and Clos models like why set those why put those limits in in place yeah so the two big ones are you're right you so on the um the competition so on kaggle and kaggle enforces no internet and you have limited compute so specifically you get 1 p100 for 12 hours and no internet means you can't use Frontier close models they're you know available through apis like Claude Sonet or gemini or gbd4 or 40 um maybe a little take an order I think the compute one is maybe more interesting the reason for the compute limit is to Target efficiency first and foremost because if there wasn't any compute limit at all then you could simply Define AGI as saying it's just a system that can that can acquire skill with no degree of efficiency attached to it and if that was true that would mean that this system could Brute Force basically every possible program think through every possible future outcome here generate every possible single Arch type of puzzle and use that in order to sort of win the challenge and we know that's not actually what happens in human general intelligence um the thought you can read more in sort of Fran paper about why but the way that I think about it is you know you can think about it you can introspect even yourself while you're taking the arc puzzles and see that when you're trying to solve one of these that you're not brute forcing every possible transformation from the pattern trying to recognize the pattern and apply it to the test instead you're using your intuition you're using your prior experience to try and identify maybe three four five possible possibilities of like what the pattern is and then you check them right in your head and I think this shows that like humans that the sort of efficiency humans have is not brute forcing every possible you know solution and checking it's actually there's there efficiency so the the compute limit um it it sort of forces researchers to reckon with that definition now I think it is worth important acknowledging like we don't know exactly how much compute is necessary to beat arit and we're going to like keep upping the compute bar over time is what what I expect for example we already up it we over 2x it from uh prior versions the competition so I think a prior us you got somewhere between like two and five hours to run on the GPU we we we bumped that up to 12 um interestingly all of the stavr techniques right now are actually maxing out that 12h hour run time as well so I do expect we'll we'll keep continue to increase it over time but but I think it is an important tool in order to like force the generality out of the sort of full solution that we're looking for and then new internet is a little more of a practical reason you know we're trying to reduce cheating reduce contamination reduce overfitting not be able to leak the private test set and largely just increase confidence that when we reach the 85% grand prize Mark that someone has actually beaten Arc um and be able to sort of say that with some sense of sort of authority and confidence that's that's a true statement one of France wm's goals for Arc prize is to establish a public Benchmark of progress towards or or maybe towards AGI or maybe the lack of progress towards AGI and have it be sort of a trusted public tool that you know policy makers you know students entrepreneurs Venture capitalists um you know employees everyone can look at to get a sense of you know how close or or far are we away from this sort of important technology existing and then using that Insight in order to help try to drive more AI researchers to work again on exploring new ideas which is something that's unfortunately kind of fallen out of favor in the last several years as LMS have taken off what what have you seen or maybe what do you expect to be true about the efforts that are successful or more successful toward AR AGI that makes them different from what we're seeing out of the frontier models and the big research Labs yeah so the it gets into like the details of like how does an LM work because that's kind of the B most Frontier AI researchers lives have been taking the last coule years is we're going to scale up language models and that's going to more scale more data is going to get us age out and even though that's the dominant story I actually don't think it's what most of the labs actually believe internally most of them are working on new ideas um so I think there's like an interesting story there but it is definitely in their interest to sort of promote a very strong Narrative of like scale is all you need don't compete with us you know we're just going to steamroll you to see here yeah um I think there's true competitive dynamics that ofers in the market that have that that are unfortunately I think shaping a lot of attention investment um effort away from exploring new ideas and if it is true that new ideas are needed which I believe it is and I think AR egi and Arc price show that like at least some new idea is needed then due to the sort of competitive dynamics that emerged in the market over the last couple years we're kind of like headed in the wrong direction right there's like um you know the all the frontier research has basically gone close Source the gp4 paper had no technical details shared in it the GPD the gemi paper had no technical deal shared on its longer context Innovation things like that and and yet this is like in direct contrast to the history of how we even got here today uh right the sort of innovation set that led the sort of chain of of research that led from you know ilot sequence to sequence paper at Google out to Jacob's University back to Google then to Alec gradford and back to ilot open AI like there's like a six or seven year chain of research that only happened because of open sharing open progress and open science and I think that's a bit unfortunate that we don't we don't really have that right now um again somewhat just due to the market dynamics and Commercial Success of of of language models kind of forcing a lot of that closed frener research up so um yeah one of the goals of our priz is to help counterbalance a lot of those things um you were asking about difference between look like it what you said resonates because it seems like a lot of the foundation model companies are going down very similar somewhat clearly defined paths and I'm sure that internally there's all sorts of work being done to find the next breakthrough in architecture but in terms of what's working today they're all fairly similar paths Y and I imagine though BS yes and I imagine though what works for the sake of rgi is going to be a little bit of a different shape yep and I'm wondering if you're starting to see what shape that may take got it and have a sense for what may be different about this more General architecture than what we're seeing out of the foundation models great so I think um you know a a useful um shortcut on how to think about language models is that they are effectively doing very high dimensional memorization right they're bble to train and memorize tons and tons of examples and apply them in slightly adjacent contexts I I don't want to under I don't want to dismiss language models too much because I think that they are very special something very magical and something very that has lots of economic utility zappier is an existence proof of that fact alone so um you know I I don't want to like um throw it under the bus too much I think there's some like really good things that it has like unlocked um uh as a technology goes but there are limits to it and you know there are the the sort of limits are um you know not being able ble to effectively leverage its training data to compose it or combine it at test time to go attack and like accomplish novel tasks that had never seen before in its training data that's what Arc sort of shows right is that this is like a skill that these language models don't don't sort of possess I think it's kind of maybe um useful to look at the history of the high score so far and maybe where where we expect it to go so from uh when when the eval was first introduced in 2019 2020 the first there was a small cgo competition that ran to kind of get a B Baseline when it was 20% and from 20% to 30% the uh techniques that worked were effectively um researchers cre crafted a like handcrafted domain specific language by looking at the puzzles that were in part of the public test set there's there's two test sets there's a public set and then the private one that's the sayate of the art is measure on and they looked at the public Des set and they try to infer and write down programs in Python code or C or whatever uh what are the like individual Transformations that like you do in your head to go from the you know one puzzle to the next and so that they called this a DSL and then they wrote a Brute Force search to try and search through all possible like permutations and combinations of those subprograms order to like find the general pattern and then apply it at real time and that got to about 30% what's gotten from 30% up to close to 40% now is a slightly different technique um this is uh Jack Cole and his approach is effectively using a code-based open source language model and doing test time fine-tuning so he has some pre-training down on the code gen model and then at test time taking the the puzzles they get the novel puzzles that's never been seen before and perating variations of it and then training this like cod genen based model in order to write that program then and find a program that fits the pattern and then apply it a test time and that's gotten to 40% I suspect that we probably have I bet we have the ideas in the air already to get to like the 50% Mark May maybe even a little beyond the 50% Mark without a lot of new innovation I bet just ensembling or combining the sort of existing idea sets that have already worked towards AR probably gets you about halfway um I think to get to the 85% Mark in or Beyond I think it the solution the ultimate solution probably looks more like the shape of um at least to solve Arc something that looks like a you know a deep learning guided DSL generator where you have some sort of instead of hand coding and hand crafting the DSL um like ahead of time by trying to infer from the public test set what those like sub programs would be you need would it generate that DSL dynamically right by looking at the puzzles um in real time and being able to learn from past puzzles and apply that towards future puzzles this is also another important thing humans do when they're going through the arc set sometimes the first or second puzzle are actually a little trickier because you're orienting yourself around what am I doing what task am I doing what does the possible solution space look like and then as you get further into the task set they tend to get a little easier because you've they start you know some of the rec the the sort of space of possible Transformations is just finite so you start kind of recognizing patterns there and then combining that with some sort of Deep based program synthesis engine something that can um not bir force all possible programs of how to combine those dsls together but something that has some sort of you know deep learning approach to like shape which program traces do you try to generate or test and try against the pattern and then again it kind of goes back to this human introspection of how how how we take the puzzles which is we're we're not brute forcing all possible you know programs in our head instead we're trying to identify just a handful of likely candidates and then testing those deterministically in our head and app finding the one that works it's really interesting that code generation and program synthesis kind of underlies all of the methods you just talked about um it's and there's something something very special there like program synthesis is very general allows you to actually get closer to that definition of generalized intelligence that you mentioned at the beginning it's very exacting and I think this is one of the reasons why the solution to RI is going to be useful um very quickly so uh there's um we were talking this before there's a history of like toy AI benchmarks over the last 10 15 years that you know kind of looked like Arc there were games there were puzzles and really never amounted to much in terms of being beaten they they all got handily beaten as sort of scale emerged and um you know they they really didn't add to or our understanding of how to build useful AI systems and so what's one of the common questions I've been feeling the last couple weeks is like what's different about Arc what is is that you know isn't that likely to just happen here again and I think the reason why we're likely to see something much more useful assuming we get like a really good solution Arc um from the sort of first you know grand prize win is that the number one problem at the application layer and this we see this with zpp here too with our new AI Bots that we launched a couple months ago um been surprising to me in how that has gotten adopted actually by our users um you know there there's like how I kind of describe it there's a lot of concentric rings of use cases that you can use ai ai automation for and what we're seeing is people are sort of restricting the use cases for the AI Bots where they're sort of fully automated totally hands off to the use cases where there's um a sort of a very low need for user trust or where where the sort of let say that different way if um if it goes wrong it's not catastrophically bad uh so they deployed for use cases like personal productivity or team based like workflow automation things where you know if it's wrong or it's right only nine out of 10 times or it takes me you know maybe a couple days to like really work with the system to like do the prompt in engineering to steer it towards getting maybe 95 99% reliability that's acceptable because the risk of being wrong is just you know quite low in order to get much higher up and expand the number of concentric rings to you know moderate risk to high risk deployment scenarios where we want these systems working autonomously we're going to need that the main thing that is missing is user confidence in the exacting nature of what it can do and what it can't do um this is what zapier like core classic gives us right is like it's a terministic engine to execute automation so you know once you build it and set up it's going to do the exact same thing every single time but that's also what makes it fragile and hard to use and on contrast you know these like AI Core based LM systems that are totally autonomous have the opposite set of trade-offs right they're much easier to use you can steer them guide them and fix them entirely through natural language but because the accuracy is still inexact confidence is low and I think that's what Arc gets us a solution to Arc at 85 or 100% means that you've written a again you've written a computer program that can generalize from like very simple core knowledge priors to Sol with exacting accuracy 100% reliability these like sight unseen puzzles and I think that that tool as a for that will be a new tool in like the programs toolkit basically in terms of building products and Building Systems that can achieve that same thing we're two weeks in I think to when you launched Arc AGI prize um what have you learned so far like what what types of people are working and and competing on this is it like the pedigree researchers are the big Labs is it Scrappy Hustler types like who's competing how many teams are submitting Solutions yeah let's see so um the the response by the way after launch was phenomenal was much bigger than we expected uh I think we had we were like trending on Twitter twice during the launch week the number one kle competition in the world over a million social views I think over all the launch channels so just very phenomenal I'm really thankful for everyone who helped sort of promote and helped share Arc um hopefully we actually like can get a solution here in in some some short time um I think that like the most interesting thing about the folks that are working towards there probably um a historical answer here and then what I've seen over the last two weeks so the historical answer is most of the people that have worked on Arc are Outsiders to the field they like um this is not actually the first year that there's ever been a contest about there was a past competition called arthon it was much smaller it was hosted of a um uh this lab 42 AI lab in Switzerland um and so last year there was actually 300 teams that worked on trying to beat Arc and again no one sort of Beat It and almost to my knowledge all of those teams were a effectively individuals or effective you know Outsiders in some way they're not you know people at like big AI Labs they're folks with backgrounds and you know engineering mechanical engineering or video game programming or physics um folks that just kind of like got curious and interested in the problem at hand and and I actually think that's more likely than not where the Breakthrough for Arc is going to come from is I think it's going to come from an outsider somebody who like some of you just thinks a little bit differently or has a different set of life experience so they're able to like you know cross-pollinate a couple like really ideas across Fields um that's one of the reasons why um I I put as much money as we did at AR priz I felt like the progress was idea rate limited actually and one of the best ways to sort of increase the amount of ideas is to try and blow up awareness which is what the the launch kind of did over the last two weeks the um I've kind of seen like two probably like camps of people at least on Twitter emerge I think there's one Camp of people who are sort of the uh um you know they're in it for the mission they they they they agree with the underlying concept they they think that like we do need to some new ideas they're excited to like try and figure out what those are and then there's like a second group of people that are sort of like I'm going to prove you wrong LMS are definitely enough scale is definitely what we need and I'm going to do my best to go like feed this Benchmark just using like existing off-the-shelf technology and uh and sort of prove you wrong so I'm I'm actually quite happy for both those games to exist uh I one of those approaches is is currently up in the leaderboards right um so uh yeah we can we can break break some sort of news here so we're uh this week this Thursday we're launching or I guess when this comes out have launched just a couple days in the past a brand new public task leaderboard so we talked about how um Arc doesn't allow internet access and there's compute limits I know personally how unsatisfying that is to not be able to use Frontier models though I also want to know how good can GPD 4 how good can cloudon it like do against this Benchmark um and also because um you no computer internet kind of also is a bit of a barrier entry right you have to use open source models you have to do um just like quite a bit of intering work have to do before you can even start just testing and experimenting so we're goingon to be we're we're launching a new public task leaderboard it's going to be a secondary leaderboard we're going to be committing about $150,000 for a reproducible fund towards this secondary leaderboard it won't be officially part of the competition this year um you know we want to incre we want to like maintain that aspect of like Assurance on you know cheating and contamination over fitting with the private test set and that's also the test set that has sort of the most empirical evidence against it over the last four years but the secondary leaderboard is going to allow folks to um basically submit scores towards against the 400 public task set and we'll verify reproduce the scores locally to sort of Ensure good like fitting with um with the approach and and we'll publish that and you're right the the I think the top scor one of the top scores on that is this guy Ryan greenblat he came out a couple days after um the competition launched with a pretty interesting novel approach actually towards beating it and he's using gbd 40 and but not not just gbd4 I think the interesting thing is he's he created a like an outer loop around 40 where he is using 40 to generate programs or sample from GPD 40 these like program these reasoning traces to um beat the task or identify the patterns then testing these patterns against uh ATT against the demonstrations that then finding the one who works on applying it and that's that approach seems to be getting in the like uh 40 like low 40s maybe 40% 41 somewhere in that range and um it's pretty interesting because I think it's you know someone might look at that I think and it sort of at first blush say well isn't that evidence and skill is all you need and I do think there is something interesting there right it's like it's showing that hey the more training data these things have the more sort of um you know programs that they can spit out that might be kind of right uh but it also shows that I think that new ideas are needed still uh like this outer loop is novel like that might actually be Frontier llm reasoning that Ryan published and we're going to make all the approach whenever we put um um similar to Arc prize we're going to like open source all the code for all the reproducible solutions so folks can can take these and apply them and try to reproduce them and sort of using private close or open close open uh open source models for the sort of the closed private data set um yeah I do think it's pretty interesting uh what how much Innovation you get when you um how much ination we've gotten over the last few weeks is just a result of putting even just the awareness against the public test are the faults at the big research Labs like why are they not working towards this Benchmark because it almost like when you explain the Benchmark it's seems so clear that obviously this is the thing you want to solve you don't want to solve the memorizing the textbook um use case why do you think the folks with the big research Labs aren't trying to sell this Benchmark or are they so I am aware of a handful of big a Labs that have tried in the past um seever several years ago so like you know this was perhaps at smaller scale with weaker models and things like that I would uh one of the things I would hope is that actually more do in the future actually um love to see if there was if we could make ARG AGI an actual measure on some of the model cards that get reported against future models I think that would be like a really cool thing we're willing to do it so if anyone is listening to this and wants to reach out and make that happen uh more than happy to work with them and find some way to do that um if I had to guess well let let me say let me not guess unless say what I have more sort of confidence in um I've been surveying once I got exposed to Fran's work again and was sort of for deep think deeping thinking deeply about rgi I started serving a lot of my like friends and researchers in SF in the Bay Area about had they heard of France SW and had they heard of Arc and France is pretty good name recognition because he's like really big on Twitter um been big on Twitter for many years probably nine out of 10 people I talked to like knew who France was was maybe one in 10 two in 10 had heard about the arcal and uh probably half those were confused because there's like five other AI evals called Arc uh and I had to like sort of do some uh so I had to like you know disambiguate uh with them so it had really low awareness this was one of the first things I asked Fran about when I when I met him for the first time in person this year I asked him why do you think that is why do you think you have such high awareness but Ark has such low awareness and as the answer effectively was that it's hard you know the way that benchmarks um G like gain popularity and notoriety is we make progress towards them right researchers um are working against it some has an idea they have a breakthrough they publish that in a paper that paper gets picked up and cited by others other that generates awareness and attention other researchers say o interesting okay something might be possible now on this like really hard Benchmark and and so you get the Snowball Effect right of like attention and because Arc has sort of endured with very low rates of progress in fact decelerating progress over the last you know four years um I think anyone kind of in a lab looking at that would just say well maybe the time's not right for it yet maybe we don't have the idea set in the world Maybe we don't have the scale we need yet in order to sort of beat this thing and it looks like a toy and it doesn't like you know I don't fully understand why I don't get the necessary import how it's qualitatively different haven't just spent that much time we got a million other benchmarks I could use and um you know I I think that's some of somewhat of the dynamic that has existed in the past and and is one of the again reasons why we launch dark prize right I think there there are some there's lots of like Market um tools you can use to like shape markets and shape Innovation and I think prizes do have a narrow uh there's a narrow spot where prizes can be like outrageously effective and it's where like the idea is small and it's and it's like idea rate limited one person or a small team can make that breakthrough and it's very quickly and easily inspectable reproducible and like build and you can build on top of it rapidly and uh so all those sort of boxes got checked and yeah one of the reasons why I uh decided to go price and well and you've mentioned curiosity around AI or AGI dating back to college that was sparked a few years ago in the context of zapier and has kind of been nurtured ever since um Beyond The Curiosity I'm curious why this is important meaning if you could paint the picture of what life looks like for the world post AGI yeah where we've defined it as the ability to efficiently acquire new skills um what do you think that version of the future looks like like why is this an important thing to to solve the thing that I I feel like I have a unique insight into at this point having spent a lot of time thinking about Arc and this AGI definition is I suspect the Advent of AGI is going to look very differently than most people expect especially of the group who are in the camp that like AGI is undefinable because it's so mythical and you know scary or big or awesome that like we can't even hope to ever Define it it's just going to be this like magical special thing and you know it turns out like you know something that I I believe quite is the definitions are really important because definitions allow us to create benchmarks and benchmarks allow us as a society to measure progress and set goals towards things that we care about and want to happen and this idea of efficiently acquiring skill one of the you we've talked about a handful of times today but one of the direct near-term things that you get from that is you get systems that can do exacting accuracy generalization from a small set of core priors and apply towards novel solutions that is again the number one problem that raate limits AI adoption for more real world use cases today and so you that's the that's what you're going to see you're going to see B basically like the application layer of AI get like amazingly good at accuracy consistency low hallucination rates which is going to allow us to use it in in a much more unfettered way in a much more trusted way because we because of because of like the underlying way that in which it's built um so I I think that's like an the reason I think that's important I think that's the reason why I think that's important is you know I think there's a lot of we don't know what that set of capabilities is going to build on top of into the future right there's lots of unknowns I think of how AI AGI evolves beyond the actual Inception moment of a system that can efficiently acquire skill but I think it's going to be a much more gradual and incremental roll out where there's a lot of contact with reality as we build an Eng these systems which is going to give us as like a society a lot of time to update based on what those capabilities what what it can do what it can't do and make decisions at that point about how do we want to like deploy this technology where might we as a society say we don't want to deploy for this set of use cases um you know I think that's one of the reasons why um I've been so sort of such an proponent I think of of uh open source AGI progress with with Arc prize is like right now I think the um what I happening is there's sort of this mythical story of a very bad outcome once we get to like super intelligence right it's a very theoretical driven story it's not grounded in sort of empirical evidence it's it's basically based on sort of reasoning our way to like this outcome and I think the only way that we can really really effectively and truly set good policy is by you have to look at what the systems can and can't do and then regulate or decide make decisions at that point about what what it can or can't do I think anything else is sort of like you know you're you're cutting off potential really really good Futures way too early and and that's sort of what's happening I think with a lot of this early AI regulation where um you know I'll try to you know be be the paint the good side of this picture it's like hey I care you know maybe the risk of this bad outcome is so high in the future that we should like pause here um I think the the risk of that is you you've like trimmed off every possible possible good path of the future way too early and the reason this way toly is because we still need new ideas um we we we need new ideas from researchers we need new ideas from students we need new ideas from young people we need new ideas from from Labs um otherwise we're actually there's like like a chance that we've like never actually the like high degree of useful AGI that we that we actually want um and so that's kind of my Nuance take I think on like probably what the like Advent of AGI looks like I think it's much less likely to be like a moment in time and much more likely to be a stair step of technology that we build on um on top of past technology and that creates a lot of moments to sort of update beliefs based on what it can and can't do do you have any predictions on when we'll cross 85% on Arc price you know before the competition started my uh there was a the first data scientist we hired at AB here um gave me this idea a long time ago I stuck with me he he said um the longer it goes the longer it goes and so it's this idea that like the longer something takes the more you should update your prior about it's going to take longer to be so coming into this year my expectation was like hey at least three or four years probably before we get to the grand prize Mark based on sort of like past um based on the past track record uh having seen what we've seen over the last like two or three weeks weeks though uh I think it is quite likely we get to 50% during this competition period um I would be very surprised and I I I'd be surprised in a good way if we actually get to the 85% grand prize in this competition period um but I think it is uh not unlikely that we crested the 50% Mark before um the uh the end of or middle of November which is when the contest period ends for for 2024 and is there a good why now because people have been trying at this for five years now and and you know you're you're galvanizing interest around it and now you know a lot more researchers around the world are interested in Ai and solving hard problems um but is there a good why now in terms of enabling techniques Technologies Etc that's different now than than five years ago when when fris first kind of defined The Benchmark if it is true that um deep learning is an important part of the solution right uh a deep learning guided program synthesis engine or a DSL that is generated on the fly through deep learning technique if that's it's true the world has a lot of experience now on building and engineering and scaling such systems over the last three or four years and there's a lot more compute online which brings the cost down into a territory where some of these things may have just been out of practical cost before um for example actually Ryan Green blats Solution right now is uh maxing out our cost limits we're going to have against the public leaderboard cost $10,000 to generate the 8,000 reasoning sample traces from GPD 40 that he then deterministically checks and so that would have been a technique that would not have been possible you know three or four years ago in in anywhere so I think if it is true that there's amount of like a minimum amount of scale that's necessary to be Arc I think hey we've gotten more of that in the last three or four years than we had when the first comp just ran and then I think the other way now is just um can't largely due to awareness uh if if it truly is um actually let me answer the opposite like I think the risk that it is uh the reason we launch dark prize is that it is actually not now right it's like actually it's not going to happen is the problem um it's not a why now it's not like oh the ideas are in the world we just need to like get people to work on it the the risk is that it is it's actually not why now um and not why now is I think a much more interesting story right around this like LM driven you know uh focused attention on sort of LM Solutions only the closed research due to the competitive Dynamics all of these things have like shifted and shaped attention away from the new ideas and towards scale towards LMS towards like you know um face yeah application lay Ai and I think that we think we need some shaping reshaping back towards the new idea set so hopefully uh the why now is because Arc has now lots of attention to be seen do you think llms will be part of the solution I'm curious what you think of it seems like in in the big research Labs right now a lot of the frontier researches around let's merge kind of llms with the insights that you get from the inference Temp comput and the qar alphao stuff I'm curious what you think of that kind of direction of research I there's some like pretty interesting research I've come across with like Transformers that the Transformers are capable as an architecture of representing um very deep deductive reasoning change with 100% accuracy I think this is interesting the challenge with them is actually we just don't have the learning algorithm back propagation is an ineffective learning algorithm in order to teach a transer architecture a set of network weights that can do deductive reasoning with 100% accuracy and so I think it's possible that the systems that we kind of or at least the Core Concepts that underly language models have sufficient capability in order to do this type of reasoning and we have not yet discovered the like algorithm that can train the model in the right way we haven't quite discovered the right outer loop around the Transformer that is going to do the program synthesis engine or the DSL generator um I I I feel more confident in saying that like deep learning is almost certainly going to be a part of the grand PR in some way I bet it's it won't I'm pretty confident it wouldn't be just like a you know a pure deterministic program is going to be how we how it gets solved um I think Transformers are effectively the technology that has the most has the highest degree of awareness on Research like literature there's a lot of Hardware now that's going towards accelerating Transformer based models I think actually just uh earlier there was like an Asic that got announced recently that's trying to accelerate like the transform architecture so to the degree that actually like some degree of like scaler computer is necessary to be Arc I think those are like things that I would say I'm bullish on sort of the transer architecture though I would point out that the um search base of alternative architectur is quite Rich right we've had maybe like nine or 10 now mainline architectures from Transformers to lstm CS CNN RNN xlsm State space models um this would suggest that the search Bas of those architectures is quite Rich actually and they all have like slightly different varying properties um so uh like I think it's certainly possible that someone comes up with some Innovation there um I'm less confident or like bullish that llms in their exact form are going to be part of the 85% solution though I would think it probably like a subcomponent of the architecture instead of the entire application system itself when somebody does ultimately hit the 85% threshold what do you hope they do with the solution like what would you like to see out of that person other than submit it to the leader word so I this one thing we didn't talk about a ton but um you know one of our prizes goals is to accelerate open progress towards AGI so we're going to be requiring that in order to claim the prize money you do have to publicly share and reprod publicly share reproducible code and put it into the public domain and this is this goes for both the public leader board and the official competition leader board as well um you know this is with the spirit of trying to re accelerate open progress again so that we have research in small ways out in public that other researchers can build on top of and hopefully stair sear we actually actually building Ai and not getting stuck in sort of the plateau that we're that we're in today um so I think that's probably my first B C I've actually seen a handful of people online that have said hey if you got a soltion in rcgi uh you know I'll give you this like you know million dollar offer my company yeah exactly which you know I on one hand I'm like okay that's like kind of interesting um but you know on the other hand I'm kind of like you know I think that's like great awareness and it shows actually like the importance of of solving this um I think more people are starting to become aware of the lack of progress lack of Frontier progress I think Arc is kind of becoming a lightning rod for um folks that want an actual measure towards this I think growing sentiment in the field today should we close out with some rapid fire questions yeah let's do it okay who do you admire most in AI I mean Fran is a bit of a cop answer I would have go found a dark prize if I didn't admire and respect his his work over the last four years I mean I think the two people that I have learned the most from like directly and have inspired my own belief and work um Brit Sutton and Chile both of them published papers in 2019 right Rich Sutton published The Bitter lesson which I think is fairly well known now in the industry at this point um I think his idea that is quite right there with maybe the one Aster that um uh you know the uh the one aspect that has not yet had skill applied to it is architectur search itself um you know we've certainly have unbiased search and learning applied on the INF side in the training side but uh every architecture still has a very human handcrafted story and journey to it which uh is an important Insight I think about that and then un measure of intelligence from cholet in 2019 and I think both of these papers are or in like or I guess maybe um sence was more of a blog post but both of these pieces of writing I think are very important because history is proven them right as time has gone on um language models Transformer scale has sort of shown sat's ideas to be even more true than they were in 2019 and I think the endurance of Arc has shown francois's definition of AI to be um more and more true as time has gone on what is your most contrarian point of view in AI I feel like everything we've talking about today most people don't agree with me hzo uh all right we'll count it I like um scale is not all you need well new ideas are needed what's your favorite AI app other than zappier uh let me look and see what do I have I have a handful I think I'm not going to like surprise you with anything so I've got chat TBT perplexity and Claude and I'm a pain user of all three of those Services you know um one interesting thing actually I'll add I have gotten way more value out of language model based tooling over the last like call it six months than um I ever did in the first aspect when I was starting to start work on it at zapier and it's because one of the things are really really good at the thing they're perhaps like best at is summarizing tons of unstructured text and helping be like an educational tutorial for you so it's significantly like ramped up my learning rate on actually building with AI build like learning like these fundamental different architectures um you know starting to actually do like model training something app really hasn't done but I've started to do myself over the last six months to get you know a sense of of of that type of work and yeah language models and AI tooling has definitely accelerated my learning process awesome all right last question let's do something optimistic something that we can all dream about what change in the world are you most excited to see over the next five or 10 years as a result of AI I think the I've always wanted to like live in the future I think that's maybe a something that has always driven me towards like working on like Frontier Tech I've always you know bought the latest gadget I always tried the latest app I think it's led me to work on zapier and and Ai and um it's one of the reasons I'm working AI right now is because I think it's like the biggest thing you can I can potentially have an influence on trying to pull forward pull forward that future you know I personally get I I think like one of the things that feels very limited from AI right now is that with the narrow form of AI that we have if we never get to AI what that will mean is that we will always be rate limited on developing things by the human that's in the loop and that means we'll never have ai systems that can invent and discover and sort of innovate alongside humans and really help pull forward and push forward the frontier in I think a lot of really interesting ways like I you know understand more about the universe and vent discover new pharmaceutical things um you know and discover new physics um discover how to build AI you know I think we're always going to be rate limited by the human today and I think if you just sort of care about living in the future and you want to pull forward the good aspects of the future some form of AGI is necessary to do that awesome thank you Mike thank you both for having me [Music] [Music] [Music]

========================================

--- Video 69 ---
Video ID: BDgGDCQX3G4
URL: https://www.youtube.com/watch?v=BDgGDCQX3G4
Title: Factory’s Matan Grinberg and Eno Reyes Unleash the Droids on Software Development | Training Data
Published: 2024-06-25 11:00:08 UTC
Description:
Archimedes said that with a large enough lever, you can move the world. For decades, software engineering has been that lever. And now, AI is compounding that lever. How will we use AI to apply 100 or 1000x leverage to the greatest lever to move the world?

Matan Grinberg and Eno Reyes, co-founders of Factory, have chosen to do things differently than many of their peers in this white-hot space. They sell a fleet of “Droids,” purpose-built dev agents which accomplish different tasks in the software development lifecycle (like code review, testing, pull requests or writing code). Rather than training their own foundation model, their approach is to build something useful for engineering orgs today on top of the rapidly improving models, aligning with the developer and evolving with them. 

Matan and Eno are optimistic about the effects of autonomy in software development and on building a company in the application layer. Their advice to founders, “The only way you can win is by executing faster and being more obsessed.”

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

(01:36) Personal backgrounds
(10:54) The compound lever
(12:41) What is Factory? 
(16:29) Cognitive architectures 
(21:13) 800 engineers at OpenAI are working on my margins 
(24:00) Jeff Dean doesn't understand your code base
(25:40) Individual dev productivity vs system-wide optimization 
(30:04) Results: Factory in action 
(32:54) Learnings along the way 
(35:36) Fully autonomous Jeff Deans
(37:56) Beacons of the upcoming age
(40:04) How far are we? 
(43:02) Competition 
(45:32) Lightning round
(49:34) Bonus round: Factory's SWE-bench results

Transcript Language: English
I would have thought this would be different 13 months later, but this is still very much the case where agent is synonymous with unreliable, stochastic, demoware, vaporware, and I think something very important for us is we want to build these systems that aren’t just like cool examples of what is to come, but rather valuable today and not just valuable for like a hacker on a side project but valuable to enterprise engineers today. Hi, and welcome to training data. We have with us today Matan Grinberg and Eno Reyes, founders of Factory. Factory is building autonomous software engineering agents, or “droids,” that can automate everything from the drudgery of maintaining your documentation to actually writing code for you. In doing so they are building the ultimate compound lever. Last week, Factory also announced some impressive results on the key AI coding benchmarks SWE-bench, beating state of the art by a wide margin. Stay tuned at the end of the episode for more context on how they built it. We’re here with Matan Grinberg and Eno Reyes, founders of Factory. Gentlemen, thank you for joining us. Thank you so much for having us. Yeah, thanks for having us. Let’s start. Let’s start with a little bit of personal background. And Matan, maybe we’ll start with you and then go to Eno. So Matan. One thing that I believe you and I share in common is an affinity for a well-executed cold call. I know at least two cold calls that have had some bearing on your life. Why don’t we start with the one that you did, as an undergrad at Princeton, to somebody who my partner, Shaun Maguire tells me is quite a famous physicist. Can we start with that cold call? Yeah, absolutely. So while I was at Princeton, I was studying string theory. And the most famous string theorist happened to be working at the Institute for Advanced Study, which is an academic institution, right next to Princeton University, but not technically affiliated with it. Part of the allure of going to the IAS is that you don’t have to take on graduate students, much less undergrads. That said, you know, there’s a professor there, Juan Maldacena, who is, you know, by far the kind of leader of the string theory movement. And, you know, being a young, ambitious undergrad, I, I decided, you know, might as well see if I could snag him as an advisor. And so with some advice from some graduate students, I sent him an email, asked if we could meet. And the thing about Juan is the way he works with people, you know, he’ll take a meeting with anyone, basically, and will spend about two hours at the chalkboard with you. And in this two hour chalkboard session, he’ll subtly drop a problem that you basically have 24 hours to solve, and get back to him with the solution. And then you’ll officially, you know, be a student of his. Luckily, I was warned about this, about this rite of passage. So yeah, I was paying close attention to any hints he was dropping. Indeed, yes, yes. So I found the problem, ended up spending, you know, basically the entire night working on it, and, you know, luckily ended up having him as an advisor. We were able to then publish a paper together, which was very exciting. So, yeah. Typical undergrad experience. Yes. Yes, exactly. So there’s, there’s a second cold call that I want to ask you about. Before we get to that, why don’t we go to Eno, so Eno, you similarly went to Princeton, you have a CS degree from there, you spent some time as a machine learning engineer at Hugging Face, which is where we first intersected spent some time at Microsoft. But like a lot of great founders, your story before then started with some humble beginnings. Could you say a word about the stuff that doesn’t appear on LinkedIn that has helped to shape who you are today? Yeah, absolutely. And I think that, you know, my family on my dad’s side came from Mexico in the late 60s to San Francisco. And my grandparents were both working for a bit. But when my dad was born, they started a Mexican restaurant in Los Altos. And that was in the 70s. They moved it to Haight and Cole in the 80s, and were a very kind of San Francisco immigrant story. They actually ended up leaving, to Georgia, where I grew up. But really, I think it's the drive that they had to give my dad a successful life in America. And it was my dad and my mom that drove that same kind of mentality and to me growing up, and I think it’s really cool because this story is one that I think a lot of Americans share and something that makes it really exciting to be back in San Francisco, kind of building, building something to potentially make the world a better place for everyone. Very cool. That is the dream. Matan, I want to get back to that other cold call, because I think it leads directly into the founding or the forming of Factory. So our partner, Shaun Maguire, who I mentioned earlier, who I believe shares a similar academic background to your own, Ireceived an email from you a year or so ago, that led to a walk. And very shortly thereafter, Factory was formed. So I’m curious. What caused you to cold call Shaun Maguire and this is less of a Shaun Maguire question because we know plenty about Shaun Maguire. This is more of like, you’re on a very good path, you do really good research, you’re on track to get a PhD in physics. And something inspired you to go in a different direction. And I’m curious, what was it that inspired you that led to that cold call? And maybe tell us a quick story about what happened shortly thereafter? Yeah, absolutely. So, you know, I was like you said, I was doing my PhD at Berkeley. About a year in though I realized that I was only doing theoretical physics, and string theory, because it was hard. And not because I actually loved it, which is obviously a bad reason, bad reason to do anything. And, you know, I had such tunnel vision on this path that, you know, when I came to this realization, it was kind of earth shattering, and I looked at the paths ahead of me. And there were basically three options that seemed realistic. And so it was either going into quantitative finance, going into big tech, or going into startups. And by this time, I’d already kind of switched my research at Berkeley from being purely physics to ML in physics, and then slowly more ML, and then mostly AI. So it was kind of quickly cascading there. At the time, I think I saw a video of Shaun speaking, I think over Zoom to some founders at Stanford or something. And I recognize his name from string theory research, because I had read his papers way back in the day. And it was particularly shocking to me, because I’m not sure how much time you spent with string theorists. But normally, they’re quite introverted. Not, Not, Not... not the most socialized. Yeah, and so Shaun is, you know, this very different example. And so, to me, I kind of like, like, I looked at his background, and it was just it was just shocking to see someone who was like, so deep and like a bonafide string theorist, then go in and like, start his own companies, invest in some of the best companies, join Sequoia and be, you know, a partner there. And to me, it was just like, Oh, my God, this seems like someone who is of my kind of background, of my nurturing, I guess. And so I sent him an email. And I was just like, hey, you know, we both were string theorists, I don’t want to do string theory anymore. I’m thinking about AI, we’d love to get your advice, like you mentioned that then turn into a walk, it actually was supposed to be a 30 minute walk. We ended up going from the Sequoia offices in Menlo Park, all the way to Stanford, and then back. And so it ended up in three hours. He missed a lot of his meetings that day. So it was pretty amusing. And basically at the conclusion of the walk, he said, the one thing was for sure, he was saying you must drop out of your PhD, there’s way too many exciting things to do. And he kind of left me with the advice of, you should either join Twitter right now, because this is just after Elon took over. And he was saying, you know, only the most badass people are going to join Twitter right now, too. You could join a company of mine as just like a general glue is what he said, his is Foundry by the way. Yep. Or three, if there’s some ideas that you’ve been thinking about, you should start a company. And I was like, you know, very grateful for all the time that he spent and, you know, we kind of left off there. Beautifully, in parallel, Eno and I had just reconnected at a LangChain hackathon. And he was in Atlanta the weekend prior, and he basically got back the next day. So that next day, Eno and I got coffee, and I think we got coffee at noon. And then basically every hour since then, until now, Eno and I’ve been working together, talking constantly about code generation and what became Factory. Did you and Eno know each other in undergrad? We had like the maximal overlap of mutual friends without ever having had a one-to-one conversation. Yeah, it’s pretty funny. We were in eating clubs at the time opposite from each other. And we had just so many mutual friends and it really wasn’t until I moved to the Bay Area that we had a face to face convo and it was a very fruitful conversation for sure. It was intellectual love at first sight, you could say. I love that and it’s so serendipitous about the LangChain connection. How did you guys decide on, I’m curious, I mean, you’re both brilliant. And I think for a lot of founders starting out in AI right now, a lot of them find it hard to resist the siren call of training a foundation model. So like, how did you decide to, you know, build in the application layer? I’m curious. And then why software engineering? Yeah. So I think from my perspective, like going deep from academia, I think, throughout all the years of spending time on math and physics, the theory of beauty that I learned to be drawn to was things being fundamental. And, you know, spending, you know, time doing AI research, it was so clear that code is fundamental to machine intelligence. And so I was just naturally attracted to the role that it plays there. And I think that kind of joined quite well with Eno’s attraction to the space. You’ve referred to it a couple of times as a compound lever. Can you unpack that for us? And let us know what that means? Yeah, I mean, so there's the famous Archimedes quote about, you know, software, or well, his quote is rather, you know, if you have a large enough lever, you can move the world. And then I think that’s been co-opted for software engineering, right, that software is a lever upon the world. And for us, we see AI and in particular AI code generation, as a lever on software, the impacts of that being, you know, compounding, exponential. And sorry, Eno, I think I cut you off, I think you were maybe mentioning how you got to the founding inspiration for Factory? Oh, yeah, absolutely, I mean, I think Matan story is really indicative of kind of the energy at the time I was at Hugging Face working on, you know, training, optimizing, deploying LLMs, for enterprise kind of use cases, was actually working with Harrison on early like LangChain kind of integrations. And it was so clear that the work that was happening in open source was directionally moving towards modeling human cognition with LLMs, where the LLM was just one piece of the system. The idea of chains, and I think Harrison calls them cognitive architectures and the LangChain folks call it that. And seeing that happening, and seeing that, within the code gen space, the most advanced players were basically looking at autocomplete, it felt like there was a huge opportunity to take that to the next step. And take some of those lessons that were happening both in the kind of the fringe research and open source communities and applying them towards a kind of massive organization. I realize we haven’t set explicitly yet, what is Factory? So Matan, what is Factory? And then maybe what are a couple of the key decisions that you’ve made about the way Factory is built. And you know, for example, one of them is to start by benefiting from all the ongoing improvements in the foundation model layer. You know, one of them might be the product itself, but can you just say what is Factory? And what are some of the key decisions you’ve made that have shaped Factory today? Yeah, absolutely. So Factory is a cutting edge AI startup. Our mission is to bring autonomy to software engineering. What that means more concretely, we are automating tasks in the software development lifecycle. And in particular tasks like code review, documentation, testing, debugging, refactoring. And, you know, as I list these off, you’ll kind of hear quickly that these are the tasks that engineers don’t particularly enjoy doing. And that’s very much intentional, right? Like, obviously, we are doing code generation. And that’s really important. But I think, an equally important thing, too, you know, generating some inspirational and like forward looking demos, it’s also important to understand what engineers are actually spending their time on. And in most organizations, it’s not actually fun development work. In most organizations, they’re spending a lot of their time on things like review and testing and documentation. Normally, they’ll do these things way too late. And they’re suffering because they’re missing deadlines, right. And so our approach is, we want these tools to be useful in the enterprise. And so to do that, we need to kind of meet engineers where they are with the tasks that they are very eager to automate away. We call these autonomous systems "droids," and like Eno was alluding to earlier, these are kind of, there’s a droid for each category of task. And in this kind of a paradigm where we want to frame these problems as games. It’s very convenient that software development has a clearly defined software development lifecycle. And so for each kind of category of tasks, where each step in the software development lifecycle, we have a corresponding droid. So that's kind of a first pass there. I guess there were, I think there was a second part of your question that I missed. Oh, we’ll get into the rest of it. Where did the name droid come from? It's a pretty catchy name. It's very memorable and distinct to Factory. Where’d that come from? Yeah, yeah. So I mean, keep in mind, you know, when Factory started, this was, like you mentioned about a year and a month ago. And you know, actually, I would have thought this would be different 13 months later, but this is still very much the case where agent is synonymous with unreliable, stochastic, demoware vaporware. And I think something very important for us is we want to build these systems that aren’t just like cool examples of what is to come, but rather valuable today. And not just valuable for like a hacker on a side project but valuable to enterprise engineers today. We felt very strongly that agents just don't really capture what we’re trying to deliver. And so fun fact, we were originally incorporated as the San Francisco Droid Company. But upon legal advice and given I guess the eagerness with which Lucasfilm pursues its trademarks, we changed our name to Factory. Fair enough. So is it? Is it fair to say that a droid is sort of like a job specific autonomous agent that actually works? Is that a reasonable way to think about it? Yeah. Okay, exactly. You just said the words cognitive architecture. And I know my partner, Sonya Huang well enough to know that this is her love language. So I’m sure that Sonia’s mind just lit up with a whole bunch of questions for you. So I don’t want to get in the way. Sonya, have at it. We just had Harrison on the podcast, who talked about custom cognitive architectures as well, I guess, what are you doing on that front? And how do your implementations dovetail with the multi-droid strategy that you’re talking about? Yeah, absolutely. I mean, it’s a great question. And I think the way that we think about reasoning, and, and cognition within the standpoint of these systems, there are clearly huge innovations happening on both layers, the foundation model layer, as well as on the kind of orchestration or kind of application layer. The way that you can kind of think of our technical approach on this is that, you know, traditionally labs like DeepMind, and kind of some of these, these orgs that are really focused on solving problems, that you can model like a game, where you have rules and an environment and feedback loops, you can build out systems which model the reasoning of humans and even outperform them, they did this with the Alpha series of models, protein folding, Go, code. And for us, most of the reasoning work we do is similarly focused on kind of inference time reasoning, search through kind of decisions, and, and in what we kind of think of as you know, maybe it’s something of intuition. Maybe it’s something of planning, but we aren’t training foundation models yet. And I think that a lot of the innovation that’s going to happen at the foundation model, there will be things like latency and context window and kind of performance on some subset of tasks. But anytime that you need action and environmental feedback, and kind of long-term planning, it’s going to be really difficult to build a single foundation model that does that. And I think it’s really the application layer where those types of innovations are going to happen. Yeah, I thought the Princeton SWE-agent paper that came out last week or so. It was really interesting as an example, that if like, you can get incredible agentic reasoning performance on code tasks from small open source models. I thought that was really nice, a proof point of what you’re saying. We love the whole team that put that together. And the SWE-bench work, I think, is a popular benchmark in the space. I think it’s, you know, it’s clear that a lot of the efforts towards building these systems relies on not just like any one, benchmark, or eval or set of tasks, but rather collaboration across a bunch of different areas, whether it’s the model layer, whether it’s the tasks themselves, it’s how, what data are you using to evaluate? And ultimately, like the overall architecture, and yeah, they’re, they’re a really great team. We’re super pumped to see their work. Okay, last question on this? And then and then we’ll, I will, I will pause myself. Any favorite cognitive architectures? Like is it the tree of thought stuff, chain of thought stuff like any favorite cognitive architectures that you think are especially promising or fruitful in your field? Yeah, I think that’s, that’s a great question. I mean, I think kind of what I alluded to Previously, when you have almost like the game-like problem space where there are kind of simulatable, analyzable and optimizable boundaries, then that means that you can search through those decisions. And there’s a bunch of techniques like Monte Carlo tree search, language agent tree search, that people have talked about in research papers that I think are interesting approaches here. I think that, in my mind, there isn’t a singular cognitive architecture that makes sense for all tasks. And a lot of the benefit of breaking down the software development lifecycle into kind of semantically meaningful segments is that developers, when they have these workflows that move from one step to the next, they’ve kind of defined the boundaries of the game, so to speak. And so a lot of the work we do is figuring out which cognitive architecture or what design makes sense for a given task. You’re reminding me of the Rich Sutton, Bitter Lesson, search and learning are the two techniques that scale. Yeah, absolutely. And I think you definitely need both. And then, Eno, you’re talking about this a bit, how the sort of the reasoning layer on top of the foundation model is really the focus for a lot of the fundamental research and a lot of the fundamental work that you guys are doing. Matan you had a line a couple of months ago, when we were talking. That was and and hopefully this doesn’t come across as snarky, because it’s not meant to, but if something to the effect of their interest in engineers at OpenAI working on my margins for me, can you say a word about that? Because I thought that was, first it was incredibly well put. And then second, a pretty good insight in terms of how you’re building the business and really benefiting from the work of the foundation models? Can you just say a couple words about that? Yeah, absolutely. So you know, there are a lot of companies, a lot of startups that are, you know, pursuing training foundational models or fine tuning models. And then there are a lot of huge research labs like OpenAI, and Anthropic, they're also putting a ton of resources behind making these foundational models better, cheaper, faster. And from our perspective, right, like, we don’t want to run a race that’s, you know, not suited to our abilities, right, or we don’t want to, we don’t want to fight a battle that we know we won’t win. Training foundational models, we’re not going to win that battle. And similarly, I also don’t think it’s a particularly unique battle, at this point. I think these companies were incredibly unique and innovative, clearly based on what they’re delivering. But now, I think the stage is set in terms of training foundation models. And I think similarly, with a lot of the infrastructure for fine tuning and that sort of thing. What has not really come to fruition yet is actually making products with AI that people are using, there’s so much talk about all these foundational models, all this infrastructure, and there’s still very few real products that use this AI. You know, in the analogy that, you know, VCs like to talk about a lot, we have a ton of picks and shovels, and no one’s actually going for gold. And so the thesis behind how we’re building this company is, let’s first you know, use these beautiful black boxes that OpenAI, Anthropic and Google are spending billions of dollars and you know, hundreds of engineers to make, let’s use these black boxes, and build a product that people are actually using. And once we do that, then we can earn the right to do the fancier things like fine tuning and training. If you’re unable to build a product that people are actually using, with these incredible models, then chances are fine tuning and training will not save you. And it’s probably just not a good product. And so that’s kind of the approach that we’re taking there. And so, you know, we do you know, we do get a lot of improvements when new models come out. But yet, we are very much grateful for the work that’s being done at at these cutting edge research labs. I want to, you’ve said a lot about how what you’re doing is kind of like making AI immediately practical for engineers and like an enterprise setting. And so I want to throw another I think it’s a Matan quote, and I’m not sure if you were quoting somebody else, but you said you, you’re telling us last time and you said If Jeff Dean shows up in your office, and he doesn’t understand your code base, he won’t be productive. And unpack that for us. Like what does it take to kind of make a coding agent not just good for anybody that boots up a computer but somebody that's a full time engineer, they’re a real software company? Yeah, totally. Yeah. So the analogy here is that, you know, Jeff Dean is the analog of a really, really good foundational model, let’s say like GPT-6, with incredible reasoning, right? But if it comes into your engineering organization, with all your nuances, and all your engineering best practices, just having that good inference, and good reasoning is not enough to actually contribute and automate these tasks reliably. Some given isolated task, and sure you can solve, like, give it some, like LeetCode problem and this, give Jeff Dean a LeetCode problem, I’m sure he will solve it. But if you have some, you know, 20 year old legacy code base, some part of it is dead code. The other part of it, the last person who was contributing to it just retired. And so no one else knows what’s going on there. You need a deep understanding of the engineering system, not just the code base, but like why you made certain decisions, how things are being communicated, what top of mind priorities are for the engineering organization. And it’s kind of these, less sexy, but incredibly important details that were really focused on in order to deliver this to the enterprise. What about, a lot of these AI coding companies are kind of focused on the individual developers productivity? How do you think about the individual level optimization versus maybe the system, the system wide optimization? I think the the important thing to think about with respect to the the whole org is, when a VP of engineering comes into the room, that it’s they’re not really focused on whether or not an individual completed, like one task an hour faster, they’re concerned about how many tasks are being completed, and aggregate metrics of speed. But if that person completed that task an hour faster, but it’s 40%, worse code, right? It’s churning code, where people are gonna rewrite on top of it, or that person took that task, and, you know, they did it an hour, but it took them four hours to plan that, and they were blocking five other engineers. And so when you start to actually add the nuance of what does it mean to be successful? Measuring an engineering org, you start to bump into a lot of challenges with understanding what needs to be improved, and what is a bottleneck? And what is just kind of a secondary metric. I think a lot of the initial attempts at making AI coding tools are really focused on first order effects, how quickly is somebody tabbing to autocomplete a statement? Or, you know, how quickly is somebody completing an individual task? But I think that at Factory, a lot of what we’re trying to do is understand from an engineering leader’s perspective, how are you measuring performance? And what are the metrics that you look at to understand, hey, we’re doing really well as an org, or, hey, we need to make improvements and targeting those and I think metrics like code churn, end to end open to merge time, you know, time to first answer within the eng org, all these things are much more impactful to to an organization’s speed of shipping code. And so that’s kind of how we think about it. I think this really ties into what Eno was just saying quite well, which is, you know, the clearly we were talking about products earlier as well, like, clearly the AI product that has penetrated the enterprise the most is Copilot, right? Unfortunately, with a tool like Copilot the things that are kind of the metrics that are really held up as success are things like autocomplete acceptance rate. And the problem is exactly to your point, if you’re a CTO or VP of engineering, how do you then go to the executive team and say, hey, look, our autocomplete acceptance rate is this high. They don’t know what that means. They don’t understand how that translates into, like business objectives, right. And also, you know, Eno was alluding to this, there’s kind of a hidden danger to some of these autocomplete tools, which is, orgs that use tools like this end up increasing their code churn by anywhere from 20 to 40%. There’s some studies that look into this. There’s some problems with these studies. But you know, directionally what’s clear is that, as the percentage of AI generated code increases, code churn, if you’re not, you know, doing anything different in your review process, code churn is going to go up. If we look at things like how fast are you completing your cycles, what is your, you know, code churn across the org, or across these different repos that divides out these kind of like, a smaller like intermediate metrics, and gives you a sense of, hey, we are shipping faster, and we’re churning less code. So that’s really how we talk about this with these engineering leaders. At the end of the day, the three main axes we look at are saving engineering time, increasing speed, and improving code quality. So these are three and again, there’s kind of different complexity of metrics for different parts of the org. These are the three that we discussed with engineering leaders, but we want to arm them with information when they’re talking to let’s say, their CFO. And so really, we kind of break that down into one main metric, which is engineering velocity. And that’s really what all of these droids are targeted towards is increasing engineering velocity, Let me try to recap a couple parts of the story thus far. So in some ways, this is a compound lever, meaning AI is a lever on software software is a lever on the world. And so building an autonomous software development system is one of the most impactful things you can possibly do with your lives, which is pretty cool. There are a few unique angles to the approach that you guys have taken and maybe not unique, but distinctive, you know, one of which is the decision to write on top of the foundation models, which means that you get to benefit from all their ongoing innovation. It also frees you up to really focus on the reasoning and the agentic behavior on top of those foundation models, which is part of the reason why you can deploy your product as a series of droids which are basically job specific, autonomous agents that do something like test or review end to end in a way that is practically useful to an engineering organization. And instead of focusing on just producing more code, you’re actually focused on kind of the system wide output, which requires you to have really detailed context around, not just the codebase, but all of the systems and processes and kind of nuanced around the entire environment. And having done so you can increase, you know, velocity for an organization. I think that’s a bunch of the story that we’ve talked about so far. Let's talk a bit about the results. Are there any good sort of customer examples you can share of, you know, Factory in action and the results that you’ve been able to have for people? Yeah, so I think some of the some of the main things that we’re seeing, like across the board, and we’re not super public on case studies just yet, but something that we see across the board is, you know, I think our average cycle time increase is around 22%. On average, we are lowering code churn by 13%. Tools, like, I guess, we haven’t even gotten into the specific droids, but tools like the test droid, end up saving engineers, like around 40 minutes a day, which is pretty exciting. And yeah, I think, kind of going back to what we were talking about in terms of benchmarks, one of the most exciting things about having 1000s of developers who are actually using these tools, is that we get this live set of benchmarks, and we get evals and feedback from these developers about how these droids are performing. And so, you know, like I mentioned, we’re huge fans of SWE-bench and what that’s done, kind of for the general community and giving people like an open source benchmark to really compare these models. But you know, strategically for us, having this deployed in the real world has allowed us to dramatically increase our iteration speed in terms of quality for these droids. What, what have you guys learned a lot, since you have a bunch of people using this in the real world, what have you learned along the way? And have there been any, any big surprises? Engineers love ownership. Yeah, alright, say more. Absolutely. I mean, I think it really is that, you know, when you’re building an autonomous product, and the goal is to take, you know, take over a task, you have to deal with developers who are fickle, for good reason, that they’re constantly bombarded with developer tools, and, and automations. And anything that’s kind of being enforced from a top down perspective needs to be very flexible. And so making sure that, you know, when we’re building these products, we think about what are the different preferences or ideas that people have about how this task should be done, and then building as much flexibility into that. I think a great example of this is the review process. Everybody has a different idea of what they want code review to look like. Some people want superhuman linters. Some people want, you know, really deep kind of analysis of the code change. Some people don’t even like code review. They get annoyed by it entirely. Matan has a great quote about what code reviews like, I don’t know, if you want to share that. Yeah, yeah. So I mean, in general, we've kind of internally realized that code, the code review process is very much like going to the DMV, in that no matter how clean the DMV is, no matter how fast the line is. No one loves code review, right? Because at the end of the day, someone is criticizing you. Someone’s going in and looking out, you know what you did, and saying better ways you could have done it. So in general, the review process, it's the type of thing that as an engineering leader, it’s great to see like moving the needle on these organization wide metrics. As a developer, it’s maybe not the most fun thing. Whereas something like the test droid, right, which is generating tests for you so you don’t spend hours writing your unit tests. That’s incredible as a developer, but you know, for the engineering leader, it’s slightly less obvious how that connects directly to business metrics. So I think this is part of why it’s important for us to have this fleet of droids. Because we’re not just building this for the engineering leader, nor are we just building this for the developer, but rather for the engineering organization as a whole. Part of what I heard there was that I don’t have to go to the DMV anymore. You can just send me my driver’s license in the mail. Yeah, basically. Yeah. Well, that's a good way to sum it up. Have you guys seen Pat drive? I don’t think they should be sending him a driver’s license. There’s Waymo for that. Speaking of Waymo, how far out do you think we are from having fully autonomous software engineers? Like if you thought about Waymo, like it felt like it was gonna come really fast. And then it felt like we went through a valley of despair. And now the future is coming out super fast again, like, which inning are we in for the kind of fully autonomous software engineer cycle? And when do you think we’ll have fully autonomous Jeff Deans? This is a great question. And I think one that we get, we get a lot, I think one thing that’s worth is kind of like reframing what a fully autonomous software engineer will do. There have been many moments where technical technical progress has led to kind of, you know, labor dynamic changes, and increases in the level of abstraction in which people work. And I think that historically, enabling people to operate or impact the construction of software with, you know, at a higher level of abstraction with less domain knowledge has generally led to huge increases in demand for software. And I think that what we’re seeing with the the customers we’re working with today, is that when you free people up from these types of kind of secondary tasks like generating unit tests that map to a pull request, or, or writing and maintaining documentation on a code base that 95% of people know, but that documentation comes into play for that 5%, that doesn’t, they start to shift their attention to higher level thinking, they think about orchestration, they think about architecture, they think about, you know, what is this PR actually trying to do, and less about, did they follow the style guide. did they follow the style guide. I think that what we’re seeing is that this is happening today, already because of AI tools. And over time, as they get better and better, we’ll see that shift towards software engineers becoming a little bit more like architects or designers of software. And so in the future, I think there’s gonna be 10 times more people involved in software creation, where every individual has the impact of maybe 100 or 1000 people, it just may not look exactly like the individual steps of the development lifecycle that we see today. You know, that reminded me of a quote that you guys have on your website, which that, and I’m gonna, I’m gonna read this, it says, “we hope to be a beacon of the coming age of creativity and freedom that on-demand intelligence will unlock.” And that really resonated with me when I read it, because it sort of implies a very positive and optimistic view of the world that we’re heading into. I wonder if you guys want to say a couple more words on that or, or sort of what you think the sort of relationship between man and machine will be in the fullness of time. This kind of goes back to our original approach, which is, you know, it’s very tempting to go after the sexiest parts of software development in particular, you know, like, building an app from scratch, right. But that’s also the sort of thing that will make a developer defensive, because that’s the part that they enjoy, right? And so in a world where you automate the development, then an engineer is just left reviewing, testing and documenting, which is like a depressing hellscape, if you were to ask any, any software engineer, right? So for us, it’s very important that we position ourselves aligned with the developer instead of you know, going into these organizations and being antagonistic with them, right? Like, by going in and automating the things that we don’t want to do, or rather, by going in there, and automating the things that developers don’t want to do, we are positioning ourselves with them. Right? Five years from now, I don’t think anyone really knows what software engineering will be, or even if it’ll be called that anymore, you know, to this point, you might be, you know, you’re like a software curator, or cultivator or orchestrator. But by positioning ourselves this way, with the developer, wherever that role goes, we will be there side by side, to allow them to have this higher leverage. And so yeah, completely agree, to your point, this is one of the most incredible things that is going to happen to you know, our ability as humans to create. And I think for us, it’s just incredibly important that we are aligned with the users of this product. and not, you know, antagonistic, trying to rule it to replace them. How far do you think we are from having these reliable, kind of maybe call it intern level engineers? Is it a year out? Is it already here today? Is it a decade out? I think I mean, it depends on on on the the task first, for things like code review and testing, I think we’re here, we’re already there, where we’re able to operate at a level that, you know, for many, or like, there’s feedback from one organization that we got, in particular, that where we brought them the review droid. And, this was pretty early on. And they said, you know, the review droid is the best reviewer on our team. And I think that every once in a while, you kind of hear something like that, and it gives you a lot of confidence that directionally we’re definitely moving towards something that is valuable. And for tasks like, you know, hey, we’ve got to decompose our mono repo into a ton of micro services. And you know, that type of thing that you might arm like a staff level engineer, and hat with an armed with a team of engineers under them, I think that we won’t see like a binary moment of oh, well, now this is done by an AI, I think that their responsibilities will slowly start to get decomposed into the tasks of planning and implementing the refactor going, you know, one file at a time. And when they start handing off those sub tasks to AI, I think that role will will kind of start to be called something different, because when you’re no longer as focused on what is the individual line of code that I’m writing tomorrow, and more focused on what is our mission, or what is our goal, as an engineering team, you know, you really are more of an architect and less of an implementer. And concrete example of, you know, us eating, eating the, the food that we’re creating, right, we were dreading for months creating a GitLab integration, some of our customers use GitLab, we want to build cool AI stuff, we didn’t want to spend time building a GitLab integration, we had our code droid, fully spec out what the steps of building a GitLab integration would look like. And then it actually implemented every one of these sub tickets, we were of course, monitoring it just to make sure it wasn’t breaking anything. And we now have a GitLab integration. And so this is something that we genuinely were considering getting an intern to do. Because we just, we were just, we really didn’t want to do a GitLab integration. Shout out to GitLab? Yeah. But like, materially, the droid saved us, like hours of time, none of us had built a Git lab integration before. And also, it’s just like, relatively complicated to like, abstract away the like source code manager. And so that was materially intern work that we did today. So to answer your question, it is now. It’s just kind of slowly climbing up more and more the level of complexity of these tasks. The future’s really here. I have a question about competition, and not specifically the competition in your space. But I think how you more generally think about navigating competition, I think you guys are the type of founders that a lot of companies in the application layer really look up to, because you’re insanely ambitious, building a real company of meaning. You’re making a lot of smart decisions, like, you know, riding on other people’s models. I think the obvious kind of, like, scary thing, scary, kind of other side of that is, you know, every other competitor in the space has access to the same models as you. And so I’m curious how maybe just mentally and then I guess, overall, you think about approaching competition in this space? Do you think it’s more elevated in the application layer AI market, than other startup markets historically? And how do you think about navigating that? Totally. Yeah, I think that’s a great question. I think that’s really, you know, our approach to that has defined how we’ve built out this team. And really, I think there are a lot of ways you can respond to competition and like mentally, kind of justify your existence versus competitors. I think for us, on the team side, we’re just a team of people who are more obsessed than anyone else out there. And I think that is like something that just has the compounding benefit of I am willing to bet everything that the people that we have assembled are just more obsessed than everyone else working in this space. I think a kind of a corollary to that is, the only way you can win is by executing faster. Everything else is all just like sprinkles on top. The only way you can really win is by executing faster and being more obsessed. And that is what our team is and I think I guess one last thing is having a group of people who respond to kind of external pressures, as more motivating. And, you know, responding in that way, also being very mission driven, right? Like, if you know, a competitor that does something big and then suddenly you’re deflated? Well, if you’re truly obsessed with a mission, it’s irrelevant, right? If you’re truly obsessed with our goal of bringing autonomy to software engineering, all of that is noise. What we need to do is execute as fast as possible in this direction that we’ve set and the rest will sort itself out. Love it. Really well said. Maybe a few final questions to close this out. If you weren’t solving the kind of autonomous software engineering problem, what problem would you be solving? I guess I’d have to be banned from coding agents for this. Perhaps robotics? I find robotics very interesting. I think a lot of the time the team here, a lot of the team comes from backgrounds working on autonomy and robotics. And we talked about how what we’re building really kind of resembles that in many ways. I think multimodal function-calling LLMs are here, and the robotics company with decreased hardware costs that are coming out, are clearly making progress. So it feels like a fun area. So you'd be making physical droids? Exactly. It's on the roadmap. How about you, Matan? Yeah, I think this is one of my blind spots where I just suffer from severe tunnel vision. I genuinely cannot fathom working on anything else. I’m just genuinely obsessed with our mission to bring autonomy to software engineering. If I wasn’t working on this, I’d figure out a way to work on this. I know that’s a cop out answer. But I genuinely can't, It does not compute. So that is, in fact, a cop out answer, but it is a fantastic cop out answer. So we will take it. One of the questions that I always like to ask is who do you admire most in the world of AI, and tell you what Matan because of your background will let you look at the superset of AI and physics, if you like. I would say there’s some sort of name that comes to mind when you say that is Jeff Dean. I think we mentioned him earlier already, actually. But you know, his impact and research is one huge side of that. I think TensorFlow and the kind of the work that that whole team has done at Deep Mind and related. But I’ve also heard he's a nice guy. And I think that the thing is, having responsible leadership in the AI community, I think is really important, and there’s a lot of folks who are on Twitter all the time, you know, clashing. And I think that seeing folks who are outside of that side of it, I think is pretty great. Yeah. And I think not to not to give you guys a double cop out. But at Factory, we very highly emphasized collaboration. And I think like in AI in particular everything has been done by groups of people. And so it’s hard to really think about one individual. I think in physics there are a lot more like solo geniuses doing something crazy. But I think a team like recently that I think we really admire at Factory is Mistral, and how kind of quickly they basically came into open source and brought both those models to basically the cutting edge in a super short amount of time. And I think, yeah, I speak not just for myself, but I think all of our team really admires both the mission that they have and the speed with which they executed on that. So yeah, I would say Mistral. Awesome. Alright, last question, if you had to offer one piece of advice to founders or would-be founders, hoping to build in AI. What piece of advice would you offer them? We are in a land of picks and shovels. And no one has struck gold yet clearly. So I’d say go for gold. I would say try to build something that you think is going to get 10x Better if OpenAI releases GPT-6 or 7. I think internally we think of our product as something that will multiply in value and uniqueness when new models are released. And it’s I think, for us, it’s always like we’re listening to the OpenAI announcement yesterday. And you know, everyone is excited, everyone’s pumped when a new model comes out when open source does something great. If you’re stressing about new product releases or demos, it might mean it’s worth adjusting your product strategy. Congratulations on launching Factory and beating state of the art on SWE-bench by such a wide margin last week. It’s incredible. Just for our audience, can you maybe quickly recap what SWE-bench is? Yeah, absolutely. And thank you all credit goes to the Factory team for making it happen. SWE-bench is a benchmark designed to test an AI systems ability to solve real world and software engineering tasks. So it’s around 2300 issues which were taken from contributions made to 12, popular open source Python projects. And typically, these issues are bug reports or unexpected behavior that people reported on these open source projects. And the idea is, you know, all of these real world issues were addressed by other humans. And so you have the ground truth of what a human software engineer would do when faced with an issue. And the benchmark is trying to test, can your AI system go through each of these issues, and generate a code change that properly addresses it, and comparing it to the human solution with tests that a human wrote. And so there’s a lot of asterisks, but it is a somewhat useful approximation of your system’s ability to take natural language, and then turn that into code. And I think the previous high watermark on SWE-bench was 14% or so from cognition, Devin until last week, and you put up a really impressive new result at 19%, which is such a wide margin. This is such a competitive field right now. And unlike such a competitive benchmark that everyone is trying to beat, which makes you know, your result even more impressive. Could you maybe share a little bit about your approach? And how’d you get there? Definitely. And one of the main reasons we were instead in SWE-bench is that there’s a lot of companies and research labs that made submissions. So you can see Microsoft Research, Amazon, IBM, ByteDance. And I think that’s a testament to the suite bench team’s effort and making this benchmark a household name, which is great. I think one of the reasons we were able to out-compete well funded tech giants and other AI code gen startups is that we’re honestly not building the code droid for a benchmark, but rather to support real world customers. And we’ve always said customers are the best benchmark. And I think there’s some great evidence for the success of that approach. There’s a few areas our technical report goes into around planning and task decomposition, environmental grounding, codebase understanding. But overall, I think that the thing that matters most when your team’s working on these types of general software problems, is kind of like what is the northstar? What are you iterating against? And so having kind of a real world data set can make a huge difference. And we just had Harrison on the podcast last week actually talking about cognitive architectures. To what extent did prompt engineering and cognitive architectures play a role here and your results? I would characterize our research as continuously pushing the question of how can we model each droid’s architecture to more closely resemble the human cognitive process that takes place during the task? It’s funny, we actually have internally been referring to the flow of data and LLM calls as the droid to architecture, basically, since the first droid. And when Harrison first wrote about cognitive architectures, it really became apparent that that concept, cognitive architecture, is a great mental model for how to characterize the systems that have complex LLM interactions and data flow. And so, you know, for us, I think the meta problem of designing a good cognitive architecture is balancing flexibility with rigidity in the actual workflow. You want very rigid entry points, and certain common trajectories, like error recovery need to be really consistent. But then you want the flexibility in the dynamics during the majority of the problem solving process. But I think it’s one of the most interesting problems when building the droids is how do you know, kind of when to add structure and when to let the droid so to speak handle it? Really cool. So every droid has its own cognitive architecture that mirrors as closely as possible with a kind of human equivalent of that task would be doing? Yeah, exactly. 19% is amazing compared to prior state of the art. It also still feels quite far away from, you know, reliable code droids that people will just trust to run wild in their codebase. What do you think is the threshold at which engineers will actually start to use these code droids reliably and just, you know, let them run? Are we there yet? Or what is the threshold? Yeah, for sure. I think that one thing to keep in mind is that the percentage on a benchmark like SWE-bench is kind of like one of many possible measures, because the answer really is that they are already using it in production. But the use cases that might kind of highlight what the code droid is designed for, may not necessarily have a ton of overlap with what is tested in a given benchmark. So if you take human eval or some of the other coding benchmarks, that maybe tests your ability to pass a coding interview, but it doesn’t really test like your real world software engineering. SWE-bench I think actually does test a lot of real world software engineering. But in the particular context of debugging or kind of, you know, unexpected behavior identification, there are some feature requests, and there are a lot of kinds of not explicitly debugging style problems. But tasks like a migration, or refactor, a modernization that takes place over multiple changes that oftentimes have humans very heavily collaborating, are really a pretty different problem. And our internal evaluations are much more focused on those customer tasks. And so we have way higher reliability rates, for like those styles of tasks. And I also think that a huge part of the role of human AI interaction design is acknowledging where the systems are currently falling short, and building into your interaction pattern accommodation for the weak points of the AI system. This isn’t going to 100% of the time, perfectly capture the intent of what you were doing. So how do you kind of have failure trajectory handling? How do you introduce the ability to kind of edit midway, as the code droid is working to observe and have kind of some interpretability into why a code droid is making a decision. So that when it does something, the human being actually can step in, or at least understand what went wrong. And so I think that those allow you to say, well, we may not be at, you know, 100% on someone like SWE-bench, but we can still use this and get kind of real productive gains, in the meantime. Totally makes sense. And I hear you that SWE-bench is not the be-all, end-all. But since you have a good crystal ball into this space, do you have a prediction, at what point we’ll get to 80 or 90% on SWE-bench? I think that the pace right now is really, really fast. There’s a kind of interesting question of, will we get to 80 to 90%, on SWE-bench or will there be a better benchmark that kind of comes out before we can really meaningfully start hill climbing past like the 50-60%. There’s honestly a lot of tasks and SWE-bench, which are, I wouldn’t say impossible, but it almost feels like getting them right would almost only indicate that you’re cheating. It’s like they test for really, really specific claims, or a string match. And so I think that before we see 80 to 90%, on SWE-bench, what we’ll actually see is kind of like SWE-bench two and three, bench three that focuses on trying to think deeply about how can we evaluate when, you know, a piece of code is correct, but also kind of ideal or useful for a given codebase. The SWE-bench folks actually have a lot of really great thoughts about how to make these benchmarks better, but I think probably in the next two, three years, we’ll see that. Yeah, and they’re Princeton guys as well. Right? Yeah, yeah, they are. We actually shared a thesis advisor. No way. That’s very cool. Well, Eno, Matan, thank you so much for the conversation. Congratulations again on these results and on launching Factory. We are so excited. Thank you. Thank you very much. For.

========================================

--- Video 70 ---
Video ID: 6XZLoW0-mPY
URL: https://www.youtube.com/watch?v=6XZLoW0-mPY
Title: LangChain’s Harrison Chase on Building the Orchestration Layer for AI Agents | Training Data
Published: 2024-06-18 11:00:04 UTC
Description:
Last year, AutoGPT and Baby AGI captured our imaginations—agents quickly became the buzzword of the day…and then things went quiet. AutoGPT and Baby AGI may have marked a peak in the hype cycle, but this year has seen a wave of agentic breakouts on the product side, from Klarna’s customer support AI to Cognition’s Devin, etc.

Harrison Chase of LangChain is focused on enabling the orchestration layer for agents. In this conversation, he explains what’s changed that’s allowing agents to improve performance and find traction. 

Harrison shares what he’s optimistic about, where he sees promise for agents vs. what he thinks will be trained into models themselves, and discusses novel kinds of UX that he imagines might transform how we experience agents in the future.     

(01:21) What are agents? 
(05:00) What is LangChain’s role in the agent ecosystem?
(11:13) What is a cognitive architecture? 
(13:20) Is bespoke and hard coded the way the world is going, or a stop gap?
(18:48) Focus on what makes your beer taste better
(20:37) So what? 
(22:20) Where are agents getting traction?
(25:35) Reflection, chain of thought, other techniques?
(30:42) UX can influence the effectiveness of the architecture
(35:30) What’s out of scope?
(38:04) Fine tuning vs prompting?
(42:17) Existing observability tools for LLMs vs needing a new architecture/approach
(45:38) Lightning round

Hosted by: Sonya Huang and Pat Grady, Sequoia Capital

Read the Transcript: https://seq.vc/TDHC
Read the Inference Essay: https://seq.vc/TDHCIA

Transcript Language: English (auto-generated)
it's so early on that like it's so early on there's so much to be built yeah like you know GPT 5 is going to come out and it'll probably make some of the things you did not relevant but you're going to learn so much along the way and this is I strongly strongly believe like a transformative technology and so the more that you learn about it the better [Music] [Music] hi and welcome to training data we have with us today Harrison Chase founder and CEO of Lang chain Harrison is a legend in the agent ecos system as the product Visionary who first connected llms with tools in action and Lang chain is the most popular agent building framework in the AI space today we're excited to ask Harrison about the current state of Agents the future potential and the path ahead Harrison thank you so much for joining us and welcome to the show of course thank you for having me uh so maybe just to set the stage agents are the topic that everybody wants to learn more about and you've been at the the epicenter of agent building pretty much since the llm wave first got going and so maybe first just to set the table what exactly are agents I think defining agents is actually a little bit tricky and people probably have different definitions of them um which I think is pretty fair because it's still pretty early on in the life cycle of everything llms and agent related the way that I think about agents is that it's when an llm is kind of like deciding the control flow of an application um so what I mean by that is if you have a more traditional kind of like rag chain or retrieval augmented Generation chain the steps are generally known ahead of time first you're going to maybe generate a search query then you're going to retrieve some documents then you're going to generate an answer and you're going to return that to a user and it's a very fixed sequence of events um and I think when when I think about things that start to get agentic it's when you put an llm at the center of it and let it decide what exactly it's going to do so maybe sometimes it will look up a search query other times it might not it might just respond directly to the user um maybe it will look up a search query get the results look up another search query look up two more search queries and then respond and so you kind of have the llm deciding the control flow um I I I think there are some other maybe more buzzwordy things that fit into this so like tool usage is often uh associated with agents and I think that makes sense because when you have an LM deciding what to do the main way that it decides what to do is through tool usage um so it's so so I think those kind of go hand in hand um there's some aspect of memory that's commonly associated with agents and I think that also makes sense because when you have an llm deciding what to do it needs to remember what it's done before um and so like tool usage and memory are kind of like loosely associated but to me when I think of an agent it's really having an llm decide the control flow of of your application and Harrison a lot of what I just heard from you is around decision making and I've always thought about agents as as sort of action taking uh do those two things go hand inand is agentic behavior more about one versus the other how do you think about that I think they go hand in hand I think like a lot of what we see agents doing is deciding what actions to take um for for all intents and purposes um and I think the big uh difficulty with action taking is deciding what the right actions to take are um so I do think that solving one kind of leads naturally to the other and after you decide the action as well there's generally the system around the llm that then goes and executes that action and and kind of like feeds it back into the agent um so so I think that in yes so I do think they go kind of hand in hand so Harrison it seems like the main distinction then uh between an agent and something like a chain is that the llm itself is deciding what step to take next what action to take next as opposed to these things being hardcoded is that like a fair way to distinguish agentes yeah I think that's right and there's different gradients as well so as like an extreme example you could have basically a router that decides between which path to go down and so there's maybe just like a classification step in in your chain and so the lm's still deciding like what to do but it's a very simplistic way of deciding what to do um and you know at the Other Extreme you've got these autonomous agent type things and then there's this whole Spectrum in between so I'd say that's largely correct although I'll just note that there's a bunch of nuance in gray area as there is with most things in the llm space these days got it it's like a spectrum from control to like fully autonomous decision making and logic um all those are kind of the spectrum of Agents interesting uh what role do you see Lang chain playing in the agent ecosystem I think um right now we're really focused on making it easy for people to create something in the middle of that Spectrum um and for a bunch of reasons we've seen that that's kind of the best spot to be building agents in at the moment um so we've seen uh some of these more fully autonomous things get a lot of interest and and prototypes out the door and and and there's a lot of benefits to the fully autonomous things they actually quite simple to build um but we see them going off the rails a lot and we see people wanting more constrained things um but a little bit more flexible and powerful than chains um and so a lot of what we're focused on recently is the being this orchestration layer that enables the creation of these agents particularly these things in the middle between chains and autonomous agents um and I can dive into a lot more about what uh exactly we're doing there but at a high level that's that being that piece of orchestration uh uh framework is is kind of where we imagine Lang chain sitting got it so there chains there's autonomous agents there's a spectrum in between and your sweet spot is somewhere in the middle enabling people to build agents yeah and obviously that's changed over time so it's fun to like reflect on the evolution of Lang chain um so you know I think when Lang chain first started it was actually a combination of chains and then we had this one class this agent executor class which was basically this agent thing and we started adding in like a few more controls to that class um and but eventually we realized that people wanted way more flexibility and control than we were giving them with that one class so like recently we've been really heavily invested in Lang graph which is an extension of Lang chain that's really aimed at like customizable agents that sit somewhere in the middle and so kind of like our Focus you know has has evolved over time as as the space has as well fascinating maybe maybe one more final kind of setting the stage question um one of our our core beliefs is that agents are the next big wave in AI um and that we're moving as an industry from from co-pilots to agents I'm curious if you agree with that take um and and why why not yeah I I generally G with that take I think um The Reason Why That's so exciting to me is that a co-pilot still relies on having this human in the loop and so there's a little bit of almost like an upper bound on the amount of work that you can have done by an external kind of like uh by another system um and and so it's a little bit limiting in in that sense I do think there's some really interesting thinking to be done around what is the right ux and human agent interaction patterns um but I do think they'll be more along the lines of an agent doing something and maybe checking in with you as opposed to a co-pilot that's constantly kind of like in the loop I just think it's I just think it's more powerful and and gives you more leverage if the more that they're doing which which is very paradoxical as well because it comes the the more you let it do things by itself there's more risk that it's messing up or going off the rails and so I think striking this right balance is is going to be really really interesting I remember back in I think it was marchish of 2023 uh there were a few of these autonomous agents that really captured everyone's imaginations like baby AI autog GPT a few of these um and I I just remember Twitter was very very excited about it and it seems like that first itation of an Asian architecture hasn't quite met people's expectations um I think why why do you think that is and and where do you think we are in the agent hype cycle now yeah I think um maybe thinking about the agent hype cycle first I think Auto GPT was definitely the start and and and then so I mean it's it's one of the most popular GitHub projects ever so one of one of the peaks of of the hype cycle um I think and and I'd say that started in the spring 2023 to Summer of 2023 is then I personally feel like there is a bit of kind of like uh law slashdown trend from the the late summer to basically the start of the new year um in in 2024 and I think starting in 2024 we've started to see a few more realistic things come online um i' I'd point out some of the work that we've done at linkchain with elastic for example they have kind of like an elastic assistant an elastic agent in production um and so we're seeing that we saw kind of like the Clara customer support bot um kind of like come online and get a lot of hype we've seen Devon we've seen Sierra these other these other companies start to emerge um in in the agent space and so I think uh with that hype cycle in mind talking about why the auto GPT style architecture didn't really work it it was very general and and and very unconstrained um and I think that made it really exciting and captivated people's kind of like imaginations but I think practically for things that people wanted to automate to provide immediate business value there's actually a lot it it's a much more specific thing that they want these agents to do and there's really like a lot more rules that they want the agents to follow or specific ways they want them to do things and so I think in practice what we're seeing with these agents is they're much more kind of like custom cognitive architectures is kind of like what we we call them where there's a certain way of doing things that you generally want an agent to do and there's some flexibility in there for sure otherwise you know you would you would just code it um but it's a very like directed way of thinking about things and that's most of the agents and assistants that we see today and that's just more engineering work and that's just more kind of like trying things out and and and seeing kind of like what works and what doesn't work and it's harder to do so it just takes longer to build and I think that's kind of why you know that that's why that didn't exist a year ago or something like that since you mentioned cognitive architectures I love the way that you think about them maybe can you just explain like what is what is a cognitive architecture and like is there a good mental framework for how we should be thinking about them yeah so the the way that I think about a cognitive architecture is basically what's the system architecture of your llm application um and so what I mean by that is if you're building an LM application there's some steps in there that use llms what are you using these llms to do are you using them to just generate the final answer are you using them to route between two different things are you uh have do you have like a pretty complex one with a lot of different branches and maybe some Cycles repeating um or do you have uh kind of like you know a pretty a loop you basically run this LM in a loop these are all kind of differents of cognitive architectures um and cognitive AR is just a fancy way of saying like from the user input to the user output what's the flow of data of information of llm calls that happens along the way um and what we've seen more and more especially as people are trying to get agents actually into production is that the flow is specific to their application and their domain um so there's maybe some specific checks they want to do right off the bat there's maybe three specific steps that it could take after that and then each one maybe has an option to loop back or has two separate substeps um and so we see these more like if you think about it as a graph that you're drawing out we see more and more basically custom and B spoke graphs as people kind of try to constrain and guide the agent along uh their application um the reason I call it a cognitive architectures is just you know I think a lot of the power of LMS is around reasoning and thinking about what to do um and so you know I would maybe have like a cognitive mental model for how to do a task and I'm basically just encoding that that mental model into some kind of like Software System some some architecture that way and do do you think that's the direction the world is going because I kind of heard two things for me there one was it's very bespoke and second was it's fair barly Brute Force like it's fa hardcoded in a lot of ways do do you think that's where we're headed or do you think that's a stop Gap and at some point more elegant architectures or or a series of default sort of reference architectures will emerge that is a really really good question and one I spend a lot of time thinking about I think so like at at an extreme you could make an argument that if the models get really really good and reliable at planning then the best thing you could possibly have is just this four Loop that runs in a loop calls the llm decides what to do takes the action and Loops again and like all of these constraints on how I want the model to behave I just put that in my prompt and the model follows that kind of like explicitly um I I do think the models will get better at planning um and and reasoning for sure I don't quite think they'll get to the level where that will be the best way to do things for a varet of reasons one I think uh efficiency if you know that you always want to do step a after step B you can just put that in order um and two reliability as well like these are still not a terministic things we're talking about especially in Enterprise settings you probably want a little bit more comfort that if it's always supposed to do step a after step B it's actually always going to do step a over step B um or after step B I think it will get easier to create these things things like I think they'll they'll maybe start to become a little bit less and and less complex um but actually this is maybe a hot take or interesting take that it had you could say like so the the architecture of just running it in a loop um you could think of as like a really simple but General um cognitive architecture and then what we see in production is like custom and complicated kind of like cognitive architectures I think there's a separate access which is like complicated but generic custom or complicated but generic cognitive architectures and so this would be something like a really complicated like planning step and reflection Loop or or like tree of thoughts or something like that and I actually think that quadrant will probably go away over time because I think a lot of that generic planning and generic reflection will get trained into the models themselves but there will still be a bunch of not generic training or not generic planning not generic reflection not generic control loops that are never going to be in the models basically yeah no matter what um and so I think like those two ends of the spectrum I'm pretty I'm pretty bullish on I guess you can almost think about it as like the llm does the kind of like General the very general um agentic reasoning um but then you need domain specific reasoning uh and and that's the sort of stuff that that you can't really build into one General model 100% like I think I think a way of thinking about like the custom C of architectures is you're basically taking you're taking the planning responsibility away from the llm and putting it onto the human and some of that planning you'll you'll move more and more towards the model and and and more and more towards the prompt but I think they'll always be like I think a lot of a lot of tasks are actually quite complicated in some of their planning um and so I think it will be a while before we get things that are just able to do that super super reliably off the shelf it seems like we've simultaneously made a ton of progress on agents in the last six months or so like um I was reading a paper the the Princeton swe paper um where their coding agents can now solve 12.5% of GitHub issues versus I think 3.8% um when it was just rag um so it feels like we've we've you know we've made a ton of progress in the last six months but 12 and a half% is like not good enough to you know replace even an intern right and so um it feels like we still have a ton of room to go um I'm curious where you think we are both for General agents and also for your customers that are building agents like are they kind of getting to I I assume not 5 9's reliability but are they getting to kind of like the thresholds they need to kind of deploy these agents out to actual customer facing deployments yeah so so theu agent is I would say a relatively Gish agent in that it is expected to work across a bunch of different GitHub repos I I think if you look at something at like v0 by versel um that's probably much more reliable than 12.5 um% right and so I think that speaks to like yeah there there are there are definitely custom agents that not 59 of reliability but that like are being used in in production so like elastic I think we've talked publicly about how how they've done I think multiple agents at this point and I think this week is RSA and I think they're announcing something new at RSA um that that that's in agent um and yeah those are um I I don't have the exact numbers on reliability but they're reliable enough to be shipped into production um General agents are still tough yeah this is where this is where kind of like longer longer context Windows better planning better reasoning will help those General agents you shared with me this great Jeff bezos's quote of just like focus on what make makes your beer better and I think it's referring to the fact that um at the turn 20th century breweries were you know trying to make their own on electricity generat on electricity um I think similar question um a lot of companies are thinking through today like do you think that having control over your cognitive architecture really makes your your beer taste better uh so to speak metaphorically or like or or do you seed control that the model and and just build kind of UI and product I I think it maybe depends on the type of cognitive architecture that you're building going back to some of the discussions earlier if you're building like a generic cognitive architecture I don't think that makes your beer taste better I think the model providers will work on this General planning I think like well work on these General cognitive architectures that you can try off the bat on the other hand if your cognitive architectures are basically you codifying a lot of the way that your support team thinks about something or internal business processes or the best way that you know to kind of like develop code or develop this particular type of code or this particular type of application yeah I think that absolutely makes your your beer taste better especially if we're going towards a place where these applications are are are doing work than like the logic the bespoke kind of like uh business logic or or mental models for for and anthropomorphizing these LMS a lot right now but like the the models for these things to to do the best work possible 100% like I think that's the key thing that you're you're selling in in some capacity I think ux um and UI and distribution and everything absolutely still plays a part but like yeah I draw this distinction between General versus custom Harrison before we get into some of the details on how people are building these things can we um pop up a level real quick so our our founder Don Valentine was famous for asking the question so what and so my question to you is so what let let's imagine that autonomous agents are working flawlessly what does that mean for the world like how is life different if and when that occurs I think at a high level it means that as humans we're focusing on just a different set of things so I think there's a lot of like Ro repeated kind of like work that goes on in a lot of Industries at the moment um and so I think the idea of Agents is a lot of that will be kind of like automated away leaving us to think maybe higher level about like what these agents should be doing and maybe leveraging their outputs to do more creative or or building upon those outputs to do more uh kind of like uh higher leverage things um basically and so I I think uh you know you could imagine bootstrapping a uh an entire company where you're Outsourcing a lot of the functions that you would normally have to hire for and so you could play the role of a CEO with the an an an an agent for marketing an agent for sales something like that um and allow you to basically Outsource a lot of this work to to agents leaving you to do a lot of the interesting strategic thinking product thinking and maybe this depends a little bit on on what your interests are but I think at a high level it will free us up to do what we want to do and what we're good at and automate a lot of the things that we might not necessarily want to do and are you are you seeing any interesting examples of this today sort of Live And in production I mean I think the biggest there there's there's two kind of like categor ories or areas of agents that are starting to get more traction one's customer support one's coding um so I think customer support is a pretty good example of this like I think um you know often times uh people need customer support we need customer support at linkchain um and so if we could hire agents to do that um that would be really powerful um coding is interesting because I think there's some aspects of coding that I mean yeah this is maybe a more philosophical but I think there's some aspects of coding that are really creative and do require like really I mean lots of product thinking lots of uh uh positioning and things like that there's also aspects of coding that limit some of the or or not limit but get in the way of a lot of the creativity that people might have so if my mom has an idea for a website she she um she doesn't know how to code that up right but if there was an agent that could do that she could focus on the idea for the website and basically the scoping of the website but automate that a and so I'd say customer support absolutely that's having an impact today coding there is a lot of interest there um I don't think we're at I don't think it's as mature as customer support but in terms of areas where there is a lot of people doing interesting things that would be a second one to call out your your comment on coding is interesting because I think this is one of the things that has us very optimistic about AI it's this idea of sort of closing the Gap from idea to execution or closing the Gap from you know dream to reality where you can come up with a very creative compelling idea but you may not have the tools at your disposal be able to to be able to put it into into reality and AI seems like it's well suited for that I think Dylan and figma talks about this a lot too yeah I think I think it goes back to this idea of like automating away the things that yeah get in the way of of make I I like the phrasing of idea to reality it automates away kind of like the the the things that you don't necessarily know how to do or want to think about but are needed to to create what whatever you want to create I think it also one of the things that I spend a lot of time thinking about is like what does it mean to be a builder in the age of kind of like gend of AI and in the age of Agents um so what it means to be you know a a builder of software today means you know you you either have to be an engineer or hire Engineers or something like that right um but I think what it means to be a builder in the age of agents and generative AI just allows people to build a way larger set of things than they could build today um because they have at their fingertips all this other knowledge and all this other kind of like all these other builders they can hire and and use for very very cheap I mean I think like you know some of the um language around like commoditization of kind of like uh intelligence or something like that is these llms are providing intelligence for free um I think does does speak to enabling a lot of these new Builders to emerge you mentioned reflection and and chain of thoughts and other techniques like maybe can you just say a word on like what we've learned so far about what some of these I guess cognitive architectures um are capable of of doing uh for a gentic performance and maybe just um I'm curious what you think are the most promising cognitive architectures yeah I think there's um maybe it's worth talking a little bit about why kind of like the auto GPT things didn't didn't work um because I think a lot of the cph architectures are kind of like emerged to counteract some of that um I guess way back when there was basically the problem that llms couldn't even reason well enough about a first step to do in and like what they should do as the first step and so I think prompting techniques uh like Chain of Thought turned out to really helpful there they basically gave the LM more uh space to think about and think step by step about like what they should do for for a specific kind of like single step um then that actually started to get trained into the models more and more and they kind of did that by default um as that kind of like is basically everyone wanted the models to do that anyways and so yeah you should train that into the models um I think then there there was a great paper by shenu uh called react which basically uh was the first cognitive architecture for agents or something like that and and the the thing that it did there was one it asked the LM to predict what to do that's the action but then it added in this reasoning component and so it's kind of similar to Chain of Thought and that it basically added in this reasoning component he put it in a loop he asked us to do this reasoning thing before each step and you kind of run it there um and so that was kind of like and actually that's that like explicit reasoning step has actually become less and less necessary as the models have that trained into them like just like they have kind of like the Chain of Thought trained into them that explicit reasoning step has becomeing less less necessary so if you see people doing kind of like react style agents today they're often times just using function calling without kind of like the explicit like uh thought process that was actually in the original react paper um but it's still this like Loop that has kind of become synonymous with the react paper um so that's a L of the that's a lot of the uh difficulties initially with agents and I wouldn't entirely describe those as cognitive architectures I describe those as prompting techniques but okay so now we've got this working now what are some of the issues the two main issues are basically planning and then kind of like realizing that you're done and so by planning I mean like uh when I think about what to do things subconsciously or consciously I like put together a plan of the order that I'm going to do the steps in and then I kind of like go and and do each steps and basically models uh struggle with that they struggle with long-term planning um they struggle with uh coming up with a good long-term plan and then if you're running it in this loop at each step you're kind of doing a part of the plan and maybe it finishes or maybe it doesn't finish and so uh there's this uh you know if you just run it in this Loop you're implicitly asking the model to first come up with a plan then kind of like track its progress on the plan and continue along that so I think some of the planning cognitive architectures that we've seen have been okay first let's add an explicit step where we ask the llm to generate a plan um then you know let's go step by step in that plan and we'll make sure that we do each step and that's just a way of like enforcing that the model generates a long-term plan and like actually does each step before going on and it doesn't just like you know generate a five-step plan do the first step and then say okay I'm done I finished or something like that um and then I think a a separate but kind of related thing is this idea of reflection which is basically like has a model actually done its job well um right so like I could generate a plan where I'm going to go get this answer I could go get an answer from the internet maybe it's just like completely the wrong answer or I got like bad search results or something like that um I shouldn't just return that answer right I should kind of like think about whether I I I got the right answer um and and and or whether I need to do something again and again like if you're just running it in a loop you're kind of asking the model to do this implicitly so so there have been some cognitive architectures that have emerged to overcome that that basically add that in as an explicit step where they they they do an action or a series of actions and then ask the model to explicitly think about whether it's done it correctly or not um and so planning and reasoning are probably like two of the more popular generic kind of like cognitive architectures there's a lot of like custom cognitive architectures but that's all super tied to like business logic and things like that um so so but planning reasoning are generic I'd expect these to become more and more trained into the models by default although I do think there's a very interesting question of how good will they ever get in the models um but that that's probably a separate longer term conversation Harrison one of the things that you talked about that um AI Ascent was ux um which we normally think about as kind of being on the opposite end of the spectrum from architecture you know the architecture is behind the scenes the ux is the thing out in front um but it seems like we're in this interesting world where the ux can actually influence the effectiveness of the architecture by allowing you like for example with Devon to rewind to the point in the planning process where things started to go off track can you can you just say a couple words about ux and the importance of it in in agents or llms more generally and maybe some interesting things that you've seen there yeah I I'm I'm super fascinated by uh ux and and I think there's a lot of really interesting work to be done here I think the reason it's so important is because these LM still aren't perfect and still aren't kind of like reliable and and have a tendency to mess up and I think that's why chat is such a powerful uex for some of the initial uh kind of like interactions and applications you can easily see what it's doing it streams its backs its response you can easily correct it by responding to it you can easily ask follow-up questions um and so I think chat has clearly emerged as the dominant ux at the moment um I do think there are downsides to chat um you know it's generally like one AI message one human message the human is very much in the loop it's very much a co-pilot esque type of thing and I think the more and more that you can remove the human out of the loop um the more it can do for you and and it can kind of like work for you and I just think that's incredibly powerful and enabling um however again going llms are not perfect and they mess up so how do you kind of like balance these two things um I think some of the interesting IDE that we've seen talking about Devon are this idea of basically having a uh like really transparent list of everything the agent has done right like you should be able to know what the agent has done that seems like step one step two is probably like being able to modify what it's doing or what it has done so if you see that it you know messed up step three you can maybe rewind there give it some new instructions or even just like edit it kind of like uh uh decision manually um and and and and go from there um I think other like interesting ux patterns besides this rewind and edit um one is like the idea of kind of like a inbox where the agent can reach out to the human as needed so you've maybe got like you know 10 10 agents running in in in in parallel in the background and every now and again it maybe needs to ask the human for clarification um and so you've got like an email inbox where the agent is sending you like help help me I'm at this point I need help or something like that and and you kind of go and help it at that point um a similar one is like reviewing its work right and so I think this is really powerful for we've seen a lot of like um agents for writing different types of things doing research like research style agents there's there's a great project GPT researcher um which which has some really interesting kind of like architectures around agents um and I think that's a great place for this type of like review right like you can have an agent write a first draft and then I can review it and I can leave comments basically um and and and and there's a few different ways that it can actually happen so uh you know what the the most um maybe like the least involved way is I just leave like a bunch of comments in one go send those all to the agent and then it goes and fixes all of them another ux that's really really interested is this like collaborative um at the same time so like Google Docs um but a human and an agent working at the same time like I leave a comment the agent fixes it while on making another comment or something like that I think I think that's a separate ux that is pretty complicated to think about setting up and getting working um and I yeah I I I think that's interesting um there's uh there's one other kind of like ux thing that I think is interesting to think about which is basically just like how how how do these agents learn from these interactions right like we're talking about a human kind of like correcting the agent a bunch or giving feedback it would be so frustrating if I had to give the same piece of feedback 100 different times right that would suck and so like what are what's the architecture of the of the system that enables it so that it can start to learn from that I think is really interesting and and you know I think all of these are um all all all of these are still to be figured out like we're super early on in in in the game for figuring out a lot of these things but this is this is a lot of what we spend a lot of time thinking about h it well actually that reminds me you you are um I don't know if you know this or not but you're sort of legendary for the degree to which you are present in the developer community and paying very close attention to what's happening in the developer community and um and the problems that people are having in in the developer community so there are the problems that Ling Chene sort of directly addresses and you're building a business to solve and then I imagine you encounter a bunch of other problems that are just sort of out of scope and so I'm curious within the world of problem developers who are trying to build with llms or trying to build an AI are encountering today what are some of the interesting problems that you guys are not directly solving that maybe you would solve if you had another business yeah I mean I think two of the obvious areas are like at the model layer and at kind of like the database layer so like we're not building a vector database I think it's really interesting to think about what the Right Storage um is but you know we're we're not doing that um we're not building a foundation model and we're also not doing fine tuning of models like we want to help with the data curation bit absolutely um but we're not kind of like building the infrastructure for for fine tuning for that there there's fireworks and and other companies like that I think I think those are really interesting um I think uh those are probably at like the immediate infray in terms of what people are uh uh running running into at at this moment um I do think there's a second question there or second thought process there which is like if agents do become kind of like the uh future like what are what are other infer problems that are going to emerge because because of that um and so like you know to and I think it's way too early for us to say like what of these we will or won't do um because to be quite Frank we're not at the place where agents are reliable enough to have this whole like economy of Agents emerge but I think like um you know identity verification for agents um permissioning for agents payments for agents there's a really cool startup for payment for agents actually this was the opposite is Agents could pay humans to do things right and so I think there's like I I think that's really interesting to think about like if agents do become prevalent like what is the tooling infra that is going to be needed for that um which I think is a little bit separate than like what's the things that are are are needed in the developer Community for building llm applications because I think LM applications are here agents are starting to get here but not fully here and so I think it's just different levels of maturity for these types of companies Harrison you mentioned fine tuning and the fact that you guys aren't going to go there um it seems like that two kind of prompting and and like going of architectures and fine tuning are almost substitutes for each other uh how do you think about kind of the I mean the current states of like how people should be using prompting versus fine tuning and and how do you think that plays out yeah I I don't think that fine tuning meaning and cognitive architectures are substitutes for each other um and the reason I don't think they are and actually think they're kind of complimentary in a bunch of Senses is that when you have a more custom cognitive architecture the scope of what you're asking each agent or each node or or each piece of the system to do becomes much more limited and that actually becomes really really interesting for fine tuning maybe actually um on that point can you talk a little bit about Lang Smith and Lang graph Like Pat had just asked you what problems are you not solving I'm curious what what problems are you solving and and as as it relates to kind of all the problems with agents that we were talking about earlier like the things that you were doing to I guess making to make managing State more um uh uh more manageable um to make you know the agents more kind of controllable so to speak like how how do how do your products help people with that yeah so maybe even backing up a little bit and talking about Lang chain when it first came out I think the the the L chain open source project um really solved and tackled a few problems there I think one of the ones is basically standardizing the interfaces for all these different components so we have tons of Integrations with different models different Vector stores different tools um uh different databases things like that and and so that's a big that's always been a big value prop of of Lang chain and why people use Lang chain um in Lang chain there uh also is a bunch of higher level interfaces for easily getting started off the shelf with like rag or or SQL Q&A or things like that and there's also a lower level runtime for dynamically constructing chains um and and by chains I kind of mean uh we can call them dags as well like directed directed flows um and I think that distinction is important because when we talk about L graph and why L graph exists is to solve a slightly different orchestration problem which is you want these customizable and controllable things that have loops both are still in the orchestration space um but I draw like this distinction between kind of like a chain and and and these cyclical Loops I think with L graph and when you start having Cycles um there's a lot of other uh problems that come into play one of the main ones being this persistent layer uh persistence layer so that you can resume so that you can you can kind of like uh uh has uh uh them running in the background um in kind of like an async Manner and so we're starting to think more and more around deployment of these long running cyclical human in the loop type applications and so we we'll start to tackle that more and more and then the piece that kind of like spans across all of this is Lang Smith um which we've been working on basically since the start of the company and and and that's kind of like observability and testing for llm applications um and so basically from the start we noticed that you're putting an llm at the center of your system llms are not deterministic got to have good observability and testing for these types of things in order to have confidence to to put it in production um so we started building L Smith works with and without Lan chain um there's uh some other things in there like a prompt Hub so that you can manage promps um a human annotation cue to allow for this human review which I actually think is crucially one like I think in all of this it's important to ask like so what's actually new here um and I think like the main thing that's new here is these llm and I think the main new thing about llms is they're non-deterministic so observability matters a lot more and then also testing is a lot harder and specifically you probably want a human to review things more often than you want them to review like a software test um or something like that um and so a lot of the tooling we're adding in Lang Smith kind of helps at that actually that Harrison do you have a turistic for where existing observability existing testing you know existing fill-in the blank will also work for llms versus where llms are sufficiently different that you need a new product or you need new architecture you need a new approach yeah I think I've I've thought about this a bunch on the testing side from the observability side I feel like it's almost like I I feel like it's almost more obvious that there's something new that's needed here and I think that's maybe that's just um because of these multi-step applications like it's you just need need a level of observability to to get these insights and I think a lot of the like data dog I think is really aimed Dat Dog is great kind of like monitoring but for like specific traces um I don't think you get the same level of insights that you can easily get with something like Lang Smith for example and I think a lot of people spend time looking at specific traces because they're trying to debug things that went wrong on specific traces because there's all this non- determinism that happens when you use an llm um and so observability has always kind of felt like um there's there's something new to kind of like be built there testing's really interesting um and and I've thought about this a bunch I think there's two maybe like new unique things about testing um one is basically this idea of like pairwise comparisons so when I run software tests I don't generally like compare the results of like it's either pass or fail for the most part um and if if I am comparing them maybe I'm like comparing like the latency spikes or something but it's not like necessarily pairwise of two individual unit tests um but if we look at like some of the evals for llms the main uh the main eval that's trusted by people is this llm CIS uh kind of like Arena chatbot Arena style thing where you literally judge two things side by side and so I think this pairwise thing is pretty important um and pretty distinctive from kind of like tradition software tting um I think another component is basically depending on how you set up evals you might not have kind of like a 100% pass rate um at any given point in time and so it actually becomes important to track that over time and see that you're improving or at least not not regressing um and I think that's different than software testing because you generally have everything kind of like passing um and then the the third bit is just a human in the loop component um so I think you still want humans to be looking at the results of like it I don't wants maybe the wrong word because there's a lot of uh downsides to it like it takes a lot of human time to look at these things but like those are generally more reliable um than having some automated system if you compare that to software testing like software can test whether 2 equals 2 just as well as I can tell that 2 equals 2 by looking at it and so figuring out like how put the humans in the loop for this testing process is also really interesting and and unique and new I think I have a couple of very general questions for you cool I love general questions um who do you admire most in the world of AI um that's a good question I mean I I I think what open AI has done over the past year and a half is incredibly impressive um so so I think um Sam but also everyone there um I think across the board has has has I have a lot of admiration for the way they do things I think Logan when he was there did a fantastic job at kind of like some of of bringing these Concepts to folks Sam obviously deserves a ton of credit for a lot of the things um that has happened there uh what lesser known but like David Dohan is a researcher that I think is absolutely incredible he did some uh early model Cascades papers and I chatted with him super early on in in Lang chain and he's been like he he's he's like he's been incredibly uh just influential in the way that I thinks about things and so I have a lot of admiration for the way that he does things I separately you know like I'm I'm touching all all different uh possible answers for this but I think like uh uh Zuckerberg and and Facebook like I think they're crushing it with with llama and a lot of the open source um and I also think like as a CEO and as a leader the way that he and the company have embraced that has been incredibly impressive to watch so I have a lot of admiration for that um speaking of which is there a CEO or a leader um who you try to model yourself after or who you've learned a lot about your own leadership style from it's a good question I think um I I I definitely think of myself as more of kind of like a a product Centric kind of like CEO um and so I think like Zuckerberg has been interesting to watch there Brian chesy I I saw him uh talk or I listened to him talk at the seoa base camp uh last year um and really admired the way that that he kind of like thought about product and thought about kind of like company building um and so Brian's usually my go-to answer for that um but I can't say I've gone incredibly into the depths of everything that he's done if if you have one piece of advice for current or aspiring Founders trying to build an AI what would your one piece of advice for them be um just just build and just try building stuff it's so it's it's so early on that like it's so early on there's so much to be built yeah like you know GPT 5 is going to come out and it will probably make some of the things you did not relevant but you're going to learn so much along the way and this is I strongly strongly believe like a transformative technology and so the more that you learn about it better one quick anecdote on that um just because I gotta kick out of that answer I remember at our first AI asent in early 2023 when we were just starting to get to know you better um I remember you were sitting you were sitting there pushing code the entire day like like people were up on stage speaking and you were listening but you were sitting there pushing code the entire day and so so when the advice is just build you're clearly somebody who takes your own advice I think well that that that was the day open AI released like uh plugins or something and so there was a lot of scrambling to be done and I don't think I did that at this year's Sequoia asent so I'm sorry to disappoint and regress some that capacity uh thank you for joining us we really appreciate it [Music] [Music]

========================================

--- Video 71 ---
Video ID: jHwGYbWxwG4
URL: https://www.youtube.com/watch?v=jHwGYbWxwG4
Title: AI-augmented game development with Inworld co-founder Kylan Gibbs
Published: 2024-04-08 20:14:16 UTC
Description:
Kylan Gibbs, co-founder and CEO of Inworld, demonstrates Inworld's AI-powered end-to-end, vertically integrated platform for game developers.

Transcript Language: English (auto-generated)
our final demo of the day is kylin who will be giving a demo um from inworld AI on how folks are using llms to actually change video games and fundamental gameplay mechanics thank you kylin thank you so much it's been a crazy week GDC GTC gaming is hot right now um so uh I'm excited to show what I'm going to be sharing is actually a demo of an actual game that's kind of going live now so um just for a quick preview for those who aren't familiar um basically imor is a kind of endtoend vertically integrated platform specifically for gaming uh you know CJ was mentioning developers are hard to work with game developers are really hard to work with they are both technically Savvy and also extremely creatively want a lot of control so basically we have an engine that actually drives a lot of the experiences at runtime and it connects to anything like unre or unity and actually drives the behavior by taking in the game State and then putting that out we have a studio that actually is actually how you Craft um and augment existing game development using AI uh and we have a core solution so a lot of the challenges that I think lots of folks here are encountering games want to run locally they want you know no latency and all those kinds of things and so we're solving for that and what I'll be showing here today is actually mainly a demonstration of the engine component of that so we we this week launched with uh an experience with Nvidia a partnership with Microsoft and their Studios called project Explora and we released an early experience of a game U working with Ubisoft so I'll be showing a video of the Ubisoft soft experience so the main thing I guess is you know games have always been one of those places where people are super excited to apply AI but it's really hard because the controls that you have to put in place need to actually be beholden to what the narrative designers and writers of the games actually want to do and so in this video first you'll see sort of a conversational Dynamic where they've added on game design elements that are actually kind of making the dialog more engaging so it's not just a chatbot experience but it actually fits within what you feel like a gamer should be a gamified type of experience so we'll start with that and then we'll pause and I'll explain the next parts so what is this resistance when did it form like why do we even not like these Mega Corps like I like you know eating [Music] burgers I see okay our resistance has been around for years fighting against the oppression of these Mega Corps who exploit resources for their own greed plus with our sabotage missions we can provide a satisfying outlet for your skills while also fighting for a greater cause but enough about us what about your burger preference uh ah the classic case of burgers versus Revolution who needs Justice and equality when you can have a juicy burger instead time to re your priorities my friend well for me it's personal seeing my home and the world I love destroyed by their actions has fueled my passion to fight back but as a team we can make a a bigger impact and take down these corporations for good and who knows maybe we'll celebrate with some burgers after a success oh my gosh okay I love it Bloom absolutely love it let's do it that's what I like to hear insulting him okay welcome to the resistance Nelson let's make some history and maybe later we can grab some burgers and toast to a successful mission so a few things that I just want to point out here is because everybody here's very familiar with AI um the key things is you know of course we can do updates to emotional states relationship Dynamics all those things in the back end but what we realized for game Studio specifically is actually how we expose those as super important to kind of give the idea of progression because if you're just doing all of that in the back end they ultimately don't care so it's how we expose them kind of mapping that with game design is actually what allows it to come to life okay team the new drones in the air let's check out how Shadows does with this recon mission since it's your first rodeo don't be shy shoot any questions my way all right so Shadow the target is in sight but we H up any more guards show up let's hack that laptop and get out of [Music] here yes success the Drone has successfully hacked into the laptop and is downloading the map as we speak now we we just need to get out of there before anyone realizes what's happened watch out there's a guard watch out don't worry we got this just keep an eye out for any other guards and let me know if you see anything suspicious we're almost there hey Bloom how tall do you think that guard is around Oh no I got shot got that though around six feet that's pretty cool and uh so there as well because we're you know we have to use a lot of these models in parallel and so it's super challenging when you're putting together The Voice the multimodal input all those different things so large models don't necessarily work there and so it ends up being a series of a lot of models that are running in parallel to effectively process that then send the signals to the ultimate prompt so we've got and in this case you'll see the actual Dynamic game State as well so it's not just changing the character Dynamics but actually dynamically changing the actual game State Itself by triggering different things on the UI for the first step the only way to enter the building without triggering an alarm of any sort is through the window of the gallery on the second well for the entry point the best option is to access the gallery's window on the second floor from the outside but the obstacle is we'll need a ladder to reach it what do you think I don't know if that's the best option why do you think that's the best option because the other sides of the Villa are heavily guarded and monitored by cameras plus Shadows Intel showed that the guard in the L biggest threat we can't risk being seen or her so we have to be strategic any ideas I see that you got a a grappling hook why did you not decide to do that one that one looks pretty cool you know you're right that could work we could use the grappling hook to access the gallery's window good call Nelson let's go with that plan and a lot of what we're doing here as well is actually generating quests on the Fly and so effectively we're able to actually produce new quests that are dynamically generated based on the interactions that the player has and then basically grounding those in the goals that they have and so the fun of what we're building is we get to bring together basically everything that's you know consistently coming out and research but applying that with a user who has very specific needs and Creative Vision so yeah super fun check us out and if you're interested in games or or applying your Technologies to games as well we're always looking for partners thank you

========================================

--- Video 72 ---
Video ID: -hUJTEDOQDs
URL: https://www.youtube.com/watch?v=-hUJTEDOQDs
Title: AI integration for enterprise ft. CJ Desai of ServiceNow
Published: 2024-04-03 19:02:14 UTC
Description:
Chirantan "CJ" Desai, President and Chief Operating Officer of ServiceNow, discusses implementing AI at enterprise scale, including: how he prioritizes where to integrate AI, the power of constraint-driven innovation, what CIOs are looking for, and the importance of outcome-based selling for AI products.

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
this year at aient we are doing a few things differently one of those things is the what's next section that we talked about these Visionary ideas another is the what's now that Sonia has been talking about about how are people taking things in Ai and implementing them as part of this we don't just have the amazing Foundation model CEOs like Sam an Arthur and danela we've also brought in two exceptional leaders in the space of taking Ai and making it Enterprise ready at scale uh one of those is CJ uh CJ has been the president and COO of service now for the past seven years and in addition to service now being a sequoia backed company for over a decade service now is in an exceptional business it's the kind of business anyone in this room should aspire to be like uh Pat I believe sourced the business in 2012 is that right9 2009 and what was the scale of the business then Pat 20 what million of ARR okay anyone have a guess as to where service now is now in terms of ARR Andy I feel like you know this 1.4 billion one more guess 20 billion okay somewhere somewhere in that somewhere in that range is correct um so service now is the third largest software of Service Company in the world it has a market cap of $155 billion it is about to cross 10 billion of ARR it's at 9.75 billion of ARR remarkably uh when you took over as president and coo 7 years ago it was at a billion of arrs so it's 10x growth and it was 13 billion of market cap so it's it's been a 12 plus X multiple from there um it's adding 600 million of ARR a quarter so let that be something to Aspire to and I mentioned it's the third largest SAS company in the world well number one is growing at 11% a year number two is growing at 12% a year and service now is growing at 26% a year so we all are good at math that uh that does that ranking doesn't last very long with that kind of growth and CJ told me virtually all of this has been organic they've made some Acquisitions like element AI we'll talk about that in a little bit they've been very ahead of the curve on AI but really it's been organic it's called service now but it does not have Services margins this is an 82% gross margin business the free cash flow is 30% operating income 27% this is a rule of 56 business on the rule of 40 terms uh and since IPO since Pat sourced in 07 I don't know what kind of multiple it's had what was the valuation then Pat what did when we invested 260 million post that's pretty good from 260 million post to 155 billion nicely done since IPO it's been up 42 times 42x returns so an exceptional story and one that I think is both grounding and also aspirational for everyone in this room one of the main reasons why uh we we were so excited to have you come CJ is service now has been way ahead of the curve on AI and we first met actually in a conversation where uh you were talking about the AI Vision with Nvidia with Jensen and the Nvidia team and we might have a clip actually but just yesterday at at the Nvidia conference service now was a main feature uh the specific use cases on the Nvidia platform and at the previous conference it comes up as main user um my first question for you CJ is please tell us how you got here to this exceptional role as coo where you do much of the product work and then please tell us how you've gotten the AI Suite what it is today and how you've gotten the AI Suite to where it is yeah so first of all uh thank you for inviting me and you know he asked me a fun fact uh and the fun fact is that I'm a failed stand-up comedian so I'm constantly working on the material to try something out uh very high self-deprecating humor that you will see from time to time uh and at the highest level I want to say that we are extremely extremely grateful to sakoa and here is a simple story the story is that before we were going public in 2012 is when we went public with a modest market cap of $3.9 billion okay that's when we went public in 2012 and it was a meh uh IPO people like you know Facebook was pretty good back then it was called Facebook the same year and a work day was another one and we were a me IPO but before that um VMware made an offer to service now for single digit billions single- digit billions below five and Doug Leoni and the sakoa team convinced the management team which would have been happy to take that offer because they're like wow we don't have to go public we can be part of a great company like VMware at the time uh and Doug Leone to his credit convince not only the board but also the management team that you can become big so we are extremely grateful for sakoya coaching at that point in time and for that $2 billion offer which we could have taken versus today's 155 billion in just a matter of 12 years is incredible so thank you to seoa and the team for sourcing us and believing in us and uh allowing us to get here because it would have been a very easy thing for Doug to say who was on the board and the return would have been amazing for sakoa at that point in time and we still walked away CJ txting to me before and he said excited to see you can't wait to be part of the cult that is seoa yeah you're already in it man I am I I I am I am but I just wanted to be thankful and grateful and you guys work with great people at sakoa and I never take that lightly and on the path to our growth uh we never forget our friends and supporters so I just wanted to start there okay uh number two on the scale so I joined the company so Frank slutman hired me and Frank slutman was also placed by sakoa uh at service now and Frank had two choices at the time which is I think public knowledge but Frank's data domain gets Acquired and then he decided and I make fun of Jess all the time on this topic but he decides that he's going to be a VC and he joined Greylock and if you have met Frank slutman which I'm sure some of you have he is not a sandill guy so he said no I need to go back to operational CEO job and it was sakoa that convinced him Frank had two offers palalo Network CEO or service now CEO and he picked service now wow okay at that point in time and there was also Saka was behind it so when Frank hired me we were doing billion plus in ARR and Frank said CJ by 2020 if we can get to 4 billion of AR that'll be a massive W and let's go for it and our founder who created the company close to at age of 50 Fred ly who became bankrupt so this is not a classic Stanford Harvard story but it's like he went to Indiana University and it's originally from Indiana and he said I'm going to create a platform that can solve multiple use cases and so we always knew that Tam was pretty much unlimited and the platform provided everything you need to create a product or multiple products and so when I joined we were oneish billion we had one large product and two or three small products and since then it has been brutal execution on which buyers we go out of like the simple question on when my product team comes and says hey we can create this great product uh my first question is who is the buyer do we have access to that buyer is that buyer next door or two doors down or five doors down and will current buyer introduce us to that buyer or those two buyers don't talk to each other those kind of simple questions on who is the buyer who are we competing against okay and even though this is very simple the third one what is the size of the price that if we nail this use case can we create a billion dollar AR product again another product so we have been doing that level of precise execution and that's what has helped us on organic Innovation we haven't bought Revenue we are the only SAS company that has not bought revenue on our path to 10 uh and we always buy amazing companies uh which has great people and then we make them work on our platform but that's how we have scaled now from 2016 billion plus to 2023 when we exited the December quarter we already reached 10 billion of ACV and then of course the revenue Trails a little bit and we guided for 10.75 billion growing at 21% I mean these are some of the numbers but it has been hey you have an underlying platform that's cloud-based what products do you create what are you solving for who is the buyer right that brutal focus on who is the buyer do we have access to the buyer and what is the size of the price and without that with my product team and engineering team and I joined as a head of products and Engineering Frank hired me and left um after that but that's been the focus and on AI is as simple as we had a fundamental belief from supervised machine learning and as AI evolved all the way to geni today um we have been very focused on AI in service of our use cases because if we can Infuse AI in our use cases it's a very easy conversation with a JP Morgan Chase or a City Bank or United States Army that hey you are using service now for this use case AI will help accelerate X or accelerate Y and so we have been acquiring or gaining small teams that are AI experts uh at various stages all the way from 2016 uh 2017 was our first one 2016 we started the journey and then when we bought element AI they were trying to be the next Google of Canada um and they had somewhere between 170 to 180 Engineers between phds data scientist and engineers and they were in this amazing team lot of um very well-known people yosua benj who won the touring award was part of that team there are people who have written some seminal paper on Transformer model that's the kind of team we got and the call I got from alen and Company was hey this is a great team they have no Revenue zero and they are trying to figure out what use case AI can be applied to and this was during pandemic and I went to my boss our CEO and I said hey man these people don't have any Revenue but it's a great talent and we need to spend some money and to Bill's credit he said absolutely if you believe this is a great talent let's take them and that they showed me chat GPT 10152 Demos in 2020 late and early 2021 and then when this whole thing just blew up in 2022 we exactly knew where we could apply llms to our use cases and again I don't know if it's a term but slms uh we are very use case specific llms that we apply uh in service now and we started our monetization strategy in September so that's the story I know it was a simple question but I had to give a pretty long answer because there's a lot of history to it really incredible and I I didn't realize frankly until this conversation that you guys are the only SAS business ever to cross the 10 billion ARR Point fully organically corre um that's that's remarkable and frankly I've talked to a bunch of people people at service now you are so much of the product brain um it's it's a it's such a pleasure to get to learn from you and this question is on product yeah um so you guys had a little bit of a head start a few years because I know you and Bill had been talking about AI even ahead of the element acquisition but then with the element acquisition you got to think about how you're going to integrate into your product tell us about how you got up the curve and now how AI is in service Now Products maybe a couple of examples yeah so I'll just take the recent example uh we are a big fan of open- source Community when it comes to AI even on the nlu models we worked with uh Stanford to figure out which libraries we can use which was four or five years ago um but we are a big fan of Open Source community and the team in Canada with working with hugging face figured out for which use cases of service now you can apply AI so then said okay now listen we uh you talked about our gross margins our gross margins are 82% and all of you run the companies your profitability starts at your gross margin level right that's your first step or staircase 82 then you add R&D cost sales and marketing cost GNA cost and then you get to profitability so we are world class in terms of our gross margin at scale so I don't have the luxury it's a constraint train driven optimization problem that I don't have the luxury to say I'm going to run open AI everywhere in my farm in our Cloud because we are 100% Cloud company uh with 170 billion who knows 2 trillion parameters now with 40 I don't have that luxury so the constraint was can I run smaller models faster with lower latency for service now use cases and there was a constraint driven Innovation and we part partnered with hugging face our science and research team and we came up with the first model on text to cord and we are not trying to do text to Cod like GitHub copilot with Java or anything our text to code was specifically service now code uh how you configure service now and that was our first breakthrough working with hugging face and then once we do that and you know one you talked about Jensen um is he's a big fan of Canada so when we acquired element AI me too Canada yes uh so when we acquired element AI he was it was the first phone call he made and said CJ love the Canadian talent we should do more together and that was in 2020 uh because his history with UT Toronto and imet and all of those things so what we did is that Canada team working with hugging face we figured out smaller models uh one tenth the size of open Ai and I told Jens and he man dude you are constantly pushing the next rev HCL class uh Plus+ I need these to run on A1 100s and that's what will work for because we are a public company so you have 1% gross margin Dell and the number of questions I get from investors like you get from VCS all the time are not fun so uh hey hey hey yes so uh we basically said I want smaller model that can run on A1 100s and I can replicate that in every cloud he's still always trying to push me he's a great salesman even though he acts like he isn't uh he's always trying to push for H to say h is faster more efficient which is right uh but we wanted something to run on a so the smaller model smaller models is where we are going with use cases well I hear you're really going to need Blackwell yes I know the high I will I will $30,000 exactly what's that to you yeah um fabulous team I have one more question for CJ and I'm giving you that heads up so that you come up with a couple questions top of Mind CJ we've got a room of Builders here and they're building many consumer companies but also many Enterprise companies frankly you're a dream customer uh for a lot of the companies in this room what can you tell people building products in this room to help guide them towards being a great AI Builder that service now might consider partnering with and being a customer of yeah so I said there are two places so just when you look at me look at my forehead and it's a$1 billion spend I have in my cloud and software so if you want to sell something to me make it quick and I'll buy okay but I spend $1 billion doar no jokes uh on cloud and software and on the infrastructure a year uh and growing at 25% in line with our Revenue so that's how much uh I spend so I can be a great customer of yours or at least a prospect um in terms of what works is if you understand service now which if you go to our website you will not understand what service now does but but but uh if you understand service now we do basically lot of workflows as in tasks that get orchestrated digitally in a certain sequence between human and machines that's what we do right because you may think you know people at a large Bank our customers tell me that they can get a Tesla faster than getting a PC from the bank or a Mac from the bank when they order something right I mean that's the reality uh of large corporations large governments and so on so when you request a PC at a bank say uh and Banks try to be very efficient the processes that it goes through does it require two levels of approval some banks have for Mac four levels of approval then once those approvals are done it needs to go to shipping Department do they have inventory they need to base image it they need to put security creds on it and then it goes to okay what's his is going to her home address or it's going to these are the workflows and these are the things that we automate behind the scene so all you say is I want to pick this PC I want to pick this monitor I want it to be delivered via fadex tomorrow morning that's the ideal right but because of these complex workflows and the banks want to harden the image of the Mac Mac that they give you if you're doing on a trading floor that's where it takes so that's what service now does and then we Infuse AI in making it simpler and faster so for us if you understand service now and you can say hey CJ for your use cases here is the great technology that we have built and you can consume this technology whether it's your llms or whether it's a use case specific AI that you have done or some kind of analytic whatever it is then you have my attention that if I can make the use cases for our customers better I have your attention and I'll buy your product uh to make to make us go faster so we can deliver for our customers right we have only 8,000 customers only 8,000 and if you think about 8,000 customers 10 billion ARR you can do the math pretty fast but we have only 8,000 customers and so I'm obsessed like uh OCD level obsessed that if you come in and say here is what it can do for your use cases I will listen to your pitch every day okay so that's one uh and I have enough money to spend uh if we can serve our customers better and number two is we have a great go to market team so besides our uh engineering AI science research team we have a great goto Market team and if you have something coming back to the buyer next door down person or two doors down person and you want to leverage CIO is our Prime buyer if you think about cios 10 15 20 years ago till service now came cios were serving other sea Suite hey for sales I need to put Salesforce in for marketing I may need to put Adobe in for finance sap I need to put for the CFO we were the first platform we said this is the cio's platform so if you say that CIO is your buyer and you want access to the CIO there is no better company to partner with than service now right the two companies that really sell to CIO well is service now and Microsoft these are the two companies that sell really well so brilliant um I'm sure quite a few people in this room are interested in in that $1 billion of of cloud spend all right we've got time for I'd say let's say three to four questions Michelle hello thanks DJ many of us have dreams of an act two some of us might already be thinking about act two in terms of a product any advice in the early days of how you think about resourcing and philosophy around experimentation versus intentional bets around act to product development correct so uh um I will share a story that the reason our IPO was very m is because um people said that our Tam was only 1.8 billion so one of the industry analyst said these guys are not going to do well and their time is limited same thing happened to many many companies where people say I don't know if the Tam is there so for us that actually created a chip on our shoulder because we believed that the T for service now was lot bigger than what the industry analyst Community said it was which was sub two billion okay and this was in 2012 not too long ago so one thing is on your core core being core you really really have to understand what is the Tam which is an art combined with science before you start saying I want to go multiproduct I want to go now to different buyers or the same buyer but multi products whatever the strategy you want to go after with the buyer access but the core has to be core the reason you exist is for something you have try to solve a problem so really understand the T behind that core and then figure out before you go into the next ACT why are you really going after that next act so we prevented going after the next ACT till we hit 1 billion in AR and then overnight we flipped it and we said we are going go after these three buying Center security HR and customer service in addition to it and here is the go to market for it here is the buyer for it and we're going to rely on CIO to make introduction to those buyers because we nailed the CIO so core has to be core and you really have to understand the Tam before you say I'm now taking because it's so easy to say oh sales you know typical thing I hear from entrepreneurs uh CEOs smart people like yourself hey is you know we have a great product but I don't have a great sales team and then they flip constantly Chief Revenue officer right uh they do some of some of you probably do uh and I always when I'm asked for advice which rarely happens but when I'm asked for advice I say are you what problem you're really trying to solve is it the chief Revenue officer or you really don't know what product you are building so that Focus that like maniacal focus on main thing being the main thing and what is the Tam really in there before you pivot or before you go to second act is something that we look out for so that's what we learned we said main things should be the main thing till it hits billion before we go to the three other things any Sequoia company can discuss act two after a billion of ARR is is what I'm hearing um and I I think a very powerful insight there CJ while we get the next question set up is just the power of the CIO I think a lot of people overlooked that that that HR and security would look to them Charlie I have a question which is just service now has a broad platform with many capabilities you have other ways you you know interface with customers like customer support how do you think about prioritizing where you want to integrate AI so um one of the thing it has been hard in figuring out where AI could truly disrupt a use case right because that's always the hardest thing because it's still very bleeding edge and on the buyer side if you're talking to a large bank if you're talking to a large government right and I'll share one story on this so US public sector which is federal state and local okay you have to invest a lot in US public sector for certification of your product the cloud you know Microsoft has regions for il5 six so our chief Revenue officer came to me and said we want to go all in on us public sector and I said okay uh what and we had to invest 100 million plus in infra before we can start really making money in US public sector and now whether it's US Army US Navy Air Force to all public sector institution even on civilian side are all service now wall to- wall customers and coming back to the question we always try to figure out the pain point and can AI really disrupt that use case in a positive way that customers gets higher value from service now so if I Infuse AI in a use because not all use cases are created equal right you have a product for multiple use cases not all use cases are created equal but which use case will really provide higher value because when customers spend money on you all they are looking for is how much value I'm going to get out of this investment and that software roic including AI roic is hard like right now we are trying to tell everybody that okay we have Genna infused in service Now Products number one question is how much will it cost and what's the return I'm going to get and if you're not doing outcome based selling so if you're not doing outcome based selling it's like you are another guy coming in there and giving the AI pitch to The to the customer and Truth is nobody gives a ship I mean they don't because you have to be very very clear and specific on here is where you will get the roic on that so wherever the highest roic is that's what we prioritize that customers can say okay I could see if CJ is saying $10 million productivity for this large Bank most likely because it's CJ is going to be 3 million but 3 million is still better than zero yeah Andy um you you have such a close understanding what the CIO wants y um Beyond HR Support Services security it yeah what's the next set of use cases that the CIO is super excited about beyond what service now is currently harvesting yeah I would say if you think and one thing you all should know this is what I learned is in my seven years at service now they constantly told me about CIO being Chief irrelevant officer okay and they said all the power is with developers and CJ you are selling to the wrong door you need to be like other people and sell to developers yes developers will buy XYZ all that is fine but the irrelevancy of CIO has been exaggerated and right now CIO is the most technical person on a SE suite at most of the large companies which are your buyers right you may have a CTO the product person right product Tech person and you have the CIO but product Tech person is focused on Innovation not about what they will buy from you so coming back to cio's irrelevance has been exaggerated and OV exaggerated year after year and now you know we mainly sell to Fortune 500 in Fortune 500 the CEO are only asking C give us the AI road map give us this give us that and so on so that's number one that CIO is still very relevant and very important okay Dev it's not that developers are not but if you're telling if you're selling to developers I mean I was one very fickle people right they churn and uh sell thing and someday they like this and I read this on freaking Reddit and now I like this that's a hard hard hard thing to sell so um cios right now with the the state of economy today are focused on two main things one where can I take out the cost just Enterprise wide using technology so where can I take out the cost and second how can I help in the path to revenue if I can help on the path to revenue whether it's quot to cash or whether it's any part of the uh sales front end or front office CIO is constant like today I'll tell you at service now it still takes us a time when we propose a bill of material to a customer and customer says well I want to change quantity here quantity there then we have to respin the order form make sure it's in sap make sure it's in our cpq system that process is still 3 4 hours sometimes and at end of quarter 3 four hours feels like eternity so how how can we do that fast but cios right now are the two the CFO and cro are their big stakeholder and if your product is in path to that you'll always get that CIO meeting but you have to make it like really quick uh because that's I talk to seven eight cios a day and that's what I constantly see on the pattern magic excellent we have time for one more question Peter all right um you talked a lot about use cases I'm curious if you have any stories of use cases with AI that worked really well and use cases with AI that really did not work at all yeah so uh one of the things I would say trying to read documents and understand documents which is Accounts Payable invoices and this and that it is still very hard for AI to crack that and we have tried multiple different ways and people talk about OCR and this and that and there is a junkyard of technologies that we have tried and not been able to crack through because if you can autom the paperwork invoice matching and other things there is still a lot of dollars to be had on productivity on booking the revenue and so on where we have seen the most is simple things like predicting X um for our use cases specifically for our use cases so for example say a large Bank their policy is that if you use your computer every 3 years you can refresh your computer say your Mac we can do that now with AI and we can say hey Julie uh you're 4 months away from your entitled computer refresh and we have already notified it and you'll get your new computer on that third so the depreciation and all the schedules work and say yes if you agree because we still want human in the loop because nobody likes uh so far companies don't like if we just make the decision that's where we see that when we see the pattern matching and can we predict better and make it easy is where we are seeing the highest leverage on un generative AI amazing CJ this is fabulous I want to tie together some findings here really this was unique in that we had two separate areas first is application layer we've been talking about this throughout the day and here's someone who has done it incredibly successfully at massive scale you talked about who's the buyer what is the size of the prize and who are you competing against and also the overlooked customer yeah in this case the chief uh irrelevant officer you said that really actually is incredibly powerful I'll tell a quick quick story which is a dinner with you and Bill from service now a little while ago you guys said to me hey you get to a building you go to the elevator what floor do you go to I said I don't know what floor the top okay great you get off at the top floor where do you go from there uh bathroom no the corner that's the person who's the buyer and that seems like a premier your mentality and two actually focusing on the customer each of you in this room if you succeed we'll try to sell to the likes of service now a billion dooll Cloud spend business and you gave us a great guide here with AI for you you're looking at small language models not just the biggest ones you're looking at Cost you're looking at open source 1% gross margin matters to you that's how you build an 82% gross margin $160 billion business and actually understand the customer don't just go to the marketing website and uh and try to guess what service now does talk to people who built this is fabulous CJ any parting words for the team nothing it was a pleasure and I'm always around if you want any words of wisdom I have made a lot of mistakes as well uh too many uh in terms of scaling service now from a product perspective engineering perspective which use cases you prioritize which you don't how do you ring fence the team when you go for the second act and really really get focused so lots of mistakes as well uh but we are constantly learning Fab CJ one last question for you will you do stand up for us later on tonight no I will not do stand up today it's standup when I used to do it uh nowadays I'll get cancelled very fast and Bill will not appreciate that so I I I will not there we go that one off record thank thank [Applause] you

========================================

--- Video 73 ---
Video ID: twiog1bzvYs
URL: https://www.youtube.com/watch?v=twiog1bzvYs
Title: AI-powered workflow automation with Zapier co-founder Mike Knoop
Published: 2024-04-02 21:41:26 UTC
Description:
Mike Knoop, co-founder and Head of AI at Zapier, demostrates automating workflows from different web applications by leveraging Zapier's powerful AI interface.

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: Error: ParseError
--- Transcript not available or fetch failed ---

========================================

--- Video 74 ---
Video ID: jl_aABkYQ3s
URL: https://www.youtube.com/watch?v=jl_aABkYQ3s
Title: Text-to-music AI generation with Suno co-founder Mikey Shulman
Published: 2024-04-02 21:41:23 UTC
Description:
Mikey Shulman, co-founder of Suno, demonstrates how to harness AI to generate music of any style or genre with just a text-based prompt.

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
up next we have Mikey Schulman who's the CEO of sunno in AI music company that can help anybody create great music uh and Mikey if if I remember correctly you're actually we're all going to make a song together that's the plan okay let's let's do it so everyone get get thinking on lyrics I love music uh I think most humans are kind of hardwired to love music and the thing that's really cool about music the thing that's really special is much more so than like images or certainly text people have really welldeveloped tastes in music you walk down the street someone's got really well like what kind of music do you like they're going to say I really like Japanese rock but like there's some some weird bbop that I like too and I don't think these make sense and when you give people the ability to fill the Gap that's between we call it their fingers because maybe they don't play an instrument and their ears they love it so I thought it would be fun just to play what's on the top of our trending page right now night electric dreams flight feel the r feel [Music] the underneath the nearon SK heart beats Echo night as we dance until the [Music] sun in the chaos we find high aost in music come Al so hopefully you guys get the idea um and I think one of the things that nobody gets to do is make music together and so I don't know there's 50 people here let's make music together um somebody call out a genre grunge grunge call out another genre drum BAS Sal cool and uh what should we what should we make it about I I made one before about um uh being working on top of a chocolate factory but uh give someone give me a topic what was that hot girl summer I I got to I'm got to do something more vanilla uh like uh let's let's let's just do it like that so we're making something um a little more topical and uh um I think this is like really fun this is really cool this is a genre that I promise you doesn't exist right now and let's see let's see what came out grunge R&B salsa uh let's see how we [Music] do of algorithms dancing to and yeah yeah this is some interpretation of what the Confluence of those genres is you know I think I think the thing that's really fun is to then go and start smithing this we think about culture really pushes us toward not wanting to make music I'm sorry I have to talk over it I only get five minutes but culture really pushes us to not want to make music there's the artist class and then there's the rest of us who just um listen sometimes we don't even pay attention I'm sure a lot of us here who code are like half listening to music when we code and um so much more as possible if you give people some tooling to be able to do that so many more experiences are possible this is huge with kids actually kids are really hardwired to love music um and it's pretty special so let's see let's see what happens when we do this EDM of Al dancing and we in the and there's lots of different use cases for me this is like a really cool tool it it is expanding both like our creative taste for listening and also our creative taste for for producing stuff I want to leave you with something personal which is a weird song that a user made that clearly a tremendous amount of effort was poured into this and I don't know why it touches me this way this is kind of like a German lead not a style of music I'm super into this is Estonian not a language I speak at all and it just I I don't know why this resonates with me so I will play [Music] it I won't make everyone listen to the whole thing I've listened to the whole thing you know this has a tremendous number of plays and they're probably like 80% me um uh I I need to get in contact with the person who made this so um maybe there I'll [Applause] end

========================================

--- Video 75 ---
Video ID: pBBe1pk8hf4
URL: https://www.youtube.com/watch?v=pBBe1pk8hf4
Title: What's next for AI agents ft. LangChain's Harrison Chase
Published: 2024-03-29 19:47:43 UTC
Description:
Harrison Chase, founder of LangChain, speaks at Sequoia Capital's AI Ascent about what's next for agents in AI and the evolution of using language model to interact with the external world. Harrison identifies three critical areas of development for the next generation of agents that would make them production-ready and more impactful in the real world: planning, user experience and memory.

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
I'm delighted to introduce Harrison Chase um you know one of the reasons I was really excited to come back today was because I think it was a year ago at this event that I met Harrison and I thought boy if I get to meet super cool people at Harrison I'm definitely going to come back this year um quick question how many of you use Lang chain yeah wow okay it's almost everyone those you that don't you know P up your laptop run pip andall Lang chain if you're aren using Lang Swift yet I'm a huge fan but Harrison Works a massive developer Community if you look at the PIP You Know piie download stats I think um Lin is by far the leading generative AI orchestration platform I think and this gives a huge view into a lot of things happen in gen of AI so I'm excited to have him share of us what he's seeing with AI agents thanks for the intro and and thanks for having me excited to be here so today I want to talk about agents uh so Lang chains a developer framework for building all types of LM applications but one of the most common ones that we see being built are agents um and we've heard a lot about agents uh from a variety of speakers before so I'm not gonna I'm not going to go into too much of of a deep kind of like overview but at a high level it's using a language model to interact with the external world in in a variety of forms um and and so tool usage memory planning taking actions is is kind of the high level gist and the simple form of this you can maybe think of as just running an llm in a for Loop so you ask the llm what to do you then go execute that and then you ask it what to do again and then you keep on doing that until it decides it's done so today I want to talk about some of the areas that I'm really excited about that we see developers spending a lot of time in and really taking this idea of of of an agent and making it something that's production ready and and and real world and and really you know the future of Agents as the title suggests so there's three main things that I want to talk about and we've actually touched on uh all these in some capacity already so I think it's a great round up so planning uh the user experience and memory so for planning Andrew uh covered this really nicely in his talk um but we see a few the basic idea here is that if you think about running the LM in a for Loop often times there's multiple steps that it needs to take and so when you're running it in a for Loop you're asking it implicitly to kind of reason and plan about what the best next step is see the observation and then kind of like Zoom from there and think about the what the what the next best step is right after that right now at the moment language models aren't really good enough to kind of do that reliably and so we see a lot of external uh uh papers and external prompting strategies kind of like enforcing planning in in some method whether this be uh planning steps explicitly up front um or reflection steps at the end to see if it's kind of like done everything correctly as as it should I think the interesting thing here thinking about the future is whether these types of prompting strategies and these types of like cognitive architectures continue to be things that developers are building or whether they get built into the model apis as we heard Sam talk a little bit about um and so for all three of these to be clear like I don't have answers uh and I just have questions and so one of my questions here is you know are these planning prompting things short-term hacks or long-term uh necessary components um another another kind of like aspect of this is just the importance of basically flow engineering and so this term I heard come out of this paper Alpha codium it basically achieves state-of-the-art kind of like coding performance not necessarily through better models or better prompting strategies but through better flow engineering so explicitly designing this uh kind of like graph or or or state machine type thing and I think one way to think about this is you're actually offloading the planning of what to do to the human Engineers who are doing that at the beginning and so you're relying on as a little bit of a crutch the next thing that I want to talk about is the ux of a lot of agent applications this is actually one area I'm really excited about I don't think we've kind of nailed the the right way to interact with these agent applications I think uh human in the loop is kind of still necessary because they're not super reliable but if it's in the loop too much then it's not actually doing that much useful thing so there's kind of like a weird balance there one ux thing that I really like uh from from Devon uh which came out you know a week two weeks ago um and and and Jordan B kind of like uh put this nicely on Twitter is is the presence of like a rewind and edit ability so you can basically go back to a point in time where the edit or where the agent was and then edit what it did or edit the state that it's in so that it can make a more informed decision and I think this is a really really powerful ux um that we're really excited about uh at Lang train and exploring this more and I think this brings a little bit more reliability um but at the same time kind of like steering ability to the agents and speaking of kind of like steering ability the the last thing I want to talk about is the memory of of Agents um and so Mike uh as zapier showed this off a little bit earlier where he was basically interacting with the bot and kind of like teaching it what to do and correcting it and so this is an example where I'm teaching um in a chat setting an AI to kind of like write a tweet in a specific style and so you can see that I'm just correcting it in natural language to get to a style that I want I then hit thumbs up the next time I go back to this application it it remembers the style that I want but I can keep on editing it I can keep on making it a little more differentiated and when I go back a third time it remembers all of that and so this I would kind of classify as kind of like procedural memory so it's remembering the correct way to do something I think another really important aspect is is basically personalized memory so remembering facts about a human that you might not necessarily use to to do something more correctly but you might use to make the experience kind of like more personalized um so this is an example kind of like journaling app that that we're building and playing around with for exploring memory and you can see that I mentioned that I went to a cooking class and it remembers that I like Italian food and so I think bringing in these kind of like personalized aspects um whether it be procedural or or kind of like these personalized facts will be really important for the next generation of Agents um that's all I have uh thanks for having me and excited to chat about all this if anyone want chat about this after

========================================

--- Video 76 ---
Video ID: 6Bo-T9XNTvU
URL: https://www.youtube.com/watch?v=6Bo-T9XNTvU
Title: What's next for photonics-powered data centers and AI ft. Lightmatter's Nick Harris
Published: 2024-03-29 19:45:00 UTC
Description:
Nick Harris, founder of Lightmatter, speaks at Sequoia Capital's AI Ascent about what's next for data centers and how we will power the next generation of AI models. The compute required to scale AI models comes at a high cost. To achieve new levels of scale, the next generation of super computers will replace electrons with photonics. Their product, Passage, is an optical interconnect for GPUs that reduces energy consumption and enables scaling to a million nodes and beyond.

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
it's my privilege to introduce the first Speaker Nick Harris um who CEO of light matter we all know that a lot of AI progress has been driven by scaling laws and training very large Foundation models um Nick and his company light matter is a key player in that and he's building very very large data centers hundreds of thousands of gpus maybe millions of notes someday uh that that will be coming online soon and hopefully powering hyperscalers next generation of AI models for all of us to build on so that hand over to Nick all right thank you seoa team for the invite uh Sean Maguire for for putting my name up and Constantine for accepting the talk uh I have to say that the talks at seoa I've attended two events now um really been world class seoa is able to pull together some of those interesting people in the world um so yeah let's let's talk about white matter and and the future of the data center um one of the things that I thought was incredibly exciting from earlier today was seeing Sora and the example that was very near to my heart was looking at what happened as you scaled the amount of compute in the a AI model it went from this sort of goofy munge of like some kind of furry thing to the physics of a dog with you know a hat on in a person and their hair flowing and this is the difference that uh the amount of compute you have makes on the power of AI models um so let's go ahead and talk about the future of the data center so this is pretty wild uh very rough estimate on sort of the capital expenditure for the supercomputers that are used to train AI models so let's start here in the bottom so 4,000 gpus something like $150 million to deploy this kind of system 10,000 we're looking at about 400 million I 60,000 4 billion this is an insane amount of money um it turns out that the power of AI models and AI in General Scales uh very much with the amount of compute that you have and the spend for these systems is astronomical um and if you look at you know what's coming next what's the next Point here you know 10 billion 20 billion there's going to be an enormous amount of pressure on companies to deliver a return on this investment um and but we know that the the AGI is potentially out there at least we suspect it is if you spend enough money but this comes at a very challenging time my background is in physics I love computers and I'll tell you that scaling is over you're not getting more performance out of computer chips uh Jensen had GTC announcement yesterday I believe where he showed a chip that was twice as big for twice the performance and that's sort of what we're doing in terms of scaling today today so the core technology that's driven you know Mo's law and dinard scaling that make computers faster and cheaper and has democratized you know Computing for the world and made this AGI hunt that we're on possible is coming to an end so at light matter what we're doing is we're looking at how do you continue scaling and everything we do is centered around light we're using light to move the data between the chips allow you to scale it to be much bigger so that you can get to you know 100,000 nodes a million nodes and Beyond try to figure out what's required to get to AGI what's required to get to these nextg models so this is kind of what a present day supercomputer looks like uh you'll have racks of networking gear and you'll have racks of computing gear and there are you know a lot of interconnections when you're inside one of the Computing racks but then you kind of get a spaghetti you know a few links over to the networking racks and this very weak sort of interconnectivity in these clusters and what that means is that when you map a computation like an AI training workload onto these supercomputers you're basically having to slice and dice it so that big pieces of it fit in the tightly interconnected clusters you're having a really hard time scaling getting a really good unit performance scaling as you get to you know 50,000 gpus running a workload so I would basically tell you that a th000 gpus is not just a th000 gpus it really depends how you wire these together and that wiring is where a significant amount of the value is this is present day data centers what if we deleted all the networking racks what if we deleted all of these and what if we scaled the compute to be a hundred times larger and what if instead of the spaghetti we have linking everything together what if we had an all toall interconnect what if we deleted all of the networking equipment in the data center CER this is the future that we're building at light matter we're looking at how you get these AI supercomputers to get to the next model it's going to be super expensive and it's going to require fundamentally new technologies and this is the core technology this is called passage and this is how all gpus and switches are going to be built um we work with companies like AMD Intel Nvidia Qualcomm places like this and we put their chips on top of our Optical interconnect substrate it's the foundation for how AI Computing will make progress it will reduce in it'll reduce the energy consumption of these clusters dramatically and it will enable scaling to a million nodes and Beyond um this is how you get to wait for scale the biggest chips in the world and this is how you get to AGI thank you

========================================

--- Video 77 ---
Video ID: c3b-JASoPi0
URL: https://www.youtube.com/watch?v=c3b-JASoPi0
Title: Making AI accessible with Andrej Karpathy and Stephanie Zhan
Published: 2024-03-26 21:18:23 UTC
Description:
Andrej Karpathy, founding member of OpenAI and former Sr. Director of AI at Tesla, speaks with Stephanie Zhan at Sequoia Capital's AI Ascent about the importance of building a more open and vibrant AI ecosystem, what it's like to work with Elon Musk, and how we can make building things with AI more accessible. 

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
I'm filed to introduce our next and final speaker Andre karpathy I think karpathy probably needs no introduction most of us have probably watched his YouTube videos at length uh but he's a um he's renowned for his research in deep learning he designed the first deep learning class at Stamford was part of the founding team at open AI led the computer vision team at Tesla and is now a mystery man again now that he has just left open AI so we're very lucky to have you here here than Andre you've been such a dream speaker and so we're excited to have you and Stephanie close out the day thank [Applause] you Andre's first reaction as we walked up here was oh my God to his picture it's like a very intimidating I don't know what year was taken but he's he's impressed um okay amazing um Andre thank you so much for joining us today and welcome back yeah thank you um fun fact that most people don't actually know how many how many folks here know where open ai's original office was that's amazing um Nick I'm gonna guess right here right here right here on the opposite side of our uh San Francisco office where actually many of you guys were just in huddles so this is fun for us because it brings us back to Our Roots back when I first started at seoa and when Andre first uh started co-founding open aai um Andre in in addition to living out the Willy Wonka working a top a chocolate factory dream uh what were some of your favorite moments working from here yes opening I was right there um and this was the first office after I guess Greg's apartment which maybe doesn't count uh and so yeah we spent maybe two years here and the Chocolate Factory was just downstairs so it always smelled really nice uh and uh yeah I guess the team was you know 10 20 plus and uh uh yeah we had a few very fun episodes here one of them was eluded to by um by Jensen at GTC that happened just yesterday or two days ago so Jensen was describing how he brought the DG the first dgx and how he delivered it to open AI so that happened right there uh so that's where we all signed it it's in the room over there um so Andre needs no introduction but I wanted to give a little bit of backstory on some of his journey to date um as Sonia had introduced he was trained by Jeff Hinton and then uh Fay um you know his first claim to fame was his deep learning course at Stanford um he co-founded open AI back in 2015 and 2017 he was poached by Elon I remember this very very clearly for folks who don't under who don't remember the context then Elon had just transitioned through six different autopilot leaders each of whom lasted six months each and I remember when Andre took this job I thought congratulations and good luck um not too long after that uh you know he went back to open aai and has been there for the last year now unlike like all the rest of us today he is Basking in the ultimate Glory of freedom in all time and responsibility um and so we're really excited to to see what you have to share today a few things that I appreciate the most from Andre are that he is an incredible fascinating futurist thinker um he is a Relentless Optimist and he's a very practical Builder and so I think he'll share some of his insights around that today to kick things off um AGI even seven years ago seemed like an incredibly impossible task to achieve even in the span of our lifetimes now it seems within sight what is your view of the future over the next n years uh yes so I think you're right I think a few years ago I sort of felt like AGI was um um it wasn't clear how it was going to happen it was very sort of academic and you would like think about different approaches and now I think it's very clear and there's like a lot of space and everyone is trying to fill it and uh uh so there's a lot of optimization um and I think roughly speaking the way things are happening is um everyone is trying to build what I refer to as kind of like this llm OS um and basically I like to think of it as an operating system you have to get a bunch of like basically peripherals that you plug into this new CPU or something like that the peripherals are of course like text uh images audio and all the modalities and then you have a CPU which is the llm Transformer itself and then it's also connected to all the software 1.0 infrastructure that we've already built up for ourselves and so I think everyone is kind of trying to build something like that and then um make it available as something that's customizable to all the different nukes and crannies of the economy and so I think that's kind of roughly what everyone is trying to build out and what um uh what we sort of also heard about earlier today uh so I think um that's roughly where it's headed is um we can bring up and down these relatively uh you know self-contained agents that we can give high level tasks to and specialize in various ways so yeah I think it's going to be very interesting and exciting and it's not just one agent it's many agents and what does that look like and if that view of the future is true how should we all be living Our Lives differently um I don't I don't know I guess we have to try to build it influence it make sure it's good and uh yeah just uh try to try to make sure it turns out well so now that you're a free independent agent um I want to address the elephant in the room which is that open AI is um uh dominating the ecosystem and most of our audience here today are founders who are trying to carve out a little niche praying that open aai doesn't take them out overnight where do you think opportunities exist for other players to build new independent companies versus what areas do you think open AI will continue to dominate even as its ambition grows uh yes so my high level impression is basically open is trying to build out this lmos and I think uh as we heard earlier today like um is trying to develop this platform on top of which you can position different companies and different verticles now I think the OS analogy is also really interesting because when you get when you look at something like Windows or something like that these are also operating systems they come with a few default apps like a browser comes with Windows right you can use the edge browser and so I think in the same way openai or any of the other companies might come up with a few default apps quote unquote but it doesn't mean that you can have different browsers that are running on it just like you can have different chat agents uh sort of running on that infrastructure and so there will be a few default apps but there will also be potentially a vibrant ecosystem of all kinds of apps that are fine tune to all the different NS and cares of the economy and I really like the analogy of like the early um iPhone apps and what they looked like and they were all kind of like jokes and it took time for that to develop and I think absolutely I agree that we're going through the same thing right now people are trying to figure out what is this thing good at what is it not good at how do I work it how do I program with it how do I debug it how do I just you know uh actually get it to perform real tasks and what kind of oversight because it's quite autonomous but not fully autonomous so what does the oversight look like what does the evaluation look like there's many things to think through and just to understand sort of like the psychology of it and I think uh that's what's going to take some time to figure out exactly how to work with this infrastructure uh so I think we'll see that over the next few years so the race is on right now with llms open AI anthropic mol llama Gemini um the whole ecosystem of Open Source models now a whole longtail of small models how do you foresee the future of the ecosystem playing out yeah so again I think the open source anal sorry the operating systems analogy is interesting because we have say like we have basically an oligopoly of a few proprietary systems like say windows uh Mac OS Etc and then we also have Linux and so and Linux has an Infinity of distributions uh and so I think maybe it's going to look something like that I also think we have to be careful with the naming because a lot of the ones that you listed like Lama mrone I wouldn't actually say they're open source right and so like it's kind of like tossing over a binary for like an operating system you know like you can you can kind of work with it it's like it's like useful but um but it's not fully useful right and um there are a number of um what I would say is like fully uh open source llms uh so there's um know Pia models llm 360 Almo Etc so and they're fully releasing the entire infrastructure that's required to compile the the operating system right to train the model from the data to gather the data Etc and so when you're just given a binary it's much better of course because um you can fine-tune the model which is useful but also I think it's subtle but you can't fully fine-tune the model because the more you fine tune the model the more it's going to start regressing on everything else and so what you actually really want to do for example if you want to add capability is you uh and not regress the other capabilities you may want to train on some kind of um um like a mixture of the previous data set distribution and the new data set distribution because you don't want to regress the old distribution you just want to add knowledge and if you're just given the weights you can't do that actually you need the training Loop you need the data set Etc so you are actually constrained in how you can work with these models and um again like I think it's definitely helpful but it's uh I think we need like slightly better language for it almost so there's open weights models open source models and then um proprietary models I guess and that might be the ecosystem um and yeah probably it's going to look very similar to the ones that we we have today and hopefully you'll continue to help build some of that out um so I'd love to address the other ele in the room which is scale um simplistically it seems like scale is all that matters scale of data scale of compute and therefore the large research Labs large Tech Giants have an immense advantage today um what is your view of that and and is that all that matters and if not what else does um so I would say scale is definitely number one uh I do think there are details there to get right and I think you know um a lot also goes into the data set propriation and so on making it uh very good clean Etc that matters a lot these are all sort of like compute efficiency gains that you can get so there's the data the algorithms and then of course the um the training of the model and making it really large so I think scale will be the primary determining factor is like the first principal component of things for sure uh but there are many of many of the other things uh that um that you need to get right so it's almost like the scale set some kind of a speed limit almost uh but you do need some of the other things but it's like if you don't have the scale then you fundamentally just can't train some of these massive models if you are going to be training models uh if you're just going to be doing fine tuning and so on then I think um maybe less scale is is necessary but we haven't really seen that just yet to fully play out and can you share more about some of the ingredients that you think also matter maybe lower in priority behind scale um yeah so the first thing I think is like you can't just train these models if you have if you're just given the money and the scale it's actually still really hard to build these models and part part of it is that the infrastructure is still so new and it's still being developed not quite there but uh training these models at scale is extremely difficult and is a very complicated distributed optimization problem and there's actually like the talent for this is fairly scarce right now and uh it just basically turns into this uh insane thing running on tens of thousands of gpus all of them are like failing at random at different points in time and so like instrumenting that and getting that to work is actually extremely difficult challenge uh gpus were not like intended for like 10,000 GPU workloads until very recently and so I think a lot of the infrastructure is sort of like creaking under that pressure and uh we need to like work through that but right now if you're just giving someone a ton of money or a ton of scale or gpus it's not obvious to me that they can just produce one of these models which is why uh you know it's not it's not just about scale you actually need a ton of uh expertise both on the infrastructure side the algorithm side um and then the data Side and being careful with that so I think those are the major components the ecosystem is moving so quickly um even some of the challenges we thought existed a year ago are being solved more more today um hallucinations context Windows multimodal capabilities inference getting better faster cheaper um what are the llm research challenges today that keep you up at night what do you think are medy enough problems but also solvable problems that we can continue to go after so I would say on the algorithm side one thing I'm thinking about quite a bit is uh the this like distinct split between diffusion models and autoaggressive models they're both ways of presenting problem the distributions and it just turns out that different modalities are apparently a good fit for one of the two I think that there's probably some space to unify them or to like connect them in some way uh and also um get some Best Best of Both Worlds or um sort of figure out how we can get a hybrid architecture and so on so it's just odd to me that we have sort of like two separate SP points in the space of models and they're both extremely good and it just feels wrong to me that there's nothing in between uh so I think we'll see that sort of carved out and I think there are interesting problems there and then the other thing that maybe I would point to is there's still like a massive Gap in just um the energetic efficiency of running all this stuff so my brain is 20 watts roughly uh Jensen was just talking at GTC about you know the massive super computers that they're going to be building now these are the numbers are in mega megawatts right and so maybe you don't need all that to run like a brain I don't know how much you need exactly but I think it's safe to say we're probably off by a factor of a thousand to like a million somewhere there in terms of like the efficiency of running these these models uh and I think part of it is just because the computers we've designed of course are just like not a good fit for this workload um and I think part Nvidia gpus are like a good step in that direction uh in terms of like the you need extremely high parallelism we don't actually care about sequential computation that is sort of like data dependent in some way we just have these uh we just need to like blast the same algorithm across many different uh sort of U array elements or something you can think about it that way so I would say number one is just um adapting the computer architecture to the new uh data workflows number two is like pushing on a few things that we're currently seeing improvements on so number one maybe is uh Precision we're seeing Precision come down from what originally was was like 64 bit for double we're now to down to I don't know it is 456 or even 1.58 depending on which papers you read and so I think Precision is one big lever of um of getting a handle on this and then second one of course is sparsity so that's also like another big Delta would say like your brain is not always fully activated and so sparity I think is another big lever but then the last lever I also feel like just the V noyman architecture of like computers and how they built where you're shuttling data in and out and doing a ton of data movement between memory and you know the cores that are doing all the compute this is all broken as well kind of and it's not how your brain works and that's why it's so efficient and so I think it should be a very exciting time in computer architecture I'm not a computer architect but I think there's uh it seems like we're off by a factor of a million thousand to a million something like that and there should be really exciting um sort of Innovations there that um that bring that down I think there are at least a few builders in the audience working on this problem um okay Switching gears a little bit um you've worked alongside many of the greats of Our Generation Um Sam Greg from openai and the rest of the open AI team Elon Musk um who here knows the the joke about the uh rowing team the American team versus the Japanese team okay great so this will be a good one uh Elon shared this at Al LS base camp and I think it reflects a lot of his philosophy around how he builds uh cultures and teams so you have two teams um the Japanese team has four rowers and one steerer and the American team has four steerers and one rower and can anyone guess when the American team loses what do they do shout it out exactly they fire the rower and and Elon shared this example I think as a reflection of how he thinks about hiring the right people building the right people building the right teams at the right ratio um from working so closely with folks like these incredible leaders what have you learned uh yeah so I would say definitely Elon runs this company is an extremely unique style I don't actually think that people appreciate how unique it is you sort of like even read about in some but you don't understand it I think it's like even hard to describe I don't even know where to start but it's like a very unique different thing like I I like to say that he runs the biggest startups and I think um it's just um I don't even know basically like how to describe it it almost feels like it's a longer sort of thing that I have to think through but well number one is like so he likes very small strong highly technical uh teams uh so that's number one so um I would say at companies by default they sort of like the teams grow and they get large Elon was always like a force against growth I would have to work and expend effort to hire people I would have to like basically plead to higher people um and then the other thing is at big companies usually you want um it's really hard to get rid of low performers and I think Elon is very friendly to by default getting getting rid of low performance so I actually had to fight for people to keep them on the team uh because he would by default want to remove people and so uh that's one thing so keep a small strong highly technical team uh no middle management that is kind of like uh non-technical for sure uh so that's number one number two is kind of like The Vibes of how this is how everything runs and how it feels when he sort of like walks into the office office he wants it to be a vibrant place people are walking around they're pacing around they're working on exciting stuff they're charting something they're coding you know he doesn't like stagnation he doesn't like to look for it to look that way he doesn't like large meetings he always encourages people to like leave meetings if they're not being useful uh so actually do see this or you know it's a large meeting and some if you're not contributing and you're not learning just walk out and this is like fully encouraged and I think this is something that you don't normally see so I think like Vibes is like a second big big lever that I think he really instills culturally uh maybe part of that also is like I think a lot of bigger companies they like pamper employees I think like there's much less of that it's like the the culture of it is you're there to do your best technical work and there's the intensity and and so on and I think maybe the last one that is very unique and very interesting and very strange is just how connected he is to the team uh so usually a CEO of a company is like a remote person five layers up who talks to their VPS who talk to their you know reports and directors and eventually you talk to your manager it's not how your ask companies right like he will come to the office he will talk to the engineers um many of the meetings that we had were like uh okay um 50 people in the room with Elon and uh he talks directly to the engineers he doesn't want to talk just to the VPS and the directors uh so I you know um normally people would talk spend like 99% of the time maybe talking to the VPS he spends maybe 50% of the time and he just wants to talk to the engineers so if if the team is small and strong then engineers and the code are the source of Truth and so they have the source of Truth not some manager and he wants to talk to them to understand the actual state of things and what should be done to improve it uh so I would say like the degree to which he's connected with the team and not something remote is also unique and um and also just like his large hammer and his willingness to exercise it within the organization so maybe if he talks to the engineers and they bring up that you know what's blocking you okay I I just I don't have enough gpus to run my my thing and he's like oh okay and if he if he hears that twice he's going to be like okay this is a problem so like what is our timeline and when when you don't have satisfying answers he's like okay I want to talk to the person in charge of the GPU cluster and like someone dials the phone and he's just like okay double the cluster right now like let's let's have a meeting tomorrow from now on send me daily updates until cluster is H twice the size and then they kind of like push back and they're like okay well we have this procurement set up we have this timeline and Nvidia says that we don't have enough GP gpus and it will take six months or something and then you get a rise of an eyebrow and then he's like okay I want to talk to Jensen and then he just kind of like removes bottlenecks so I think the extent to which he's extremely involved and removes bottlenecks and applies his hammer I think is also like not appreciated so I think there's like a lot of these kinds of aspects that are very unique I would say and very interesting and honestly like going to a normal company outside of that is is uh you you like definitely miss aspects of that uh and so I think yeah that's maybe maybe that's a long rent but that's just kind of like I don't think I hit all the points but it is a very unique uh thing and uh it's very interesting and yeah I guess that's my brand hopefully tactics that most people here can employ um taking a step back you've helped build some of the most generational companies you've also been such a key enabler for many people many of whom are in the audience today of getting into the field of AI um knowing you what you care most about is democratizing AC access uh to AI education tools uh helping uh create more equality in the in the whole ecosystem at large there are many more winners um as you think about the next chapter in your life what gives you the most meaning uh yeah I think like I think you've described it on in the right way like where my brain goes by default is um like you know I've worked for a few companies but I think like ultimately I care not about any one specific company I care a lot more about the ecosystem I want the ecosystem to be healthy I want it to be thriving I want it to be like a coral reef of a lot of cool exciting startups and all the nukes and crannies of the economy and I want the whole thing to be like this boiling soup of cool stuff and genuinely Andre dreams about coral reefs you know I want it to be like a cool place and I think um yeah that's why I love startups and I love companies and I want uh there to be a vibrant ecosystem of them and um by default I would say a bit more hesitant about kind of like you know uh like five Mega Corps kind of like taking over especially with AGI being such a magnifier of power uh I would be kind of I'm kind of uh worried about what that could look like and so on so uh so I have to think that through more but yeah I like I love the ecosystem and I want it to be healthy and vibrant amazing um we'd love to have some questions from the audience yes Brian hi um Brian hallan would you recommend Founders follow elon's management methods or is it kind of unique to him and you shouldn't try to copy him um yeah I think that's a good question I think it's up to the DNA of the the founder like you have to have that same kind of a DNA and that some some kind of a Vibe and I think when you're hiring the team it's really important that you're like the you're you're making it clear upfront that this is the kind of company that you have and when people send up for it they're uh they're very happy to go along with it actually but if you change it later I think people are happy with that and that's very messy uh so as long as you do it from the start and you're consistent I think you can run a company like that um and uh you know uh but uh you know it has its own like pros and cons as well and I think uh um so you know up to up to people but I think it's a consistent model of company building and running yes Alex hi um I'm curious if there any types of model composability that you're really excited about um maybe other than mixture of experts I'm not sure what you think about like merge model merges Franken merges or any other like things to make model development more composable yeah that's a good question um I see like papers in this area but I don't know that anything has like really stuck maybe the composability I don't exactly know what you mean but you know there's a ton of uh work on like uh primary efficient training and things like that I don't know if you would put that in the category of composability in the way I understand it but um it's only the case that like traditional code is very composable and I would say neural lots are a lot more fully connected uh and less composable by default but they do compose and confine tune as a part of a whole so as an example if you're doing like a system that you want to have chpt and just images or something like that it's very common that you pre-train components and then you plug them in and fine tune maybe through the whole thing as an example so there's composability in those aspects where you can pre-train small pieces of the cortex outside and compose later uh so through initialization and fine tuning so I think to some extent it's um so maybe those are my scattered thoughts on it but I don't know if I have anything very coherent otherwise yes Nick um so you know we've got these next word prediction things do you think there's a path towards building a physicist or a Von noyman type model that has a mental model of physics that's self consistent and can generate new ideas for how do you how do you actually do Fusion how do you get faster than light if it's even possible is is there any path towards that or is it like a fundamentally different Vector in terms of these AI model developments I think it's fundamentally different in some in one aspect I guess like what you're talking about maybe is just like capability question because the Curr models are just like not good enough and I think there are big rocks to be turned here and I think people still haven't like really seen what's possible in the space uh like at all and I like roughly speaking I think we've done step one of alpha go this is what the team we've done imitation learning part uh there's step two of Alo which is the RL and people haven't done that yet and I think it's going to fundamentally like this is the part that actually made it work and made something superum uh and so I think uh this is uh I think there's like big rocks in capability to still be turned over here um and uh you know the details of that like are are kind of tricky potentially but I think this is we just haven't done step two of alphao long story short and we've just done imitation and I don't think that people appreciate like for example um number one like how terrible the data collection is for things like jpt like say you have a problem like some prompt is some kind of mathematical problem a human comes in and gives the ideal solution right to that problem the problem is that the human psychology is different from the model psychology what's easy or hard for the mo for the human are different to what's easy or hard for the model and so human kind of fills out some kind of a trace that like comes to the solution but like some parts of that are trivial to the model and some parts of that are massive leap that the model doesn't understand and so um you're kind of just like losing it and then everything else is polluted by that later and so like fundamentally what you need is the model my the model needs to practice itself uh how to solve these problems it needs to figure out what works for it or does not work for it uh maybe maybe it's not very good at four-digit Edition so it's going to fall back and use a calculator uh but it needs to learn that for itself based on its own capability and its own knowledge so that's number one is like that's totally broken I think it's a good initializer though um for something agent likee and then the other thing is like we're doing reinforcement learning from Human feedback but that's like a super weak form of reinforcement learning doesn't even count as reinforcement learning I think like what is the equivalent in Alpha go for rhf it's like what is what is the reward model it's it's a what I call it's a Vibe check U like imagine like if you wanted to train like an alpha go rhf it would be giving two people two boards and like said which one do you prefer and then you would take those labels and you would train model and then you would ARL against that what are the issues with that it's like number one that's it's just Vibes of the board that's what you're training against number two if it's a reward model that's a neural nut then it's very easy to overfit to that reward model for the model you're optimizing over and it's going to find all these spous uh uh ways of uh hacking that massive model is the problem uh so alphago gets around these problems because they have a very clear objective function you can ARL against it so rlf is like nowhere near I would say RL is like silly and the other thing is imitation learning super silly RL HF is nice Improvement but it's still silly and I think people need to look for better ways of training these models so that it's in the loop with itself and its own psychology and I think we're uh there will probably be unlocks in that direction so it's sort of like graduate school for AI models it needs to sit in a room with a book and quietly question itself for a decade yeah I think that would be part of it yes and I think like when you are learning stuff and you're going through textbooks like there is an exerc you know there's exercises in the textbook what are those those are prompts to you to exercise the material right uh so and when you're learning material not just like reading left or right right like number one you're exercising but maybe you're taking notes you're rephrasing reframing like you're doing a lot of manipulation of this knowledge in a way of you like learning that knowledge and we haven't seen equivalence of that at all in llms so it's like super early days I think um yes Yi yeah uh it's cool to be to be uh optimal and uh and and practical at the same time so I would I would be asking like how would you be align the priority of like a either doing cost reduction and revenue generation or be like finding the better quality models with like better reasoning capabilities how would you be aligning that so maybe I understand the question I think what I see a lot of people do is they start out with the most capable model that doesn't matter what the cost is so you use uh gp4 you use super prompt it Etc you do rag Etc so you're just trying to get your thing to work so you go after you're go you're going after uh sort of accuracy first and then you make concessions later you check if you can fall back to 3.5 for certain types of queries you check if you um and you sort of make it cheaper later so I would say go after performance first and then you make it cheaper later um it's kind of like the Paradigm that I've seen a few people that I talked to about this kind of U say works for them um uh and uh maybe it's not even just a single prom product think about what are the ways in which you can even just make it work at all because if you just can make it work at all like say you make 10 prompts or 20 prompts and you pick the best one and you have some debate or I don't know what kind of a crazy flow you can come up with right like just get your thing to work really well because if you have a thing that works really well then one other thing you can do is you can distill that right so you can get a large distribution of possible problem types you run your super expensive thing on it to get your labels and then you get a smaller cheaper thing that you find you on it and so I would say I would always go after sort of get it to work as well as possible no matter what first and then make it cheaper is the thing I would suggest Hi Sam hi um one question um so this past year we saw a lot of kind of um impressive results from open source ecosystem I'm curious what your opinion is of how that will continue to keep Pace or not keep Pace with closed Source development um as the models continue to improve in scale uh yeah I think that's a very good question um yeah I think that's a very good very good question I don't I don't really know fundamentally like these models are so Capital intensive right like one thing that is really interesting is for example you have Facebook and meta and so on who can afford to train these models at scale but then it's also not part of it's not the thing that they do and it's not invol like their money printer is unrelated to that and so they have actual incentive to um potentially release some of these models so that they uh empower the ecosystem as a whole so they can actually borrow all the best ideas so that to me makes sense uh but so far I would say they've only just done the open weights model and so I think they should actually go further and that's what I would hope to see and I think it would be better for everyone and I think potentially maybe there's squeamish about some of the uh some of the aspects of it eventually with respect to data and so on I don't know how to overcome that um maybe they should like try to just uh uh find data sources that they think are you know uh very easy to use or something like that and try to constrain themselves to those so I would say like those are kind of our Champions um potentially and uh that's I would like to see more transparency also coming from you know and I think meta and Facebook are doing pretty well like they released paper they published a log book and sorry was yeah log book and so on so they're doing um I think they're doing well but they're they could do uh much better in terms of fostering the ecosystem and I think maybe that's coming we'll see Peter yeah uh maybe this is like an obvious answer given the previous question but what do you think would make the AI ecosystem cooler and more vibrant or what's holding it back is it you know openness or do you think there's other stuff that is also like a big thing that you'd want to work on um yeah I certainly think like one big aspect of is just like the stuff that's available I had a tweet recently about like number one build the thing number two build the ramp I would say there's a lot of people building a thing I would say there's lot a lot less happening of like building ramps so that people can actually understand all this stuff and you know I think we're all new to all of this we're all trying to understand how it works we all we all need to like ramp up and collaborate to some extent to even figure out how to use this effectively so I would love for people to be a lot more open uh uh with respect to you know what they've learned how they've trained all this how what works what doesn't work for them Etc and um yes just from us to like learn a lot more from each other that's number one and then uh number two I also think like there is quite a bit of momentum of in the open ecosystems as well uh so I think that's already good to see and maybe there's some opportunities for improvement I talked about already um so yeah last question from the audience Michael to get to like the the next big performance leap uh from Models do you think that it's sufficient to modify the Transformer architecture with say uh thought tokens or activation beacons or do we need to throw that out entirely um and come up with a new fundamental building block to take us to the next big step forward or AGI um yeah I think I think that's a good question um I think well the first thing I would say is like Transformer is amazing is just like so incredible I don't think I would have seen that coming for sure um like for a while before the Transformer arrived I thought there would be a insane diversification of neural networks and that was not the case it's like complete opposite actually it's a complete like it's like all the same model actually so it's incredible to me that we have that I don't know that it's like the final neural network I think there there will definitely be I would say it's really hard to tell to say that given the history of the of the field and I've been in it for a while it's really hard to to say that this is like the end of it absolutely it's not and I think uh I feel very optimistic that someone will be able to find a pretty big change to how we do things today I would say on the front of the autoaggressive or diffusion which is kind of like the modeling and the the law setup um I would say there's definitely some fruit there probably but also on the Transformer and like I mentioned these levers of precision and sparcity and as we drive that and together with the codesign of the hardware and how that might evolve uh and just making Network architectures there a lot more sort of well tuned to those constraints and how all that works um um I to some extent also I would say like Transformer is kind of designed for the GPU by the way like that was the big leap I would say in the Transformer paper and that's where they were coming from is we want an architecture that is fundamentally extremely paralyzable and because the recurrent neural network has sequential dependencies terrible for GPU uh Transformer basically broke that through the attention and uh this was like the major sort of insight there and it has some predecessors of insights like the neural GPU and other papers at Google they were sort of thinking about this but that is a way of targeting the algorithm to the hardware that you have available so I would say that's kind of like in that same Spirit but long story short like I I think it's very likely we'll see changes to it still but it's been it's been proven like remarkably resilient I have to say like it came out you know many years ago now like I don't know yeah something six seven yeah so uh you know like the original Transformer and what we're using today are like not super different um yeah as a parting message to all the founders and builders in the audience what advice would you give them as they dedicate the rest of their lives to helping shape the future of AI uh so yeah I don't I don't have super I don't usually have crazy generic advice I think like maybe the thing that's top of my mind is I I think uh founders of course care a lot about like their startup I would I also want like how do we have a vibrant ecosystem of startups how do startups continue to win especially with respect to like big Tech and how do we how how's the E how how does the ecosystem become healthier and what can you do sounds like you should become an investor amazing um thank you so much for joining us Andre for this and also for the whole day [Applause] today

========================================

--- Video 78 ---
Video ID: 3JLekB-NV8o
URL: https://www.youtube.com/watch?v=3JLekB-NV8o
Title: Trust, reliability, and safety in AI ft. Daniela Amodei of Anthropic and Sonya Huang
Published: 2024-03-26 21:18:19 UTC
Description:
Daniela Amodei, co-founder and president of Anthropic, sits with with Sonya Huang at Sequoia Capital's AI Ascent to discuss the launch of Claude 3, solving the business problems of trust and reliability in AI, the importance of transparency in research, and implementing technical safety approaches to make AI more aligned with the values of the human race. 

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
we are thrilled to have our next speaker with us uh Daniela is the uh president and co-founder of anthropic um which recently just launched the really impressive Claude 3 Model uh please welcome Danielle in conversation uh thank you so much for being here Daniela you're welcome M uh yes you do here take this oh that's so nice of you thank you I think everybody in the audience is amiliar with anthropic as probably a customer of yours uh but can you just do a quick refresher for everyone in the audience about anthropic the company what is your mission what's the future you imagine and um how are you building towards that future sure thing uh so first of all thanks so much for for having me uh great to be with all of you today so uh I'm Daniela I am a co-founder and president at anthropic we uh are a generative AI company that is really working to build uh powerful transformative generative AI tools that really have humans at the center of them so we have a huge focus on building this Tech in a way that is trustworthy and uh reliable and we've been around for just about uh three years a little over three years and uh in that time have been able to advance the State ofth art uh across generative AI on a number of of Dimensions wonderful and what are the things that what are the unique approaches that you're taking now that the foundation model space is getting very crowded what are the things that make you uniquely anthropic uh I love that question so uh first of all I would say there's there's a few different ways that I kind of uh like think about or or interpret that question one is really how do we kind of differentiate ourselves at the model level right what do we do when we're training the models or how do we want the models to sort of have people feel when they when they use them and here what I would say is we really uh again thinking about this kind of commitment to trustworthiness reliability of our models we Implement a number of different sort of technical safety approaches to help uh make the models really more aligned with uh what humans want them to be doing so we pioneered a technique called uh constitutional AI which really enables uh the models to incorporate documents like the UN Declaration of Human Rights the Apple terms of service to really make it more aligned with uh with values of of the the sort of human race um from a sort of business perspective we really have tried to make uh Claud as approachable as possible in particular for Enterprise businesses so large businesses uh in particular I think have really resonated with our approach because they also value models that are helpful and honest and harmless right in general very large Enterprise businesses um tend to be uh concerned about models that will you know hallucinate or say something very very offensive wonderful uh let's talk about use cases I think one of the major questions people in the audience have today is uh where companies are finding the most product Market fit and I think you have a unique vantage point on that uh from anthropic what are the use cases that you see that are already reaching real product Market fit and what are the use cases that you think are on the come that are about to reach prodct Market fit so I think it it varies a little bit uh first of all just kind of depending on industry so there's some uh there's kind of some industries that I think are kind of quite advanced in in generative AI um unsurprisingly the technology industry has been you know an early adopter that's that's often how how it goes but I think something that has sort of been interesting for us to see is we we just released this new sort of Suite of of models the the CLA 3 Model we call it the model family and so the kind of biggest model CLA 3 Opus is the kind of state-of-the-art we sort of joke it's like the Rolls-Royce of the models it's incredibly capable and powerful and really what we've seen is you know not everybody needs the kind of top tier state-of-the-art model for all of their use cases is but the times when you do need it is when you need a model that is just incredibly intelligent capable and and Powerful so things like you know if you're doing scientific research or you're trying to have a model uh write very complex code for you at a fast pace or do you know complex macroeconomic policy analysis um CLA 3 Opus is like a great fit for that um CLA 3 Haiku which is the smallest model this is like the Ducati it's sort of the like racing motorcycle is amazing for things like customer support so really what we've seen in the industry is that um you know speed and cost are very important for anything that kind of requires real-time response rates and then Claud 3 Sonet which is sort of that middle Model A lot of Enterprise businesses are using for things like um day-to-day retrieval summary of information if they have unstructured data that they need to uh pull together and analyze and so I would say it varies by industry but it also sort of varies by use case and just how much uh ability C customers have to kind of choose between uh between what's available for them wonderful can you share one or two of your favorite use cases that people have built on anthropic yeah for sure um I would say I'm I'm like a do goter at heart so uh one of my favorite use cases is the Dana Farber Cancer Institute uses Claud to help with a genetic genetic analysis so looking for sort of cancer markers um I think there's also like much more kind of a a sort of boring application but there's a lot of kind of financial services firms like Bridgewater and street that are really using CLA to help them you know analyze financial information uh in in real time I think I like both of those because they really just sort of represent such a wide spectrum right I think it illustrates how truly general purpose these models are right it's a model that can help you to literally try and cure cancer faster but also to do sort of the day-to-day bread and butter of illegal services or financial services firms work Wonderful are you seeing more success uh in your customers finding product Market fit from startups or from Enterprises right now so I would say you know for anthropic in particular uh we have really focused on kind of the Enterprise use case and again this is really because we have felt such a resonance you know in approach um for businesses that are interested in building in ways that are you know trustworthy and reliable right all of the things we we've sort of been talking about um that being said I think there's a ton of innovation that is always happening in the start space and so something that I think is really interesting to watch is sometimes we'll have kind of a startup um sort of prototype something and we'll see like wow that's you know that's a really fascinating use case like we wouldn't have thought that you know you could use Claud that way and then that will become something that like Enterprise businesses sort of like later learn about because they know someone who works at that startup or they've kind of seen it in production so my sense is for us personally we're much more sort of you know building for and pivoted towards the Enterprise but I think there's really a wide wide ecosystem uh of development that that's happening uh in the business space wonderful on the Spectrum from prototyping to you know experimentation all the way to production where do you think most of your customers are today on that Journey yeah um I think on the kind of I think for this I'll like talk about Enterprise and then and then startups because they're a little bit different um I think for Enterprises it it actually ranges like pretty pretty widely um there's some businesses that I would even say have multiple kind of production use cases right where they might be using CLA internally to uh you know analyze health records or help doctors or nurses um you know analyze notes and save themselves administrative time so they can be with patients more but if they're a big company they might also be using it for a chat interface right so depending on the business use case sometimes they have you know multiple use cases in production but it's a little spiky right there might be times where one of those one of those use cases is like quite far along they've already been in production for like a year um they really like know the question right they come to us and they're like we really really want to optimize like this metric or we really care about price or we really care about latency and then there's businesses all the way on the other end of the spectrum who come to us and are like I've been hearing about generative AI like from my board can you help us understand is there a solution here right and so I think it um it does it does vary a lot but I will say Industries I have personally been surprised that some um industries that are not necessarily historically known for being early adopters like insurance companies or financial services um or Healthcare I think are actually um great candidates for incorporating this technology and and many of them have wonderful let's move on to Cloud 3 and and research uh maybe you just you just launched Cloud 3 maybe tell us a little bit about what into what went into it um and how the reception has been so far so uh yes we just uh just a couple of weeks ago launched uh clae 3 as I mentioned it's this sort of model uh family right so there's uh different uh models kind of available for different use cases again for businesses uh and really I think what has been so interesting is uh we've gotten great you know positive feedback about Claude of course there's always things that that we're improving and wanting to do better but some something that I have found you know really um just interesting is customers have sort of simultaneously commented on how kind of capable and Powerful the models are right they're the most intelligent state-of-the-art models available on the market today but people have also commented hey it's way harder to jailbreak these or the hallucination rates have kind of gone down a lot and so there has been this kind of dual language around both capability and safety and then the last piece which I always find um really interesting is um many customers have told us part of the appeal of Claude is that Claude feels more human um and so when people kind of interact with or talk to Claude we've sometimes heard folks say it really feels like talking to you know a trusted person versus talking to a robot that was kind of trained to sound like a human I love that uh and I've I think everyone here has seen all the eval charts I think Claude really one of the areas where it really spikes is in coding where I think the performance is is just off the charts right now maybe can you tell us a little bit about how you made the model so good at coding in particular and then how you see the role uh how you see AI software engineering playing out and anthropics role in it m so I think something that uh that is interesting that I've like learned from my research colleagues so I don't sort of pretend to be an expert uh on this is as the models just become generally more performative they kind of like get better at everything and so I think much of the same training uh techniques that we used to improve the model's you know accuracy and uh reading comprehension and general reasoning were also used to to improve its ability to code and I think that's something that again is kind of a fundamental interesting sort of research thing which is like Rising boat sort of lifts all tides that being said there's a lot of variety in these models and something I've always found interesting is certain models like people are like I always use this model for like task X right at the consumer level and other times folks will say this model like you absolutely have to use for for task y so I I think they're there is a little bit of almost um pull through personality that happens with these kind of regardless of of the Improvement it's kind of a useful caveat in terms of you know what are people doing in the sort of software engineering space and and kind of what is the role of these models um I'm I'm not a programmer so I feel like I'm I I probably can't opine on this as well as others but um much of what we have heard from our customers is that Claude is a great tool in helping you know people who write code so Claude cannot replace a human engineer uh you know yet but it can be a great kind of co-pilot in in helping love that maybe more of a Phil philosophical research question question um how do you think about the role of transparency in AI research especially as it seems like the AI field has become more and more closed anthropic has always uh felt very strongly about publishing um a large portion of our research so uh we don't publish everything but we have published something like two dozen uh papers the vast majority of them are actually technical uh safety uh or policy research papers and the reason that we choose to publish those are um as a public benefit Corporation we really view uh our job as helping to raise the watermark really across the industry in areas like safety so uh we have a team that focuses on something called mechanistic interpretability which is uh essentially the art of trying to figure out you know what is happening inside the black box that is these neural networks and it's a very kind of emerging field of of research uh there's like two or three teams in the entire world that work on it and we really feel like there's a lot of opportunity when kind of sharing that more broadly with the scientific Community to just increase understanding around around topics like that particularly in sort of the element of of safety so we've shared uh all of these research papers and then additionally we do a lot of work in kind of the policy sphere and try and publish uh research results papers our you know red teaming uh red teaming results as well thank you uh one of the big themes of today's event is trying to think about what's next um so I was hoping to ask from your from your Vantage Point what are the biggest challenges that you see your customers facing or your researchers thinking about when they're trying to build with llms like where are they you know hitting a wall uh and how is anthropic working to address some of those problems so I think there's a a few kind of classes of ways that that these models are still sort of they're still not perfect right um I think one big one is there are just fundamental kind of challenges to how these models are developed and trained and used so the kind of prototypical one that's talked about is this hallucination problem right I'm sure everyone in the room knows this but models are just trained to predict the next word and so sometimes they don't know the right answer and so they just make something up and we have made a huge amount of progress as an industry in reducing hallucination rates from like the gpt2 era but they're still not perfect I'm not entirely sure like what the sort of like decrease Curve will look like for hallucination rate right we keep getting better at it I'm not sure if we'll ever be able to get models to zero um that is a fundamental challenge for businesses right if your model is going to even very occasionally hallucinate for some of the highest Stakes decisions you probably wouldn't choose to use a model alone right you would say hey we need a human in the loop and I do think something that's kind of very interesting is there's there's a really small set of cases today where llms alone can do the majority of the task right like their best again I think in t with a human for the majority of of kind of use cases I also think there's just sort of this interesting um it it almost feels a little more philosophical which is just what are humans actually comfortable with giving to models right I think part of the sort of human in the loop story is also about helping um you know businesses and industries and individuals feel more comfortable with an AI tool making fundamental decisions thank you for sharing that uh a few of the folks here uh spoke about plan and reasoning is that something you all are thinking about at anthropic and could you share a few words on that yeah definitely um so that can obviously mean a a few things so I think on the kind of dimension of like how do you get these models to sort of like execute sort of multi-step instructions right I'm assuming that's kind of what what planning means um you know it's it's really interesting there's a lot of research and and kind of work that has gone into uh this this sort of concept of like agents right like how do you give the models ability to like take control of something and like you know execute multiple actions in a row and like can they plan right can they can they sort of think through like a a set of steps I do think that Claude 3 sort of represented for us a leap uh between kind of the last generation of models in it sort of ability to do that but I actually think that level of kind of agentic behavior is still really hard like I think the models cannot quite quite do that reliably yet again this feels like such a sort of fundamental research question that I don't know how long it will be until that's not the case but I don't think it's the the sort of you know the dream of like can I just ask Claude to book my flight for me like please go book my reservation hotel just plan my vacation I don't actually think that that's like immediately around the corner I think there's still some some research work and and uh engineering work that needs to go into making that possible yep yep okay so the the future is coming but maybe not as quickly as we think the future is coming quickly it's also coming choppily it's a little unclear exactly which parts of it are going to come where okay very cool uh can we talk about AI safety for for a moment anthropic really made a name for itself on AI safety and I think you were the first major research institution to publish your responsible scaling policies um how do you balance Innovation and and accountability and how would you encourage other companies in the ecosystem to do that as well so something that we um that we kind of get asked a lot is is you know how do you all plan to compete if you're you know so committed to safety and something that I think has been you know really interesting is many fundamental safety challenges are actually business challenges and rather than sort of thinking of these two as something that is you know two sides that are kind of opposed to each other I actually think the path to kind of mline success in generative AI development runs through many of the safety topics we've been talking about right uh most businesses don't want models that are going to like spout harmful garbage right like that's just not a useful product the same thing is true like if the model refuses to answer your questions if it's if it's uh if it's dishonest right if it makes things up those are sort of fundamental business challenges in addition to kind of technical safety challenges I also think something we have really aimed to do as a business is sort of take the responsibility of developing this very powerful technology quite seriously right we uh we sort of have the benefit of being able to look back on several decades of social media and say like wow much of what social media did for the world was incredibly positive and there were these externalities that nobody predicted that it created which I think are sort of now widely believed to be quite negative for people so I think anthropic has always aimed to say what if we could try and sort of build this technology in a way that better anticipates what some of those risks are and helps to prevent them and the responsible scaling policy is basically our first attempt to do that right it might not be perfect there could be things about it that are sort of laughably wrong later but really what we've said are you know what are the dimensions on on which something can go wrong here right and um you know our CEO my brother Dario testified to Congress about the potential risks for generative AI to develop things like chemical and biological weapons and what we've said is we actually have to do proactive work to ensure that these models are not able to do that and the responsible scaling pact is really just a way of sort of saying hey we're committing to doing that work thank you for sharing that uh let's see any questions from the audience yes thanks so much um one of the things that I think was really awesome about the the Claude Opus release was that it was really strong specific performance in a few domains of interest and so I was wondering if you could talk more about um kind of like technically how you view the importance of research versus compute versus data for specific domain outperformance and what the road map looks like for where CLA um will continue to get better yeah um that's a that's a great question I think my real answer is that I think you're probably giving the industry more credit than it deserves for having some like perfectly uh sort of planned structure between like we'll we'll sort of you know research area X and like increased compute will improve y um I think I think there's a way in which training these large models is more a process of uh Discovery by our researchers than kind of uh intentional deliberate decisions to like improve particular areas to kind of go back to that like Rising tide lifts all boat sort of analogy um making the models just generally more performative tends to just make everything better sort of across the board that being said there is sort of particular targeted work that we did do in some sub areas with constitutional Ai and reinforcement learning from from Human feedback where we just saw that performance wasn't wasn't quite as good um but it's actually a smaller fraction than you might think compared to just generally improving the models and making them better it's a great question yes Sam um I've been loving playing with Claude 3 Claud Opus it's fantastic and I totally agree it feels way more human to talk to one thing I've noticed that it almost feels like a specific human like it has a a personality and I'm kind of curious as you guys continue to work in this domain and make other models how you see the boundary of um kind of like personality development if people are kind of trying to create specific characters um is there kind of a stance you guys are taking from the constitutional perspective of the boundaries of how Claude can actually play a character other than itself so something that is really I think unusual about kind of Claude is just how like seriously Claude will take feedback about about its tone right if you're like Claude you are this is this is too wordy like please just be very factual and talk to me like I am a financial analyst like try it out Claude will absolutely sort of adjust its style to be more kind of in that in that sort of you or hey I'm writing you know a creative writing story like please use very flowery language or talk to me like you're angry at me or talk to me like you're sort of you know friendly or whatever um I think I think there's sort of an interesting other thing you're asking though which is like what is the default mode that we should be setting these models kind of personalities to be and I don't think we've I don't think we've sort of landed on kind of the perfect the perfect spot but really what we were aiming for was like what is a slightly wiser better version of us kind of how would they react to questions right like some humility I'm oh I'm sorry I missed that um or thanks so much for the feedback like I'll try to do that better I think there's kind of an interesting fundamental question though which is as the kind of marketplace evolves do people want like particular types of of kind of chat Bots or chat interfaces to sort of treat them differently right like you might want to sort of coax a particular form of customer service bot to be like particularly obsequious or um I don't know there there are just kind of other potential use cases my guess is that's probably going to end up being the province of like startups that are built on top of tools like Claude um and I think our stance might might vary a little bit there but in general we've tried to start from a like friendly humble uh base and then let people tweak them as they as they go within boundaries of course hey um so the developer experience on Claude and the you know the new generation of Claude 3 models is markedly different than other llm providers um especially the use of XML as like a prompt templating format how are you thinking about introducing switching costs here and especially in the long term do you want it to be an open ecosystem where it's very easy to switch between um anthropic and your various competitors or are you thinking about making more of a closed ecosystem where you know I'm working directly with anthropic for all of my model needs so I think I think maybe the best way to answer this is what we've seen kind of in the market today which is that most like big businesses are interested in at some point you know some some of them just use one model but they they like to try them out and my guess is that likely developers will have that same Instinct right so I think the more kind of open hey like it's whatever it's easy to download your data move it over um I think that's the sort of goal that we're trying to eventually aim towards the one sort of difference I would say is that often developers particularly when they're just getting started are like the switching costs are just more laborious for them right they're like hey I'm I'm I'm building on this tool it's annoying to switch like it's complicated to switch you have to sort of redo your prompts because all of the models like react a little bit differently just depending on and like we have great prompt engineering resources like please check them out and also it just takes some time and effort to like understand the kind of new personality of the model that you're using so I think my kind of short answer is yes we're aiming for sort of that more open ecosystem but also it's it's sort of tactically hard to do in kind of a perfect way um with interpretability research I'm curious what you think is coming first to the product like what what is looking most optimistic where I could say like turn on a switch and have it only output Arabic or something like that what what do you think is like closest working so interpretability is a is a team that is deep Dey close to my heart despite me like not being able to contribute anything of value to them other than telling them how great they are I think interpretability is to me like the coolest and most exciting area of AI research today because it's fundamentally trying to figure out like what what are these models actually doing right it's it's like the Neuroscience of of of like large models I actually think we're like not impossibly far but like not that close from being able to sort of productionize something in interpretability today right the kind of Neuroscience analogy is a little bit strange but I actually think it's it's relevant in one particular way which is that like we can have a neuroscientist like look at your brain and be like well we know that these two things light up when you think about dogs but it can't sort of like change you thinking about dogs right it's like you can sort of diagnose and understand and see things but you can't actually like go in and change them yet and I think that's about where we are at sort of the interpret level could we offer some insight like in the future I think almost certainly yes probably not even on a a crazy long time skill right we could say hey if you're playing with sort of you know this type of model and it's it's you know it's activating Strangely I think that's the type of thing we could like show a sort of visualization to a customer of I don't actually know how actionable it is if that makes sense right in sort of the same way or like well these these sort of two parts of the model are are lighting up or this set of neurons is activating um but I think it's it's it's an interesting area of like very basic science or basic research that I think could have incredible potential applications like a couple of years from now I'll ask a question uh what maybe give the folks here A Taste of what's going to come on the product road map let's assume that Claude gets smarter and smarter but what are you all going to add on the developer facing product and then what should we expect in terms of first party products from you so uh first of all we uh we are just sort of scrambling day in day out to try and keep up with the uh incredible demand that we have so we are incredibly grateful for everybody's patience but I think really on the kind of you know developer side we really want to just uplevel the tools that are available for developers to be able to kind of make make the most use of of Claud sort of broadly um I think something that's really interesting just sort of speaking to the kind of ecosystem point is there's so much opportunity for like knowledge sharing and sort of learning between developers and between people that are kind of using these models and tools so we're also very interested in just sort of figuring out how to host more information sharing about how to get the most out of these models as well wonderful yes oh you have the mic yes go for it um given your um focus on safety I was hoping you could comment on how you see the regulatory landscape evolving um maybe not so much for you specifically but for the companies that are using your models and others so something that I think is just always an unknown is like what what's going to happen in the regulatory landscape and how is it going to impact like how we build and do our work kind of in in this space I think I mean first of all I don't have any amazing pressions to say like this set of regulations I expect will happen but I I imagine what we'll see is kind of on it will probably start from a place of the consumer because that's really what kind of governments and Regulators are sort of most well positioned to try and defend or protect and I think a lot of the kind of narrative around data privacy is one that I expect will sort of see emerge right around just hey what are you doing with my data right people put personal things into these into sort of these interfaces and they want to know like are the companies being responsible with that information right what are they doing to protect it are they de anonymizing it we don't train on people's data but if C if other companies do like what does that mean for that person's information um completely speculative but that sort of is my guess of of where things will start I also think there's a lot of um interest and activation in sort of the policy space right now around like how to develop these models in a way that is safe from a sort of bigger picture like capital S perspective right some of the sort of scary things I I talked about but again like regulation is is a sort of it's a long process and I think something we' have always aimed to do is work closely with policy makers to give them as much information as possible so that there is thoughtful reg regulation that will you know prevent some of the potentially bad outcomes without sort of stifling Innovation thank you danela thank you do we have time for one more question okay one more I'm getting I'm getting looks from Emma sorry hey Danel Claud fre is awesome thank you um when you think about the model family and the hierarchy of models you have any thoughts on whether um it is effective to use prompts or you've done any work internally on giving the smaller models Insight that larger models are available uh to kind of say hey this is beyond my knowledge but this is a good time to use the larger model that is such a good idea are you looking for a job that is a that's a great idea um that has not been something we have currently uh trained the models to do I actually think it's a great idea something we something we have thought about is just how to kind of make the process of switching between models within a business just much more seamless right you can imagine that over time the model should know like hey you're you're not actually like trying to look at like macroeconomic Trends in like the 18th century right now you're just like trying to answer a sort of Frontline question you don't need Opus you need Haiku and I think some of that is sort of a research Challenge and some of it is actually just a product and engineering challenge right which is how well can we kind of get the models to self-identify the level of difficulty and really sort of price optimize right for customers to say you don't actually need Opus to do this task it's really really simple pay you know a tiny fraction of the cost for haou and we'll just switch you to Sonet if it's sort of somewhere in the middle um we're we're not like we're not there yet but I think that's definitely something we've been we've been thinking about and a request we've been hearing from from customers but I love your idea of adding in the sort of um the sort of like self- knowledge of the models it's a cool idea the callif friend exactly yeah wonderful thank you so much Daniela thank you for sharing with us today thanks for having appreciate [Applause] it

========================================

--- Video 79 ---
Video ID: yinHx5UnYs0
URL: https://www.youtube.com/watch?v=yinHx5UnYs0
Title: Open sourcing the AI ecosystem ft. Arthur Mensch of Mistral AI and Matt Miller
Published: 2024-03-26 21:18:16 UTC
Description:
Arthur Mensch, founder of Mistral AI, speaks with Matt Miller at Sequoia Capital's AI Ascent about his mission to bring AI to all developers, pushing for more open platforms and spreading the adoption of AI, as well as the balancing open source efforts while pursuing commercial opportunities.

Transcript Language: English (auto-generated)
I'm excited to introduce our first Speaker uh Arthur from mistol uh Arthur is the founder and CEO of mistal AI despite just being nine months old as a company uh and having many fewer resources than some of the large Foundation model companies so far I think they've really shocked Everybody by putting out incredibly high quality models approaching GPT 4 and caliber uh out into the open so we're thrilled to have Arthur with us today um all the way from BRS to share more about the opportunity behind building an open source um and please uh interviewing Arthur will be my partner Matt Miller who is dressed in his best French wear to to honor Arthur today um and and and helps lead lead our efforts in Europe so please Welcome Matt and [Applause] Arthur with all the efficiency of a of a French train right just just just just right on time right on time we we're sweating a little bit back there cuz just just just walked in the door um but good to see you thanks for thanks for coming all this way thanks for being with us here at aisn today thank you for hosting us yeah absolutely would love to maybe start with the background story of you know why you why you chose to start mrra and and maybe just take us to the beginning you know you we all know about your career at Deep your successful career at Deep Mind your work on the chinchilla paper um but tell us maybe share with us we always love to hear at seoa and I know that our founder commun also L to hear that spark that like gave you the idea to to launch and to to start to break out and start your own company yeah sure um so we started the company in April but I guess the ID was out there for a couple of months before uh timot and I were in master together G and I were in school together so we knew each other from before and we had been in the field for like 10 years uh doing research uh and so we loved the way AI progressed because of the open exchanges that occurred between uh academic Labs uh industrial Labs uh and how everybody was able to build on on on top of one another and it was still the case I guess when uh in between even in the beginning of the llm era where uh openi and deep mine were actually uh like uh contributing to another one another road map and this kind of stopped in 2022 so basically the one of the last uh paper doing important changes to the way we train models was chinchila and that was the last Model that uh Google ever published uh last important model in the field that Google published and so for us it was a bit of a shame that uh we stopped uh that the field stopped doing open uh open contributions that early in the AI Journey because we are very far away from uh finishing it uh and so when we saw chat GPT at the at the end of the year and um and I think we reflect on the fact that there was some opportunity for doing things differently for doing things from France because in France you have as it turned out there was a lot of talented people that were a bit bored at in big tech companies and so that's how we figured out that there was an opportunity for building very strong open source models going very fast with a lean team uh of experienced people uh and show yeah and try to correct the the the direction that the field was taking so we wanted to push it to push the open Source model is much more and I think we did a good job at that because we've been followed by various companies uh in our trajectory wonderful and so it was really a lot of the open source move movement was a lot of the a lot of the drive behind starting the company yeah that's one of one of the yeah that was one of the driver uh Our intention and the mission that we gave ourselves is really to bring AI to the hands of every developers and the way it was done and the way it is still done by our competitors is very closed uh and so we want to push a much more open platform and we want to spread the adoption and accelerate the adoption through that strategy so that's very much uh at the core well the reason why we started the company indeed wonderful and you know just recently I mean fast forward to today You released Mr all large you've been on this tear of like amazing Partnerships with Microsoft snowflake data bricks announcements and so how do you balance the what you're going to do open source with what you're going to do commercial commercially and how you're going to think about the the tradeoff because that's something that you know many open source companies contend with you know how do they keep their Community thriving but then how do they also build a successful business to contribute to their Community yeah it's it's a hard question and the way we've addressed it is currently through uh two families of model but this might evolve with time um we intend to stay the leader in open source so that kind of puts a pressure on on the open source family because there's obviously some contenders out there um the I think compared to to how various software providers playing this strategy uh developed we need to go faster uh because AI develops actually faster than software develops faster than databases like mongodb played a very good game at that and this is a good uh a good example of what we could do uh but we need to adapt faster so yeah uh yeah there's obviously this tension and we're constantly thinking on how we should contribute to the community but also how we should show and start uh getting some commercial adoption uh Enterprise deals Etc and this is uh there's obviously a attention and for now I think we've done a good job at at doing it but it's it's very it's a very Dynamic thing to to think through so it's basically every week we think of uh what we should release next on the on both families and you have been the fastest uh in developing models fastest reaching different benchmarking levels you know one of the most leanest in amount of expenditure to reach these benchmarks out of any of the any of the foundational model companies what do you think is like giving you that advantage to move quicker than your predecessors and more efficiently well I think we like to do uh the like get our hands dirty uh it's uh machine learning has always been about crunching numbers uh looking at your data uh doing a lot of uh extract transform and load and things that are uh oftentimes not fascinating and so we hired people that were willing to do the dot stuff uh and I think that's a uh that has been critical to our speed and that's something that we want to to keep awesome and the in addition to the large model you also have several small models that are extremely popular when would you tell people that they should spend their time working with you on the small models when would you tell them working on the large models and where do you think the Economic Opportunity from mrol lies is it in doing more of the big or doing more of the small I think and I think this is um this is an observation that every llm provider has made uh that like one size does not fit all and uh depending on what you want to when you make an application you typically have different large language model calls and some should be low latency and because they don't require a lot of intelligence but some should be higher latency and require more intelligence and an efficient application should leverage both of them potentially using the large models as an orchestrator for the small ones um and I think the challenge here is how do you make sure that everything works so you end up with like a system that is not only a model but it's really like two models plus an out Loop of of calling your model calling systems calling functions and I think some of the developer challenge that we also want to address is how do you make sure that this works that that you can evaluate it properly how do you make sure that you can do continuous integration how do you how do you change like one how do you move from one version to another of a model and make sure that your application has actually improved and not deteriorated so all of these things are addressed by various companies uh but these are also things that we think should be core to our value proposition and what are some of the most exciting things you see being built on mrra like what are the things that you get really excited about that you see the community doing or customers doing I think pretty much uh every young startup in the Bay area has been using it for like fine tune fine-tuning purposes for fast application making uh so really I think one part of the value of mix for instance is that it's very fast and so you can make applications that uh are more involved uh and so we've seen uh we've seen web search companies using us uh we've seen uh I mean all of the standard Enterprise stuff as well like uh Knowledge Management uh marketing uh the fact that you have access to the weights means that you can pour in your editorial tone much more uh so that's yeah we we see the typical use cases I think the the but the value is that uh for of the open source part is that uh developers have control so they can deploy everywhere they can have very high quality of service because they can uh use their dedicated instances for instance and they can modify the weights to suit their needs and to bump the performance to a level which is close to the largest ones the largest models while being much cheaper and what what's the next big thing do you think that we're going to get to see from you guys like can you give us a sneak peek of what might be coming soon or how what we should be expecting from MRA yeah for sure so we have uh so Mr Large was good but not good enough so we are working on improving it quite quite heavily uh we have uh interesting open source models uh on various vertical domains uh that will be announcing very soon um we have uh the platform is currently just apis like serverless apis uh and so we are working on making customization part of it so like the fine tuning part um and obviously and I think as many other companies we we're heavily betting on multilingual uh data and and multilingual model uh because as a European company we're also well positioned and this is the demand of our customers uh that I think is higher than here MH um and then yeah eventually uh in the months to come we are we will also release some multimodal models okay exciting we we look forward to that um as you mentioned many of the people in this room are using mrol models many of the companies we work with every day here in the silan valley ecosystem are working already working with mrol how should they work with you and how should they work work with the company and what what type of what's the best way for them to work with you well well they can reach out so we have uh some developer relations that are really uh like pushing the community forward making guides uh also Gathering use cases uh to Showcase what you can build uh with mral model so this is we're very uh like investing a lot on the community um something that basically makes the model better uh and that we are trying to set up is our ways to for us to get evaluations benchmarks actual use cases on which we can evaluate our models on and so having like a mapping of what people are building with our model is also a way for us to make a better generation of new open source models and so please engage with us to uh discuss how we can help uh how discuss your use cases we can advertise it uh we can uh also gather some insight of of the new evaluations that we should add to our evaluation suit to verify that our model is are getting better over time MH and on the commercial side our models are available on our platform so the commercial models are actually working better than than the the open source ones they're also available on various Cloud providers so that it facilitates adoption for Enterprises um and customization capabilities like fine-tuning which really made the value of the open source models are actually coming very soon wonderful and you talked a little bit about the benefits of being in Europe you touched on it briefly you're already this example Global example of the great innovations that can come from Europe and are coming from Europe what you know talk a little bit more about the advantages of building a business in France and like building this company from Europe the advantage and drawbacks I guess yeah both both I guess what one advantage is that you have a very strong junior pool of talent uh so we there's a lot of uh people coming from Masters in France in Poland in the UK uh that we can train in like three months and get them up to speed get get them basically producing as much as a as a million dollar engineer in the Bay Area for 10 times 10 10 times less the cost so that's that's kind of efficient sh don't tell them all that they're goingon to hire people in France sure uh so that like the the workforce is very good engineers and uh and machine learning Engineers um generally speaking we have a lot of support from uh like the state which is actually more important in Europe than in in the US they tend to over regulate a bit bit too fast uh we've been telling them not to but they don't always listen uh and then generally uh I mean yeah like European companies like to work with us because we are European and we we are better in European languages as it turns out like French uh the the French Mr Large is actually probably the strongest French model out there uh so yeah that's uh I guess that's not an advantage but at least there's a lot of opportunities that are geographical and that we're leveraging wonderful and you know paint the picture for us 5 years from now like I know that this world's moving so fast you just think like all the things you've gone through in the two years it's not even two years old as a company almost two years old as a company um but but five years from now where does mrr sit what do you think you have achieved what what does this landscape look like so our bet is that uh basically the platform and the infrastructure uh of int of artificial intelligence will be open yeah and based on that we'll be able to create uh assistance and then potentially autonomous agent and we believe that we can become this platform uh by being the most open platform out there by being independent from cloud providers Etc so in five years from now I have literally no idea of what this is going to look like if you were if you looked at the field in like 2019 I don't think you could bet on where we would be today but we are evolving toward more and more autonomous agents we can do more and more tasks I think the way we work is going to be changed profoundly and making such agents and assist is going to be easier and easier so right now we're focusing on the developer world but I expect that like AI technology is in itself uh so uh easily controllable through human languages human language that potentially at some point the developer becomes the user and so we're evolving toward uh any user being able to create its own assistant or its own autonomous agent I'm pretty sure that in five years from now this will be uh uh like something that you learn to do at school awesome well we have about five minutes left just want to open up in case there's any questions from the audience don't be shy son's got a question how do you see the future of Open Source versus commercial models playing out for your company like I think you made a huge Splash with open source at first as you mentioned some of the commercial models are even better now how do you imagine that plays out over the next cample of years well I guess the one thing we optimize for is to be able to continuously Produce open model with a sustainable business model to actually uh like fuel the development of the Next Generation uh and so that's I think that's as I've said this is uh this is going to evolve with time but in order to stay relevant we need to stay uh the best at producing open source models uh at least on some part of the spectrum so that can be the small models that can be the very big models uh and so that's very much something that basically that sets the constraints of whatever we can say we can do uh staying relevant in the open source uh World staying the best best uh solution for developers is really our mission and and we'll keep doing it David there's got to be questions for more than just the Sequoia Partners guys come on you talk to us a littleit about uh llama 3 and Facebook and how you think about competition with them well lfre is working on I guess uh making models I'm not sure they will be open source I have no idea of what's going on there uh so far I think we've been delivering faster and smaller models so we expect expect to be continuing doing it but uh generally the the good thing about open source is that it's never too much of a competition because uh uh once you have like uh if you have several actors normally that should actually benefit to everybody uh and so there should be some if if they turn out to be very strong there will be some cination and and we'll welcome it one thing that's uh made you guys different from other proprietary model providers is the Partnerships with uh snowflakes and data bricks for example and running natively in their clouds as to sort of just having API connectivity um curious if you can talk about why you did those deals and then also what you see is the future of say data bricks or snowflake in the brave new LM world I guess you should ask them but uh I think generally speaking AI models become very strong if they are connected to data and grounding uh yeah grounding information as it turns out uh the Enterprise data is oftentimes either on snowflake or on data rcks or sometimes on AWS uh and so being able for customers for customers to be able to deploy the technology exactly where their data is uh is I think quite important I expect that this will continue continue doing the ca being the case uh especially as I believe we'll move onto more stateful AI deployment so today we deploy serverless apis with not much State it's really like Lambda uh Lambda functions but as we go forward and as we make models more and more specialized as we make them uh more tuned to use cases and as we make them um self-improving you will have to manage State and those could actually be part of the data cloud or so there there's an open question of where do you put the AI State and I think that's the uh my understanding is that Snowflake and datab Bricks would like it to be on their data cloud and I think there's a question right behind him the grace I'm curious where you draw the line between uh openness and proprietary so you you're releasing the weights would you also be comfortable sharing more about how you train the models the recipe for how you collect the data how you do mixure experts training or do you draw the line at like we release the weights and the rest is proprietary so that's where we draw the line and I think the the reason for that is that it's a very competitive landscape uh and so it's uh similar to like the tension there is in between having a some form of Revenue to sustain the Next Generation and there's also tension between what you actually disclose and and everything that yeah in order to stay ahead of of the curve and not to give your recipe to your competitors uh and so again this is this is the moving line uh if there's also some some Game Theory at at stake like if everybody starts doing it then then we could do it uh but for now uh for now we are not taking this risk indeed I'm curious when an when another company releases weights for a model like grock for example um and you only see the weights what what kinds of practices do you guys do internally to see what you can learn from it you can't learn a lot of things from weights we don't even look at it it's actually too big for us to deploy a gr is is quite big or uh was there any architecture learning I guess they have they are using like a mixture of expert uh pretty standard setting uh with a couple of Tricks uh that I knew about actually but uh yeah that's uh uh there's there's not not a lot of things to learn from the recipe themselves by looking at the weights you can try to infer things but that's like reverse engineering is not that easy it's basically compressing information and it compresses information sufficiently highly so that you can't really find out what's going on coming the cube is coming okay it's okay uh yeah I'm just curious about like um what are you guys going to focus on uh the model sizes your opinions on that is like you guys going to still go on the small or uh yeah going to go to the larger ones basically so model size are kind of set by like scaling lows so it depends on like the compu you have based on the computer you have based on the The Landing AR infrastructure you want to go to you make some choices uh and so you optimize for training cost and for inference cost and then there's obviously um uh there's the weight in between between uh like for depends on the weight that you put on the training cost amortization uh the more you amortize it the more you can compress models uh but basically our goal is to be uh low latency and to be uh relevant on the reasoning front so that means having a family of model that goes from the small ones to the very large ones um hi are there any plans for mistol to exp expand into uh you know the application stack so for example open a released uh the custom gpts and the assistance API is that the direction that you think that M will take in the future uh yeah so I think as I've said the we're really focusing on the developer first uh but there's many um like the the frontier is pretty thin in between developers and users for this technology and so that's the reason why we released like a an assistant demonstrator called lha which is the cat in English and uh it's uh the point here is to expose it to Enterprises as well and be make them able to connect their data connect their context um I think that's that that answers some some need from our customers that many of of the people we've been talking to uh are willing to adopt the technology but they need an entry point and if you just give them apis they're going to say okay but I need an integrator and then if you don't have an integrator at end and often times this is the case it's good if you have like an off the shelf solution at least you get them into the technology and show them what they could build for their core business so that's the reason why we now have like two product offering there the first one which is the platform and then we have the sh uh which should evolve into an Enterprise off the shelf solution more over there there there I'm just wondering like where would you be drawing the line between like stop doing prompt engineering and start doing like fine tuning because like a lot of my friends and our customers are suffering from like where they should be stopped doing more PRT engineering yeah I think that's that's the number one pain Point uh that is hard to solve uh from from a product product standpoint uh the question is normally your workflow should be what should you evaluate on and based on that uh have your model kind of find out a way of solving your task uh and so right now this is still a bit manual you you go and and you have like several versions of prompting uh but this is something that actually AI can can help solving uh and I expect that this is going to grow more and more automatic across time uh and this is something that yeah we would love to try and enable I wanted to ask a bit more of a personal question so like as a Founder in The Cutting Edge of AI how do you balance your time between explore and exploit like how do you yourself stay on top of like a field that's rapidly evolving and becoming larger and deeper every day how do you stay on top so I think this question has um I mean we explore on the science part on the produ part and on the business part uh and the way you balance it is is effectively hard for a startup you do have to explore it a lot because you you need to ship fast uh but on the science part for instance we have like two or three people that are like working on the next generation of models and sometimes they lose time but if you don't do that you're at risk of becoming irrelevant and this is very true for the product side as well so being right now we have a fairly simple product but being able to try out new features and see how they pick up is something that we we are we need to do and on the business part you never know who is actually quite mature enough to to use your technology so yeah the balance between uh exploitation and exploration is something that we Master well at the science level because we've been doing it for years uh and somehow it transcribes into the product and the business but I guess we're currently still learning to do it properly so one more question for me and then I think we'll be we'll be done we're out of time but you know you've in at the scope of two years models big models small that have like taken the World by storm killer go to market Partnerships you know just tremendous momentum at the center of the AI ecosystem what advice would you give to Founders here like what you have achieved in the pace of what you have achieved is truly extraordinary and what advice would you give to people here who are at different levels of starting and running and building their own businesses in it around the AI opportunity I would say it's it's always day one so I guess we yeah we are uh I mean we got some mind share but there's I mean there's still many proof points that we need to establish uh and so yeah like being a Founder is basically waking up every day and and figuring out that uh you need to build everything from scratch every time all the time so it's uh it's I guess a bit exhausting but it's also exhilarating uh and so I would recommend to be quite ambitious usually uh being more ambitious uh I mean ambition can get you very far uh and so you yeah you should uh dream big uh that's that would be my advice awesome thank you arur thanks for being with us [Applause] today

========================================

--- Video 80 ---
Video ID: sal78ACtGTc
URL: https://www.youtube.com/watch?v=sal78ACtGTc
Title: What's next for AI agentic workflows ft. Andrew Ng of AI Fund
Published: 2024-03-26 21:18:13 UTC
Description:
Andrew Ng, founder of DeepLearning.AI and AI Fund, speaks at Sequoia Capital's AI Ascent about what's next for AI agentic workflows and their potential to significantly propel AI advancements—perhaps even surpassing the impact of the forthcoming generation of foundational models. 

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
all of you uh know Andreu in as a famous uh computer science professor at Stanford was really early on in the development of neural networks with gpus of course a creator of corsera and popular courses like deeplearning.ai also the founder and Creator uh and early lead of Google brain uh but one thing I've always wanted to ask you before I hand it over Andrew while you're on stage uh is a question I think would be relevant to the whole audience 10 years ago on problem set number two of cs229 you gave me a b and I was wondering I looked it over I was wondering what you saw that I did incorrectly so anyway Andrew thank you Hansen um looking forward to sharing with all of you what I'm seeing with AI agents which I think is the exciting Trend that I think everyone building in AI should pay attention to and then also excited about all all the other uh on Sak presentations so hey agents you know today the way most of us use Lish models is like this with a non- agentic workflow where you type a prompt and generates an answer and that's a bit like if you ask a person to write an essay on a topic and I say please sit down to the keyboard and just type the essay from start to finish without ever using backspace um and despite how hard thises is L's do it remarkably well in contrast with an agentic workflow this is what it may look like have an AI have an LM say write an essay outline do you need to do any web research if so let's do that then write the first draft and then read your own first draft and think about what parts need revision and then revise your draft and you go on and on and so this workflow is much more iterative where you may have the L do some thinking um and then revise this article and then do some more thinking and iterate this through a number of times and what not many people appreciate is this delivers remarkably better results um I've actually been really surprised myself working these agent workflows how well how well they work I's do one case study at my team analyzed some data uh using a coding Benchmark called the human eval Benchmark released by open a few years ago um but this says coding problems like given the nonent list of integers return the sum of all the all elements are an even positions and it turns out the answer is you code snipper like that so today lot of us will use zero shot prompting meaning we tell the AI write the code and have it run on the first spot like who codes like that no human codes like that just type out the code and run it maybe you do I can't do that um so it turns out that if you use GPT 3.5 uh zero shot prompting it gets it 48% right uh gp4 way better 607 7% right but if you take an agentic workflow and wrap it around GPT 3.5 I say it actually does better than even gbd4 um and if you were to wrap this type of workflow around gb4 you know it it it also um does very well and you notice that gbd 3.5 with an agentic workflow actually outperforms gp4 um and I think this has and this means that this has signant consequences fighting how we all approach building applications so agents is the ter of around a lot there's a lot of consultant reports talk about agents the future of AI blah blah blah I want to be a bit concrete and share of you um the broad design patterns I'm seeing in agents it's a very messy chaotic space tons of research tons of Open Source there's a lot going on but I try to categorize um bit more concretely what's going on agents reflection is a tool that I think many of us should just use it just works uh to use I think it's more widely appreciated but actually works pretty well I think of these as pretty robust technology when I use them I can you know almost always get them to work well um planning and multi-agent collaboration I think is more emerging when I use them sometimes my mind is blown for how well they work but at least at this moment in time I don't feel like I can always get them to work Rel Lively so let me walk through these four design patterns in the few slides and if some of you go back and yourself will ask your engineers to use these I think you get a productivity boost quite quickly so reflection here's an example let's say ask a system please write code for me for a given task then we have a coder agent just an LM that you prompt to write code to say you def du task write a function like that um an example of self-reflection would be if you then prompt the LM with something like this here's code intended for a toas and just give it back the exact same code that they just generated and then say check the code carefully for correctness sound efficiency good construction CRI just write prompt like that it turns out the same l that you prompted to write the code may be able to spot problems like this bug in line Five May fix it by blah blah blah and if you now take his own feedback and give it to it and reprompt it it may come up with a version two of the code that could well work better than the first version not guaranteed but it works you know often enough for this be wor trying for a lot of applications um to foreshadow to use if you let it run unit test if it fails a unit test then he why do you fail the unit test have that conversation and be able to figure out fail the unit test so you should try changing something and come up with V3 by the way for those of you that want to learn more about these Technologies I'm very excited about them for each of the four sections I have a little recommended reading section at the bottom that you know hopefully gives more references and again just the foreshadow multi-agent systems I've described as a single coder agent that you prompt to have it you know have this conversation with itself um one Natural Evolution of this idea is instead of a single code agent you can can have two agents where one is a coder agent and the second is a Critic agent and these could be the same base LM model but that you prompt in different ways where you say one your expert coder right code the other one say your expert code review to review this code and this Tye of workflow is actually pretty easy to implement I think it's such a very general purpose technology for a lot of workflows this would give you a significant boost in in the performance of LMS um the second design pattern is to use many of where already have seen you know LM based systems uh uh using tools on the left is a screenshot from um co-pilot on the right is something that I kind of extracted from uh gp4 but you know LM today if you ask it what's the best coffee maker web search for some problems um will generate code and run code um and it turns out that there are a lot of different tools that many different people are using for analysis for gathering information for taking action for personal productivity um it turns out a lot of the early work in two use turned out to be in the computer vision Community because before large language models lm's you know they couldn't do anything with images so the only option was that the LM generate a function called that could manipulate an image like generate an image or do object detection or whatever so if you actually look at literature it's been interesting how much of the work um in two years seems like it originated from Vision because LMS would blind to images before you know gp4 and and and lava and so on um so that's two use and it expands what an LM can do um and then planning you know for those of you that have not yet played a lot with planning algorithms I I feel like a lot of people talk about the chat GPT moment where you're wow never seen anything like this I think if not used planning alums many people will have a kind of a AI agent wow I couldn't imagine the AI agent doing this I've run live demos where something failed and the AI agent rerouted around the failures I've actually had quite a few of those moment wow you can't believe my AI system just did that autonomously but um one example that I adapted from a hugging GPT paper you know you say this general image where the girls read where a girl is reading a book and it posts the same as a boy in the image example. jpack and please subscribe the new image for your voice so give an example like this um today we have ai agents who can kind of decide first thing I need to do is determine the post of the boy um then you know find the right model maybe on hugging face to extract the post then next need to find a post image model to synthesize a picture of a of a girl of as following the instructions then use image to text to and then finally use text of speech and today we actually have agents that I don't want to say they work reliably you know they're kind of finicky they don't always work but when it works is actually pretty amazing but with agentic loops sometimes you can recover from earlier failures as well so I find myself already using research agents for some of my work where one of piece of research but I don't feel like you know Googling myself and spend a long time I should send to the research agent come back in a few minutes and see what it's come up with and and it sometimes works sometimes doesn't right but that's already a part of my personal workflow the final design pattern multi- Asian collaboration this is one of those funny things but uh um it works much better than you might think uh uh but on the left is a screenshot from a paper called um chat Dev uh which is completely open which actually open source many of you saw the you know flashy social media announcements of demo of a Devon uh uh Chad Dev is open source it runs on my laptop and what Chad Dev doeses is example of a multi-agent system where you prompt one LM to sometimes act like the CEO of a software engine company sometimes Act designer sometime a product manager sometimes I a tester and this flock of agents that you built by prompting an LM to tell them you're now Co you're now software engineer they collaborate have an extended conversation so that if you tell it please develop a game develop a GOI game they'll actually spend you know a few minutes writing code testing it uh iterating and then generate a like surprisingly complex programs doesn't always work I've used it sometimes it doesn't work sometimes it's amazing but this technology is really um getting better and and just one of design pattern it turns out that multi-agent debate where you have different agents you know for example could be have ch GPT and Gemini debate each other that actually results in better performance as well so having multiple simulated air agents work together has been a powerful design pattern as well um so just to summarize I think these are the these are the the the uh patterns of seen and I think that if we were to um use these uh uh patterns you know in our work a lot of us can get a prity boost quite quickly and I think that um agentic reasoning design patterns are going to be important uh this is my small slide I expect that the set of T AI could do will expand dramatically this year uh because of agentic workflows and one thing that it's actually difficult people to get used to is when we prompt an LM we want to response right away um in fact a decade ago when I was you know having discussions around at at at Google on um it called a big box search we type a long prompt one of the reasons you know I failed to push successfully for that was because when you do a web search you one of responds back in half a second right that's just human nature we like that instant grab instant feedback but for a lot of the agent workflows um I think we'll need to learn to dedicate the toss and AI agent and patiently wait minutes maybe even hours uh to for a response but just like I've seen a lot of novice managers delegate something to someone and then check in 5 minutes later right and that's not productive um I think we need to it be difficult we need to do that with some of our AI agents as well I saw I heard some loss um and then one other important Trend fast token generation is important because with these agented workflows we're iterating over and over so the LM is generating tokens for the elm to read so be able to generate tokens way faster than any human to read is fantastic and I think that um generating more tokens really quickly from even a slightly lower quality LM might give good results compared to slower tokens from a better LM maybe it's a little bit controversial because it may let you go around this Loop a lot more times kind of like the results I showed with gbd3 and an agent architecture on the first slide um and cand I'm really looking forward to Cloud 5 and uh CL 4 and gb5 and Gemini 2.0 and all these other wonderful models that may are building and part of me feels like if you're looking forward to running your thing on gp5 zero shot you know you mayble to get closer to that level performance on some applications than you might think with agenting reasoning um but on an early model I think I I I I think this is an important Trend uh uh and honestly the path to AGI feels like a journey rather than a destination but I think this typ of agent workflows could help us take a small step forward on this very long journey thank [Applause] you

========================================

--- Video 81 ---
Video ID: TDPqt7ONUCY
URL: https://www.youtube.com/watch?v=TDPqt7ONUCY
Title: The AI opportunity: Sequoia Capital's AI Ascent 2024 opening remarks
Published: 2024-03-26 21:18:10 UTC
Description:
The landscape is wide open. The opportunity set is massive. At our second Sequoia Capital AI Ascent, Sonya Huang, Pat Grady, and Konstantine Buhler discuss the way AI is already providing glimpses of enduring value, and how this technology will help us do more with less so that we can solve more problems, create more, and build a better future.

#AI #AIAscent #Sequoia #Startup #Founder #entrepreneur

Transcript Language: English (auto-generated)
my name is pack Rady I'm one of the members of team seoa I'm here with my partners Sonia and Constantine who will be your MC's for the day and along with all of our partners at seoa we would like to welcome you to AI Ascent there's a lot going on in the world of AI we have an objective to learn a few things while we're here today we have an objective to meet a few people who can be helpful on our journey while we're here today and hopefully we'll have a little bit of fun so just to frame the opportunity what is it well a year ago it felt like this magic box that could do wonderful amazing things I think over the last 12 months we've sort of been through this contracted form of the hype cycle we had the peak of inflated expectations we had the trough of disillusionment we're crawling back out into the plateau of productivity and I think we've realized that what what llms what AI really brings to us today are three distinct capabilities that can be woven into a wide variety of magical applications the first is the ability to create hence the name generative AI you can create images you can create text you can create video you can create audio you can create all sorts of things not something software has been able to do before so that's pretty cool the second is the ability to reason could be one shot could be multi-step agentic type reasoning but again not something software's been able to do before because it can create because it can reason we've sort of got the right brain and the left bra covered which means that software can also for the first time interact in a humanlike capacity and this is huge because this has profound business model implications that we're going to mention on the next slide so what a lot of times we try to Reason by analogy when we see something new and in this case the best analogy that we can come up with which is imperfect for a million reasons but still useful is the cloud transition over the last 20 years or so that was a major tectonic shift in the technology landscape that led to new business models new applications new ways for people to interact with technology and if we go back to some of the early days of that cloud transition this is Circa about 2010 the entire Pi the entire Global Tam for software was about 350 billion of which this tiny slice just $6 billion doar is cloud software fast forward to last year the Tam has grown from about 350 to 650 but that slice has become 400 billion of Revenue that's a 40% ker over 15 years that's massive growth now if we're going to Reason by analogy Cloud was replacing software with software because of what I mentioned about the ability to interact in a humanlike capability one of the big opportunities for AI is to replace services with software and if that's the T that we're going after the starting point is not hundreds of billions the starting point is possibly tens of trillions and so you can really dream about what this has a chance to become and we would posit and this is a hypothesis as everything we say today will be we would posit that we are standing at the precipice of the single greatest value creation opportunity mankind has ever known why now one of the benefits of being part of SEO is that we have this long history and we've gotten to sort of study the different waves of technology and understand how they interact and understand how lead us to the present moment we're going to take a quick trip down memory lane so 1960s our partner Don Valentine who founded SEO was actually the guy who ran the goto market for Fairchild semiconductor which gave Silicon Valley its name with silicon based transistors we got to see that happen we got to see the 1970s when systems were built on top of those chips we got to see the 1980s when they were connected up by by networks with PCS as the endpoint and the Advent of package software we got to see the 1990s when those Networks Works went public facing in the form of the internet change the way we communicate change the way we consume we got to see the 2000s when the internet matured to the point where it could support sophisticated applications which became known as the cloud and we got to see the 2010s where all those apps showed up in our pocket in the form of mobile devices and change the way we work and so why do we bother going through this little build well the point here is that each one of these waves is additive with what came before and the idea of AI is nothing new it dates back to the 1940s I think neural Nets first became an idea in the 1940s but the ingredients required to take AI from idea from dream into production into reality to actually solve real world problems in a unique and compelling way that you can build a durable business around the ingredients required to do that did not exist until the past couple of years we finally have compute that is cheap and plent we have networks that are fast and efficient and reliable seven of the 8 billion people on the planet have a supercomputer in their pockets and thanks in part to covid everything has been forced online and the data required to fuel all of these delightful experiences is readily available and so now is the moment for AI to become the theme of the next 10 probably 20 years and so we we we have as strong conviction as you could possibly have in a hypothesis that is not yet proven that the next couple of decades are going to be the going to be the time of AI what shape would that opportunity take again we're going to analogize to the cloud transition and the mobile transition these logos on the left side of the page those are most of the companies born as a result of those transitions that got to a billion dollars plus of Revenue the list is not exhaustive but this is probably 80% or so of the companies formed in those transitions that got to a billion plus of Revenue not valuation Revenue the most interesting thing about this slide is the right side and it's not what's there it's what isn't there the landscape is wide open the opportunity set is massive we think if we were standing here 10 or 15 years from today that right side is going to have 40 or 50 logos in it chances are it's going to be a bunch of the logos of companies that are in this room this is the opportunity this is why we're excited and with that I will hand it off to Sonia [Applause] than wow what a year chat GPT came out a year and a half ago I think it's been a whirlwind for everybody here it probably feels like just about all of us have been going non-stop with the ground shifting under our feet constantly so let's take a pause zoom out and take stock on what's happened so far last year we were talking about how AI was going to revolutionize all these different fields and provide amazing productivity gains a year later it's starting to come into Focus who here has seen this tweet from Sebastian at Clara show fans um it's pretty incredible Clara is now using open aai to handle two-thirds of customer service inquiries they've automated the equivalent of 700 full-time agents jobs we think you know there are tens of millions of call center agents globally and one of the most most exciting areas where we've already seen AI find product Market fit is in this customer support Market Legal Services a year ago the law was considered one of the least Tech forward Industries one of the least likely to take risks uh now we have companies like Harvey that are automating away a lot of the work that lawyers do from day-to-day grunt work and drudgery all the way to more advanced analysis or software engineering I'm sure a bunch of people in this room have seen some of the demos floating around on Twitter recently it's remarkable that we've gone from a year ago AI theoretically writing our code uh to entirely self-contained AI software engineers and I think it's really exciting the future is going to have a lot more software and AI isn't all about revolutionizing work it's already increasing our quality of life now the other day I was in a zoom with Pat and I noticed that he looked a little bit suspicious uh didn't speak the entire time and having reflected on it more I'm pretty sure that he actually sent in his virtual AI Avatar um was actually hitting the gym which would explain a lot hi this is Pat Grady this is definitely me I'm definitely here and not at the gym right now and it even gets the facial scrunches right this is courtesy of hen it's it's pretty amazing um this this is how far Technologies come in a year it's it's just it's scary to think about um it's scary and exciting to think about how this all plays out in the coming decade um all getting a two years ago uh when we thought that generative AI might usher in the next great technology shift we didn't know what to expect would real companies come out of it would real Revenue materialize I think the sheer scale of user poll and revenue momentum has surprised just about everybody uh generative AI we think is now clocking in around $3 billion doll of revenues in Aggregate and that's before you count all the incremental revenue generated by the Fang companies and the cloud providers in AI to put 3 billion in context it took the SAS Market nearly a decade to reach that level of Revenue generative AI got there it's first year out the gate so the rate and the magnitude of the C change make it very clear to us that generative AI is here to stay and the customer pull in AI isn't restricted to one or two apps it's everywhere I'm sure everyone's aware of how many users chat GPT has but when you look at the revenue and the usage numbers for a lot of AI apps both consumer companies and Enterprise companies startups and incumbents uh many AI products are actually striking a cord with customers and starting to find product Market fit across Industries and so we find the diversity of use cases that are starting to hit really exciting the number one thing that has surprised me at least about the funding environment over the last year has been how uneven the share of funding has been if you think of generative AI as a layer cake where you have Foundation models on the bottom uh you have developer tools and infro above and then you have applications on top a year ago we had expected that there would be a Cambrian explosion in the application layer due to the new enabling technology in the foundation layer instead we've actually found that new company formation in capital has formed in an inverse pattern more and more Foundation models are popping up and raising very large funding rounds while the application layer feels like it is just getting going our partner David is right here uh and posed a thought-provoking question last year with his article ai's $200 billion question if you look at the amount that at the amount of money that companies are pouring into gpus right now we spent about $50 billion doar on Nvidia gpus just last year and everybody's assuming if you build it they will come AI is a field of dreams but so far remember on the previous slide we've identified about3 billion dollars or so of AI Revenue plus change from the cloud providers we've put 50 billion into the ground plus Energy Plus data center costs and more we've gotten three out and to me that means the math isn't mathing yet uh the amount of money it takes to build this stuff has vastly exceeded the amount of money coming out so far so we got some very real problems to fix still and even though the usage and uh even though the revenue and the user numbers in AI look incredible the usage data says that we're still really early and so if you look at for example the ratio of daily to monthly active users or if you look at one month retention generative AI apps are still falling far short of their mobile peers to me that is both a problem and an opportunity it's an opportunity because AI right now is a once a week once a month kind of tinkery phenomenon for the most part for people but we have the opportunity to use AI to create apps that people want to use every single day of their lives when we interview users one of the biggest reasons they don't stick on AI apps is the gap between expectations and reality so that magical Twitter demo becomes a disappointment when you see that the model just isn't smart enough to reliably do the thing that you asked it to do the good thing is with that $50 billion plus of GPU spend last year we now have smarter and smarter base models to build on and just in the last month we've seen Sora we've seen Claud 3 we saw grock over the weekend and so as the level of intelligence of the Baseline Rises we should expect ai's product Market fit to accelerate so unlike in some markets where the future of the market is very unclear uh the good thing about AI is you can draw a very clear line to how those apps will get predictably better and better let's remember that success takes time we said this at last year's aent and we'll say it again if you look at the iPhone some of the first uh some first apps in the V1 of the App Store were the beer drinking app or the light saer app or the flip cup app or the the flashlight kind of the fun lightweight demonstrations of new technology those eventually became either native apps uh aka the flashlight Etc or utilities and gimmicks um the iPhone came out in 2007 the App Store came out in 2008 it wasn't until 2010 that you saw Instagram and door Dash uh 2013 so it took time for companies to discover and harness the net new capabilities of the iPhone in creative ways that we couldn't just imagine yet we think the same thing is playing out in AI we think we're already seeing a peak into what some of those next legendary companies might be here are a few of the ones that have captured our attention recently but I think it's much broer than the the set of use cases on this page as I mentioned we think customer support is one of the first handful of use cases that's really hitting product Market fit in the Enterprise as I mentioned with the Clara story I don't think that's an exception it's the rule I think that is the rule AI friendship has been one of the most surprising applications for many of us I think took a few months of thinking for us to wrap our uh our heads around but I think the user and the usage metrics in this category imply very strong user love um and then horizontal Enterprise knowledge we'll hear more from glean and dust later today we think that Enterprise knowledge is finally starting to be become unlocked so here are some predictions for what we'll see over the coming year prediction number one 2024 is the year that we see real applications take us from co-pilots that are kind of helpers on the side and suggest things to you and help you to agents that can actually take the human out of the loop entirely AI that feels more more like a coworker than a tool we're seeing this start to work in domains like software engineering um customer service and we'll hear more about this topic today I think both Andrew in and Harrison Chase are playing this PE on it prediction number two one of the biggest knocks against llms is that they seem to be paring the statistical patterns in text and aren't actually taking the time to reason and plan through the tasks at hand that's starting to change with a lot of new research um like inference time compute and game gameplay style value iteration like what happens when you give the model the time to actually think through what to do we think that this is the uh this is a major research thrust for many of the foundation model companies and we expect it to result in AI That's more capable of higher level cognitive tasks like cogn like uh planning and reasoning over the next year and we'll hear more about this later today from noan Brown of open AI prediction number three we are seeing an evolution from fun consumer apps or prosumer apps where we don't really care if the AI says something wrong or crazy occasionally uh to real Enterprise applications where the stakes are really high like hospitals and defense the good thing is that there's different tools and techniques emerging to help bring these llms sometimes into the 59 reliability range from rhf to prompt chaining to Vector databases and I'm sure that's something that you guys can compare notes on later today I think a lot of folks in this room are doing really interesting things to make llms more reliable in production and finally 2024 is the year that we expect to see a lot of AI prototypes and experiments go into production and what happens when you do that that means latency matters that means cost matters that means you care about model ownership you care about data ownership and it means we expect the balance of compute to begin shifting from pre-training over to inference so 2024 is a big year there's a lot of pressure and expectations built into some of these applications as they transition in production and it's really important that we get it right with that I'll transition to Constantine who will help us dream about AI over an even longer time Horizon thank you Sonia and thank you everyone for being here today Pat just set up the so what why is this so important why are we all in the room and Sonia just walked us through the what now where are we in the state of AI this section is going to be about what's next we're going to take a step back and think through what this means in the broader concept of technology and Society at large so there are many types of Technology Revolution there are communication revolutions like telefony there are Transportation revolutions like the locomotive there are productivity revolutions like the mechanization of food Harvest we believe that AI is primarily a productivity Revolution and these revolutions follow a pattern it starts with a human with a tool that transitions into a human with a machine assistant and eventually that moves into a human with a machine Network the two predictions that we're going to talk about in this section both relate to this concept of humans working with machine networks let's look at a historical example the sickle has been around as a tool for the human for over 10,000 years the mechanical reaper which is a human and a machine assistant was invented in 1831 a single machine system uh being used by a human Today We Live in an era where we have a combined Harvester combined Harvester is tens of thousands of machine systems working together as a complex Network we're starting to use language in AI to describe this language like individual machine participants in the system might be called an agent we're talking about this quite a bit today uh the way the topology and the way that the information is transferred between these agents we're starting to talk about as reasoning for example in essence we're creating very complicated layers of abstraction Above The Primitives of AI I'll talk about two examples today two examples that we're experiencing right in front of us in knowledge work the first is software so software started off as a very manual Pro process here's a love who wrote logical programming uh with pen and paper was able to do these computations but without the assistant of a machine we've been living in an era where we have significant machine assistance for computation uh not just the computer but the integrated development environment and increasingly more and more Technologies to accelerate development of software we're entering a new era in which these systems are working together in a complex machine Network what you see is a series of processes that are working together in order to produce uh complex Engineering Systems and what you would see here is agents working together to produce codee not one at a time but actually in unison and Harmony the same pattern is being applied in writing very commonly writing was a human process human and a tool over time this has progressed to human and a machine assistant and now we have a human that's actually leveraging not one but a network of assistants I'll tell you in my own personal workflow now anytime I call an AI assistant I'm not just calling gp4 I'm calling Mist large I'm calling Claud 3 I'm having them work together and also uh against each other to have better answers this is the future that we're we're seeing right in front of us so what what does this type of revolution mean for everyone in this room and frankly everyone outside of this room in cold hard economic terms what this means is significant cost reduction so this chart is the number of workers needed at an S&P 500 company to generate 1 million of Revenue it's going down rapidly we're entering an era where this will continue to decline what does that mean faster and fewer the good news is it's not so that we can do less it's so that we can do more and we'll get to that in the next set of predictions also fortunate is all the areas where we've had this type of prog progress in the past have been deflationary I'll call out computer software and accessories the process of computer software because we're constantly building on each other has actually gone down in cost over time uh televisions are also here but some of the most important things to our society education college tuition Medical Care housing they've gone up far faster than inflation and it's perhaps a very happy coincidence that artificial intelligence is poised to help drive down costs in these and many other crucial areas so that's the first conclusion about the long-term future of artificial intelligence as a massive cost driver a productivity Revolution that's going to be able to help us do more with less in some of the most critical areas of our society the second is related to what is it really doing one year ago on the stage we had Jensen hang make a powerful prediction he said that in the future pixels are not going to be rendered they're going to be generated any given image even information will be generated what did he mean by this well as everyone in this room knows historically images have been stored as rope memory uh so let's think about the letter a asky character number 97 okay that is stored as a matrix of pixels either the presence or absence if we use a very simple black and white presence or absence of those pixels well we're entering a period in which we already are representing Concepts like the letter A not as Road storage not as a presence or absence of pixels but as a concept a multi-dimensional point I mean the the image to think about here is the concept of an a which is generalizable to Any Given format for that letter A so many different type faces in this multi-dimensional space we're sitting at the center and where do we go from here well the powerful thing is the computers are now starting to understand not just this multi-dimensional point not just how to take it and render it and generate that image like Jensen was talking about we are now at the point where we're going to be able to contextualize that understanding the computer's going to understand the a be able to render it understand it's an alphabet understand it's an English alphabet and understand what that means in the broader context of this rendering computer's going to look at the word multi-dimensional and not even think about the a but rather understand the full context of why that's being brought up and amazingly this future is how we think how humans think no longer are we going to be storing uh the wrote pixels in a computer memory that's not how we think I wasn't taught about the letter A as the presence or absence of a of a pixel on a page instead we're going to be thinking about that as a concept powerfully this is how we' thought about it philosophically for thousands of years here's my fellow Greek Plato 2,500 years ago who said this idea of a platonic form is what we all ascribe to or all striving for that you have this concept in this case of a letter A or this concept of software engineering that we actually are able to build a model around so what now we've talked about the second pattern this idea that we're going to have generalization in inside Computing itself what does that mean for each of us well it's going to mean a lot for company building uh today we're already integrating this into specific processes and kpis Sonia just mentioned how Clara is using this in order to accelerate their kpis around customer support they know that they have certain kpis that they can drive towards and they can have a system that's actually retrieving information generating great customer experiences tomorrow and this is already happening alongside new user interfaces that might be a different interface for how the support is actually being communicated and this is what I'm personally incredibly excited about is because of this future in which concepts are rendered because of this future in which everything is generated eventually the entire company might start working like a neural network let me break that down in a specific example this is a caricature as with everything in this presentation it's in reality everything is continuous these are all discreet this is a caricature of the customer support process you have customer service that has certain kpis these are driven by Text to Voice language generation customer personalization and the like this feeds into sub patterns sub trees that you're optimizing and eventually yourx going to have a fully connected graph here yourx going to have feedback from the language generation to the end kpi for the servicing of the customers this is is going to be at some point a layer of abstraction where customer support is managed optimized and improved by the neural network now let's think about unique customers another part of the important job of building a business well again you have Primitives of artificial intelligence from language generation to a growth engine to add customization and optimization this will all feed into each other once again the powerful conclusion here is eventually these layers of abstraction will be become interoperable to the point where the entire company is able to function like a neural network here comes the rise of the oneperson company the one person company is going to enable us not to do less but to do more more problems can be tackled by more people to create a better Society so what's next the reality is the people in the room here are going to decide what's next you are the ones who are building this future we personally are very excited about the future because we think that AI is positioned to help drive down costs and increase productivity in some of the most crucial areas in our society better education healthier populations more productive populations and that's the purpose of convening this group today you all are going to be able to talk about how are we able to take our Technologies abstract away complexity mundane details and actually build something that's much more powerful for the future I'll hand it off to Sonia to introduce our first speaker thank you

========================================

